{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyP3dXRfcXLa"
      },
      "source": [
        "# Retrieve & Re-Rank Demo on Column Y from Education Excel\n",
        "\n",
        "\n",
        "You can input a query or a question. The script then uses semantic search\n",
        "to find relevant passages in a sample of papers from our Education Excel.\n",
        "\n",
        "For semantic search, we use `SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')` and retrieve potentially passages that answer the input query.\n",
        "\n",
        "Next, we use a more powerful CrossEncoder (`cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')`) that\n",
        "scores the query and all retrieved passages for their relevancy. The cross-encoder further boost the performance,\n",
        "especially when you search over a corpus for which the bi-encoder was not trained for.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "924axXqeVtf-",
        "outputId": "1bd49561-f9eb-4479-d700-f2776465e906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2R9TjVzNV_E",
        "outputId": "dc4b0eb3-9c41-4380-9fe3-e0a943477ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=585d1f71436616656155b24817fa68dfb7765b6df064ced5231d4d3e420cddf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, rank_bm25, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.11.1 rank_bm25-0.2.2 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldREifRvVUEp",
        "outputId": "ec7b477f-3f04-4681-aa0b-5cc79547997c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import gzip\n",
        "import os\n",
        "import torch\n",
        "import nltk\n",
        "import nltk\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: No GPU found. Please add GPU to your notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aknDT9Pg8VMj"
      },
      "source": [
        "#Experiments on whole data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501ypKKFbBmY",
        "outputId": "64310174-a64b-4ff1-85d1-3767ea2765f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "Passages for paper  #2598 : 104\n",
            "Passages for paper  #17247 : 194\n",
            "Passages for paper  #17284 : 69\n",
            "Passages for paper  #17755 : 1339\n",
            "Passages for paper  #17192 : 190\n",
            "Passages for paper  #17725 : 73\n"
          ]
        }
      ],
      "source": [
        "#We split these articles into sentences and encode them with the bi-encoder\n",
        "\n",
        "data_filepath = 'information_retrieval/focused_sample_paragraphs.jsonl'\n",
        "\n",
        "with open(data_filepath, 'r', encoding='utf8') as fIn:\n",
        "    papers = json.load(fIn)\n",
        "print(len(papers))\n",
        "for paper in papers:\n",
        "  #paper['sent_tokenized_txt'] = sent_tokenize(paper['pdf_txt'])\n",
        "  print(\"Passages for paper \",paper['paper_id'],':', len(paper['paragraphs']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSVuvWOpProl",
        "outputId": "c81b8ab7-2a7e-44ed-ebf8-a5534ff94651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered passages for #2598: 97\n",
            "Filtered passages for #17247: 188\n",
            "Filtered passages for #17284: 68\n",
            "Filtered passages for #17755: 1265\n",
            "Filtered passages for #17192: 184\n",
            "Filtered passages for #17725: 71\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Filter which sentences we choose to encode\n",
        "\n",
        "import re\n",
        "general_keywords = [\"beneficiaries\",\"beneficiary\",\"service\", \"user\", \"participants\", \"eligible\",\"population\", \"eligibility\",\"criteria\",\"cohort\",\"client\",\"target\",\n",
        "                    \"intervention\",\"identified\",\"enrolled\",\"attended\",\"sample\"]\n",
        "\n",
        "currency_keywords = ['$','USD','dollar','pound','euro','£','gbp','€','₹','rupee','franc','sterling','dinar','dirham','yen']\n",
        "\n",
        "def filter(sentence):\n",
        "  k = 0\n",
        "  for keyword in general_keywords:\n",
        "    if keyword in sentence: #and any(char.isdigit() for char in sentence):\n",
        "      k = 1\n",
        "  return k    \n",
        "\n",
        "for paper in papers:\n",
        "    paper['filtered_paragraphs'] = []\n",
        "    for sentence in paper['paragraphs']:\n",
        "      if len(sentence.split())>=10:# and any([x.lower() in sentence.lower() for x in currency_keywords]): #and bool(re.search(r'\\d', sentence):\n",
        "        paper['filtered_paragraphs'].append(sentence)\n",
        "    if paper['filtered_paragraphs']== []:\n",
        "      paper['filtered_paragraphs']=['nothing']\n",
        "    print('Filtered passages for {}: {}'.format(paper['paper_id'],len(paper['filtered_paragraphs'])) )\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O0Sk9VLU6P5"
      },
      "source": [
        "File #17271 does not contain the full text of the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "f70771c693cd4169ba3b9bae4d09d35d",
            "3fd07ae167ad4db983a71edf32a561e6",
            "a96c20cb8caa4fe28aec75c3570d5045",
            "35395c2533c3499b849577a26d047d9e",
            "d89b7cadc9194c0a82306d4d02ca8168",
            "52d836bd17684e08bb62fcefaf40b0e6",
            "80e770629fce44a394e127283314f2cf",
            "1d06c77c84dd4257a94b4550f095204b",
            "c8c8919dd330447e93a8861a34ae9477",
            "06af69d1941240d1ba18586474a43dc2",
            "bbd40703ef414ef0a60f31c236bb2244",
            "ad221b5f5cb44a94977706bea8363e8c",
            "265e606c68ad44afa323b21db21a266f",
            "97936cc6bf4241958b10cb6e2dbdd5c8",
            "4b7f6ec2cc8d41e5b57aa0c39bc06422",
            "7c2e6c17e47b416da789e852267777ed",
            "916f771306ce4443b8d5e8ae43dc67ec",
            "2ccd149dccb348d6a2084f37371ed960",
            "1db76f4a2eab4e0c84b4c2d07b1187f0",
            "e09a1b58273c40daa6b636e8cf689304",
            "1e7a1a547f344ae9b308578f68d1b114",
            "ad6c88b1bca84ecebfbe3253ad8cdc1a",
            "7a41619167ad4546aba2d9daf6c13a68",
            "51b39eb413064abc8af1f279a2be210d",
            "bfccabfc8003460795f883f525a197a9",
            "b5b3c41bb94f4bbca0bb2e7fd307812f",
            "38bcdc63ffc545f993a3cad0723ea5a4",
            "34d32757088c40169f8eec7ba1c91c3d",
            "a945737a13624feb827f94404f1ea836",
            "12130eeb1e4b4ca9a0f513f3a0fe1fc6",
            "2210d51ee86a4aec8f8e91d4e7fd22cc",
            "db27dd052ca04e6e83cc96ac7c20b14e",
            "2074d2142adf4bf2b9e3ef6336091812",
            "dcd433dacd764ca89aa0830330bddfa7",
            "16606d51ede748559c3d4d714299a582",
            "6ea399690962481da4dee0c7a96dc959",
            "a3d4803fd7394eecaa37c7ff856a0f94",
            "7b15d6a8c78341dc9d4c2d90dd6eb4fe",
            "2ab72ec74c744591b1f2b90ac616056c",
            "75b1c687fb074ffc813379836e5884e4",
            "3b6f9e5ba5214811a9d0973c51581ddd",
            "5da7c37b28304c818a6e65e16bdf7f73",
            "cdb8ae7efc214171844b2b4a42f5ff5c",
            "5a168802bd2743458d27841652b34307",
            "0758d6cf8d36499f91ff76ba15ac6edc",
            "0a3836a486984c6abbdaa1c9f9444a85",
            "b69928128cb949938262471f229184b5",
            "5af8afe8fe254d05a999736033d69f00",
            "8930fac4b1fa48ed8623f0913483df41",
            "6d0f4e1283244d208032ca5b7790f6d1",
            "33caeb7513ad4126b44c7bbffb10fc4b",
            "a88492bcd6104eac8f6e723a50fbd1d7",
            "3b77494cefcd48a794f694842a16f453",
            "7fb718b9ec8440e7a22398988e2b287f",
            "8a0e465bbd2446c8b7905a9af5bdb17f",
            "207f979de05a47dfba9931a1e7f3300a",
            "baef5fb01167411b9f439b9db400ade4",
            "186340bc0aa84ed08a23bd9849b704f2",
            "6047ec96e6c94701b99a67516e2719e2",
            "01f415f0e24c4e7c85b95628ddae99a9",
            "003d692dee9d43d08004c2d1cc94ec1b",
            "3634debf505443ed9b097c529075544f",
            "0dc8d02a5705412284da8b002216bb35",
            "0ce85a23775740e6a998887e02b592bf",
            "091d147c11cc420b9fb6bd8bfb879721",
            "dce6cae3b6aa40f591d56e6fa1fedd40"
          ]
        },
        "id": "D_hDi8KzNgMM",
        "outputId": "bd25dae0-453d-4d8a-f4a2-68f92113dd7f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f70771c693cd4169ba3b9bae4d09d35d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad221b5f5cb44a94977706bea8363e8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a41619167ad4546aba2d9daf6c13a68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcd433dacd764ca89aa0830330bddfa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0758d6cf8d36499f91ff76ba15ac6edc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "207f979de05a47dfba9931a1e7f3300a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
        "bi_encoder = SentenceTransformer('multi-qa-mpnet-base-dot-v1')#('multi-qa-MiniLM-L6-cos-v1')\n",
        "bi_encoder.max_seq_length = 512     #Truncate long passages to 256 tokens\n",
        "top_k = 30                          #Number of passages we want to retrieve with the bi-encoder\n",
        "\n",
        "#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "# We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
        "all_corpus_embeddings = {}\n",
        "for paper in papers:\n",
        "  all_corpus_embeddings[paper['paper_id']] = bi_encoder.encode(paper['filtered_paragraphs'], convert_to_tensor=True, show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "0b14abb989524a538d233003d4ab54db",
            "69433fe5ef4041b6b07082c3a9090c3e",
            "f47356d978784df4b15e3805c25da741",
            "d7bc8b01726f4632bb328c8c6b0c4d6d",
            "c206f87cf3354981b8260d7a7566f45c",
            "3f892b864c974c3491859c4a8adda0a6",
            "bf780f802f68481398cd98dbab551267",
            "3e070f0208384cf7934c71b07cd7e88a",
            "6a2b2b8d5512449a9a3b9d628320ea10",
            "90a8fb4a685d4fb0859854800734f580",
            "ae8281fe85464bfebcda36358a3b3d4e",
            "586b7253719840358f0cd3349cfd4096",
            "2e22d5e099b74d74896c9413669eee9b",
            "547a9f88bc5a4ba28f95b65798c6b833",
            "6cd7fd691f1a4de389dd5a354cf97332",
            "63373539e4b34f0793fce84adf4cf62e",
            "191a2f9de1fd455ab406630a7bd066b3",
            "a023517fdb4741d3911ae3259df654f3",
            "9520c2b523544a6da8b35fd6a79c2cca",
            "af1868dcf14d464a851af10269bf8cbe",
            "89178dbfc8af4bd48c808af760f95f93",
            "b6bbc60984f247db9a5cba7afa1c7ce4",
            "09befa9a8d174a06ad93f8cebf0f5a28",
            "73fdee955bb5431d971c1e9fc5859077",
            "641f7e95c3d94c9e852cbd73387e08b3",
            "40996cdf3c594a7ebe73fc39fd6008bd",
            "971ec3d353d840b3854abab3ab090b02",
            "e5b834706d5041589fe7777a7b2ef448",
            "29624b5a8de04cf798e0b8cf13c3e1cd",
            "273b05a586f241f99bd8f9d380910066",
            "587b3bc1ebd34b49a32f811cf295f959",
            "867acac12d3b48e1a16190d1592ff467",
            "d3dad67e64af44e08705b6771324437f",
            "7e14ee033ce5498ab127b65b0c5ffdf1",
            "0f0548986e2a4bd19664c469ec2ab445",
            "86cdf5363ad64aae93b2e9ec61bd551a",
            "738ddc7c95c04788ad54a0dd4a612142",
            "5e93caa73c3643bca3dda0054df20082",
            "efa228e28ad94688a62f459b52edbfa7",
            "932b3ac3bae44c52836f8a3c3a31d275",
            "affa91f1e2774f6ba5f5e887a4d35ec5",
            "df865f4d7af346318133f2c268fb8f07",
            "15735cf436aa4972bc5134481b5f3977",
            "ea6424b92a6a49fda861fcaebb3d004a",
            "e6e614e681c0499e9ce92e4b4c88f325",
            "b46ccff723c94aa184d6dc0d30fb36b8",
            "7ea7d1af7f574837869976ed8fb1e5d5",
            "64dedb7900e540d8a396eed5f63275bd",
            "394e04a2baed473c9c3a4addc8f4489a",
            "5e8b5fd6f7c94258be451833ff50c71a",
            "d02841089d2940aca79f91360f4309a4",
            "711c752b5c09437f84427d1b1bdac148",
            "01c07f3f5b7449d79832b32c1e1c1e9e",
            "bc27ab45ee2148ceab12b3fba8951c14",
            "7dd8595b079f4fc1b885b1053fa123e4",
            "da306e6dd86247a0ae5d53d57ad82381",
            "a8ca9487ab6648099a839fac89c37e91",
            "bdd95884c84940b18524bbcb74afa883",
            "c2e2f98129224f93a6a12ca2ff929283",
            "d6c2a9ed689d4550aad7005071f3edb8",
            "03158e548ebb4c5691539aff3b4496dd",
            "c893715f26a7424589d4259447a1a319",
            "0534758537c44a289d410d70c9424938",
            "5d8fee751b3f4685b263538ad49a1100",
            "7d2fa602f01249f38a603427d5f87c5f",
            "e1df7922686e4b7cbf7660a080ae186b"
          ]
        },
        "id": "0rueR6ovrs01",
        "outputId": "23892d51-1d9f-4684-f7fc-cd67ff659954"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b14abb989524a538d233003d4ab54db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/97 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "586b7253719840358f0cd3349cfd4096",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/188 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09befa9a8d174a06ad93f8cebf0f5a28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/68 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e14ee033ce5498ab127b65b0c5ffdf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1265 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6e614e681c0499e9ce92e4b4c88f325",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/184 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da306e6dd86247a0ae5d53d57ad82381",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/71 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We also compare the results to lexical search (keyword search). Here, we use \n",
        "# the BM25 algorithm which is implemented in the rank_bm25 package.\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "import string\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# We lower case our text and remove stop-words from indexing\n",
        "def bm25_tokenizer(text):\n",
        "    tokenized_doc = []\n",
        "    for token in text.lower().split():\n",
        "        token = token.strip(string.punctuation)\n",
        "\n",
        "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
        "            tokenized_doc.append(token)\n",
        "    return tokenized_doc\n",
        "\n",
        "\n",
        "tokenized_corpus = {}\n",
        "for paper in papers:\n",
        "  tokenized_corpus[paper['paper_id']] = []\n",
        "  for passage in tqdm(paper['filtered_paragraphs']):\n",
        "    tokenized_corpus[paper['paper_id']].append(bm25_tokenizer(passage))\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlArb7kqN3Re"
      },
      "outputs": [],
      "source": [
        "# This function will search all passages that answer the query\n",
        "def search(query,passages,corpus_embeddings,n):\n",
        "   \n",
        "    ##### BM25 search (lexical search) #####\n",
        "    '''bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -5)[-5:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)'''\n",
        "    \n",
        "    \n",
        "    ##### Sematic Search #####\n",
        "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
        "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
        "    question_embedding = question_embedding.cuda()\n",
        "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
        "    hits = hits[0]  # Get the hits for the first query\n",
        "\n",
        "    ##### Re-Ranking #####\n",
        "    # Now, score all retrieved passages with the cross_encoder\n",
        "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
        "    cross_scores = cross_encoder.predict(cross_inp)\n",
        "\n",
        "    # Sort results by the cross-encoder scores\n",
        "    for idx in range(len(cross_scores)):\n",
        "        hits[idx]['cross-score'] = cross_scores[idx]\n",
        "\n",
        "\n",
        "    # Output the best hit from re-ranker\n",
        "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
        "    return [passages[hit['corpus_id']] for hit in hits[0:n]] \n",
        "    \n",
        "    #Output the best hit from BM25\n",
        "    #return [passages[hit['corpus_id']] for hit in bm25_hits[0:n]]  \n",
        "\n",
        "    #Ouput the best hit from bi-encoder\n",
        "    #hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n",
        "    #return [passages[hit['corpus_id']] for hit in hits[0:n]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J0Zxgw0artg"
      },
      "outputs": [],
      "source": [
        "from nltk.sem.drt import DrtFunctionVariableExpression\n",
        "import copy\n",
        "\n",
        "questions_tp = [\"What is the target population?\",\"Who are the intended beneficiaries of the service?\",\n",
        "                \"Who does the service try to help?\",\"Who was eligible for inclusion in the intervention?\",\n",
        "                \"target population beneficiaries service users participants eligible population eligibility criteria cohort clients\",\"target population\",\n",
        "                \"beneficiaries\", \"service users\", \"participants\", \"eligible population\", \"eligibility criteria\",\"cohort\",\"clients\"]\n",
        "\n",
        "\n",
        "questions_sd = ['What is the study design?','What is the research method?','How was data collected and analysed?',\n",
        "               'study design method methodology data collection research design','study design','method',\n",
        "               'methodology','data collection','research design']\n",
        "\n",
        "questions_fd = ['What are the costs of the contract?','How much is paid for outcomes?','What are the outcomes payments?','What is the total contract value?',\n",
        "                'What is the price per outcome?','outcomes payment price contract value contract cap rate card incentive payment costs savings',\n",
        "                'outcomes payment','price','contract value','contract cap','rate card','incentive payment','costs','savings']\n",
        "\n",
        "questions_plo = ['What outcomes were achieved?','What impact was achieved?','What were the results of the intervention?',\n",
        "  'What was the impact of the intervention?','Were the contracted outcomes achieved?','results outcomes achieved impact','results',\n",
        "    'outcomes achieved','impact']\n",
        "\n",
        "all_IR_results = {}\n",
        "for paper in papers:\n",
        "  key = paper['paper_id']\n",
        "  IR_results = {'Study Design':{},'Target Population':{},'Financial detail and costs':{},'Personal-level outcomes':{},'Financial detail and costs(filtered)':{}}\n",
        "\n",
        "  for question in questions_sd:\n",
        "    IR_results['Study Design'][question] = []\n",
        "\n",
        "  for question in questions_tp:\n",
        "    IR_results['Target Population'][question] = []\n",
        "\n",
        "  for question in questions_fd:\n",
        "    IR_results['Financial detail and costs'][question] = []\n",
        "    IR_results['Financial detail and costs(filtered)'][question] = []\n",
        "\n",
        "  for question in questions_plo:\n",
        "    IR_results['Personal-level outcomes'][question] = []\n",
        "\n",
        "  all_IR_results[key] = copy.deepcopy(IR_results)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy-weERO399j",
        "outputId": "81727465-85f0-408f-bbef-7d8b53f51620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Top  19  :   Systemic change: presence of a new, sustainable public funding stream Performance metrics: breadth of metrics South Carolina Yes—Medicaid reimbursement Utah Yes—state income tax Broad—four metrics Narrow—one metric Cost structure: comparison of public investment to investor profit Maximizes public investment— reinvests success payments Maximizes investor profit —overestimates impact Social equity: coverage of services to Yes—rural coverage vulnerable clientele Yes—low-income coverage Chicago No—funding ends after SIB Intermediate—three metrics Maximizes investor profit—overpays investors Yes—low-income coverage politically conservative states with high child poverty rates. They are difficult contexts in which to push any expansion of ECE services.\n",
            "Top  20  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  2  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  3  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  4  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  5  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  6  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  7  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  8  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  9  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  10  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  11  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  12  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  13  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  14  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  15  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  16  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  17  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  18  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  19  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  20  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  5  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  6  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  7  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  8  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  9  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  10  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  11  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  12  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  13  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  14  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  15  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  16  :   The margin, or the “razor’s edge,” for this broader interpretation is thin because the SIB’s logic privileges financial gains. Figure 2 visualizes our findings on navigating this neoliberal margin. South Carolina has no premium and no profit, instead inserting social objectives into the market to move the political conversation in a conservative state. Utah ran the financial risk of a SIB with a long view of moving state politics, but the investor payout and methodology undermined its social mission by diverting public investment to private investors. Chicago demonstrates the risk of financialized logic by providing short-term benefits to investors and clientele without the balance of long-term social objectives. This SIB falls totally on the extractive side of the market.\n",
            "Top  17  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  18  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  19  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  20  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  4  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  5  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  6  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  9  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  10  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  11  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  12  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  13  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  14  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  15  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  16  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  17  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  18  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  19  :   The margin, or the “razor’s edge,” for this broader interpretation is thin because the SIB’s logic privileges financial gains. Figure 2 visualizes our findings on navigating this neoliberal margin. South Carolina has no premium and no profit, instead inserting social objectives into the market to move the political conversation in a conservative state. Utah ran the financial risk of a SIB with a long view of moving state politics, but the investor payout and methodology undermined its social mission by diverting public investment to private investors. Chicago demonstrates the risk of financialized logic by providing short-term benefits to investors and clientele without the balance of long-term social objectives. This SIB falls totally on the extractive side of the market.\n",
            "Top  20  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  5  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  6  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  7  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  8  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  9  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  10  :   Because of the SIB, the state ultimately passed legislation to appropriate funding for preschool. The legislation also caps investor return on future SIBs at 5% above the municipal market data general obligation bond rate (State of Utah, 2014).\n",
            "Top  11  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  12  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  13  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  14  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  15  :   The number of SIBs has grown substantially around the world (A. Fraser, Tan, Lagarde, & Mays, 2018) since the first SIB was developed in the United Kingdom in 2010 (Disley, Rubin, Scraggs, Burrowes, & Culley, 2011). At least 17 have been implemented in the United States (Finlaw, 2017), primarily by cities, counties, and states, with steady Obama-era federal support for feasibility studies and technical assistance (Gustafsson-Wright, Gardiner, & Putcha, 2015; U.S. Government Accountability Office [GAO], 2015).\n",
            "Top  16  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  17  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  18  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  19  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  20  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  4  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  5  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  6  :   Because of the SIB, the state ultimately passed legislation to appropriate funding for preschool. The legislation also caps investor return on future SIBs at 5% above the municipal market data general obligation bond rate (State of Utah, 2014).\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  9  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  10  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  11  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  12  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  13  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  14  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  15  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  16  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  17  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  18  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  19  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  20  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  2  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  5  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  6  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  7  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  8  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  9  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  10  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  11  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  12  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  13  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  14  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  15  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  16  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  17  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  18  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  19  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  20  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  4  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  5  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  6  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  7  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  8  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "Top  9  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  10  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  11  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  12  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  13  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  14  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  15  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  16  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  17  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  18  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  19  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  20  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  4  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  5  :   Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure sustainable public funding for early childhood services through state legislation. The designers of each project committed to this goal first. Then they used the SIB to demonstrate cost savings for governments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.\n",
            "Top  6  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  9  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  10  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  11  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  12  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  13  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  14  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  15  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  16  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  17  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  18  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  19  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  20  :   3. Under the Affordable Care Act, preventive health programs, such as the Nurse–Family Partnership, can be funded through federal Medicaid dollars. States have wide latitude in how they use federal Medicaid dollars, so funding levels and eligibility criteria for reimbursement of services differ greatly by state. The Medicaid funding for NFP in South Carolina is the first of its kind in the country.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What outcomes were achieved?\n",
            "Top  1  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  2  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  3  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  4  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  5  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  6  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  7  :   Analysis We evaluate each case against the four foci outlined in our methodology. Chicago’s and Utah’s SIBs operate at the city or metropolitan level. The South Carolina SIB operates at the state level. Our following analysis shows how the cases fall along a continuum. We find that South Carolina is most likely to achieve systemic change by both inserting social values in the market and using market players to shift the political dialogue. Utah is next along the continuum, using market pressure to shift political investment but sacrificing social objectives to investor return. We find that Chicago ranks last because it extracts funds from the public sector in the name of expanding access to preschool. A summary of the three cases is provided in Table 2.\n",
            "Top  8  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  9  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  10  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  11  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  12  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  13  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  14  :   The third factor is coordinated networks. In both South Carolina and Utah, coalitions of local and statewide advocates and philanthropies had supported their respective interventions long before the SIBs were implemented. These coalitions worked together with state actors to pursue long-term policy solutions and funding streams, using the SIBs as leverage. But without these networks inserting broader objectives, comprehensive approaches could be lost to SIBs’ financializing logic, as occurred in Chicago.\n",
            "Top  15  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  16  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  17  :   A. E. TSE AND M. E. WARNER We use this framework to discern SIBs’ possibilities for progressive impact. We look inside the details of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs can concretize the promise of increased investment and broader public policy response depends critically on the configuration of actors and mechanisms in their local and regional context.\n",
            "Top  18  :   Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure sustainable public funding for early childhood services through state legislation. The designers of each project committed to this goal first. Then they used the SIB to demonstrate cost savings for governments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.\n",
            "Top  19  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  20  :   Testing the razor’s edge: SIBs in early childhood education Early childhood care and education services are a proving ground for early SIBs in the United States. Lessons for emerging SIBs can be drawn from the experience of using economic logics to support state, regional, and local-level economic development policies that expand access and funding for child care. Economic or financial logics have been used for more than 20 years in the United States to reimagine ECE services as investments, instead of just expenditures, and thus justify additional public and private support (Bartik, 2011; Warner, 2006). These logics have nudged policy discourse in the United States and Canada toward thinking of early education and care as a social infra- structure for economic development and thus an investable social service (Warner & Prentice, 2013). ECE has a well-documented capacity to produce important social gains. Relatively short and inexpensive interventions, such as preschool or parenting education programs, can improve chil- dren’s cognitive and social development and enhance school achievement (Heckman, Pinto, & Savelyev, 2013; Olds et al., 1997). Broader supports for working families, such as full-day childcare, reduce stress on parents and enable increased parental workforce participation and productivity (Gornick & Meyers, 2003; Halpern, 2004; Hipp, Morrissey, & Warner, 2017). Early childhood interventions can even support market returns at the regional economy level, if broadly conceived (Bartik, 2011; Warner, 2006). These economic logics reconceptualize early care and education as social infrastructure can expand social rights because it shifts ECE from a welfare service to an infrastructure and the universality that infra- structure has historically implied (Warner & Prentice, 2013).\n",
            "\n",
            "\n",
            "\n",
            "Query:  What impact was achieved?\n",
            "Top  1  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  2  :   Social impact bonds (SIBs) offer an experimental strategy for U.S. cities navigating the politics of fiscal constraint. These emerging financing mechanisms represent an alluring possibility: increased investment in social programs through private financing (Pequeneza, 2018). We explore how U.S. cities have maneuvered through the allure of SIBs, and their Trojan horse–like dangers, as they strive for more equitable social service delivery.\n",
            "Top  3  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  4  :   A. E. TSE AND M. E. WARNER We use this framework to discern SIBs’ possibilities for progressive impact. We look inside the details of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs can concretize the promise of increased investment and broader public policy response depends critically on the configuration of actors and mechanisms in their local and regional context.\n",
            "Top  5  :   Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert ECE into policy and funding conversations in which it would not have been before. SIB stakeholders are positive about the impact of their experiments with social finance. They see SIBs as helping them serve more children and opening the door to serving many more.\n",
            "Top  6  :   JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial logic when they include long-term objectives to build systemic change. These objectives are not written into the contracts. They are not part of the design. But they leverage the capacity of SIBs to produce localized information. This capacity is especially useful in conservative political contexts because strategic networks of social advocates can use that localized information to nudge discourse and alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs in the United States can be leveraged to scale up public investment to the state level by strategically targeting long-term policy change.\n",
            "Top  7  :   Journal of Urban Affairs ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/ujua20 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse & Mildred E. Warner To cite this article: Allison E. Tse & Mildred E. Warner (2020) The razor’s edge: Social impact bonds and the financialization of early childhood services, Journal of Urban Affairs, 42:6, 816-832, DOI: 10.1080/07352166.2018.1465347 To link to this article:  https://doi.org/10.1080/07352166.2018.1465347 Published online: 14 May 2018.\n",
            "Top  8  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  9  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  10  :   Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure sustainable public funding for early childhood services through state legislation. The designers of each project committed to this goal first. Then they used the SIB to demonstrate cost savings for governments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.\n",
            "Top  11  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  12  :   Systemic change: presence of a new, sustainable public funding stream Performance metrics: breadth of metrics South Carolina Yes—Medicaid reimbursement Utah Yes—state income tax Broad—four metrics Narrow—one metric Cost structure: comparison of public investment to investor profit Maximizes public investment— reinvests success payments Maximizes investor profit —overestimates impact Social equity: coverage of services to Yes—rural coverage vulnerable clientele Yes—low-income coverage Chicago No—funding ends after SIB Intermediate—three metrics Maximizes investor profit—overpays investors Yes—low-income coverage politically conservative states with high child poverty rates. They are difficult contexts in which to push any expansion of ECE services.\n",
            "Top  13  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  14  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  15  :   Analysis We evaluate each case against the four foci outlined in our methodology. Chicago’s and Utah’s SIBs operate at the city or metropolitan level. The South Carolina SIB operates at the state level. Our following analysis shows how the cases fall along a continuum. We find that South Carolina is most likely to achieve systemic change by both inserting social values in the market and using market players to shift the political dialogue. Utah is next along the continuum, using market pressure to shift political investment but sacrificing social objectives to investor return. We find that Chicago ranks last because it extracts funds from the public sector in the name of expanding access to preschool. A summary of the three cases is provided in Table 2.\n",
            "Top  16  :   The third factor is coordinated networks. In both South Carolina and Utah, coalitions of local and statewide advocates and philanthropies had supported their respective interventions long before the SIBs were implemented. These coalitions worked together with state actors to pursue long-term policy solutions and funding streams, using the SIBs as leverage. But without these networks inserting broader objectives, comprehensive approaches could be lost to SIBs’ financializing logic, as occurred in Chicago.\n",
            "Top  17  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  18  :   infrastructure for economic development. Social A comprehensive approach that includes human development, the productivity of families, and the regional economy is critical because it opens possibilities for deeper reform (Morrissey & Warner, 2007). The simple interventions favored by SIBs in the United States, however, such as preschool or parent education, fall short of this approach. Despite the possibilities for broader structural change, they do not address workplace policy and comprehensive support for ECE from birth onward, such as full-day childcare for working parents. Biases toward preschool alone are found in business and economic arguments (Rolnick & Grunewald, 2003). Feminist economists and educators voice the broader concerns of parents and ECE specialists, but these have not gained the investor attention they deserve (Folbre, 2006; Halpern, 2004; Stoney, Mitchell, & Warner, 2006; Warner, 2009).\n",
            "Top  19  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  20  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What were the results of the intervention?\n",
            "Top  1  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  2  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  3  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  4  :   The third factor is coordinated networks. In both South Carolina and Utah, coalitions of local and statewide advocates and philanthropies had supported their respective interventions long before the SIBs were implemented. These coalitions worked together with state actors to pursue long-term policy solutions and funding streams, using the SIBs as leverage. But without these networks inserting broader objectives, comprehensive approaches could be lost to SIBs’ financializing logic, as occurred in Chicago.\n",
            "Top  5  :   infrastructure for economic development. Social A comprehensive approach that includes human development, the productivity of families, and the regional economy is critical because it opens possibilities for deeper reform (Morrissey & Warner, 2007). The simple interventions favored by SIBs in the United States, however, such as preschool or parent education, fall short of this approach. Despite the possibilities for broader structural change, they do not address workplace policy and comprehensive support for ECE from birth onward, such as full-day childcare for working parents. Biases toward preschool alone are found in business and economic arguments (Rolnick & Grunewald, 2003). Feminist economists and educators voice the broader concerns of parents and ECE specialists, but these have not gained the investor attention they deserve (Folbre, 2006; Halpern, 2004; Stoney, Mitchell, & Warner, 2006; Warner, 2009).\n",
            "Top  6  :   A common refrain of interviewees was that SIBs’ cost savings argument, supported by data, is the only one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track evidence on child development. The evidence for cost savings serves as evidence for “what works.” But the SIBs’ interventions already were well documented. NFP operates in 42 states and is well researched (Olds et al., 1997). Child–parent centers have been operating in Chicago for 50 years and have been studied closely (Reynolds, 1997). Even the Granite School District preschool program had 3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well- demonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public money—and private finance generated through the SIB is much more expensive than public finance. Instead, the main benefit of data and the cost savings argument is the production of localized information to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.\n",
            "Top  7  :   JOURNAL OF URBAN AFFAIRS argue that SIBs can broaden dialogue and intervention in critical social problems by demonstrating the benefits of model programs (Jupp, 2015; Liebman, 2011), others warn that they may narrow the scope of policy conversation, reducing it to a transactional discussion of inputs and outputs which can silence broader policy goals (Lake, 2015; Tan, Fraser, McHugh, & Warner, in press; Warner, 2015).\n",
            "Top  8  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  9  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  10  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  11  :   City, county, and state governments are experimenting with SIBs to expand critical early child- hood care and education (ECE) services. We will show that in networks with a broader view and a balance of power across actors in early education, government, and business, SIBs may be able to promote expanded ECE investment. But when financial metrics are privileged, market logic narrows the focus. Though financialization risks reducing urban policy to its financial, not social, outcomes (Lake, 2016), we find local configurations of actors and their power relations matter. Cities can push back against these forces of marketization, effectively “riding the wave” of fiscal austerity by adapting market logics to justify public investment and enhance social policy (Warner & Clifton, 2014). N. Fraser (2011) and Ogman (2019) have used Polanyi’s (1944) theory of a double movement to look at how progressive coalitions of service providers seek to maintain services in times of austerity by partnering with more powerful economic actors in a progressive neoliberal compromise. Progressive lower-level service providers are using economic logics to justify enhanced investments in early childhood, making the “economic case” and hoping that this will lead to an expansion in social rights to care (Warner & Prentice, 2013). In so doing, early childhood interventions walk a razor’s edge between the potential to insert broader social objectives into a market mechanism and that mechanism’s capacity to undermine broader social goals.\n",
            "Top  12  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  13  :   Policy coalitions matter. As SIBs emerge in the ECE arena in the United States, we must assess the extent to which state and local policy actors are building links with private finance to increase ECE investment and promote further financialization of ECE services at the same time (see also Carter, in press; J. Williams, 2018). SIBs claim to demonstrate the effectiveness and cost savings of their social interventions, but the economic benefits of ECE investments are already widely known. Does the SIB mechanism itself help to produce information that can shift the policy debate? Or does it merely serve as a Trojan horse opening another form of legitimacy for neoliberal policy intrusion into social services?\n",
            "Top  14  :   Testing the razor’s edge: SIBs in early childhood education Early childhood care and education services are a proving ground for early SIBs in the United States. Lessons for emerging SIBs can be drawn from the experience of using economic logics to support state, regional, and local-level economic development policies that expand access and funding for child care. Economic or financial logics have been used for more than 20 years in the United States to reimagine ECE services as investments, instead of just expenditures, and thus justify additional public and private support (Bartik, 2011; Warner, 2006). These logics have nudged policy discourse in the United States and Canada toward thinking of early education and care as a social infra- structure for economic development and thus an investable social service (Warner & Prentice, 2013). ECE has a well-documented capacity to produce important social gains. Relatively short and inexpensive interventions, such as preschool or parenting education programs, can improve chil- dren’s cognitive and social development and enhance school achievement (Heckman, Pinto, & Savelyev, 2013; Olds et al., 1997). Broader supports for working families, such as full-day childcare, reduce stress on parents and enable increased parental workforce participation and productivity (Gornick & Meyers, 2003; Halpern, 2004; Hipp, Morrissey, & Warner, 2017). Early childhood interventions can even support market returns at the regional economy level, if broadly conceived (Bartik, 2011; Warner, 2006). These economic logics reconceptualize early care and education as social infrastructure can expand social rights because it shifts ECE from a welfare service to an infrastructure and the universality that infra- structure has historically implied (Warner & Prentice, 2013).\n",
            "Top  15  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  16  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  17  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  18  :   Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert ECE into policy and funding conversations in which it would not have been before. SIB stakeholders are positive about the impact of their experiments with social finance. They see SIBs as helping them serve more children and opening the door to serving many more.\n",
            "Top  19  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  20  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What was the impact of the intervention?\n",
            "Top  1  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  2  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  3  :   The third factor is coordinated networks. In both South Carolina and Utah, coalitions of local and statewide advocates and philanthropies had supported their respective interventions long before the SIBs were implemented. These coalitions worked together with state actors to pursue long-term policy solutions and funding streams, using the SIBs as leverage. But without these networks inserting broader objectives, comprehensive approaches could be lost to SIBs’ financializing logic, as occurred in Chicago.\n",
            "Top  4  :   infrastructure for economic development. Social A comprehensive approach that includes human development, the productivity of families, and the regional economy is critical because it opens possibilities for deeper reform (Morrissey & Warner, 2007). The simple interventions favored by SIBs in the United States, however, such as preschool or parent education, fall short of this approach. Despite the possibilities for broader structural change, they do not address workplace policy and comprehensive support for ECE from birth onward, such as full-day childcare for working parents. Biases toward preschool alone are found in business and economic arguments (Rolnick & Grunewald, 2003). Feminist economists and educators voice the broader concerns of parents and ECE specialists, but these have not gained the investor attention they deserve (Folbre, 2006; Halpern, 2004; Stoney, Mitchell, & Warner, 2006; Warner, 2009).\n",
            "Top  5  :   JOURNAL OF URBAN AFFAIRS argue that SIBs can broaden dialogue and intervention in critical social problems by demonstrating the benefits of model programs (Jupp, 2015; Liebman, 2011), others warn that they may narrow the scope of policy conversation, reducing it to a transactional discussion of inputs and outputs which can silence broader policy goals (Lake, 2015; Tan, Fraser, McHugh, & Warner, in press; Warner, 2015).\n",
            "Top  6  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  7  :   A common refrain of interviewees was that SIBs’ cost savings argument, supported by data, is the only one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track evidence on child development. The evidence for cost savings serves as evidence for “what works.” But the SIBs’ interventions already were well documented. NFP operates in 42 states and is well researched (Olds et al., 1997). Child–parent centers have been operating in Chicago for 50 years and have been studied closely (Reynolds, 1997). Even the Granite School District preschool program had 3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well- demonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public money—and private finance generated through the SIB is much more expensive than public finance. Instead, the main benefit of data and the cost savings argument is the production of localized information to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.\n",
            "Top  8  :   Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert ECE into policy and funding conversations in which it would not have been before. SIB stakeholders are positive about the impact of their experiments with social finance. They see SIBs as helping them serve more children and opening the door to serving many more.\n",
            "Top  9  :   Interviewees argued that SIBs’ appeal is their capacity to translate a complex service into quantifiable, investable variables that can change policy. But we find that translation to be overly reductive, narrowing the scope of policy dialogue from the inherent social value of ECE to its transactional monetary value. O’Neill (2010) warns us of the danger of losing key features of infrastructure—universality, bundling, access, and positive externalities. Planners know that positive externalities are primary reasons for investing in public services (Del Bo & Florio, 2012; Sclar, 2015). When a broader perspective is taken, all of society benefits, not just the client. But SIBs are limited to short-term outcomes that can appeal to investors and clearly be attributed to cost savings. Figure 1 shows how SIBs, by narrowing their focus to the monetizable elements of an intervention, draw attention away from broader social inclusion and human development.\n",
            "Top  10  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  11  :   Policy coalitions matter. As SIBs emerge in the ECE arena in the United States, we must assess the extent to which state and local policy actors are building links with private finance to increase ECE investment and promote further financialization of ECE services at the same time (see also Carter, in press; J. Williams, 2018). SIBs claim to demonstrate the effectiveness and cost savings of their social interventions, but the economic benefits of ECE investments are already widely known. Does the SIB mechanism itself help to produce information that can shift the policy debate? Or does it merely serve as a Trojan horse opening another form of legitimacy for neoliberal policy intrusion into social services?\n",
            "Top  12  :   City, county, and state governments are experimenting with SIBs to expand critical early child- hood care and education (ECE) services. We will show that in networks with a broader view and a balance of power across actors in early education, government, and business, SIBs may be able to promote expanded ECE investment. But when financial metrics are privileged, market logic narrows the focus. Though financialization risks reducing urban policy to its financial, not social, outcomes (Lake, 2016), we find local configurations of actors and their power relations matter. Cities can push back against these forces of marketization, effectively “riding the wave” of fiscal austerity by adapting market logics to justify public investment and enhance social policy (Warner & Clifton, 2014). N. Fraser (2011) and Ogman (2019) have used Polanyi’s (1944) theory of a double movement to look at how progressive coalitions of service providers seek to maintain services in times of austerity by partnering with more powerful economic actors in a progressive neoliberal compromise. Progressive lower-level service providers are using economic logics to justify enhanced investments in early childhood, making the “economic case” and hoping that this will lead to an expansion in social rights to care (Warner & Prentice, 2013). In so doing, early childhood interventions walk a razor’s edge between the potential to insert broader social objectives into a market mechanism and that mechanism’s capacity to undermine broader social goals.\n",
            "Top  13  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  14  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  15  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  16  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  17  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  18  :   Testing the razor’s edge: SIBs in early childhood education Early childhood care and education services are a proving ground for early SIBs in the United States. Lessons for emerging SIBs can be drawn from the experience of using economic logics to support state, regional, and local-level economic development policies that expand access and funding for child care. Economic or financial logics have been used for more than 20 years in the United States to reimagine ECE services as investments, instead of just expenditures, and thus justify additional public and private support (Bartik, 2011; Warner, 2006). These logics have nudged policy discourse in the United States and Canada toward thinking of early education and care as a social infra- structure for economic development and thus an investable social service (Warner & Prentice, 2013). ECE has a well-documented capacity to produce important social gains. Relatively short and inexpensive interventions, such as preschool or parenting education programs, can improve chil- dren’s cognitive and social development and enhance school achievement (Heckman, Pinto, & Savelyev, 2013; Olds et al., 1997). Broader supports for working families, such as full-day childcare, reduce stress on parents and enable increased parental workforce participation and productivity (Gornick & Meyers, 2003; Halpern, 2004; Hipp, Morrissey, & Warner, 2017). Early childhood interventions can even support market returns at the regional economy level, if broadly conceived (Bartik, 2011; Warner, 2006). These economic logics reconceptualize early care and education as social infrastructure can expand social rights because it shifts ECE from a welfare service to an infrastructure and the universality that infra- structure has historically implied (Warner & Prentice, 2013).\n",
            "Top  19  :   JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial logic when they include long-term objectives to build systemic change. These objectives are not written into the contracts. They are not part of the design. But they leverage the capacity of SIBs to produce localized information. This capacity is especially useful in conservative political contexts because strategic networks of social advocates can use that localized information to nudge discourse and alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs in the United States can be leveraged to scale up public investment to the state level by strategically targeting long-term policy change.\n",
            "Top  20  :   Journal of Urban Affairs ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/ujua20 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse & Mildred E. Warner To cite this article: Allison E. Tse & Mildred E. Warner (2020) The razor’s edge: Social impact bonds and the financialization of early childhood services, Journal of Urban Affairs, 42:6, 816-832, DOI: 10.1080/07352166.2018.1465347 To link to this article:  https://doi.org/10.1080/07352166.2018.1465347 Published online: 14 May 2018.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Were the contracted outcomes achieved?\n",
            "Top  1  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  2  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  3  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  4  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  5  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  6  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  7  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  8  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  9  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  10  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  11  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  12  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  13  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  14  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  15  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  16  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  17  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  18  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  19  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  20  :   JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial logic when they include long-term objectives to build systemic change. These objectives are not written into the contracts. They are not part of the design. But they leverage the capacity of SIBs to produce localized information. This capacity is especially useful in conservative political contexts because strategic networks of social advocates can use that localized information to nudge discourse and alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs in the United States can be leveraged to scale up public investment to the state level by strategically targeting long-term policy change.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results outcomes achieved impact\n",
            "Top  1  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  2  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  3  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  4  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  5  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  6  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  7  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  8  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  9  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  10  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  11  :   A. E. TSE AND M. E. WARNER We use this framework to discern SIBs’ possibilities for progressive impact. We look inside the details of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs can concretize the promise of increased investment and broader public policy response depends critically on the configuration of actors and mechanisms in their local and regional context.\n",
            "Top  12  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  13  :   Interviewees argued that SIBs’ appeal is their capacity to translate a complex service into quantifiable, investable variables that can change policy. But we find that translation to be overly reductive, narrowing the scope of policy dialogue from the inherent social value of ECE to its transactional monetary value. O’Neill (2010) warns us of the danger of losing key features of infrastructure—universality, bundling, access, and positive externalities. Planners know that positive externalities are primary reasons for investing in public services (Del Bo & Florio, 2012; Sclar, 2015). When a broader perspective is taken, all of society benefits, not just the client. But SIBs are limited to short-term outcomes that can appeal to investors and clearly be attributed to cost savings. Figure 1 shows how SIBs, by narrowing their focus to the monetizable elements of an intervention, draw attention away from broader social inclusion and human development.\n",
            "Top  14  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  15  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  16  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  17  :   JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial logic when they include long-term objectives to build systemic change. These objectives are not written into the contracts. They are not part of the design. But they leverage the capacity of SIBs to produce localized information. This capacity is especially useful in conservative political contexts because strategic networks of social advocates can use that localized information to nudge discourse and alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs in the United States can be leveraged to scale up public investment to the state level by strategically targeting long-term policy change.\n",
            "Top  18  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  19  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  20  :   Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert ECE into policy and funding conversations in which it would not have been before. SIB stakeholders are positive about the impact of their experiments with social finance. They see SIBs as helping them serve more children and opening the door to serving many more.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results\n",
            "Top  1  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  2  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  3  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  4  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  5  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  6  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  7  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  8  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  9  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  10  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  11  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  12  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  13  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  14  :   Analysis We evaluate each case against the four foci outlined in our methodology. Chicago’s and Utah’s SIBs operate at the city or metropolitan level. The South Carolina SIB operates at the state level. Our following analysis shows how the cases fall along a continuum. We find that South Carolina is most likely to achieve systemic change by both inserting social values in the market and using market players to shift the political dialogue. Utah is next along the continuum, using market pressure to shift political investment but sacrificing social objectives to investor return. We find that Chicago ranks last because it extracts funds from the public sector in the name of expanding access to preschool. A summary of the three cases is provided in Table 2.\n",
            "Top  15  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  16  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  17  :   A common refrain of interviewees was that SIBs’ cost savings argument, supported by data, is the only one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track evidence on child development. The evidence for cost savings serves as evidence for “what works.” But the SIBs’ interventions already were well documented. NFP operates in 42 states and is well researched (Olds et al., 1997). Child–parent centers have been operating in Chicago for 50 years and have been studied closely (Reynolds, 1997). Even the Granite School District preschool program had 3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well- demonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public money—and private finance generated through the SIB is much more expensive than public finance. Instead, the main benefit of data and the cost savings argument is the production of localized information to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.\n",
            "Top  18  :   JOURNAL OF URBAN AFFAIRS argue that SIBs can broaden dialogue and intervention in critical social problems by demonstrating the benefits of model programs (Jupp, 2015; Liebman, 2011), others warn that they may narrow the scope of policy conversation, reducing it to a transactional discussion of inputs and outputs which can silence broader policy goals (Lake, 2015; Tan, Fraser, McHugh, & Warner, in press; Warner, 2015).\n",
            "Top  19  :   The number of SIBs has grown substantially around the world (A. Fraser, Tan, Lagarde, & Mays, 2018) since the first SIB was developed in the United Kingdom in 2010 (Disley, Rubin, Scraggs, Burrowes, & Culley, 2011). At least 17 have been implemented in the United States (Finlaw, 2017), primarily by cities, counties, and states, with steady Obama-era federal support for feasibility studies and technical assistance (Gustafsson-Wright, Gardiner, & Putcha, 2015; U.S. Government Accountability Office [GAO], 2015).\n",
            "Top  20  :   In Chicago, the goal of the SIB is more limited. A priority of Mayor Rahm Emanuel is to expand early childhood education services, and the SIB uses private investment to increase access to CPC. But after the final cohort, the city will have to find new funding to sustain the expansion or else reduce the number of slots (Chicago SIB, 2017). Unlike the other cases, Chicago shows no evidence of a broader political or fiscal goal. There are no plans to further expand preschool after the SIB ends (Sanchez, 2016). Our case studies suggest that SIBs that do not have systemic change as an explicit goal and do not deliberately seek to change mainstream public funding arrangements offer less compensation for the known risks of financialization.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes achieved\n",
            "Top  1  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  2  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  3  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  4  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  5  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  6  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  7  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  8  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  9  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  10  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  11  :   Analysis We evaluate each case against the four foci outlined in our methodology. Chicago’s and Utah’s SIBs operate at the city or metropolitan level. The South Carolina SIB operates at the state level. Our following analysis shows how the cases fall along a continuum. We find that South Carolina is most likely to achieve systemic change by both inserting social values in the market and using market players to shift the political dialogue. Utah is next along the continuum, using market pressure to shift political investment but sacrificing social objectives to investor return. We find that Chicago ranks last because it extracts funds from the public sector in the name of expanding access to preschool. A summary of the three cases is provided in Table 2.\n",
            "Top  12  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  13  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  14  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  15  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  16  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  17  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  18  :   JOURNAL OF URBAN AFFAIRS argue that SIBs can broaden dialogue and intervention in critical social problems by demonstrating the benefits of model programs (Jupp, 2015; Liebman, 2011), others warn that they may narrow the scope of policy conversation, reducing it to a transactional discussion of inputs and outputs which can silence broader policy goals (Lake, 2015; Tan, Fraser, McHugh, & Warner, in press; Warner, 2015).\n",
            "Top  19  :   A. E. TSE AND M. E. WARNER We use this framework to discern SIBs’ possibilities for progressive impact. We look inside the details of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs can concretize the promise of increased investment and broader public policy response depends critically on the configuration of actors and mechanisms in their local and regional context.\n",
            "Top  20  :   In Chicago, the goal of the SIB is more limited. A priority of Mayor Rahm Emanuel is to expand early childhood education services, and the SIB uses private investment to increase access to CPC. But after the final cohort, the city will have to find new funding to sustain the expansion or else reduce the number of slots (Chicago SIB, 2017). Unlike the other cases, Chicago shows no evidence of a broader political or fiscal goal. There are no plans to further expand preschool after the SIB ends (Sanchez, 2016). Our case studies suggest that SIBs that do not have systemic change as an explicit goal and do not deliberately seek to change mainstream public funding arrangements offer less compensation for the known risks of financialization.\n",
            "\n",
            "\n",
            "\n",
            "Query:  impact\n",
            "Top  1  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  2  :   Social impact bonds (SIBs) offer an experimental strategy for U.S. cities navigating the politics of fiscal constraint. These emerging financing mechanisms represent an alluring possibility: increased investment in social programs through private financing (Pequeneza, 2018). We explore how U.S. cities have maneuvered through the allure of SIBs, and their Trojan horse–like dangers, as they strive for more equitable social service delivery.\n",
            "Top  3  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  4  :   A. E. TSE AND M. E. WARNER We use this framework to discern SIBs’ possibilities for progressive impact. We look inside the details of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs can concretize the promise of increased investment and broader public policy response depends critically on the configuration of actors and mechanisms in their local and regional context.\n",
            "Top  5  :   Journal of Urban Affairs ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/ujua20 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse & Mildred E. Warner To cite this article: Allison E. Tse & Mildred E. Warner (2020) The razor’s edge: Social impact bonds and the financialization of early childhood services, Journal of Urban Affairs, 42:6, 816-832, DOI: 10.1080/07352166.2018.1465347 To link to this article:  https://doi.org/10.1080/07352166.2018.1465347 Published online: 14 May 2018.\n",
            "Top  6  :   JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial logic when they include long-term objectives to build systemic change. These objectives are not written into the contracts. They are not part of the design. But they leverage the capacity of SIBs to produce localized information. This capacity is especially useful in conservative political contexts because strategic networks of social advocates can use that localized information to nudge discourse and alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs in the United States can be leveraged to scale up public investment to the state level by strategically targeting long-term policy change.\n",
            "Top  7  :   Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert ECE into policy and funding conversations in which it would not have been before. SIB stakeholders are positive about the impact of their experiments with social finance. They see SIBs as helping them serve more children and opening the door to serving many more.\n",
            "Top  8  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  9  :   About the authors Allison E. Tse is a master’s student in City & Regional Planning at Cornell University concentrating on economic development. She researches social impact bonds as well as fiscal policy and austerity in local governments in the United States. Her past work has focused on rural community and economic development in the United States.\n",
            "Top  10  :   Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure sustainable public funding for early childhood services through state legislation. The designers of each project committed to this goal first. Then they used the SIB to demonstrate cost savings for governments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.\n",
            "Top  11  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  12  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  13  :   infrastructure for economic development. Social A comprehensive approach that includes human development, the productivity of families, and the regional economy is critical because it opens possibilities for deeper reform (Morrissey & Warner, 2007). The simple interventions favored by SIBs in the United States, however, such as preschool or parent education, fall short of this approach. Despite the possibilities for broader structural change, they do not address workplace policy and comprehensive support for ECE from birth onward, such as full-day childcare for working parents. Biases toward preschool alone are found in business and economic arguments (Rolnick & Grunewald, 2003). Feminist economists and educators voice the broader concerns of parents and ECE specialists, but these have not gained the investor attention they deserve (Folbre, 2006; Halpern, 2004; Stoney, Mitchell, & Warner, 2006; Warner, 2009).\n",
            "Top  14  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  15  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  16  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  17  :   Interviewees argued that SIBs’ appeal is their capacity to translate a complex service into quantifiable, investable variables that can change policy. But we find that translation to be overly reductive, narrowing the scope of policy dialogue from the inherent social value of ECE to its transactional monetary value. O’Neill (2010) warns us of the danger of losing key features of infrastructure—universality, bundling, access, and positive externalities. Planners know that positive externalities are primary reasons for investing in public services (Del Bo & Florio, 2012; Sclar, 2015). When a broader perspective is taken, all of society benefits, not just the client. But SIBs are limited to short-term outcomes that can appeal to investors and clearly be attributed to cost savings. Figure 1 shows how SIBs, by narrowing their focus to the monetizable elements of an intervention, draw attention away from broader social inclusion and human development.\n",
            "Top  18  :   A common refrain of interviewees was that SIBs’ cost savings argument, supported by data, is the only one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track evidence on child development. The evidence for cost savings serves as evidence for “what works.” But the SIBs’ interventions already were well documented. NFP operates in 42 states and is well researched (Olds et al., 1997). Child–parent centers have been operating in Chicago for 50 years and have been studied closely (Reynolds, 1997). Even the Granite School District preschool program had 3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well- demonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public money—and private finance generated through the SIB is much more expensive than public finance. Instead, the main benefit of data and the cost savings argument is the production of localized information to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.\n",
            "Top  19  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  20  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  4  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  5  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  6  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  9  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  10  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  11  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  12  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  13  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  14  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  15  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  16  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  17  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  18  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "Top  19  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  20  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  2  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  3  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  4  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  5  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  6  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  7  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  8  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  9  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  10  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  11  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  12  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  13  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  14  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  15  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  16  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  17  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  18  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  19  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  20  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  2  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  3  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  4  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  5  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  6  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  7  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  8  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  9  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  10  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  11  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  12  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  13  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  14  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  15  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  16  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  17  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  18  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  19  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  20  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  2  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  3  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  4  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  5  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  6  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  7  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  8  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  9  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  10  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  11  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  12  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  13  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  14  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  15  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  16  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  17  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  18  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  19  :   The number of SIBs has grown substantially around the world (A. Fraser, Tan, Lagarde, & Mays, 2018) since the first SIB was developed in the United Kingdom in 2010 (Disley, Rubin, Scraggs, Burrowes, & Culley, 2011). At least 17 have been implemented in the United States (Finlaw, 2017), primarily by cities, counties, and states, with steady Obama-era federal support for feasibility studies and technical assistance (Gustafsson-Wright, Gardiner, & Putcha, 2015; U.S. Government Accountability Office [GAO], 2015).\n",
            "Top  20  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  2  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  3  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  4  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  5  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  6  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  7  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  8  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  9  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  10  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  11  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  12  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  13  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  14  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  15  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  16  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  17  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  18  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  19  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  20  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  4  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  5  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  6  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  7  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  8  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  9  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  10  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  11  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  12  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  13  :   A common refrain of interviewees was that SIBs’ cost savings argument, supported by data, is the only one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track evidence on child development. The evidence for cost savings serves as evidence for “what works.” But the SIBs’ interventions already were well documented. NFP operates in 42 states and is well researched (Olds et al., 1997). Child–parent centers have been operating in Chicago for 50 years and have been studied closely (Reynolds, 1997). Even the Granite School District preschool program had 3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well- demonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public money—and private finance generated through the SIB is much more expensive than public finance. Instead, the main benefit of data and the cost savings argument is the production of localized information to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.\n",
            "Top  14  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  15  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  16  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  17  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  18  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  19  :   Systemic change: presence of a new, sustainable public funding stream Performance metrics: breadth of metrics South Carolina Yes—Medicaid reimbursement Utah Yes—state income tax Broad—four metrics Narrow—one metric Cost structure: comparison of public investment to investor profit Maximizes public investment— reinvests success payments Maximizes investor profit —overestimates impact Social equity: coverage of services to Yes—rural coverage vulnerable clientele Yes—low-income coverage Chicago No—funding ends after SIB Intermediate—three metrics Maximizes investor profit—overpays investors Yes—low-income coverage politically conservative states with high child poverty rates. They are difficult contexts in which to push any expansion of ECE services.\n",
            "Top  20  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  2  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  3  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  4  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  5  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  6  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  7  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  8  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  9  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  10  :   South Carolina has the most metrics and its evaluation is the most extensive of the three. The project’s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction in child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on past evidence of the NFP model’s capacity to achieve them, to reduce the risk to NFP, and to maintain the clarity of the evaluation. The SIB’s designers considered that one or two metrics would apply too much pressure to NFP to perform within acute parameters. Five or six metrics, however, would reduce or distort each metric’s significance (SC SIB, 2017).\n",
            "Top  11  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  12  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  13  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "Top  14  :   implement public services, SIBs are tools to produce information about both cost savings and intervention performance, which can shift public investment. The evidence for these interventions’ social and financial benefits already existed but, as other studies show, states in conservative contexts with higher poverty levels and larger populations of people of color are more restrictive with social welfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener, 2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we thought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be attractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people see it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it directly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there’s a rigorous evaluation and we all agree to abide by the results, and the payment is specifically associated with the outcome we’re looking for. You look at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on either inertia—nothing drives the appropriations process like inertia—or it’s based on people telling stories in front of committee. The amount of money that we appropriate that is connected to real evidence is damagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management theory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing the importance of ECE. As conversations change, policies and public investment can expand to this underserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other direction, at least for Utah and South Carolina. In Utah, the economic return on special education avoidance changed state investment in early education. In South Carolina, the economic return on improved public health expanded the scope of policy and secured a long-term funding stream. The possibility to sway policymakers with higher-level authority means that SIBs have a surprising capacity to scale up public investment in social welfare from the city to the state level.\n",
            "Top  15  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  16  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  17  :   Child–Parent Center Pay for Success initiative This SIB launched in Chicago in 2014, the year after Utah’s, to increase the number of Child–Parent Center (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a). CPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent engagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases, this SIB does not appear to have a broader long-term financing goal.\n",
            "Top  18  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  19  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  20  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  5  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  6  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  7  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  8  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  9  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  10  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  11  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  12  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  13  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  14  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  15  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  16  :   The margin, or the “razor’s edge,” for this broader interpretation is thin because the SIB’s logic privileges financial gains. Figure 2 visualizes our findings on navigating this neoliberal margin. South Carolina has no premium and no profit, instead inserting social objectives into the market to move the political conversation in a conservative state. Utah ran the financial risk of a SIB with a long view of moving state politics, but the investor payout and methodology undermined its social mission by diverting public investment to private investors. Chicago demonstrates the risk of financialized logic by providing short-term benefits to investors and clientele without the balance of long-term social objectives. This SIB falls totally on the extractive side of the market.\n",
            "Top  17  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  18  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  19  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  20  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  4  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  5  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  6  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  9  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  10  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  11  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  12  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  13  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  14  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  15  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  16  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  17  :   investment The financialization of social policy through SIBs risks obscuring visions of greater social rights. Lake (2016) characterizes the reductive nature of financialization for urban policy: “The monetization of policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a means for reducing government spending and producing a financial return for investors” (p. 57). Similar lessons from the financialization of physical infrastructure show how broader public objectives and positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015). Private financing has undermined the broader characteristics of physical infrastructure: universality, access, and maximization of positive externalities (O’Neill, 2010). Today, physical infrastructure investments have become a new asset class, designed to attract private investment through public– private partnerships (O’Neill, 2017).\n",
            "Top  18  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  19  :   The margin, or the “razor’s edge,” for this broader interpretation is thin because the SIB’s logic privileges financial gains. Figure 2 visualizes our findings on navigating this neoliberal margin. South Carolina has no premium and no profit, instead inserting social objectives into the market to move the political conversation in a conservative state. Utah ran the financial risk of a SIB with a long view of moving state politics, but the investor payout and methodology undermined its social mission by diverting public investment to private investors. Chicago demonstrates the risk of financialized logic by providing short-term benefits to investors and clientele without the balance of long-term social objectives. This SIB falls totally on the extractive side of the market.\n",
            "Top  20  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically including the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d; SCDHHS, 2016).\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  5  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  6  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  7  :   Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped target educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to organizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce flexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success, measurement method, and evaluation rigor; and the performance of the intervention against its outcome targets are the sole factors that determine the payout in a SIB contract. Because of this narrowly defined path to success, some researchers argue that SIBs’ performance-based management scheme simplifies and distorts the “complex reality” of operating a social program, turning the intervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe & Wilson, 2017). This gamesmanship can drive management toward “creaming and parking, teaching to the test, reclassifying, and falsification of data” (Lowe & Wilson, 2017, p. 986). We provide the following insights into our cases’ selection and breadth of metrics.\n",
            "Top  8  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  9  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  10  :   Because of the SIB, the state ultimately passed legislation to appropriate funding for preschool. The legislation also caps investor return on future SIBs at 5% above the municipal market data general obligation bond rate (State of Utah, 2014).\n",
            "Top  11  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  12  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  13  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  14  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  15  :   The number of SIBs has grown substantially around the world (A. Fraser, Tan, Lagarde, & Mays, 2018) since the first SIB was developed in the United Kingdom in 2010 (Disley, Rubin, Scraggs, Burrowes, & Culley, 2011). At least 17 have been implemented in the United States (Finlaw, 2017), primarily by cities, counties, and states, with steady Obama-era federal support for feasibility studies and technical assistance (Gustafsson-Wright, Gardiner, & Putcha, 2015; U.S. Government Accountability Office [GAO], 2015).\n",
            "Top  16  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  17  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  18  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  19  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  20  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  4  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  5  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  6  :   Because of the SIB, the state ultimately passed legislation to appropriate funding for preschool. The legislation also caps investor return on future SIBs at 5% above the municipal market data general obligation bond rate (State of Utah, 2014).\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  9  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  10  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  11  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  12  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  13  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  14  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  15  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  16  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  17  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  18  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  19  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  20  :   South Carolina Nurse–Family Partnership Pay for Success Project funders supporting Nurse–Family Partnership (NFP) Originating from a consortium of in Greenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP home visitation services for 3,200 first-time mothers across the state (South Carolina Department of Health and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income mothers to improve maternal and child health. The project has four outcome metrics (South Carolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to secure sustainable public financing for NFP statewide.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  2  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  3  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  4  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  5  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  6  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  7  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  8  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  9  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  10  :   In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy to permanent state and federal sources. A group of philanthropies, including The Duke Endowment, Blue Cross Blue Shield Foundation, Greenville First Steps, and the Children’s Trust of South Carolina, had been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal Maternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services were limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy could not sustain these services in the long run. They launched the SIB to scale up NFP services and demonstrate its cost savings: It’s a much more expensive intervention than even the childcare vouchers that we provide. When the community started NFP, it was 100 clients, so we were at about $450,000. . . . That was sustainable in Greenville County. We could have done that forever. There were 10 partners that we could have gotten to put $50,000 in . . . but what we realized was we didn’t need to serve 100 clients. We needed to serve 600 clients, because we started looking at our birth rates in Greenville County alone; [there] was no way that local philanthropy was going to be able to sustain a $3 million project. Our United Way’s entire early childhood investment is $2 million. That’s for everything that they do. . . . Early on, we realized we want to take this program to scale, and we don’t want to be left with the bill because there’s no way we can sustain that. (personal interview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina’s Department of Health and Human Services secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years to achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners plan to advocate for permanent Medicaid reimbursement or other state appropriations so that the program will have a stable, long-term funding source (SC SIB, 2017). South Carolina’s experience demonstrates the greater value added from an SIB with a long-term perspective and the relative lack of value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn’t make sense for a known recurring cost you’re going to face forever more. It can make sense for something where you either want to try to figure how to get something to scale or something that’s a little more experimental potentially. In this case, the question was, is there a way to scale NFP while also bringing its cost structure down to a place where maybe it could become sustainably financed in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment before the SIB. State income tax funding for education was restricted to kindergarten to grade 12 (Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high- quality preschool and alter the state’s education funding formula to include preschool. The first bill to do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a “proof-of-concept” to further educate legislators about the benefits of high-quality preschool in terms of cost savings and child development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first cohort’s success payments, on the condition that, if successful, the state would pick up the success payments for the second through fifth cohorts.\n",
            "Top  11  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  12  :   Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also raise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are considered the “gold standard” for evaluation. South Carolina’s Department of Health and Human Services required an RCT to ensure the credibility of the SIB’s results (SC SIB, 2017). But RCTs require a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB designers: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to have a control group that we would also track, but cannot benefit from, not only NFP services but any other prenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers that we found during pregnancy and know they need our help. Because we need to have some sort of objective comparative data, not only can we not help them, but we can’t tell them other places to go to get that help. . . . We’re looking at probably 100 families now a year that I can’t help that I should be helping. I totally get that if it wasn’t for those 100 families, we wouldn’t be able to help all these other families. I totally get that five years ago, there was no NFP and none of these families were getting help. The world is better off because we’re doing what we’re doing, but I look at those 100 families and I think, “But their kids won’t be better off.” That is probably the one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control group that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its project participants two additional resources beyond NFP, such as Reach Out and Read pro- gramming and free childcare vouchers, but not all families participate. The Chicago SIB includes additional resources for parent engagement through its partnership with Metropolitan Family Services.\n",
            "Top  13  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  14  :   The second factor is a strong public actor. Complex contracting requires clear values and sophisticated partners who can overcome information asymmetries. The South Carolina state government was a strong and creative partner that helped steer the project toward broader aims and a sustainable funding stream. Philanthropic investors were not looking to extract economic rents from the project. By contrast, Utah’s partners had to use their position to attract private finance, at great cost, to pressure state policy. Private financial interests could take advantage of information asymmetries to extract rents. When the state did come on board, it used its political power to legally limit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city exacted no special requirements and thus no broader aims were met. The rent extraction in Chicago is the most usurious—claiming success payments for up to 15 years for a single-year intervention, without a sustainable funding plan. As with the Chicago Skyway and parking public–private partnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense of further public investment.\n",
            "Top  15  :   The uncertainty of the Utah SIB’s risk influenced its high payout rate. But Granite School District’s preschool program had a history of success and the SIB’s metrics were skewed to increase outcome payments. The risk to investors should be relatively low. So why should investors receive a high interest rate? Though demonstrating to the state legislature that preschool is good for children, the SIB handed over a massive amount of money to investors and has generated negative press about its usurious pricing structure (Popper, 2015).\n",
            "Top  16  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  17  :   Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and third-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program attrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility factor ensures that only Chicago Public Schools students who remain in the school district from preschool onward will be counted toward success payments. Compared to Utah, this more sophis- ticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of a single metric. The special education metric, however, holds by far the greatest weight in the contract. The other two are weak measures by contrast.\n",
            "Top  18  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  19  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  20  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Despite details in available documentation about the Chicago SIB, its precise costs are uncertain. In the SIB contract, success payments are expected to reach over $25 million, about $21.4 million from the Board of Education and $4.3 million from the city. But one clause caps possible board payments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each investor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the return could be up to 6% (Gustafsson-Wright et al., 2015).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   2. Performance metrics: the scope of outcome metrics. 3. Cost structure: the costs and returns in the transaction. 4. Social equity: the consideration of social inclusion.\n",
            "Top  4  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  5  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  6  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  7  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  8  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "Top  9  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  10  :   . . . In general, politicians don’t feel A. E. TSE AND M. E. WARNER government, but it is government’s role to pay Medicaid costs because that’s a role that’s been assigned to government. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South Carolina already has broadened the scope of ECE services. Although the program’s eligibility for Medicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB. In Utah, the SIB was launched in a political atmosphere that provided no funding for preschool (Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in the nation. Children in Utah represent a greater share of the population than in any other state, so state investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology, demonstrating evidence for public savings induced legislative action to finance preschool.\n",
            "Top  11  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  12  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  13  :   Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding for critical social interventions, but they should be wary of SIBs’ marketizing framework. Although our cases show the potential for new investment, SIBs may undermine their own social objectives through their financializing metrics. The risks of high transaction costs, overpayment to investors, inflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit of scaling up public investment and shifting policy, cities invite a Trojan horse of public value into their neediest communities when they implement a SIB. We caution cities to pay attention to these risks as they launch future SIB experiments.\n",
            "Top  14  :   The most surprising factor in Chicago is that investors will receive success payments for 15 years after the intervention. This has been criticized for doubling investor return (Spielman, 2014). Justification for this continued return from a single point-in-time intervention is unprecedented and not supported by research on ECE. Before its launch, five city council members voted against the SIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago’s infamous parking meter privatization scheme (Spielman, 2014). Chicago’s public school system has struggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015 (Gillers, 2015) and, heading into the 2017–2018 school year, faced a budget deficit in the hundreds of millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a pattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots without regard to long-term funding and the SIB returns constitute a substantial overpayment to investors.\n",
            "Top  15  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  16  :   Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity. In all three cases, the target clientele are low-income families. In Utah, over half of the students in Granite School District qualify for free or reduced-price lunch and nearly half speak English as a second language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African American families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In South Carolina, the Department of Health and Human Services went a step further by pushing for the inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first- time mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which are in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016). NFP initially pushed back against this request because of the additional cost of rural outreach and recruitment, but the state persevered and the metric was included (SC SIB, 2017).\n",
            "Top  17  :   This SIB is unique in the U.S. context in that it does not offer an investor return. The philanthropic consortium invested $17 million, and the state secured $13 million in Medicaid funding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.\n",
            "Top  18  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  19  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  20  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   The SIB’s designers clearly considered cost savings for the public, but how substantial these can be when nearly all of the savings are given to the investors is unclear: “They were using the cost savings of special ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money remains with the state, so in theory that helps bolster the state’s education budget as well” (personal interview, SC SIB, January 2017). Utah’s SIB is based on expected cost savings, but how close the forecasts are to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).\n",
            "Top  2  :   In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a $260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort, calculated using the special education cost rate of $2,607 per student (United Way of Salt Lake, 2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.\n",
            "Top  3  :   But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston & Nicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are expensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017; Warner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu- mented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016; Maier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe & Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich & Choi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by marketizing the “public finance value” of their vulnerable clientele (Neyland, 2017; Sinclair, McHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a variety of “welfare conventions,” organizing objectives and stakeholders according to frameworks that prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner Sibley Hall, Ithaca, NY 14853-6701. © 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.\n",
            "Top  4  :   Cost structure financialize public services. Cost structure is a key indicator of whether or not a SIB will Comparative analysis of the three SIBs reveals critical differences in the cost structures of each. Utah and Chicago overpaid their investors at the expense of public savings and future investment in early childhood because nearly all savings go back to investors as returns. South Carolina will reinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a vehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows money cheaper than governments. You go out and have other entities borrow money and then you repay them that with some kind of interest or something on top of it. From a financial argument, you’re basically throwing money away for whatever the difference is between the state’s potential borrowing cost and then whatever you pay out instead at the end of the project. The solution for us was, there’s no private investor premium being paid in our project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB, January 2017) South Carolina’s cost structure demonstrates that investor profit is not the focus. NFP reduced its cost of service by 25% for the SIB by increasing client–nurse ratios in order to make its services affordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed and variable components to determine success, so that results can indicate a zero, intermediate, or high rate of payment. The fourth metric has a minimum threshold for any success payment (South Carolina Department of Health and Human Services, 2016). Overpayment is not possible because success payments go toward further service expansion.\n",
            "Top  5  :   Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure sustainable public funding for early childhood services through state legislation. The designers of each project committed to this goal first. Then they used the SIB to demonstrate cost savings for governments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.\n",
            "Top  6  :   Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB. Initial outcome payments have been made, but education experts criticized them based on ques- tionable metrics, methodology, and financial agreements, claiming that investors were overpaid (Popper, 2015).\n",
            "Top  7  :   Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17 million (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a program-related investment (City of Chicago, 2014d). The maximum potential success payment is about $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An initial success payment has been made (Sanchez, 2016). The project has been criticized for using special education avoidance as a metric, under concerns that this incentivizes service reduction, and for using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).\n",
            "Top  8  :   Negotiating the razor’s edge of financialization Cities that walk the “razor’s edge of financialization” do so at some risk to their most vulnerable citizens. By making a simple link between intervention and payout, SIBs may narrow the scope of social to low-cost programs with short-term returns, when more comprehensive approaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political contexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term investment. Cities must negotiate this razor’s edge if they are to ensure that SIBs actually enhance long-term investment, rather than simply serving as a Trojan horse of financialization.\n",
            "Top  9  :   Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social investment while simultaneously risking further financialization of the sector. ECE services in the United States have suffered from chronic underinvestment compared to other countries. ECE clients, young children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services’ proven economic benefits, SIBs could be suitable tools to expand invest- ment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina Nurse–Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and the Chicago Child–Parent Center Pay for Success Project. For each case, we conducted a document review of publicly available contract and loan agreements,1 press releases, and journalistic articles. We also conducted 11 semistructured interviews in January and February 2017,2 including five for the South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these interviews, one was a public agency, one was an evaluator, two were funders, two were service providers, three were technical assistance providers, and two were intermediaries. Interviewees were selected based on their involvement in the design, launch, and implementation of the SIB. Interview questions covered program design, intervention scope, and management. During coding of these interviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis: systemic change, performance metrics, cost structure, and social equity. We define these constructs below and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.\n",
            "Top  10  :   The conception of SIBs as the imposition of the market economy onto society is not uniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin- ing whether progressive neoliberal coalitions will push back and redirect the market or merely justify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for social rights and protection; South Carolina pushed back against the financializing nature of SIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs. Philanthropic funders acted in the public interest by foregoing the standard investor premium to sustain future investment.\n",
            "Top  11  :   An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the Public Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and supported by the Humboldt Foundation.\n",
            "Top  12  :   SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for current costs and does not consider how to sustain investment is not worth the transaction cost or the interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago and Utah. South Carolina shows that an investor premium is not necessary and that philanthropies can significantly reduce SIB costs through their traditional grant-making roles.\n",
            "Top  13  :   Utah’s SIB designers were concerned with attracting initial investors and compensating them for their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early education SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at least half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a higher interest rate. This higher rate was eliminated for the second through fifth cohorts and the potential base rate increased (Utah SIB, 2017). The maximum return for the first and second cohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah’s new pre- school legislation caps the interest rate at 5% above the municipal market general obligation data bond rate. The idea behind the cap is to relieve the state from a degree of interest rate negotiation while still enabling them to attract investors (Utah SIB, 2017) and be “good govern- ment stewards” (GAO, 2015, p. 47).\n",
            "Top  14  :   Financialization risks marginalization of those sectors and people most in need of services by delivering prescriptive social interventions while undermining social inclusion (Shortall & Warner, 2010) and ignoring the broader structural reforms that should be at the core of social and urban policy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention’s value, as a social service and public expenditure. They position public expenditures as defensible, accountable transactions. But by framing social services as transactions, SIBs risk becoming static, point-in-time estimates of social need. If their returns were not so rigidly linked to specific out- comes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes, however, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).\n",
            "Top  15  :   Intervention Clientele South Carolina Nurse–Family Partnership maternal and child health model 3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from Medicaid Birth spacing Child injury Preterm births Low-income ZIP code coverage Government Success payment Contract duration (including $7.5 million 5 years success payments) Utah Granite School District preschool model 3,500 Salt Lake City students City, with statewide implications Commercial and philanthropy $7 million Special education avoidance Nonprofit and government Maximum unknown 12 years Chicago Child–Parent Centers preschool model 2,618 Chicago students City Commercial and philanthropy $17 million Special education avoidance Kindergarten readiness Third-grade literacy Government $25–34 million 16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS, 2016).\n",
            "Top  16  :   A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature passed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6 million from the general fund and created the Utah School Readiness Board. The board participates in Pay for Success transactions for the state and administers grants to public and private high-quality preschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High- Quality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7 million for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way is participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool and higher education. They also are working with Salt Lake County to achieve universal access to preschool in the county (Utah SIB, 2017).\n",
            "Top  17  :   Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS 2020, VOL. 42, NO. 6, 816–832 https://doi.org/10.1080/07352166.2018.1465347 The razor’s edge: Social impact bonds and the financialization of early childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT In a growing number of U.S. cities, social impact bonds (SIBs) introduce an experimental strategy into the politics of fiscal constraint. With limited political willpower and public funding, some have used SIBs to leverage new support for social programs. We argue that cities that engage in SIBs walk a razor’s edge between promoting public investment and the risk of deepening financialization in the social service sector. We explore efforts in 3 cities to expand early childhood services through SIBs: Salt Lake City, Utah; Chicago, Illinois; and Greenville, South Carolina. We test the balance between promise and risk through four foci: systemic change, performance metrics, cost structure, and social equity. We show that the context of political fiscal climate and strategic policy change matters in SIBs’ justifica- tion and impact; whereas Salt Lake City and Greenville scaled investment up to the state level, Chicago merely plugged short-term local budget gaps.\n",
            "Top  18  :   This metric led to an overidentification of at-risk children because of the “faulty assumption that many of the children in the program would have needed special education without the preschool,” which in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper, 2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only one actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort—an unprecedented level of impact for a preschool program (Popper, 2015). Although the SIB’s partners considered these criticisms, they argued that their evaluation was legitimate and they did not make dramatic changes to the payment structure (Utah SIB, 2017). But the criticisms reveal deficiencies in the SIB’s metrics. Repayment to investors is maximized by reducing the number of students receiving special education, something against which the U.S. Department of Education (2016) has warned.\n",
            "Top  19  :   The common use of special education avoidance as a metric carries the risk of reducing services for children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to “set strong guard rails when using special education as an outcome measure” (¶ 8). Interviews and document review in the Utah and Chicago cases show that the projects are not trying to remove special education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB payments to special education can create a financial incentive to keep services away from children who need them.\n",
            "Top  20  :   3. Under the Affordable Care Act, preventive health programs, such as the Nurse–Family Partnership, can be funded through federal Medicaid dollars. States have wide latitude in how they use federal Medicaid dollars, so funding levels and eligibility criteria for reimbursement of services differ greatly by state. The Medicaid funding for NFP in South Carolina is the first of its kind in the country.\n",
            "\n",
            "\n",
            "\n",
            "Begin experiment for key  #17755\n",
            "Query:  What is the study design?\n",
            "Top  1  :   Annex A: Case study reports The  case  studies  summarise  findings  from  consultations  undertaken  as  part  of  the  DFID  commissioned independent evaluation of the DIBs pilot programme. The case study reports  focus on the DIB model and early successes and lessons learned during the design and set  up phase. Consultations were undertaken with the main stakeholders involved in the design  and set up of the four DIBs. A full list of consultations is set out in the Annex H. Interviewees  have  been  given  the  opportunity  to  review  the  case  studies  and  rectify  the  findings  when  needed, and their feedback has been incorporated in the version of the case studies inserted  below.\n",
            "Top  2  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  3  :   The methodology enables the collection and analysis of disaggregated data  to show difference between groups.  Any methodological limitations are acknowledged and their impact on the  evaluation discussed. The limitations are acceptable and/or they are  adequately addressed.  Any departures from the TOR, inception phase and / or original evaluation  design are adequately explained.  The product discusses any inherent imbalances or biases that interviews  and other data collection may have created.\n",
            "Top  4  :   •  The evaluation design and implementation must meet standard ethical practices.\n",
            "Top  5  :   On this basis, the report makes recommendations on the conditions that are needed for DIBs to  be suitable and optimal, and recommend possible ways to reduce costs in the design, structuring  and implementation of DIBs. The report has been complemented by specific case study reports  focusing on each of the four DIBs, set out in Annex A.\n",
            "Top  6  :   Relevant project level learning activities: A range of learning activities are planned for each  DIB, focused on the DIB design process and the effects of using the DIB model. The supplier  will therefore be required to work with learning providers to take advantage of any synergies  (see Ways of Working and Annex C).\n",
            "Top  7  :   •  The  design  report  should  also  include  the  instruments  that  the evaluator will use in upcoming evaluation activities e.g. to  produce first evaluation report.\n",
            "Top  8  :   In terms of the lessons learned: •  Discussion focused on the importance of clearly defining roles and responsibilities at the  outset, and take advantage of the process of co-design that characterises the DIB, in terms  of shared experience and shared learning.\n",
            "Top  9  :   The  evaluation  process  has  been  set  out  transparently.  The  inception  report  has  been  published  and  clearly  sets  out  the  scope  and  proposed  approach  to  the  evaluation.  Each  interview began with a clear explanation of the research process, aims, and objectives. This  included  an  explanation  of  how  collected  data  would  have  been  used,  and  in  what  form.  Interviewees were then provided with an opportunity to ask questions.\n",
            "Top  10  :   For  the  qualitative  analysis,  this  has  been  organised  into  two  distinct  phases  -  data  management and data interpretation39. The evaluation team drew upon the topic guides and  early stages of fieldwork to develop a framework of themes and sub-themes organised around  the key research questions. This has been reviewed as the fieldwork progressed. The data  from the transcripts and field notes will be summarised and synthesised under the headings  and sub headings within the Evaluation Framework.\n",
            "Top  11  :   Section 4 presents the analysis and findings of the evaluation in relation to EQ1, assessing how  the DIB model affects the design and set up phase of development interventions.\n",
            "Top  12  :   We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We  welcome efforts by the evaluator to find savings during the life of the evaluation.\n",
            "Top  13  :   •  The report should also include an updated financial plan for  the  evaluation  –  including  highlighting  any  savings  that  are  possible  following  detailed  design  phase  and  engagement  with project level learning providers.\n",
            "Top  14  :   Table 6.1: Project focus and measurement approach Projects focused on  Innovation  Building evidence  Replication, drawing on an established evidence  base  Scaling,  using  established,  highly  evidence- based interventions Measurement  Non-experimental  Quasi-experimental or experimental  Against a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for  participation in a DIB. It then sets out five lessons learned around the process of identifying  stakeholders and managing relationships, before concluding with lessons learned around how  the  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different  benefits.\n",
            "Top  15  :   E.6.1.  Analysis The evaluation generated a variety of qualitative and quantitative evidence, which provided  multiple  lines  of  enquiry  and  enabled  the  triangulation  of  different  data  sources,  including  literature and document review, consultations with DIB and DFID stakeholders, cost analysis  and  research  on  comparator  sites.  To  ensure  detailed  and  consistent  analysis  a  clearly  structured  approach  to  the  analysis  is  essential.  The  recommended  analytical  stages  and  tasks are as follows.\n",
            "Top  16  :   The two evaluation questions are: •  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.\n",
            "Top  17  :   The two evaluation questions are: •  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.\n",
            "Top  18  :   The 2 Key Evaluation Questions are: •  EQ1: Assess how the DIB model affects the design, delivery, performance and effectiveness of development interventions.\n",
            "Top  19  :   E.3 Programme-level Research The purpose of this level of research is to compare the findings on the individual DIBs, in order  to  understand  further  how  the  DIB  effect  differs  (or  remains)  across  different  contexts. We  contextualised these findings within the wider DIB sector, and considered the implications of  the findings for both improving the DIB mechanism and how DFID could utilise the model in  the future. To achieve this, we undertook the following tasks: •  DFID consultations •  Programme document review •  Learning workshops E.3.1.  DFID consultations The purpose of the consultations with DFID was to further understand the programme aims;  DFID’s perspective on the progress and success of the programme and its implications for the  wider  DIB  landscape;  and  changes  to  relevant  DFID  strategies,  such  as  the  DIB  or  PbR  Strategies. This information helped ensure the reports and recommendations are relevant and  situated within wider developments within DFID. We had consulted with the PbR team during  the  inception  phase,  and  consultations  with  DFID  in  this  research  wave  focused  on  the  selection and structuring of the 3 DIBs under the pilot programme.\n",
            "Top  20  :   Report structure The remainder of this report is structured as follows: Section  2  sets  out the evaluation  framework that  has  been  used  to guide  the  evaluation,  and  summarises the main features of the methodology and the limitations of the available evidence.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the research method?\n",
            "Top  1  :   providers' own Research  Wave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection  methods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of  stakeholders, before concluding with the main methodological limitations, and the mitigations  undertaken. Further detail is set out in Annex E.\n",
            "Top  2  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  3  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  4  :   Yes Are  sound  research  methods  being  used  to  collect the data?\n",
            "Top  5  :   A150 Question Are sound research methods being used to  collect the data?\n",
            "Top  6  :   Annex E.12  Annex E.10 and E.12 Annex E.10 A115 Annex E: Evaluation methodology This Annex sets out the full evaluation methodology. The annex focuses on Research Wave  1, but where appropriate, reference is also made to the following research waves.\n",
            "Top  7  :   The  evaluation  process  has  been  set  out  transparently.  The  inception  report  has  been  published  and  clearly  sets  out  the  scope  and  proposed  approach  to  the  evaluation.  Each  interview began with a clear explanation of the research process, aims, and objectives. This  included  an  explanation  of  how  collected  data  would  have  been  used,  and  in  what  form.  Interviewees were then provided with an opportunity to ask questions.\n",
            "Top  8  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  9  :   Following the review and validation of the methodology and research tools by DFID, Research  Wave 1 included research and analysis at the individual DIB level for the four DIBs included in  the  scope  of  the  evaluation,  at  the  pilot  programme  level  and  at  the  sector  level.  This  was  conducted between June – December 2018. Further detail is set out in section 2.2.\n",
            "Top  10  :   Sector level research: This level of research seeks to provide the wider contextualisation to our findings.\n",
            "Top  11  :   E.6.1.  Analysis The evaluation generated a variety of qualitative and quantitative evidence, which provided  multiple  lines  of  enquiry  and  enabled  the  triangulation  of  different  data  sources,  including  literature and document review, consultations with DIB and DFID stakeholders, cost analysis  and  research  on  comparator  sites.  To  ensure  detailed  and  consistent  analysis  a  clearly  structured  approach  to  the  analysis  is  essential.  The  recommended  analytical  stages  and  tasks are as follows.\n",
            "Top  12  :   Methodology and evidence base The evaluation is based on an evaluation framework that builds on a range of hypothesised  DIB effects and indicators. As part of the inception phase, the evaluation team drew on the  literature  in  order  to  understand  hypotheses  around  how  the  DIB  model  might  affect  interventions, and developed a list of DIB effects and indicators.\n",
            "Top  13  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  14  :   The evidence base for this research wave is derived from the consultations and programme  document review undertaken at the individual DIB level, the programme level and sector level.  The table below sets out the list of data sources we have drawn upon, mapped against the  three levels of the evaluation.\n",
            "Top  15  :   The  focus  for  this  first  research  wave  has  been  on  the  costs  incurred  in  a  DIB,  and  in  comparison to grant based financing, and payment by results. The focus of the next research waves will be to identify actual costs incurred during implementation, and to review the benefits  arising  from  use  of  a  DIB  (in  comparison  to  grant  based  financing  and payment  by  results  contracts).\n",
            "Top  16  :   Methods: Qualitative   Sources: As above + access to  programme design documents;  and project level process  review/ evaluation activities  focused on design and  implementation of DIB projects  – including service provider  selection, outcome funder  engagement, metric selection.\n",
            "Top  17  :   Delivery of Research Wave 1 has drawn on the preparatory work undertaken during the inception  phase, which included: •  a literature review on the context and progress of the wider SIBs and DIBs sector, and an initial comparison of these mechanisms with alternative funding tools; •  a  review  of  programme  documentation,  at  the  individual  DIB  level  and  DFID  pilot • programme level;  the  refining  of  the  conceptual  framework  and  evaluation  questions  against  the  OECD-DAC  criteria,  development  of  DIB  effect  indicators  and  preparation  of  research  tools; •  preparatory  consultations  with  key  stakeholders  across  the  DIBs  and  scoping  of potential comparison sites; and •  a preliminary stakeholder mapping.\n",
            "Top  18  :   Programme level research: This  level  relates  to  the  DIBs  pilot  programme  and  synthesises  the  finding  across  the  four  DIBs.\n",
            "Top  19  :   2.2.1 Data collection DIB level research: There were three levels of research activity in this first research wave (RW1), at the individual  DIB level, programme level and sector level. Further detail is set out below: This level of research relates to the four DIBs under the scope of the evaluation.\n",
            "Top  20  :   Table 6.1: Project focus and measurement approach Projects focused on  Innovation  Building evidence  Replication, drawing on an established evidence  base  Scaling,  using  established,  highly  evidence- based interventions Measurement  Non-experimental  Quasi-experimental or experimental  Against a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for  participation in a DIB. It then sets out five lessons learned around the process of identifying  stakeholders and managing relationships, before concluding with lessons learned around how  the  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different  benefits.\n",
            "\n",
            "\n",
            "\n",
            "Query:  How was data collected and analysed?\n",
            "Top  1  :   2.2.2 Analysis The data collection generated a variety of qualitative and quantitative evidence, which enabled  the triangulation  of  different  data  sources set  out  above. The  data from  the  transcripts  and  field notes were summarised and synthesised under the headings and sub headings within  the  Evaluation  Framework.  Findings  from  different  data  sources  were  triangulated.  Where  findings between the data sets contradicted each other, each data set was further interrogated  to examine possible explanations. Analysis took place at three levels, focusing firstly on the  individual DIBs; bringing this together to analyse progress at a programme level; and finally  considering the implications for the wider DIB sector. We also held debriefings with all team  members, including the external experts, to support in this analysis stage.\n",
            "Top  2  :   Data  was  collected  in  an  appropriate  and  respectful  manner,  taking  into  account  cultural,  ethical and legal concerns. Given the focus of research wave 1 on the set up and delivery of  the DIB, interviews were undertaken with outcome funders, intermediaries/advisors, service  providers, investors and outcome verifiers only. No interviews were undertaken directly with  beneficiaries. Interviews were undertaken with individuals from different backgrounds, and the  evaluation  team  liaised  with  ‘gatekeepers’  in  terms  of  the  best  way  to  undertake  these  interviews. For example, in the case of the QEI DIB, the evaluators worked closely with BAT,  Dalberg and MSDF in order to understand how best to conduct interviews with the India based  service  providers.  Data  was  collected  with  due  consideration  to  ethical  concerns.  The  evaluation took a participatory, collaborative process, working closely with DIB stakeholders  in order to tailor the evaluation process. For each DIB, the evaluation worked closely with the  DIB stakeholders in order to finalise the particular research approach and information required.  The evaluation team considered how best to communicate research findings to participants, A136 in order to actively engage participants with findings and implications. This is discussed further  in the next section. Data was collected in compliance with GDPR.\n",
            "Top  3  :   Procedures  to  be  followed  to  collect  and  analyse  are  documented  in  M&E  protocol.\n",
            "Top  4  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  5  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  6  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  7  :   Are data collection and analysis methods  documented in writing and being used to  ensure the same procedures are followed  each time?\n",
            "Top  8  :   Reliability  –  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis  methods over time.\n",
            "Top  9  :   Reliability  –  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis  methods over time.\n",
            "Top  10  :   Reliability – Data should reflect stable and consistent data collection processes and analysis  methods over time.\n",
            "Top  11  :   Yes Yes/No TBD Comments Data  is  collected  by  service  providers  against  the  performance  management  framework  provided  by  Dalberg,  which  complements with other data collection  methods  and  independently  assesses  data.\n",
            "Top  12  :   E.5 Approach to data collection The process of data collection took place between August and December 2018.\n",
            "Top  13  :   N/A Has the margin of error been reported along  with  the  data?  (Only  applicable  to  results  obtained through statistical samples) Is the data collection method/tool being used  to collect the data fine-tuned or exact enough  to  register  the  expected  change?  (e.g.  a  yardstick may not be a precise enough tool  to measure a change of a few mm).\n",
            "Top  14  :   Has the margin of error been reported along  with  the  data?  (Only  applicable  to  results  obtained through statistical samples) Is the data collection method/tool being used  to collect the data fine-tuned or exact enough  to  register  the  expected  change?  (e.g.  a  yardstick may  not be a precise enough tool  to measure a change of a few mm).\n",
            "Top  15  :   Has the margin of error been reported along  with the data? (Only applicable to results  obtained through statistical samples) Is the data collection method/tool being used  to collect the data fine-tuned or exact  enough to register the expected change?  (e.g. a yardstick may not be a precise  enough tool to measure a change of a few  mm).\n",
            "Top  16  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  17  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  18  :   Yes Are  sound  research  methods  being  used  to  collect the data?\n",
            "Top  19  :   A150 Question Are sound research methods being used to  collect the data?\n",
            "Top  20  :   Yes Is there reasonable assurance that the data  collection  methods  being  used  do  not  produce  systematically  biased  data  (e.g.  consistently over- or under-counting)?\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design method methodology data collection research design\n",
            "Top  1  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  2  :   •  Desk review of work that is happening in the field  that we can learn from (including  existing research and evaluation of development and social impact bonds) so as to  draw on learning outside of the DFID DIBs Pilot programme •  Design of data collection instruments (which should be reviewed by DFID)  •  Data collection. Proposal should specify how qualitative and quantitative methods  (if  proposed)  are  going  to  be  used  together  in  a  complimenting  fashion.  The  methods and scope of data collection should be supported with clear arguments for  need. Mechanisms for ensuring quality of data should be included in the proposal.  •  Analysis  and  reporting.  Details  should  be  provided  on  how  the  analysis  will  be conducted, especially if mostly qualitative methods are used.\n",
            "Top  3  :   The methodology enables the collection and analysis of disaggregated data  to show difference between groups.  Any methodological limitations are acknowledged and their impact on the  evaluation discussed. The limitations are acceptable and/or they are  adequately addressed.  Any departures from the TOR, inception phase and / or original evaluation  design are adequately explained.  The product discusses any inherent imbalances or biases that interviews  and other data collection may have created.\n",
            "Top  4  :   providers' own Research  Wave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection  methods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of  stakeholders, before concluding with the main methodological limitations, and the mitigations  undertaken. Further detail is set out in Annex E.\n",
            "Top  5  :   Methods: Qualitative   Sources: As above + access to  programme design documents;  and project level process  review/ evaluation activities  focused on design and  implementation of DIB projects  – including service provider  selection, outcome funder  engagement, metric selection.\n",
            "Top  6  :   E.5 Approach to data collection The process of data collection took place between August and December 2018.\n",
            "Top  7  :   26 The evaluation firm will conduct two instances of data collection, through which end line data will be collected from a sample  of households from each cohort. Baseline data collected by Village Enterprise may be used for creation of covariates to be  used during the analysis. Accordingly, each group of cohorts will have its own impact estimation based on which the trustee will  pay Village Enterprise. The RCT design is an improved version of the RCT performed between 2014 and 2017 to evaluate  Village Enterprise’s intervention in Uganda. The randomization will be made at the village level. The evaluator will randomly  assign the villages to receive the Village Enterprise program.\n",
            "Top  8  :   Yes Are  sound  research  methods  being  used  to  collect the data?\n",
            "Top  9  :   2.2.2 Analysis The data collection generated a variety of qualitative and quantitative evidence, which enabled  the triangulation  of  different  data  sources set  out  above. The  data from  the  transcripts  and  field notes were summarised and synthesised under the headings and sub headings within  the  Evaluation  Framework.  Findings  from  different  data  sources  were  triangulated.  Where  findings between the data sets contradicted each other, each data set was further interrogated  to examine possible explanations. Analysis took place at three levels, focusing firstly on the  individual DIBs; bringing this together to analyse progress at a programme level; and finally  considering the implications for the wider DIB sector. We also held debriefings with all team  members, including the external experts, to support in this analysis stage.\n",
            "Top  10  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  11  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  12  :   A150 Question Are sound research methods being used to  collect the data?\n",
            "Top  13  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  14  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  15  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  16  :   Are data collection and analysis methods  documented in writing and being used to  ensure the same procedures are followed  each time?\n",
            "Top  17  :   Data  was  collected  in  an  appropriate  and  respectful  manner,  taking  into  account  cultural,  ethical and legal concerns. Given the focus of research wave 1 on the set up and delivery of  the DIB, interviews were undertaken with outcome funders, intermediaries/advisors, service  providers, investors and outcome verifiers only. No interviews were undertaken directly with  beneficiaries. Interviews were undertaken with individuals from different backgrounds, and the  evaluation  team  liaised  with  ‘gatekeepers’  in  terms  of  the  best  way  to  undertake  these  interviews. For example, in the case of the QEI DIB, the evaluators worked closely with BAT,  Dalberg and MSDF in order to understand how best to conduct interviews with the India based  service  providers.  Data  was  collected  with  due  consideration  to  ethical  concerns.  The  evaluation took a participatory, collaborative process, working closely with DIB stakeholders  in order to tailor the evaluation process. For each DIB, the evaluation worked closely with the  DIB stakeholders in order to finalise the particular research approach and information required.  The evaluation team considered how best to communicate research findings to participants, A136 in order to actively engage participants with findings and implications. This is discussed further  in the next section. Data was collected in compliance with GDPR.\n",
            "Top  18  :   2.2.1 Data collection DIB level research: There were three levels of research activity in this first research wave (RW1), at the individual  DIB level, programme level and sector level. Further detail is set out below: This level of research relates to the four DIBs under the scope of the evaluation.\n",
            "Top  19  :   The  evaluation  process  has  been  set  out  transparently.  The  inception  report  has  been  published  and  clearly  sets  out  the  scope  and  proposed  approach  to  the  evaluation.  Each  interview began with a clear explanation of the research process, aims, and objectives. This  included  an  explanation  of  how  collected  data  would  have  been  used,  and  in  what  form.  Interviewees were then provided with an opportunity to ask questions.\n",
            "Top  20  :   For  the  qualitative  analysis,  this  has  been  organised  into  two  distinct  phases  -  data  management and data interpretation39. The evaluation team drew upon the topic guides and  early stages of fieldwork to develop a framework of themes and sub-themes organised around  the key research questions. This has been reviewed as the fieldwork progressed. The data  from the transcripts and field notes will be summarised and synthesised under the headings  and sub headings within the Evaluation Framework.\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design\n",
            "Top  1  :   4.6  The design provides for multiple lines of inquiry and/or triangulation of data.\n",
            "Top  2  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  3  :   Relevant project level learning activities: A range of learning activities are planned for each  DIB, focused on the DIB design process and the effects of using the DIB model. The supplier  will therefore be required to work with learning providers to take advantage of any synergies  (see Ways of Working and Annex C).\n",
            "Top  4  :   A107 Ernst, L. (2017). Using Human-Centred Design at DFID. Development Impact and You (DIY).  http://diytoolkit.org/using-human-centred-design-at-dfid/ Evans, A. 2016. Results based financing in Zambia – an informal, unpublished annex, mimeo,  available at: https://www.researchgate.net/publication/308985858 Fleming, F. 2013. Evaluation Methods for Assessing Value for Money. Better Evaluation.\n",
            "Top  5  :   •  Desk review of work that is happening in the field  that we can learn from (including  existing research and evaluation of development and social impact bonds) so as to  draw on learning outside of the DFID DIBs Pilot programme •  Design of data collection instruments (which should be reviewed by DFID)  •  Data collection. Proposal should specify how qualitative and quantitative methods  (if  proposed)  are  going  to  be  used  together  in  a  complimenting  fashion.  The  methods and scope of data collection should be supported with clear arguments for  need. Mechanisms for ensuring quality of data should be included in the proposal.  •  Analysis  and  reporting.  Details  should  be  provided  on  how  the  analysis  will  be conducted, especially if mostly qualitative methods are used.\n",
            "Top  6  :   5.0  Analysis and Findings – Costs of designing and delivering DIBs Summary Economy The  emphasis  in this  research  wave has  been on  establishing the  additional  costs  of the  DIB.  Future research waves will explore the link between additional costs and outcomes,  and  whether  the  DIB  led  to  efficiency  savings.  The  analysis  is  framed  around  the  4Es  of  Value for Money, exploring economy, efficiency, effectiveness and equity.\n",
            "Top  7  :   •  The evaluation design and implementation must meet standard ethical practices.\n",
            "Top  8  :   •  The report should also include an updated financial plan for  the  evaluation  –  including  highlighting  any  savings  that  are  possible  following  detailed  design  phase  and  engagement  with project level learning providers.\n",
            "Top  9  :   The final sub-section then summarises the findings against the first sub-questions under EQ2,  in terms of conditions required for a DIB to be a suitable tool for stakeholders. Findings against  the second sub-question under EQ2, in terms of lessons learned around how the DIB design  process can be improved, are summarised in section 7.\n",
            "Top  10  :   In terms of the lessons learned: •  Discussion focused on the importance of clearly defining roles and responsibilities at the  outset, and take advantage of the process of co-design that characterises the DIB, in terms  of shared experience and shared learning.\n",
            "Top  11  :   4.  DIBs  should  be  open  by  design,  and  donors  and foundations  to  lead on establishing  a research data protocol.\n",
            "Top  12  :   •  The  design  report  should  also  include  the  instruments  that  the evaluator will use in upcoming evaluation activities e.g. to  produce first evaluation report.\n",
            "Top  13  :   Section 4 presents the analysis and findings of the evaluation in relation to EQ1, assessing how  the DIB model affects the design and set up phase of development interventions.\n",
            "Top  14  :   7.0  Lessons Below we set out the lessons of potential wider relevance for the design and set up phase of  DIBs. These are split out against the DIB effects and different stages of designing and setting  up DIBs. As discussed in section 3,   there is not yet a predominant design for DIBs, and it is  perhaps  more  helpful  to  understand  DIBs  as  a  funding  class  within  which  there  is  great  variation. The precise structure and nature of the DIBs depend on the stakeholders involved,  their objectives for using the DIB and the organisational and regulatory requirements in place.  These have implications for the DIB effects and for the process of design and set up phase.  This diversity must be borne in mind when taking stock of the lessons learned to date.\n",
            "Top  15  :   Design effects In this section we examine how the DIB affected the design of the projects and interventions.  This includes the extent to which the DIB mechanism fostered innovation, how it impacted on  the rigour of the project design, and how it impacted on the costs involved in designing and  setting up the projects.\n",
            "Top  16  :   • In  terms  of  the  design  effects,  the  discussion  raised  the  importance  of  distinguishing  between solution and process innovation, as well as innovation in the design of the DIB  itself.  It  was  noted  that  DIBs,  by  design,  may  not  be  the  most  appropriate  funding  mechanism  for  solution  innovation.  One  DIB  stakeholder  highlighted  that  a  substantial  innovation is the involvement of the private sector in the humanitarian sector, thanks to  the  DIB  mechanism.  The  importance  of  distinguishing  between  different  types  of  innovation is reflected in the updated DIB effect table set out in Annex E.1.\n",
            "Top  17  :   For  the  qualitative  analysis,  this  has  been  organised  into  two  distinct  phases  -  data  management and data interpretation39. The evaluation team drew upon the topic guides and  early stages of fieldwork to develop a framework of themes and sub-themes organised around  the key research questions. This has been reviewed as the fieldwork progressed. The data  from the transcripts and field notes will be summarised and synthesised under the headings  and sub headings within the Evaluation Framework.\n",
            "Top  18  :   The two evaluation questions are: •  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.\n",
            "Top  19  :   The two evaluation questions are: •  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.\n",
            "Top  20  :   We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We  welcome efforts by the evaluator to find savings during the life of the evaluation.\n",
            "\n",
            "\n",
            "\n",
            "Query:  method\n",
            "Top  1  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  2  :   providers' own Research  Wave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection  methods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of  stakeholders, before concluding with the main methodological limitations, and the mitigations  undertaken. Further detail is set out in Annex E.\n",
            "Top  3  :   Methods: Qualitative  Sources: As above + access to  past performance data for at  least 2 of the 3 DIB projects  (ICRC & VE) – including past  cost & effect data for same A87 providers, delivering similar  interventions in similar contexts.\n",
            "Top  4  :   Methods: Qualitative   Sources: As above + access to  programme design documents;  and project level process  review/ evaluation activities  focused on design and  implementation of DIB projects  – including service provider  selection, outcome funder  engagement, metric selection.\n",
            "Top  5  :   2.0  Evaluation Framework and Methodology This section sets out the evaluation framework that has been used to guide the evaluation (section 2.1), summarises the main features of the  methodology (section 2.2) and the limitations of the available evidence (section 2.3). Further details on the methodology undertaken are set  out in Annex E.\n",
            "Top  6  :   Are data collection and analysis methods  documented in writing and being used to  ensure the same procedures are followed  each time?\n",
            "Top  7  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  8  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  9  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  10  :   Yes When  the  same  data  collection  method  is  used  to  measure/observe  the  same  thing  multiple  times,  is  the  same  result  produced  each time?\n",
            "Top  11  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  12  :   Yes Yes When  the  same  data  collection  method  is  used  to  measure/observe  the  same  thing  multiple  times,  is  the  same  result  produced  each time?\n",
            "Top  13  :   Methodological limitations The table below sets out the key methodological limitations and the mitigations undertaken.\n",
            "Top  14  :   A152 Question When the same data collection method is  used to measure/observe the same thing  multiple times, is the same result produced  each time?\n",
            "Top  15  :   Yes Are  sound  research  methods  being  used  to  collect the data?\n",
            "Top  16  :   Annex E.12  Annex E.10 and E.12 Annex E.10 A115 Annex E: Evaluation methodology This Annex sets out the full evaluation methodology. The annex focuses on Research Wave  1, but where appropriate, reference is also made to the following research waves.\n",
            "Top  17  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  18  :   A150 Question Are sound research methods being used to  collect the data?\n",
            "Top  19  :   E.5 Approach to data collection The process of data collection took place between August and December 2018.\n",
            "Top  20  :   The sampling strategy is described, and is appropriate. Primary and  secondary data sources are appropriate, adequate and reliable. Sample  sizes are adequate.\n",
            "\n",
            "\n",
            "\n",
            "Query:  methodology\n",
            "Top  1  :   The methodology enables the collection and analysis of disaggregated data  to show difference between groups.  Any methodological limitations are acknowledged and their impact on the  evaluation discussed. The limitations are acceptable and/or they are  adequately addressed.  Any departures from the TOR, inception phase and / or original evaluation  design are adequately explained.  The product discusses any inherent imbalances or biases that interviews  and other data collection may have created.\n",
            "Top  2  :   providers' own Research  Wave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection  methods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of  stakeholders, before concluding with the main methodological limitations, and the mitigations  undertaken. Further detail is set out in Annex E.\n",
            "Top  3  :   2.0  Evaluation Framework and Methodology This section sets out the evaluation framework that has been used to guide the evaluation (section 2.1), summarises the main features of the  methodology (section 2.2) and the limitations of the available evidence (section 2.3). Further details on the methodology undertaken are set  out in Annex E.\n",
            "Top  4  :   Annex E.12  Annex E.10 and E.12 Annex E.10 A115 Annex E: Evaluation methodology This Annex sets out the full evaluation methodology. The annex focuses on Research Wave  1, but where appropriate, reference is also made to the following research waves.\n",
            "Top  5  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  6  :   Methodology and evidence base The evaluation is based on an evaluation framework that builds on a range of hypothesised  DIB effects and indicators. As part of the inception phase, the evaluation team drew on the  literature  in  order  to  understand  hypotheses  around  how  the  DIB  model  might  affect  interventions, and developed a list of DIB effects and indicators.\n",
            "Top  7  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  8  :   Report structure The remainder of this report is structured as follows: Section  2  sets  out the evaluation  framework that  has  been  used  to guide  the  evaluation,  and  summarises the main features of the methodology and the limitations of the available evidence.\n",
            "Top  9  :   Table 6.1: Project focus and measurement approach Projects focused on  Innovation  Building evidence  Replication, drawing on an established evidence  base  Scaling,  using  established,  highly  evidence- based interventions Measurement  Non-experimental  Quasi-experimental or experimental  Against a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for  participation in a DIB. It then sets out five lessons learned around the process of identifying  stakeholders and managing relationships, before concluding with lessons learned around how  the  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different  benefits.\n",
            "Top  10  :   Methods: Qualitative   Sources: As above + access to  programme design documents;  and project level process  review/ evaluation activities  focused on design and  implementation of DIB projects  – including service provider  selection, outcome funder  engagement, metric selection.\n",
            "Top  11  :   Methods: Qualitative  Sources: As above + access to  past performance data for at  least 2 of the 3 DIB projects  (ICRC & VE) – including past  cost & effect data for same A87 providers, delivering similar  interventions in similar contexts.\n",
            "Top  12  :   Are data collection and analysis methods  documented in writing and being used to  ensure the same procedures are followed  each time?\n",
            "Top  13  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  14  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  15  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  16  :   E.6.1.  Analysis The evaluation generated a variety of qualitative and quantitative evidence, which provided  multiple  lines  of  enquiry  and  enabled  the  triangulation  of  different  data  sources,  including  literature and document review, consultations with DIB and DFID stakeholders, cost analysis  and  research  on  comparator  sites.  To  ensure  detailed  and  consistent  analysis  a  clearly  structured  approach  to  the  analysis  is  essential.  The  recommended  analytical  stages  and  tasks are as follows.\n",
            "Top  17  :   2.2.2 Analysis The data collection generated a variety of qualitative and quantitative evidence, which enabled  the triangulation  of  different  data  sources set  out  above. The  data from  the  transcripts  and  field notes were summarised and synthesised under the headings and sub headings within  the  Evaluation  Framework.  Findings  from  different  data  sources  were  triangulated.  Where  findings between the data sets contradicted each other, each data set was further interrogated  to examine possible explanations. Analysis took place at three levels, focusing firstly on the  individual DIBs; bringing this together to analyse progress at a programme level; and finally  considering the implications for the wider DIB sector. We also held debriefings with all team  members, including the external experts, to support in this analysis stage.\n",
            "Top  18  :   Yes Are  sound  research  methods  being  used  to  collect the data?\n",
            "Top  19  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "Top  20  :   Yes Yes Yes Are sound research methods being used to  collect the data?\n",
            "\n",
            "\n",
            "\n",
            "Query:  data collection\n",
            "Top  1  :   E.5 Approach to data collection The process of data collection took place between August and December 2018.\n",
            "Top  2  :   Reliability  –  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis  methods over time.\n",
            "Top  3  :   Reliability  –  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis  methods over time.\n",
            "Top  4  :   Reliability – Data should reflect stable and consistent data collection processes and analysis  methods over time.\n",
            "Top  5  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  6  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  7  :   Are  data  collection  and  analysis  methods  documented  in  writing  and  being  used  to  ensure  the  same  procedures  are  followed  each time?\n",
            "Top  8  :   Are data collection and analysis methods  documented in writing and being used to  ensure the same procedures are followed  each time?\n",
            "Top  9  :   Is there reasonable assurance that the data  collection  methods  being  used  do  not  produce  systematically  biased  data  (e.g.  consistently over- or under-counting)?\n",
            "Top  10  :   Is  there  reasonable  assurance  that  the  data  collection methods being used do not produce  systematically  biased  data  (e.g.  consistently  over- or under-counting)?\n",
            "Top  11  :   Is there reasonable assurance that the data  collection methods being used do not  produce systematically biased data (e.g.  consistently over- or under-counting)?\n",
            "Top  12  :   Yes Yes Reliability  –  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis  methods over time.\n",
            "Top  13  :   Yes Is there reasonable assurance that the data  collection  methods  being  used  do  not  produce  systematically  biased  data  (e.g.  consistently over- or under-counting)?\n",
            "Top  14  :   Yes Yes/No TBD Comments Data  is  collected  by  service  providers  against  the  performance  management  framework  provided  by  Dalberg,  which  complements with other data collection  methods  and  independently  assesses  data.\n",
            "Top  15  :   Yes When  the  same  data  collection  method  is  used  to  measure/observe  the  same  thing  multiple  times,  is  the  same  result  produced  each time?\n",
            "Top  16  :   Yes Yes When  the  same  data  collection  method  is  used  to  measure/observe  the  same  thing  multiple  times,  is  the  same  result  produced  each time?\n",
            "Top  17  :   Yes Is there independence in key data collection,  management and assessment procedures?\n",
            "Top  18  :   Yes A155 Question Yes/No Comments Is there independence in key data collection,  management and assessment procedures?\n",
            "Top  19  :   TBD When  the  same  data  collection  method  is  used  to  measure/observe  the  same  thing  multiple  times,  is  the  same  result  produced  each time?\n",
            "Top  20  :   G.1 ICRC Question Yes/No Comments Validity – Data should clearly and adequate represent the intended results Indicator and collection methods  are clear and sensible.\n",
            "\n",
            "\n",
            "\n",
            "Query:  research design\n",
            "Top  1  :   4.  DIBs  should  be  open  by  design,  and  donors  and foundations  to  lead on establishing  a research data protocol.\n",
            "Top  2  :   A107 Ernst, L. (2017). Using Human-Centred Design at DFID. Development Impact and You (DIY).  http://diytoolkit.org/using-human-centred-design-at-dfid/ Evans, A. 2016. Results based financing in Zambia – an informal, unpublished annex, mimeo,  available at: https://www.researchgate.net/publication/308985858 Fleming, F. 2013. Evaluation Methods for Assessing Value for Money. Better Evaluation.\n",
            "Top  3  :   5.0  Analysis and Findings – Costs of designing and delivering DIBs Summary Economy The  emphasis  in this  research  wave has  been on  establishing the  additional  costs  of the  DIB.  Future research waves will explore the link between additional costs and outcomes,  and  whether  the  DIB  led  to  efficiency  savings.  The  analysis  is  framed  around  the  4Es  of  Value for Money, exploring economy, efficiency, effectiveness and equity.\n",
            "Top  4  :   •  Desk review of work that is happening in the field  that we can learn from (including  existing research and evaluation of development and social impact bonds) so as to  draw on learning outside of the DFID DIBs Pilot programme •  Design of data collection instruments (which should be reviewed by DFID)  •  Data collection. Proposal should specify how qualitative and quantitative methods  (if  proposed)  are  going  to  be  used  together  in  a  complimenting  fashion.  The  methods and scope of data collection should be supported with clear arguments for  need. Mechanisms for ensuring quality of data should be included in the proposal.  •  Analysis  and  reporting.  Details  should  be  provided  on  how  the  analysis  will  be conducted, especially if mostly qualitative methods are used.\n",
            "Top  5  :   4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation  questions, data sources and methods for data collection.\n",
            "Top  6  :   For  the  qualitative  analysis,  this  has  been  organised  into  two  distinct  phases  -  data  management and data interpretation39. The evaluation team drew upon the topic guides and  early stages of fieldwork to develop a framework of themes and sub-themes organised around  the key research questions. This has been reviewed as the fieldwork progressed. The data  from the transcripts and field notes will be summarised and synthesised under the headings  and sub headings within the Evaluation Framework.\n",
            "Top  7  :   Delivery of Research Wave 1 has drawn on the preparatory work undertaken during the inception  phase, which included: •  a literature review on the context and progress of the wider SIBs and DIBs sector, and an initial comparison of these mechanisms with alternative funding tools; •  a  review  of  programme  documentation,  at  the  individual  DIB  level  and  DFID  pilot • programme level;  the  refining  of  the  conceptual  framework  and  evaluation  questions  against  the  OECD-DAC  criteria,  development  of  DIB  effect  indicators  and  preparation  of  research  tools; •  preparatory  consultations  with  key  stakeholders  across  the  DIBs  and  scoping  of potential comparison sites; and •  a preliminary stakeholder mapping.\n",
            "Top  8  :   providers' own Research  Wave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection  methods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of  stakeholders, before concluding with the main methodological limitations, and the mitigations  undertaken. Further detail is set out in Annex E.\n",
            "Top  9  :   Level of returns and  profit made by the x x x x x x x x x A120 Research Wave DIBs level research Programme  level research Wider impact  bond sector Methods w e i v e r   t n e m u c o D s n o i t a t l u s n o c B D x I s e t i s r o t a r a p m o C s i l s y a n a t s o C i s s y a n a l a t a D s n o i t a t l u s n o c D F D x I i w e v e r   t n e m u c o d x e m m a r g o r P w e i v e r e r u t a r e t i L x s n o i t a t l u s n o c l r e d o h e k a t S x s p o h s k r o w i g n n r a e L x 1 W R 2 W R 3 W R x x x x x x x x x x x x x Key evaluation  questions Relevance, efficiency,  effectiveness (and additionality  cross cutting) Indicators What  social  issues,  target  groups,  geographies  and  project  scales  do  DIBs fit best and have the greatest  of impact?\n",
            "Top  10  :   The  evaluation  process  has  been  set  out  transparently.  The  inception  report  has  been  published  and  clearly  sets  out  the  scope  and  proposed  approach  to  the  evaluation.  Each  interview began with a clear explanation of the research process, aims, and objectives. This  included  an  explanation  of  how  collected  data  would  have  been  used,  and  in  what  form.  Interviewees were then provided with an opportunity to ask questions.\n",
            "Top  11  :   The  table  below  sets  out  the  stakeholders  consulted  in  research  wave  1,  and  the  areas  discussed. The precise areas discussed were tailored depending on the role of the interviewee  within the DIB and the point of progress of each DIB, and sent in advance to DIB stakeholders.\n",
            "Top  12  :   E.3 Programme-level Research The purpose of this level of research is to compare the findings on the individual DIBs, in order  to  understand  further  how  the  DIB  effect  differs  (or  remains)  across  different  contexts. We  contextualised these findings within the wider DIB sector, and considered the implications of  the findings for both improving the DIB mechanism and how DFID could utilise the model in  the future. To achieve this, we undertook the following tasks: •  DFID consultations •  Programme document review •  Learning workshops E.3.1.  DFID consultations The purpose of the consultations with DFID was to further understand the programme aims;  DFID’s perspective on the progress and success of the programme and its implications for the  wider  DIB  landscape;  and  changes  to  relevant  DFID  strategies,  such  as  the  DIB  or  PbR  Strategies. This information helped ensure the reports and recommendations are relevant and  situated within wider developments within DFID. We had consulted with the PbR team during  the  inception  phase,  and  consultations  with  DFID  in  this  research  wave  focused  on  the  selection and structuring of the 3 DIBs under the pilot programme.\n",
            "Top  13  :   Sector level research: This level of research seeks to provide the wider contextualisation to our findings.\n",
            "Top  14  :   Table 6.1: Project focus and measurement approach Projects focused on  Innovation  Building evidence  Replication, drawing on an established evidence  base  Scaling,  using  established,  highly  evidence- based interventions Measurement  Non-experimental  Quasi-experimental or experimental  Against a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for  participation in a DIB. It then sets out five lessons learned around the process of identifying  stakeholders and managing relationships, before concluding with lessons learned around how  the  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different  benefits.\n",
            "Top  15  :   The  focus  for  this  first  research  wave  has  been  on  the  costs  incurred  in  a  DIB,  and  in  comparison to grant based financing, and payment by results. The focus of the next research waves will be to identify actual costs incurred during implementation, and to review the benefits  arising  from  use  of  a  DIB  (in  comparison  to  grant  based  financing  and payment  by  results  contracts).\n",
            "Top  16  :   2.2.1 Data collection DIB level research: There were three levels of research activity in this first research wave (RW1), at the individual  DIB level, programme level and sector level. Further detail is set out below: This level of research relates to the four DIBs under the scope of the evaluation.\n",
            "Top  17  :   •  Research in comparator sites: In order to develop an understanding of how the DIB  affected  the  set  up  phase,  the  evaluation  team  also  undertook  data  collection  at  comparator sites. We identified two forms of comparisons: o  First,  we  identified  similar  programmes  being  delivered  by  the  same  service  providers funded by the DIBs, but which were funded under grants. As part of  the inception phase, a list of parameters which would affect the comparability  of programmes was developed based on discussion within the evaluation team  and DFID. These were: project purpose and objectives, service provider and  processes used,  countries  of  operation,  context,  time period,  size of  project,  level  of  donor  oversight/influence,  payment  structure  and  availability  of  data  and stakeholders. The evaluation team then worked with the service providers  and  intermediaries,  in  order  to  identify  potential  comparator  sites,  and  assessed the similarity to our impact bonds along these parameters. We then  interviewed staff working on this comparator sites, to determine the extent to  which  the  DIB  effect  was  also  present  in  these  sites,  to  support  our  understanding of other factors which may have also contributed to these DIB  effect indicators.\n",
            "Top  18  :   •  The report should also include an updated financial plan for  the  evaluation  –  including  highlighting  any  savings  that  are  possible  following  detailed  design  phase  and  engagement  with project level learning providers.\n",
            "Top  19  :   Research  Wave x x x x Key  evaluation  questions performance  and  effectiveness  of  development  interventions.\n",
            "Top  20  :   We  have  already  reviewed  key  documents  as  part  of  the  inception  phase,  and  will  review  further key documents during Research Waves 2 and 3.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the target population?\n",
            "Top  1  :   More incentivised to focus on target populations:   Evidence from the Employment Fund in  Nepal  (Chakravarty  et  al,  2016)  suggested that  specific targets for the  hard  to reach, such  as  greater payments for disadvantaged groups discouraged cherry picking and more focus on the  hard to reach populations.\n",
            "Top  2  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  3  :   5.4.1 Targeting strategy The approach to equity will be guided by the individual programmes’ targeting strategies, to  understand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the  effectiveness of the targeting strategy of the DIB, especially in terms of the hard to reach. The  evaluation  will  look  at  how  well  the  programmes  are  fulfilling  their  targeting  strategy  and  whether there are certain sub-groups which are not being reached. At this stage, the following  strategies are understood to be in place: • ICRC – The HIB targets disabled people in a geographic area.\n",
            "Top  4  :   •  VE – The intervention identifies individuals who live in extreme poverty and are unable  to  provide  for  their  family’s  basic  needs.  VE  assesses  poverty  levels  through  a  community-based Poverty Wealth Ranking exercise coupled with the Progress-out-of- Poverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment  Agreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the  financial model.\n",
            "Top  5  :   VE:  The  targeting  strategy  addresses  equity  concerns,  and  at  the  moment,  there  are  no  particular  risks  identified  with  the  outcome  target  or  verification  process  potentially  driving  perverse incentives. This will be monitored over the next two research waves.\n",
            "Top  6  :   Section 1.4, Annex  E.1, E.2, E.3 and E.4,  and Annex F  Section 1.2 3.3  The product describes the target audience(s) for the evaluation.\n",
            "Top  7  :   Ibidem; Pereira, J; Villota, C. 2013. Hitting the target? Evaluating the effectiveness of results- based approaches to aid. Brussels: EURODAD.\n",
            "Top  8  :   A  minimum  of  12,660  households in Kenya and  Uganda  Local  representatives  and Uganda  DFID, USAID DIV and an  anonymous donor government  in  Kenya Michael  &  Susan  Dell  Foundation,  BT, Comic Relief, Mittal Foundation.\n",
            "Top  9  :   The target audience groups for the communication activities are as follows: Phase Primary  users:  DFID  stakeholders users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs Case studies  Reports  Internal workshops  External Workshops  Annual Briefing  Learnings outputs           E.7 Involvement of stakeholders                  The evaluation has been designed and managed to meet the information and decision-making  needs of the intended users. Discussions were carried out with DFID and stakeholders of the  pilot DIBs in order to inform the approach and needs of stakeholders, as part of the inception  phase.  DFID  is  also  coordinating  the  evaluation  stakeholder  group  for  this  purpose.  Additionally, during this first research wave, the evaluation team has held bi-weekly catch up  calls  with  DFID,  to  inform  DFID  of  emerging  issues  and  to  ensure  DFID  input  in  the  implementation of the evaluation. The scope of the evaluation and individual DIB level plans,  in  terms  of  data  to  be  shared  and  consultations  to  be  undertaken  over  the  course  of  the A140 evaluation, have been discussed and agreed with the DIB level stakeholders. The individual  DIB level plans are set out in Annex E.\n",
            "Top  10  :   x x x x 6  Difference in: x x x •  Quality of outcomes  •  Sustainability of outcomes  •  Organisation  performance  (spillovers) approach to  management •  Positive  and  negative  unintended effects to (with reference 7  %  of  participants  in  the  different  sub- targeting groups  strategy).   Targeting  costs  if  relevant  (with  the  assumption that targeting costs increase  when trying to access the hard to reach)  8  Change in targeting approach based on  the identified effects of the impact bond.\n",
            "Top  11  :   o  What social issues, target groups, geographies and project scales do DIBs fit best and have the greatest impact?\n",
            "Top  12  :   to individuals in the bottom  two wealth quintiles of the  population  in  Cameroon  by the end of year 5.\n",
            "Top  13  :   estimating in Stratification by school size, urban/rural  location and school type ensures that all  parts  of  the  population  are  included  in  the sample.\n",
            "Top  14  :   A  more  socially  cohesive  and  to  a  stable  society  actively  larger  workforce  contributing  to  the  country’s  prosperity.\n",
            "Top  15  :   ambitious.   Implementers have track record of achieving  outcomes 3.  Project and performance management in place  4.  External risk factors not especially high  Med-High risk (DFID Risk score 5) 1.  Strong  evidence  base  for  the  intervention  with  extensive  historical  data  from  a  previous  RCT  conducted. Ambitious targets set.\n",
            "Top  16  :   Table 2.5: Deliverables mapped to target audiences Deliverables Primary  users:  DFID  stakeholders Case studies  Reports  Internal workshops  External Workshops  Learnings outputs         users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs              This report forms evaluation report 1, which includes early feedback on the set-up of the DIBs  (including an estimate of set-up costs) and recommendations for expanding and improving the  DIB  programme  and  these  DIB  mechanisms.  This  is  also  complemented  by  specific  case  studies focusing on each of the three DIBs (see Annex A). An internal workshop was held  to discuss emerging findings (see Annex K).\n",
            "Top  17  :   A100 Annex 1 – Initial Country Risk Assessment by DFID The programme under evaluation involves activities in multiple countries. DFID has provided  an overall initial risk assessment for the programme locations as shown below: A101 DFID Overall Initial Project/Intervention Summary Risk Assessment Matrix Dec-17Read in conjunction with the FCO Travel Advisory on each countryCountryHIGH RISK LOCATIONSMEDIUM RISK LOCATIONSDate ConductedThemeDFID Risk ScoreDFID Risk ScoreOverall Rating5 - VERY HIGH RISK3 - MEDIUM RISKFCO Travel Advice52Host Nation Travel AdviceN/AN/ATransportation55Security[*]53Civil Unrest53Violence/crime53Terrorism*54War41Hurricane13Earthquake****13Flood*****23Medical Services**53Nature of Project Intervention32Mean (ignoring nature of project)43Mode (ignoring nature of project)5312345Very Low RiskLow RiskMedium RiskHigh RiskVery High RiskMedium*The FCO travel advice for Uganda, Kenya, Nigeria and Mali advises that there is a general threat from terrorism**Medical facilities outside of Capital Cities, and particularly away from cities are limited***FCO advise against all travel to Borno State. There is also a  High Risk (4) threat of kidnapping across Nigeria and Maiduguri in particular**** Earthquake risk is (3) on Indian border with Pakistan and in Delhi***** Flash flooding can occur during the wet season in Nigeria; Eastern Uganda; and monsoon in North India.High RiskFor example: Abuja and Borno State in Nigeria; Mali; Kinshasa in DRC; parts of Kenya, including Nairobi; and the immediacte vicinity of the India-Pakistan border.For example, other project locations incl: Uganda (excluding Karamoja, which is not relevant to this project); Gujarat, Rajasthan, and Delhi in India (with exception of the area in immediate vicinity of the border between India and Pakistan where the Supplier is not required to travel).Dec-17LocationLow SUPPLEMENTARY ANNEXES Annex A1: DFID Theory of Change for DIBs A102 Annex A2: Initial Framework for Assessing Theory of Change for DIBs Initial framework for assessing the Theory of Change behind DIBs, developed during DFID evaluability assessment A103 Were(cid:9)(cid:9)/(cid:9)can(cid:9)deal-breakers(cid:9)/(cid:9)critical(cid:9)success(cid:9)factors(cid:9)identified(cid:9)early?(cid:9)What(cid:9)were(cid:9)they?Costs(cid:9)and(cid:9)cost(cid:9)drivers:(cid:9)(cid:9)What(cid:9)were(cid:9)the(cid:9)duration(cid:9)and(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stages?(cid:9)(cid:9)How(cid:9)were(cid:9)costs(cid:9)divided(cid:9)across(cid:9)the(cid:9)different(cid:9)participants?(cid:9)What(cid:9)factors(cid:9)drove(cid:9)the(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stakeholders?(cid:9)Which(cid:9)costs(cid:9)show(cid:9)potential(cid:9)to(cid:9)decrease(cid:9)in(cid:9)future(cid:9)deals?(cid:9)(cid:9)What(cid:9)steps(cid:9)can(cid:9)be(cid:9)taken(cid:9)to(cid:9)reduce(cid:9)future(cid:9)costs?Comparison(cid:9)with(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)How(cid:9)do(cid:9)costs(cid:9)compare(cid:9)(higher(cid:9)or(cid:9)lower)(cid:9)with(cid:9)alternative(cid:9)funding(cid:9)mechanisms(cid:9)(for(cid:9)both(cid:9)provider(cid:9)and(cid:9)for(cid:9)funder/(cid:9)payors)?(cid:9)For(cid:9)which(cid:9)stages(cid:9)did(cid:9)the(cid:9)costs(cid:9)differ?Cost-effectiveness:How(cid:9)does(cid:9)the(cid:9)effectiveness(cid:9)of(cid:9)the(cid:9)DIB(cid:9)funded(cid:9)projects(cid:9)(ie,(cid:9)impact(cid:9)/(cid:9)cost)(cid:9)compare(cid:9)with(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)by(cid:9)different(cid:9)mechanisms?(cid:9)Additionality(cid:9)of(cid:9)funding:(cid:9)Was(cid:9)the(cid:9)funding(cid:9)for(cid:9)the(cid:9)DIB(cid:9)net(cid:9)new(cid:9)to(cid:9)development?(cid:9)Or(cid:9)does(cid:9)DIB(cid:9)funding(cid:9)shift(cid:9)existing(cid:9)resources(cid:9)to(cid:9)more(cid:9)effective(cid:9)uses?(cid:9)How(cid:9)was(cid:9)this(cid:9)judged?InputsProcessesOutputs(cid:9)/(cid:9)impactCost-effectiveness1.(cid:9)FeasibilityAppropriate(cid:9)projects:(cid:9)What(cid:9)are(cid:9)their(cid:9)attributes(cid:9)(eg,(cid:9)sector,(cid:9)problems(cid:9)/(cid:9)opportunities(cid:9)addressed,(cid:9)innovative(cid:9)or(cid:9)scaling(cid:9)up(cid:9)mature(cid:9)interventions,(cid:9)preventive,(cid:9)measurable(cid:9)baselines(cid:9)etc)?(cid:9)Funders(cid:9)/(cid:9)payors:(cid:9)(cid:9)How(cid:9)many?What(cid:9)are(cid:9)their(cid:9)goals(cid:9)and(cid:9)motivations?(cid:9)Was(cid:9)perceived(cid:9)transfer(cid:9)of(cid:9)risk(cid:9)a(cid:9)motivation?(cid:9)Were(cid:9)they(cid:9)easy(cid:9)/(cid:9)difficult(cid:9)to(cid:9)find(cid:9)/(cid:9)engage?(cid:9)Why?Providers:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)are(cid:9)they(cid:9)resource(cid:9)&(cid:9)capital(cid:9)constrained,(cid:9)are(cid:9)they(cid:9)used(cid:9)to(cid:9)PbR(cid:9)contracts,(cid:9)do(cid:9)they(cid:9)already(cid:9)have(cid:9)an(cid:9)appropriate(cid:9)monitoring(cid:9)system(cid:9)etc.)?Investors:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)commercial(cid:9)or(cid:9)foundations,(cid:9)established(cid:9)or(cid:9)new(cid:9)to(cid:9)development,(cid:9)how(cid:9)many)?Intermediaries:(cid:9)Which(cid:9)intermediaries(cid:9)are(cid:9)involved(cid:9)What(cid:9)roles(cid:9)do(cid:9)they(cid:9)play?(cid:9)Who(cid:9)do(cid:9)they(cid:9)represent?(cid:9)How(cid:9)were(cid:9)they(cid:9)funded?Capacity-building:(cid:9)What,(cid:9)if(cid:9)any,(cid:9)support(cid:9)has(cid:9)been(cid:9)provided(cid:9)to(cid:9)help(cid:9)stakeholders(cid:9)prepare(cid:9)for(cid:9)the(cid:9)DIB?(cid:9)Has(cid:9)it(cid:9)been(cid:9)useful?Context:(cid:9)What(cid:9)contextual(cid:9)factors(cid:9)significantly(cid:9)influenced(cid:9)the(cid:9)development(cid:9)of(cid:9)the(cid:9)DIB?Estimates(cid:9)of(cid:9)impact:Was(cid:9)the(cid:9)intervention(cid:9)successful?(cid:9)Does(cid:9)it(cid:9)seem(cid:9)that(cid:9)the(cid:9)funding(cid:9)instrument(cid:9)played(cid:9)a(cid:9)role(cid:9)in(cid:9)whether(cid:9)or(cid:9)not(cid:9)it(cid:9)was(cid:9)(ie,(cid:9)via(cid:9)the(cid:9)mechanisms(cid:9)in(cid:9)(cid:9)3.(cid:9)Implementation)?Comparability(cid:9)to(cid:9)impact(cid:9)from(cid:9)using(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)Were(cid:9)the(cid:9)results(cid:9)different(cid:9)to(cid:9)past(cid:9)/(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)using(cid:9)other(cid:9)instruments?Unintended(cid:9)outcomes:(cid:9)Were(cid:9)there(cid:9)any(cid:9)unintended(cid:9)outcomes,(cid:9)positive(cid:9)or(cid:9)negative?Engagement(cid:9)with(cid:9)beneficiaries:(cid:9)Did(cid:9)the(cid:9)DIBs(cid:9)create(cid:9)more(cid:9)or(cid:9)less(cid:9)engagement(cid:9)between(cid:9)beneficiaries(cid:9)and(cid:9)service(cid:9)providers?Sustainability:(cid:9)Are(cid:9)there(cid:9)reasons(cid:9)to(cid:9)believe(cid:9)any(cid:9)outcomes(cid:9)/(cid:9)impact(cid:9)achieved(cid:9)will(cid:9)be(cid:9)more(cid:9)or(cid:9)less(cid:9)sustainable(cid:9)than(cid:9)those(cid:9)achieved(cid:9)using(cid:9)other(cid:9)instruments?Repeatability:(cid:9)Would(cid:9)the(cid:9)various(cid:9)stakeholders(cid:9)participate(cid:9)in(cid:9)a(cid:9)similar(cid:9)instrument(cid:9)in(cid:9)the(cid:9)future?(cid:9)Under(cid:9)what(cid:9)conditions?What(cid:9)factor,(cid:9)if(cid:9)any,(cid:9)drove(cid:9)improvement?1)(cid:9)change(cid:9)in(cid:9)incentives(cid:9)(mgmt.(cid:9)and/or(cid:9)front-line)2)(cid:9)increased(cid:9)flexibility(cid:9)/(cid:9)autonomy3)(cid:9)support(cid:9)from(cid:9)active(cid:9)investorDid(cid:9)these(cid:9)or(cid:9)other(cid:9)factors(cid:9)increase(cid:9)focus(cid:9)on(cid:9)outcomes(cid:9)and(cid:9)delivery?Were(cid:9)investors(cid:9)&(cid:9)funders(cid:9)/(cid:9)payorsactive(cid:9)or(cid:9)passive(cid:9)in(cid:9)this(cid:9)stage?(cid:9)If(cid:9)active,(cid:9)did(cid:9)they(cid:9)add(cid:9)value?(cid:9)(cid:9)What(cid:9)were(cid:9)challenges?(cid:9)Were(cid:9)they(cid:9)overcome?(cid:9)If(cid:9)so,(cid:9)how?What(cid:9)factors(cid:9)were(cid:9)important(cid:9)for(cid:9)projects(cid:9)that(cid:9)did(cid:9)/(cid:9)did(cid:9)not(cid:9)proceed?(cid:9)2.(cid:9)Structuring(cid:9)the(cid:9)deal3.(cid:9)Implementation4.(cid:9)Evaluation(cid:9)and(cid:9)paymentsWhat(cid:9)measures(cid:9)&(cid:9)method(cid:9)were(cid:9)used(cid:9)to(cid:9)estimate(cid:9)impact?(cid:9)Were(cid:9)these(cid:9)appropriate(cid:9)(eg,(cid:9)were(cid:9)the(cid:9)measures(cid:9)good(cid:9)predictors(cid:9)of(cid:9)positive(cid:9)effects)?What(cid:9)were(cid:9)the(cid:9)timings(cid:9)of(cid:9)the(cid:9)payments(cid:9)(and(cid:9)investments)?(cid:9)Were(cid:9)outcome(cid:9)payments(cid:9)recycled(cid:9)as(cid:9)operating(cid:9)costs?What(cid:9)were(cid:9)challenges(cid:9)in(cid:9)validating(cid:9)the(cid:9)outcome(cid:9)measures(cid:9)(eg,(cid:9)data(cid:9)quality,(cid:9)collection(cid:9)capacity(cid:9)etc.)?How(cid:9)were(cid:9)external(cid:9)factors(cid:9)that(cid:9)influence(cid:9)outcomes(cid:9)addressed?Were(cid:9)repayment(cid:9)terms(cid:9)renegotiated?(cid:9)If(cid:9)so,(cid:9)why?\fAnnex D – DFID Indicative Programme Gantt Chart (subject to change) A104 DIBs Pilot Programme timelineJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDProgrammeBusiness CaseApproval of BCXProject Appraisal , Diligence, Approval (ICRC)Project Appraisal, Diligence, Approval (VE)DFID Annual ReviewsProject Completion ReviewDFID commissioned Evaluation Tentative Timeline for OutputsIssue TenderxSuppliers BiddingxBid evaluation & contractingxEvaluation Inception (4 weeks)xDIBs Design Phase Learning Report (QA)xxxxXMid-Term Evaluation Report (QA)XFinal Evaluation Report (QA)XAnnual Evidence/Learning ReportQuality Assurance of ToR, Design, OutputsICRCDesign (largely complete b4 DFID engaged)PbR Agreement negotiation/finalisationImplementationBuilding of new centres, training staff, testing efficiency measures in 8 centresOperationalisation of the new centresProject Progress ReportsLa Caixa Outcomes Payment (~£0.88m on completion of building of centres)◊SER Outcomes Measurement & Payment (verification activities)NB: ICRC will produce monthly SER reports◊Learning Activities (no internal activities planned)VE DIBDesign Fnalisation & Contract negotiationOutcomes Verifier tender & designImplementationCohort 1dark red = targetting; light red = training and mentoringCash transfer verification & payment◊◊green shows verification of initial seed transfer (larger portion); and second smaller supplementary seed transfer; with ◊ showing donor payment $1 for every $ transferred.Cohort 2Cash transfer verification & payment◊◊Cohort 3Cash transfer verification & payment◊◊Cohort 4Cash transfer verification & payment◊◊Endline Outcomes Measurement & Payment cohorts 1-4◊Cohort 5Cash transfer verification & payment◊◊Cohort 6Cash transfer verification & payment◊◊Cohort 7Cash transfer verification & payment◊◊Endline Outcomes Measurement (cohorts 5-7) & Payment (pooled result cohorts 1-7)◊◊Learning Activities and Reports produced (✓)✓✓✓✓BAT Education DIBDesign of Education DIB IndiaxxxOutcome measurement instrument to be piloted in june/july, and baselines done in july or september)Implementation of Education DIB in IndiaOutcomes Measurement & PaymentsNB: We expect annual outcomes verification and annual results payments, but timing isn't confirmed◊◊◊◊BAT Learning ActivitiesNB: Timing of learning activities & outputs are estimated, and will be confirmed later this yearResearch Report on BAT Education DIB✓✓Selection of areas of feasibility study◊Feasibility Reports for South Asia◊Proof of Concept Reports for South Asia◊DIBs Expansion - Design?Stage 1Stage 2KeyPayments◊Reports Produced✓20232022201620172018201920202021We assume sustained service provision at centres, with maintained or increasing SER and replicated across ICRC PR programmeSome service providers will continue to deliver interventions in the schools after end of the programme.School year runs Sept -July.4 Years of schooling starting Sept 2018\fEnd of ToR Changes to the Terms of Reference Changes to the Terms of Reference were agreed during the inception phase, and set out in the inception report. No other changes have been  made during this research wave.\n",
            "Top  18  :   poverty  exclusion) and to services. The aim is to support  them  gain  mobility,  autonomy  and  dignity  so  that  they are able to become active  members  society.  of  Furthermore,  family  members  who  were  taking  care  of  them  will be able to work more, and  the  the  household  as  a  whole  can  increase its income.\n",
            "Top  19  :   Over 50% of members expressed that the primary objective is to increase the effectiveness  of  their  organisation’s  funding,  to  access  private  sector  finance,  and  to  allow  for  more  innovation in service delivery.\n",
            "Top  20  :   household  income,  increase  their  savings  and ultimately lift themselves out of poverty.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who are the intended beneficiaries of the service?\n",
            "Top  1  :   In the next two research waves, we envisage speaking directly with beneficiaries. We will work  closely  with  our  peer  reviewer,  our  local  researchers  and  the  service  providers,  in  order  to  ensure our research is conducted in an ethically appropriate manner.\n",
            "Top  2  :   •  Firstly, the interests of the service provider and investor overlap. Both are incentivised to  reach  the  outcome  targets,  because  they  bear  the  reputational  and  financial  risk,  respectively. Hence, service providers may focus on those easier to reach, or on short- term activities to trigger payments. Both actors may be incentivised to design easier to  achieve outcome targets. The outcome funder is a key counterbalance to these interests,  and ensure that pressure for success thresholds are ambitious and repayment conditions  are at least at the risk-return rate of funding alternative (i.e. at market level). The outcome  funder  plays  a  crucial  role  in  protecting  the  interests  of  beneficiaries.  This  may  be  problematic  in  cases  where  outcome  funders  cede  control  over  all  aspects,  including  grantee selection and evaluation of outcomes to private investors, for example, in the case  of the Peterborough SIB (Warner 2013).\n",
            "Top  3  :   •  Any additional costs needed to access the service (e.g. out of pocket  payments, transportation costs), or in-kind delivery on the part of  beneficiaries or local government.\n",
            "Top  4  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond  Role  of  the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond Measuring impact Validation  impact of Payment  based  experimental/quasi- experimental  or  validated  administrative data7 on Payment  based  on  validated  administrative data.   This will include verification of  records  physical  and  verification  of  mobility  of  beneficiaries.\n",
            "Top  5  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  6  :   4% on 1m)  on Presence of capital protection  measures (60%)  risk  sharing  Presence  of  arrangements  potential  –  downside for service provider 94% payment on outcomes   6%  covers  contingency  costs  on  the  DIB,  and  for  costs  including  communications evaluation 100%  payment  on  outcomes Full risk on investors.   Presence  of  risk  sharing  arrangements  –  potential upside for service provider of Full risk on investors  risk  Presence  sharing  arrangements  –  potential  upside  for  service providers Identifying and selecting stakeholders  Social  intent  of  service  providers  Social  intent  of  investors  Structuring the vehicle and developing the operating model Are the service providers /  investors  a  charity  or  company  without  explicit  social values?\n",
            "Top  7  :   •  Formed  of:  Representatives  of  the  stakeholders  involved  in  each  of  the  3  DIBs  –  including the service providers: ICRC and Village Enterprise; other donors e.g. USAID,  Belgium,  Switzerland,  British  Asian  Trust,  MSDF;  investors  e.g.  UBS  Optimus  Foundation;  and  involved  project  managers  such  as  Instiglio,  the  DFID  DIBs  team,  DFID PbR Advisor, and DFID Evaluation Advisor.\n",
            "Top  8  :   poverty  exclusion) and to services. The aim is to support  them  gain  mobility,  autonomy  and  dignity  so  that  they are able to become active  members  society.  of  Furthermore,  family  members  who  were  taking  care  of  them  will be able to work more, and  the  the  household  as  a  whole  can  increase its income.\n",
            "Top  9  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  10  :   received  a  grant from the other As  stakeholders  committed  to  the  DIB,  outcome  funders  USAID-DIV,  (DFID,  the  anonymous  and  contributed  Donor)  funds  Instiglio  to  support  to  finalisation  of the project design.\n",
            "Top  11  :   Investors: Could be the  Syndicate of the Foundation for  Community Work, an Institutional  investor or a philanthropist.  Service providers: TBC  Intermediary: D. Capital Partners  Performance manager:  mothers2mothers   Technical assistance providers:  Social Finance UK  Outcome evaluator: TBC Upfront capital  commitment USD 1.1  million across 2 impact  bonds (social  development and health),  the total potential outcome  payment of which could  reach USD 3.6 million.  Additional grants: USD  110,000.\n",
            "Top  12  :   Secondary  users  of the learning generated  by  the evaluation  will  be  organisations that are  using or thinking about using impact bonds or similar approaches to financing development  local  and  national  include  outcome  programmes.  Such  organisations  governments in developing countries as well as public and private donors who want to achieve  results  for  a  given  population),  investors  (private  and  public  sector  organisations  that  are  willing to pre-finance social impact projects in developing countries and be repaid on a pay- for-success basis), and service providers (NGOs, charities, social enterprises, private sector  organisations that deliver services to achieve development outcomes). They will benefit from  the  findings  produced  by  the  evaluation,  and the  practical  recommendations  it  contains  for  using DIBs and DIB-like structures in the future. Please see governance section for how users  are represented or engaged in the evaluation.\n",
            "Top  13  :   •  Need for risk transfer too great to be borne by service providers.\n",
            "Top  14  :   Over 50% of members expressed that the primary objective is to increase the effectiveness  of  their  organisation’s  funding,  to  access  private  sector  finance,  and  to  allow  for  more  innovation in service delivery.\n",
            "Top  15  :   Service  provider;   outcome  funder All  stakeholders Service  provider  Service  provider Service  provider Service  provider Service  provider Unlikely  to  be  able  to  receive  this  information Likely  to  be  largely  qualitative  data Unlikely  to  be  able  to  receive  disaggregated  information on  participants Unlikely  to  be  able  to  receive  disaggregated  information on  participants x x x x x x A146 Targeting  costs  will  be  difficult to  obtain Indicator Stakeholder ICRC VE QEI Cataract RW1   RW2  RW3   Comparison  programmes Different effects of the intervention on the  different sub-groups.\n",
            "Top  16  :   Outcome Funder: World Bank  Group.   Investors: Five investors from  Chile, Palestine, Holland, Britain  and Switzerland, which have not  signed yet.  Service providers: 2-4  Palestinian, not-for profit service  providers to be selected per cycle, Payment terms: The  investors will form a SPV  for the flow of funds, with a  DIB manager (contracted  by the PIA) who will  manage the SPV funds on  behalf of the investors.   Outcome metric: Likely to  be a mixture of training The outcome funds are  USD 5 million. The DIB is  part of a wider World  Bank project called  Finance for Jobs, a larger  initiative to create  employment in West Bank  and Gaza.\n",
            "Top  17  :   The four DIBs are operating in development/humanitarian contexts, and the service providers  are  primarily  non-governmental  organisations.  The  DIBs  are  similar  in  duration  (all  approximately five years in length) and timescale, operating between 2017-23.\n",
            "Top  18  :   Investors A hypothesised benefit of using DIBs, noted by a few outcome funders within the four DIBs, is  that investors not only bring capital, but can also contribute commercial sense, expertise in  pricing and quantifying risks, and market discipline in picking investments. A key advantage  cited by the private investors and service provider involved in the ICRC HIB was the fact that  it enabled commercial actors to be involved in providing upfront financing for the delivery of  services  in  humanitarian  contexts28.  One  DIB  advisor  felt  that  these  benefits  may  not  materialise when traditional donors are involved as investors due to their aversion to certain  types of risk (reputational, non-delivery) and other internal constraints, such as bureaucratic  procedures.  For  example,  one  of  the  investors  had  internal  organisation  requirements  that  required  extensive  reporting  from  service  providers,  on  areas  such  as  job  creation,  disaggregation of jobs by gender, environmental standards and salaries.  This can increase  the reporting requirements of service providers rather than moving them to a focus on delivery 28 It should be noted that investors are expected to be repaid at the end of the programme, depending on the  performance of the programme.\n",
            "Top  19  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  20  :   Who pays for these additional costs  and to what extent do they see the  benefits?\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who does the service try to help?\n",
            "Top  1  :   •  This was the first DIB that the service provider was involved with. In order to understand  the  process  fully  they  needed  both  legal  and  accounting  consultancy  support  to  help  structure the finance and set up the SPV. Stakeholders from VE said this was a lot of work  and time (over 100 hours of legal and accountancy support).\n",
            "Top  2  :   poverty  exclusion) and to services. The aim is to support  them  gain  mobility,  autonomy  and  dignity  so  that  they are able to become active  members  society.  of  Furthermore,  family  members  who  were  taking  care  of  them  will be able to work more, and  the  the  household  as  a  whole  can  increase its income.\n",
            "Top  3  :   Service  providers  do  not  or  cannot  game  the  system,  and incentives are not distorted so that actions important  for the underlying goal but not measured by the outcome  measure are ignored (i.e. tunnel vision). There may be  tension  between  this  principle  and  the  alignment  principle.\n",
            "Top  4  :   •  Firstly, the interests of the service provider and investor overlap. Both are incentivised to  reach  the  outcome  targets,  because  they  bear  the  reputational  and  financial  risk,  respectively. Hence, service providers may focus on those easier to reach, or on short- term activities to trigger payments. Both actors may be incentivised to design easier to  achieve outcome targets. The outcome funder is a key counterbalance to these interests,  and ensure that pressure for success thresholds are ambitious and repayment conditions  are at least at the risk-return rate of funding alternative (i.e. at market level). The outcome  funder  plays  a  crucial  role  in  protecting  the  interests  of  beneficiaries.  This  may  be  problematic  in  cases  where  outcome  funders  cede  control  over  all  aspects,  including  grantee selection and evaluation of outcomes to private investors, for example, in the case  of the Peterborough SIB (Warner 2013).\n",
            "Top  5  :   Furthermore,  some  service  providers  withdrew  from  the  projects,  as  they  felt  they  would  struggle with the capacity demands from the DIB and its related performance management.  For example, Educate Girls intended to be involved in the QEI DIB but the timing of this project  coincided  with  the  final  year  of  their  first  DIB,  limiting  their  organisational  capacity.  This 11 OBApproaches: Output-Based Aid in Fragile and Conflict Situations. Note Number 47, June 2015.\n",
            "Top  6  :   Over 50% of members expressed that the primary objective is to increase the effectiveness  of  their  organisation’s  funding,  to  access  private  sector  finance,  and  to  allow  for  more  innovation in service delivery.\n",
            "Top  7  :   The four DIBs are operating in development/humanitarian contexts, and the service providers  are  primarily  non-governmental  organisations.  The  DIBs  are  similar  in  duration  (all  approximately five years in length) and timescale, operating between 2017-23.\n",
            "Top  8  :   DIBs create incentives for service providers to focus on producing desired results •  Service  providers  have  the  incentive  to  be  result-focused,  which  can  incentivise  the  establishment or improvement of performance management systems. This can generate  a culture of results, together with rigorous measurement and evidence-based monitoring  and evaluation. This can spill over to other programmes not funded by the DIB and build  a culture of M&E and course correction. (it is noted that a related theory suggests that it  is increased attention, rather than the pecuniary interest, which may motivate change)   •  Service providers may be more incentivised to target populations that face the greatest  needs, as this is often where the greatest gains (social and financial) are to be had.\n",
            "Top  9  :   In the next two research waves, we envisage speaking directly with beneficiaries. We will work  closely  with  our  peer  reviewer,  our  local  researchers  and  the  service  providers,  in  order  to  ensure our research is conducted in an ethically appropriate manner.\n",
            "Top  10  :   jobs, A significant amount of time is  freed  up  for  family  members  taking  care  of  relatives  with  disabilities, who can now work  more.  The  household  as  a  whole can increase its sources  of  income  and  improve  its  living standards.\n",
            "Top  11  :   •  Need for risk transfer too great to be borne by service providers.\n",
            "Top  12  :   It is also interesting to note that the reputational risk was seen as having upsides as well as  downsides.  For  many  of  the  service  providers  across the  DIBs the  spotlight  the  DIB  would  create  on  their  organisation  was  a  motivating  factor  for  joining  the  DIB.  Furthermore,  the  backing  of  an  external  investor  signals  confidence  in  both  the  intervention  and  the  service  provider, and that the outcome targets can be met.\n",
            "Top  13  :   DIBs create incentives for service providers to focus on producing desired results Result Focus: This seems to be an area well supported by the evidence so far.\n",
            "Top  14  :   •  Any additional costs needed to access the service (e.g. out of pocket  payments, transportation costs), or in-kind delivery on the part of  beneficiaries or local government.\n",
            "Top  15  :   •  Outcome verification can lead to greater transparency around the impact of the funding and the service providers’ work, and correspondingly, improved accountability.\n",
            "Top  16  :   Yes Yes/No Comments through Dalberg  collects  data  field  observations  to  intervention  schools,  individual  and  group  interviews  with  school  staff,  and  in  person  interviews  with service provider leadership as well  as program staff.\n",
            "Top  17  :   4.4.1.4  Comparison with other impact bonds The  finding  that  impact  bonds  enable  some,  but  not  all,  service  providers  to  take  on  PbR  contracts is a common one. In the UK there is evidence that SIBs have enabled smaller service  providers to enter PbR contracts who would not have been able to do so previously because  of  the  financial  risk.  The  CBO  Evaluation  (unpublished)  found  similar  barriers  for  service  providers entering the market - as has been seen in these four DIBs, it was found that investors  work repeatedly with trusted organisations with strong and credible management teams, and  entering into an impact bond requires a degree of capability and capacity that a large number  of smaller service providers do not have.\n",
            "Top  18  :   The  Supplier  is  responsible  for  ensuring  that  appropriate  arrangements,  processes  and  procedures are in place for their Personnel, taking into account the environment they will be  working  in  and  the  level  of  risk  involved  in  delivery  of  the  Contract  (such  as  working  in  dangerous, fragile and hostile environments etc.). The Supplier must develop their response  on the basis of being fully responsible for Duty of Care in line with the details provided above  and the risk assessment matrix developed by DFID (see Annex 1) of this ToR). The Supplier  must confirm in their response that: •  They fully accept responsibility for Security and Duty of Care.\n",
            "Top  19  :   The Supplier is responsible for ensuring appropriate safety and security briefings for all of their  Personnel working under this contract and ensuring that their Personnel register and receive  briefing as outlined above. Travel advice is also available on the FCO website and the Supplier  must ensure they (and their Personnel) are up to date with the latest position.\n",
            "Top  20  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who was eligible for inclusion in the intervention?\n",
            "Top  1  :   •  VE – The intervention identifies individuals who live in extreme poverty and are unable  to  provide  for  their  family’s  basic  needs.  VE  assesses  poverty  levels  through  a  community-based Poverty Wealth Ranking exercise coupled with the Progress-out-of- Poverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment  Agreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the  financial model.\n",
            "Top  2  :   2.4 2.3 2.2  The product describes the intervention logic and/or theory of change.  The product provides a relevant and sufficient description of the local,  national and/or international development context within which the  intervention was operating.  The product identifies key linkages between the evaluated intervention and  other relevant projects / programmes / donors. If no linkages are identified,  the product justifies why other projects / programmes / donors were not  relevant to the evaluation.  There is an assessment of the policy context for the intervention and this  includes reference to poverty reduction strategies, gender equality,  environmental protection, and human rights.  The product describes the extent to which the intervention has been  managed and delivered against Paris Declaration principles.\n",
            "Top  3  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  4  :   DIB. The criteria was PbR funded interventions working in similar sector, and,  where possible, similar geographies.\n",
            "Top  5  :   3.  Shared understanding of the policy ‘problem’ and sufficient evidence for the intervention so that it is credible or knowledge-based.\n",
            "Top  6  :   6.2.1.1 Analysis from four projects Identifying metrics In identifying metrics, data from previous interventions was cited as a key enabler by a range  of stakeholders across the DIBs. The four DIBs were able to draw on similar sources of data  used to identify the intervention. These sources of data, as set out in section 6.1.2.1, were  used to identify feasible and relevant metrics.\n",
            "Top  7  :   The DIB did not enable completely new interventions to be funded, as all four had been funded  previously in a different guise, in some instances by the same funders. For example, DFID,  the  Belgian  and  Swiss  governments  all  provide  core  funding  to  ICRC;  the  same  outcome  funders  in  the  QEI  DIB  have  funded  the  same  service  providers  to  deliver  very  similar  interventions; the Magrabi Foundation had already raised USD 10m of the USD 12m required  to fund the hospital; and the Village Enterprise intervention is already operating in other parts  of Africa. Therefore, the DIB has not enabled completely new interventions to be funded, which  would not have received funding otherwise.\n",
            "Top  8  :   Identifying appropriate interventions This  sub-section  explores  the  necessary  considerations  when  selecting  appropriate  interventions to be funded through a DIB, and lessons learned on ways to reduce transaction  costs and increase the model’s benefits at this stage.\n",
            "Top  9  :   is The  the  features  of  intervention,  and  whether  it  totally  new,  an  expansion  of  an  existing  programme  or  involves  a  whose  programme  underpinning  principles  have already been tested  the  Extent  contract  a  specific  and  well-defined  intervention  and  service to  which  involves Expansion  of  the  existing  programme  of  a  service  provider.  Implementation  of  a  programme proven successful  improvement  (efficiency  measures  testing)  and  new  programme  (Digital  Centre  Management System).   Contract  involves  a  specific  and  well-defined  intervention,  though  there  is  room  to  test  and adapt Level  innovation of of Level  outcome  orientation  and  flexibility  versus the Grant received.   DFID  provided  a  grant  to  BAT  to  cover  a  proportion  of  operational,  design  and  contracting  costs;  remainder  was  covered  by  UBSOF.  Other  actors  covered  their own costs.  Instiglio  the  anonymous donor for initial design work and  stakeholder engagement.\n",
            "Top  10  :   3.5 3.6 The product clearly outlines what aspects of the intervention are and are  not covered by the evaluation.   The evaluation’s objectives are specific and realistic. They are clearly  related to the evaluation purpose.\n",
            "Top  11  :   6.1.1 Identifying appropriate interventions - necessary conditions Summary: DIBs require clear and suitable outcomes and a shared understanding of the policy  problem. Additionally, there are other practical constraints on the type of interventions suitable,  such as the timeframe of the impact bond and the level of external risk factors.\n",
            "Top  12  :   This would appear to have led to degree of risk aversion in these DIBs. This affected both the  selection of service providers and interventions; with careful selection to ensure both service  providers and the interventions had established track records. The evidence would suggest  this risk aversion limited the extent to which other DIB effects materialised, as we describe  further in this section.\n",
            "Top  13  :   However, whilst the interventions themselves have been funded before, the nature of three of  the projects is different in the DIBs (VE, ICRC and QEI), and it is the DIB element that enabled  them to be delivered in a different guise. As described in Effect 1: Transfer of financial risk,  each project had a new element that was deemed risky by the outcome funders, and in three  of the DIBs outcome funders reported they would not have funded the projects in their current  guise because they deemed them to be too risky.\n",
            "Top  14  :   Development  Stage Early-stage  design Outcome Funder: TBC  Investors: TBC  Service providers: Two service  providers have been shortlisted,  delivering employment and  entrepreneurship interventions.  Four potential service providers  have been identified and are  following detailed due diligence.  Intermediary: KOIS Invest  (feasibility study)   Technical assistance providers:  TBC Outcome Funder: N/A  Investors: N/A  Service providers: N/A  Intermediary:  Social Finance UK  Technical assistance providers:  N/A Late-stage  design – failed  to launch,  pending  availability of  outcome  funding Syrian  Refugee  Employme nt DIB 10  Uganda Sleeping  Sickness  DIB The multi-country DIB  intends to improve the  welfare of Syrian refugees  and vulnerable host  populations by funding job  market integration and  access to livelihoods  interventions in the Middle  East. This includes  employment and  entrepreneurship  interventions.\n",
            "Top  15  :   •  QEI – Poor schools were selected to participate in the programme.\n",
            "Top  16  :   Service  provider;   outcome  funder All  stakeholders Service  provider  Service  provider Service  provider Service  provider Service  provider Unlikely  to  be  able  to  receive  this  information Likely  to  be  largely  qualitative  data Unlikely  to  be  able  to  receive  disaggregated  information on  participants Unlikely  to  be  able  to  receive  disaggregated  information on  participants x x x x x x A146 Targeting  costs  will  be  difficult to  obtain Indicator Stakeholder ICRC VE QEI Cataract RW1   RW2  RW3   Comparison  programmes Different effects of the intervention on the  different sub-groups.\n",
            "Top  17  :   A100 Annex 1 – Initial Country Risk Assessment by DFID The programme under evaluation involves activities in multiple countries. DFID has provided  an overall initial risk assessment for the programme locations as shown below: A101 DFID Overall Initial Project/Intervention Summary Risk Assessment Matrix Dec-17Read in conjunction with the FCO Travel Advisory on each countryCountryHIGH RISK LOCATIONSMEDIUM RISK LOCATIONSDate ConductedThemeDFID Risk ScoreDFID Risk ScoreOverall Rating5 - VERY HIGH RISK3 - MEDIUM RISKFCO Travel Advice52Host Nation Travel AdviceN/AN/ATransportation55Security[*]53Civil Unrest53Violence/crime53Terrorism*54War41Hurricane13Earthquake****13Flood*****23Medical Services**53Nature of Project Intervention32Mean (ignoring nature of project)43Mode (ignoring nature of project)5312345Very Low RiskLow RiskMedium RiskHigh RiskVery High RiskMedium*The FCO travel advice for Uganda, Kenya, Nigeria and Mali advises that there is a general threat from terrorism**Medical facilities outside of Capital Cities, and particularly away from cities are limited***FCO advise against all travel to Borno State. There is also a  High Risk (4) threat of kidnapping across Nigeria and Maiduguri in particular**** Earthquake risk is (3) on Indian border with Pakistan and in Delhi***** Flash flooding can occur during the wet season in Nigeria; Eastern Uganda; and monsoon in North India.High RiskFor example: Abuja and Borno State in Nigeria; Mali; Kinshasa in DRC; parts of Kenya, including Nairobi; and the immediacte vicinity of the India-Pakistan border.For example, other project locations incl: Uganda (excluding Karamoja, which is not relevant to this project); Gujarat, Rajasthan, and Delhi in India (with exception of the area in immediate vicinity of the border between India and Pakistan where the Supplier is not required to travel).Dec-17LocationLow SUPPLEMENTARY ANNEXES Annex A1: DFID Theory of Change for DIBs A102 Annex A2: Initial Framework for Assessing Theory of Change for DIBs Initial framework for assessing the Theory of Change behind DIBs, developed during DFID evaluability assessment A103 Were(cid:9)(cid:9)/(cid:9)can(cid:9)deal-breakers(cid:9)/(cid:9)critical(cid:9)success(cid:9)factors(cid:9)identified(cid:9)early?(cid:9)What(cid:9)were(cid:9)they?Costs(cid:9)and(cid:9)cost(cid:9)drivers:(cid:9)(cid:9)What(cid:9)were(cid:9)the(cid:9)duration(cid:9)and(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stages?(cid:9)(cid:9)How(cid:9)were(cid:9)costs(cid:9)divided(cid:9)across(cid:9)the(cid:9)different(cid:9)participants?(cid:9)What(cid:9)factors(cid:9)drove(cid:9)the(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stakeholders?(cid:9)Which(cid:9)costs(cid:9)show(cid:9)potential(cid:9)to(cid:9)decrease(cid:9)in(cid:9)future(cid:9)deals?(cid:9)(cid:9)What(cid:9)steps(cid:9)can(cid:9)be(cid:9)taken(cid:9)to(cid:9)reduce(cid:9)future(cid:9)costs?Comparison(cid:9)with(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)How(cid:9)do(cid:9)costs(cid:9)compare(cid:9)(higher(cid:9)or(cid:9)lower)(cid:9)with(cid:9)alternative(cid:9)funding(cid:9)mechanisms(cid:9)(for(cid:9)both(cid:9)provider(cid:9)and(cid:9)for(cid:9)funder/(cid:9)payors)?(cid:9)For(cid:9)which(cid:9)stages(cid:9)did(cid:9)the(cid:9)costs(cid:9)differ?Cost-effectiveness:How(cid:9)does(cid:9)the(cid:9)effectiveness(cid:9)of(cid:9)the(cid:9)DIB(cid:9)funded(cid:9)projects(cid:9)(ie,(cid:9)impact(cid:9)/(cid:9)cost)(cid:9)compare(cid:9)with(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)by(cid:9)different(cid:9)mechanisms?(cid:9)Additionality(cid:9)of(cid:9)funding:(cid:9)Was(cid:9)the(cid:9)funding(cid:9)for(cid:9)the(cid:9)DIB(cid:9)net(cid:9)new(cid:9)to(cid:9)development?(cid:9)Or(cid:9)does(cid:9)DIB(cid:9)funding(cid:9)shift(cid:9)existing(cid:9)resources(cid:9)to(cid:9)more(cid:9)effective(cid:9)uses?(cid:9)How(cid:9)was(cid:9)this(cid:9)judged?InputsProcessesOutputs(cid:9)/(cid:9)impactCost-effectiveness1.(cid:9)FeasibilityAppropriate(cid:9)projects:(cid:9)What(cid:9)are(cid:9)their(cid:9)attributes(cid:9)(eg,(cid:9)sector,(cid:9)problems(cid:9)/(cid:9)opportunities(cid:9)addressed,(cid:9)innovative(cid:9)or(cid:9)scaling(cid:9)up(cid:9)mature(cid:9)interventions,(cid:9)preventive,(cid:9)measurable(cid:9)baselines(cid:9)etc)?(cid:9)Funders(cid:9)/(cid:9)payors:(cid:9)(cid:9)How(cid:9)many?What(cid:9)are(cid:9)their(cid:9)goals(cid:9)and(cid:9)motivations?(cid:9)Was(cid:9)perceived(cid:9)transfer(cid:9)of(cid:9)risk(cid:9)a(cid:9)motivation?(cid:9)Were(cid:9)they(cid:9)easy(cid:9)/(cid:9)difficult(cid:9)to(cid:9)find(cid:9)/(cid:9)engage?(cid:9)Why?Providers:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)are(cid:9)they(cid:9)resource(cid:9)&(cid:9)capital(cid:9)constrained,(cid:9)are(cid:9)they(cid:9)used(cid:9)to(cid:9)PbR(cid:9)contracts,(cid:9)do(cid:9)they(cid:9)already(cid:9)have(cid:9)an(cid:9)appropriate(cid:9)monitoring(cid:9)system(cid:9)etc.)?Investors:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)commercial(cid:9)or(cid:9)foundations,(cid:9)established(cid:9)or(cid:9)new(cid:9)to(cid:9)development,(cid:9)how(cid:9)many)?Intermediaries:(cid:9)Which(cid:9)intermediaries(cid:9)are(cid:9)involved(cid:9)What(cid:9)roles(cid:9)do(cid:9)they(cid:9)play?(cid:9)Who(cid:9)do(cid:9)they(cid:9)represent?(cid:9)How(cid:9)were(cid:9)they(cid:9)funded?Capacity-building:(cid:9)What,(cid:9)if(cid:9)any,(cid:9)support(cid:9)has(cid:9)been(cid:9)provided(cid:9)to(cid:9)help(cid:9)stakeholders(cid:9)prepare(cid:9)for(cid:9)the(cid:9)DIB?(cid:9)Has(cid:9)it(cid:9)been(cid:9)useful?Context:(cid:9)What(cid:9)contextual(cid:9)factors(cid:9)significantly(cid:9)influenced(cid:9)the(cid:9)development(cid:9)of(cid:9)the(cid:9)DIB?Estimates(cid:9)of(cid:9)impact:Was(cid:9)the(cid:9)intervention(cid:9)successful?(cid:9)Does(cid:9)it(cid:9)seem(cid:9)that(cid:9)the(cid:9)funding(cid:9)instrument(cid:9)played(cid:9)a(cid:9)role(cid:9)in(cid:9)whether(cid:9)or(cid:9)not(cid:9)it(cid:9)was(cid:9)(ie,(cid:9)via(cid:9)the(cid:9)mechanisms(cid:9)in(cid:9)(cid:9)3.(cid:9)Implementation)?Comparability(cid:9)to(cid:9)impact(cid:9)from(cid:9)using(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)Were(cid:9)the(cid:9)results(cid:9)different(cid:9)to(cid:9)past(cid:9)/(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)using(cid:9)other(cid:9)instruments?Unintended(cid:9)outcomes:(cid:9)Were(cid:9)there(cid:9)any(cid:9)unintended(cid:9)outcomes,(cid:9)positive(cid:9)or(cid:9)negative?Engagement(cid:9)with(cid:9)beneficiaries:(cid:9)Did(cid:9)the(cid:9)DIBs(cid:9)create(cid:9)more(cid:9)or(cid:9)less(cid:9)engagement(cid:9)between(cid:9)beneficiaries(cid:9)and(cid:9)service(cid:9)providers?Sustainability:(cid:9)Are(cid:9)there(cid:9)reasons(cid:9)to(cid:9)believe(cid:9)any(cid:9)outcomes(cid:9)/(cid:9)impact(cid:9)achieved(cid:9)will(cid:9)be(cid:9)more(cid:9)or(cid:9)less(cid:9)sustainable(cid:9)than(cid:9)those(cid:9)achieved(cid:9)using(cid:9)other(cid:9)instruments?Repeatability:(cid:9)Would(cid:9)the(cid:9)various(cid:9)stakeholders(cid:9)participate(cid:9)in(cid:9)a(cid:9)similar(cid:9)instrument(cid:9)in(cid:9)the(cid:9)future?(cid:9)Under(cid:9)what(cid:9)conditions?What(cid:9)factor,(cid:9)if(cid:9)any,(cid:9)drove(cid:9)improvement?1)(cid:9)change(cid:9)in(cid:9)incentives(cid:9)(mgmt.(cid:9)and/or(cid:9)front-line)2)(cid:9)increased(cid:9)flexibility(cid:9)/(cid:9)autonomy3)(cid:9)support(cid:9)from(cid:9)active(cid:9)investorDid(cid:9)these(cid:9)or(cid:9)other(cid:9)factors(cid:9)increase(cid:9)focus(cid:9)on(cid:9)outcomes(cid:9)and(cid:9)delivery?Were(cid:9)investors(cid:9)&(cid:9)funders(cid:9)/(cid:9)payorsactive(cid:9)or(cid:9)passive(cid:9)in(cid:9)this(cid:9)stage?(cid:9)If(cid:9)active,(cid:9)did(cid:9)they(cid:9)add(cid:9)value?(cid:9)(cid:9)What(cid:9)were(cid:9)challenges?(cid:9)Were(cid:9)they(cid:9)overcome?(cid:9)If(cid:9)so,(cid:9)how?What(cid:9)factors(cid:9)were(cid:9)important(cid:9)for(cid:9)projects(cid:9)that(cid:9)did(cid:9)/(cid:9)did(cid:9)not(cid:9)proceed?(cid:9)2.(cid:9)Structuring(cid:9)the(cid:9)deal3.(cid:9)Implementation4.(cid:9)Evaluation(cid:9)and(cid:9)paymentsWhat(cid:9)measures(cid:9)&(cid:9)method(cid:9)were(cid:9)used(cid:9)to(cid:9)estimate(cid:9)impact?(cid:9)Were(cid:9)these(cid:9)appropriate(cid:9)(eg,(cid:9)were(cid:9)the(cid:9)measures(cid:9)good(cid:9)predictors(cid:9)of(cid:9)positive(cid:9)effects)?What(cid:9)were(cid:9)the(cid:9)timings(cid:9)of(cid:9)the(cid:9)payments(cid:9)(and(cid:9)investments)?(cid:9)Were(cid:9)outcome(cid:9)payments(cid:9)recycled(cid:9)as(cid:9)operating(cid:9)costs?What(cid:9)were(cid:9)challenges(cid:9)in(cid:9)validating(cid:9)the(cid:9)outcome(cid:9)measures(cid:9)(eg,(cid:9)data(cid:9)quality,(cid:9)collection(cid:9)capacity(cid:9)etc.)?How(cid:9)were(cid:9)external(cid:9)factors(cid:9)that(cid:9)influence(cid:9)outcomes(cid:9)addressed?Were(cid:9)repayment(cid:9)terms(cid:9)renegotiated?(cid:9)If(cid:9)so,(cid:9)why?\fAnnex D – DFID Indicative Programme Gantt Chart (subject to change) A104 DIBs Pilot Programme timelineJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDProgrammeBusiness CaseApproval of BCXProject Appraisal , Diligence, Approval (ICRC)Project Appraisal, Diligence, Approval (VE)DFID Annual ReviewsProject Completion ReviewDFID commissioned Evaluation Tentative Timeline for OutputsIssue TenderxSuppliers BiddingxBid evaluation & contractingxEvaluation Inception (4 weeks)xDIBs Design Phase Learning Report (QA)xxxxXMid-Term Evaluation Report (QA)XFinal Evaluation Report (QA)XAnnual Evidence/Learning ReportQuality Assurance of ToR, Design, OutputsICRCDesign (largely complete b4 DFID engaged)PbR Agreement negotiation/finalisationImplementationBuilding of new centres, training staff, testing efficiency measures in 8 centresOperationalisation of the new centresProject Progress ReportsLa Caixa Outcomes Payment (~£0.88m on completion of building of centres)◊SER Outcomes Measurement & Payment (verification activities)NB: ICRC will produce monthly SER reports◊Learning Activities (no internal activities planned)VE DIBDesign Fnalisation & Contract negotiationOutcomes Verifier tender & designImplementationCohort 1dark red = targetting; light red = training and mentoringCash transfer verification & payment◊◊green shows verification of initial seed transfer (larger portion); and second smaller supplementary seed transfer; with ◊ showing donor payment $1 for every $ transferred.Cohort 2Cash transfer verification & payment◊◊Cohort 3Cash transfer verification & payment◊◊Cohort 4Cash transfer verification & payment◊◊Endline Outcomes Measurement & Payment cohorts 1-4◊Cohort 5Cash transfer verification & payment◊◊Cohort 6Cash transfer verification & payment◊◊Cohort 7Cash transfer verification & payment◊◊Endline Outcomes Measurement (cohorts 5-7) & Payment (pooled result cohorts 1-7)◊◊Learning Activities and Reports produced (✓)✓✓✓✓BAT Education DIBDesign of Education DIB IndiaxxxOutcome measurement instrument to be piloted in june/july, and baselines done in july or september)Implementation of Education DIB in IndiaOutcomes Measurement & PaymentsNB: We expect annual outcomes verification and annual results payments, but timing isn't confirmed◊◊◊◊BAT Learning ActivitiesNB: Timing of learning activities & outputs are estimated, and will be confirmed later this yearResearch Report on BAT Education DIB✓✓Selection of areas of feasibility study◊Feasibility Reports for South Asia◊Proof of Concept Reports for South Asia◊DIBs Expansion - Design?Stage 1Stage 2KeyPayments◊Reports Produced✓20232022201620172018201920202021We assume sustained service provision at centres, with maintained or increasing SER and replicated across ICRC PR programmeSome service providers will continue to deliver interventions in the schools after end of the programme.School year runs Sept -July.4 Years of schooling starting Sept 2018\fEnd of ToR Changes to the Terms of Reference Changes to the Terms of Reference were agreed during the inception phase, and set out in the inception report. No other changes have been  made during this research wave.\n",
            "Top  18  :   5.1.2 Costs for the four DIBs The tables below set out the additional costs per DIB, that is, costs that would not have been  incurred had the intervention been funded through a grant, using the categories introduced  above. It is important to note that: •  A significant proportion of costs were provided in-kind or pro-bono, and as such, are estimates.\n",
            "Top  19  :   No DIB Objective Stakeholders involved Structure Value Bond  Innovation  Fund –  Health Development  Stage pregnant women and  children from 0-2 years by  funding home and  community based  interventions in the  Western Cape in South  Africa. The impact bond  structure is used because  of the rigorous performance  management and  mechanism with which to  align public and private  sector outcomes funding.\n",
            "Top  20  :   received  a  grant from the other As  stakeholders  committed  to  the  DIB,  outcome  funders  USAID-DIV,  (DFID,  the  anonymous  and  contributed  Donor)  funds  Instiglio  to  support  to  finalisation  of the project design.\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population beneficiaries service users participants eligible population eligibility criteria cohort clients\n",
            "Top  1  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  2  :   More incentivised to focus on target populations:   Evidence from the Employment Fund in  Nepal  (Chakravarty  et  al,  2016)  suggested that  specific targets for the  hard  to reach, such  as  greater payments for disadvantaged groups discouraged cherry picking and more focus on the  hard to reach populations.\n",
            "Top  3  :   Service  provider;   outcome  funder All  stakeholders Service  provider  Service  provider Service  provider Service  provider Service  provider Unlikely  to  be  able  to  receive  this  information Likely  to  be  largely  qualitative  data Unlikely  to  be  able  to  receive  disaggregated  information on  participants Unlikely  to  be  able  to  receive  disaggregated  information on  participants x x x x x x A146 Targeting  costs  will  be  difficult to  obtain Indicator Stakeholder ICRC VE QEI Cataract RW1   RW2  RW3   Comparison  programmes Different effects of the intervention on the  different sub-groups.\n",
            "Top  4  :   4.  Data to build up a business case, including data on the eligible cohort and outcomes likely to be achieved.\n",
            "Top  5  :   5.4.1 Targeting strategy The approach to equity will be guided by the individual programmes’ targeting strategies, to  understand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the  effectiveness of the targeting strategy of the DIB, especially in terms of the hard to reach. The  evaluation  will  look  at  how  well  the  programmes  are  fulfilling  their  targeting  strategy  and  whether there are certain sub-groups which are not being reached. At this stage, the following  strategies are understood to be in place: • ICRC – The HIB targets disabled people in a geographic area.\n",
            "Top  6  :   •  Cataract DIB – The financial model works through cross sub-subsidisation, and  there is a specific equity target, to provide 40 percent of surgeries to individuals  belonging to the bottom two wealth quintiles of the population in Cameroon.\n",
            "Top  7  :   Stakeholder type RW2  RW3 ICRC Village Enterprise QEI Cataract Table F.1 : Proposed consultations managers Project  performance  managers  intermediaries /  / x x n/a provider:  Project x x Service  managers/service  managers/practitioners Outcome  /  donors  funders  (including  DFID  and  other  donors) x x Investors Outcomes verification agents  process  level  Project  evaluators / learning partners  National  district/local  governments and x x  x x x x  x x Instiglio (Project Manager,  Process Learning lead,  CEO, Financial Model  Developer)  Director of MEL; Kenya  and Uganda country  Director, CEO, COO DFID, USAID, the  Anonymous Donor Group of private family  foundations and SV2, via  ImpactAssets  IDInsight  Instiglio TBC Dalberg  (Performance  manager) Volta Capital (bond  manager) Gyan Shala, SARD,  Kaiyvala Education  Foundation The Magrabi  Foundation British Asian Trust,  Tata Trusts, MSDF,  Comic Relief, USB OF The Fred Hollows  Foundation, Conrad  N. Hilton Foundation  and Sightsavers  OPIC and Netri  Foundation Gray Matters India  N/A AEDES  N/A N/A Regional  governments in the  states where the  service providers are  operating PRP Lead, Director of  Finance, HIB Head, Staff at  the 3 HIB centres and  identified comparison  centres  Governments of  Switzerland, Belgium, UK  and Italy, and La Caixa  Foundation  Munich Re, Lombard Odier  pension fund, charitable  foundations and others   Philanthropy Associates  N/A Local Governments in Mali,  DRC, and Nigeria A144 Stakeholder type RW2  RW3 ICRC Village Enterprise Local  organisations  that  work  with the project  Advisors (designers) Service users / beneficiaries x x x x x x Ministry of Health in  countries of operation  KOIS TBC N/A QEI N/A Dalberg Sample  of  users  in  new  ICRC centres, and the 8 pilot  centres.\n",
            "Top  8  :   Section 1.4, Annex  E.1, E.2, E.3 and E.4,  and Annex F  Section 1.2 3.3  The product describes the target audience(s) for the evaluation.\n",
            "Top  9  :   Table 2.5: Deliverables mapped to target audiences Deliverables Primary  users:  DFID  stakeholders Case studies  Reports  Internal workshops  External Workshops  Learnings outputs         users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs              This report forms evaluation report 1, which includes early feedback on the set-up of the DIBs  (including an estimate of set-up costs) and recommendations for expanding and improving the  DIB  programme  and  these  DIB  mechanisms.  This  is  also  complemented  by  specific  case  studies focusing on each of the three DIBs (see Annex A). An internal workshop was held  to discuss emerging findings (see Annex K).\n",
            "Top  10  :   RW1   RW2  RW3   Stakeholder ICRC VE QEI Cataract x x x Service  provider Quarterly  reports Table F.3: Other data Data type M&E data  (Beneficiary  numbers  and  outcomes) Examples  of relevant  reports Internal  progress  reports;  Project  monitoring  reports  received  from  each  DIB partner;  Summary of  beneficiary  feedback Outcome  verification  reports  (baseline  and  endline) Comp arison  progra mmes  x How this data will  be used To  understand  the  status and success of  the  programme,  and  to  compare  the  DIB  funded  programmes  similar  with  other  (where  programmes  similar  M&E  data  are  collected).\n",
            "Top  11  :   x x x x 6  Difference in: x x x •  Quality of outcomes  •  Sustainability of outcomes  •  Organisation  performance  (spillovers) approach to  management •  Positive  and  negative  unintended effects to (with reference 7  %  of  participants  in  the  different  sub- targeting groups  strategy).   Targeting  costs  if  relevant  (with  the  assumption that targeting costs increase  when trying to access the hard to reach)  8  Change in targeting approach based on  the identified effects of the impact bond.\n",
            "Top  12  :   DIB. The criteria was PbR funded interventions working in similar sector, and,  where possible, similar geographies.\n",
            "Top  13  :   ICRC  HIB:  The  outcome  metric  is  based  on  people  benefitting  from  physical  rehabilitation  services.  However,  the  M&E  data  will  include  disaggregated  data  on  gender  and  age.  Furthermore, a key limitation with the verification process is that it is limited to those within  urban  areas,  accessible  by  the  verification  firm.  Over  the  next  two  research  waves,  the  evaluation team will assess the extent to which targeting may have been affected by use of  the  HIB,  and  the  extent  to  which  the  verification  process  will  be  affected  by  the  limited  geographical coverage.\n",
            "Top  14  :   •  VE – The intervention identifies individuals who live in extreme poverty and are unable  to  provide  for  their  family’s  basic  needs.  VE  assesses  poverty  levels  through  a  community-based Poverty Wealth Ranking exercise coupled with the Progress-out-of- Poverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment  Agreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the  financial model.\n",
            "Top  15  :   Sample  of  participating  households  in  Kenya  and  Uganda Sample  of  people  in  treatment schools young  the Cataract N/A Volta Capital  Aravind Foundation  Sample  of  patients  in  hospital  in  Cameroon  from  middle  and  low  income backgrounds RW1   RW2  RW3   Comparison  programmes x x x All  stakeholders Stakeholder ICRC VE QEI Cataract this  is VE  have  stated  that  data  available  and  can  be shared  with us.\n",
            "Top  16  :   2.2.3 Reporting and dissemination As part of the inception phase, we undertook an analysis of stakeholders, and identified the  three  types  of  users:  DFID  stakeholders,  stakeholders  involved  in the  pilot  DIBs  and those  interested in DIBs and/or SIBs. The reporting and communication outputs have been designed  with these stakeholders in mind. The table below maps the deliverables to the targeted users.  This is followed by a brief description of each type of deliverable.\n",
            "Top  17  :   Governments National and district governments Users of new ICRC  centres, and the 8 pilot  centres.\n",
            "Top  18  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  19  :   4% on 1m)  on Presence of capital protection  measures (60%)  risk  sharing  Presence  of  arrangements  potential  –  downside for service provider 94% payment on outcomes   6%  covers  contingency  costs  on  the  DIB,  and  for  costs  including  communications evaluation 100%  payment  on  outcomes Full risk on investors.   Presence  of  risk  sharing  arrangements  –  potential upside for service provider of Full risk on investors  risk  Presence  sharing  arrangements  –  potential  upside  for  service providers Identifying and selecting stakeholders  Social  intent  of  service  providers  Social  intent  of  investors  Structuring the vehicle and developing the operating model Are the service providers /  investors  a  charity  or  company  without  explicit  social values?\n",
            "Top  20  :   o  What social issues, target groups, geographies and project scales do DIBs fit best and have the greatest impact?\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population\n",
            "Top  1  :   More incentivised to focus on target populations:   Evidence from the Employment Fund in  Nepal  (Chakravarty  et  al,  2016)  suggested that  specific targets for the  hard  to reach, such  as  greater payments for disadvantaged groups discouraged cherry picking and more focus on the  hard to reach populations.\n",
            "Top  2  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  3  :   5.4.1 Targeting strategy The approach to equity will be guided by the individual programmes’ targeting strategies, to  understand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the  effectiveness of the targeting strategy of the DIB, especially in terms of the hard to reach. The  evaluation  will  look  at  how  well  the  programmes  are  fulfilling  their  targeting  strategy  and  whether there are certain sub-groups which are not being reached. At this stage, the following  strategies are understood to be in place: • ICRC – The HIB targets disabled people in a geographic area.\n",
            "Top  4  :   Scale: Gustafsson-et al (2015) found from a review of SIBs that scale was achieved in certain  target populations, but not as a whole.\n",
            "Top  5  :   •  Cataract DIB – The financial model works through cross sub-subsidisation, and  there is a specific equity target, to provide 40 percent of surgeries to individuals  belonging to the bottom two wealth quintiles of the population in Cameroon.\n",
            "Top  6  :   Component ICRC Village Enterprise Cataract Bond Target groups  People with physical marginalised disabilities People  living  in  extreme  poverty  (on  less  than  USD 1.90 per day) QEI 300,000  children Activities non-government  Three  (NGOs)  organisations  education  delivering  Delivery  programmes.  model  include  types  improving  whole  school  management,  learning  supplementary  and  teacher  and  school  leader training include  Activities  workshops,  trainings  and  e-resources  as  well  as  meetings  with  community  groups.\n",
            "Top  7  :   •  VE – The intervention identifies individuals who live in extreme poverty and are unable  to  provide  for  their  family’s  basic  needs.  VE  assesses  poverty  levels  through  a  community-based Poverty Wealth Ranking exercise coupled with the Progress-out-of- Poverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment  Agreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the  financial model.\n",
            "Top  8  :   Section 1.4, Annex  E.1, E.2, E.3 and E.4,  and Annex F  Section 1.2 3.3  The product describes the target audience(s) for the evaluation.\n",
            "Top  9  :   VE:  The  targeting  strategy  addresses  equity  concerns,  and  at  the  moment,  there  are  no  particular  risks  identified  with  the  outcome  target  or  verification  process  potentially  driving  perverse incentives. This will be monitored over the next two research waves.\n",
            "Top  10  :   Ibidem; Pereira, J; Villota, C. 2013. Hitting the target? Evaluating the effectiveness of results- based approaches to aid. Brussels: EURODAD.\n",
            "Top  11  :   x x x x 6  Difference in: x x x •  Quality of outcomes  •  Sustainability of outcomes  •  Organisation  performance  (spillovers) approach to  management •  Positive  and  negative  unintended effects to (with reference 7  %  of  participants  in  the  different  sub- targeting groups  strategy).   Targeting  costs  if  relevant  (with  the  assumption that targeting costs increase  when trying to access the hard to reach)  8  Change in targeting approach based on  the identified effects of the impact bond.\n",
            "Top  12  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  13  :   The target audience groups for the communication activities are as follows: Phase Primary  users:  DFID  stakeholders users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs Case studies  Reports  Internal workshops  External Workshops  Annual Briefing  Learnings outputs           E.7 Involvement of stakeholders                  The evaluation has been designed and managed to meet the information and decision-making  needs of the intended users. Discussions were carried out with DFID and stakeholders of the  pilot DIBs in order to inform the approach and needs of stakeholders, as part of the inception  phase.  DFID  is  also  coordinating  the  evaluation  stakeholder  group  for  this  purpose.  Additionally, during this first research wave, the evaluation team has held bi-weekly catch up  calls  with  DFID,  to  inform  DFID  of  emerging  issues  and  to  ensure  DFID  input  in  the  implementation of the evaluation. The scope of the evaluation and individual DIB level plans,  in  terms  of  data  to  be  shared  and  consultations  to  be  undertaken  over  the  course  of  the A140 evaluation, have been discussed and agreed with the DIB level stakeholders. The individual  DIB level plans are set out in Annex E.\n",
            "Top  14  :   Payment terms: TBC  Outcome metric: To be  finalized. Likely to include:  a) number of hospitals  attaining quality KMC  prerequisites; b) number of  infants receiving quality  KMC services; c) number  or % of infants achieving  target nutritional  status/weight at 40 weeks  gestational age and/or at  follow-up.  Range of returns: TBC The planned operating  budget USD 2.1 million,  to be spent over three-to  four years. Total outcome  commitment of USD 2.8  million. Upfront capital  commitment: USD 3.0  million (pre-capital  recycling).  Additional grants  (covering feasibility study,  baseline data study, DIB  design and structuring,  data systems design,  legal advice) USD 1  million.\n",
            "Top  15  :   o  What social issues, target groups, geographies and project scales do DIBs fit best and have the greatest impact?\n",
            "Top  16  :   Table 2.5: Deliverables mapped to target audiences Deliverables Primary  users:  DFID  stakeholders Case studies  Reports  Internal workshops  External Workshops  Learnings outputs         users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs              This report forms evaluation report 1, which includes early feedback on the set-up of the DIBs  (including an estimate of set-up costs) and recommendations for expanding and improving the  DIB  programme  and  these  DIB  mechanisms.  This  is  also  complemented  by  specific  case  studies focusing on each of the three DIBs (see Annex A). An internal workshop was held  to discuss emerging findings (see Annex K).\n",
            "Top  17  :   A  minimum  of  12,660  households in Kenya and  Uganda  Local  representatives  and Uganda  DFID, USAID DIV and an  anonymous donor government  in  Kenya Michael  &  Susan  Dell  Foundation,  BT, Comic Relief, Mittal Foundation.\n",
            "Top  18  :   ambitious.   Implementers have track record of achieving  outcomes 3.  Project and performance management in place  4.  External risk factors not especially high  Med-High risk (DFID Risk score 5) 1.  Strong  evidence  base  for  the  intervention  with  extensive  historical  data  from  a  previous  RCT  conducted. Ambitious targets set.\n",
            "Top  19  :   to individuals in the bottom  two wealth quintiles of the  population  in  Cameroon  by the end of year 5.\n",
            "Top  20  :   estimating in Stratification by school size, urban/rural  location and school type ensures that all  parts  of  the  population  are  included  in  the sample.\n",
            "\n",
            "\n",
            "\n",
            "Query:  beneficiaries\n",
            "Top  1  :   In the next two research waves, we envisage speaking directly with beneficiaries. We will work  closely  with  our  peer  reviewer,  our  local  researchers  and  the  service  providers,  in  order  to  ensure our research is conducted in an ethically appropriate manner.\n",
            "Top  2  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  3  :   RW1   RW2  RW3   Stakeholder ICRC VE QEI Cataract x x x Service  provider Quarterly  reports Table F.3: Other data Data type M&E data  (Beneficiary  numbers  and  outcomes) Examples  of relevant  reports Internal  progress  reports;  Project  monitoring  reports  received  from  each  DIB partner;  Summary of  beneficiary  feedback Outcome  verification  reports  (baseline  and  endline) Comp arison  progra mmes  x How this data will  be used To  understand  the  status and success of  the  programme,  and  to  compare  the  DIB  funded  programmes  similar  with  other  (where  programmes  similar  M&E  data  are  collected).\n",
            "Top  4  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond  Role  of  the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond Measuring impact Validation  impact of Payment  based  experimental/quasi- experimental  or  validated  administrative data7 on Payment  based  on  validated  administrative data.   This will include verification of  records  physical  and  verification  of  mobility  of  beneficiaries.\n",
            "Top  5  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  6  :   Outcome Funder: Goodbye  Malaria, underwritten by Nandos  and other corporates.   Investors: not defined.   Service providers: Lubombo  Spatial Development Initiative  (LDSI) II.   Intermediary:  D. Capital  Partners.  Technical assistance providers:  the University of Pretoria, the  Medical Research Council, and  the National Malaria Control  Programme within the Ministry of  Health (Mozambique).  Outcome evaluator: TBC Payment terms: The  payment would have been  made as a bullet payment  at the end of the third year  of bond implementation,  based on the achievement  of expected outcomes.  Outcome metric: 60%  reduction in the prevalence  and incidence of malaria  cases, compared to  baseline rates at year 1.  Incidence of malaria cases  based on prevalence  testing done at sentinel  sites in each district.    Range of returns: The  maximum potential loss of  investment for investors  would have been 30%, and  the maximum return 0.05%.\n",
            "Top  7  :   Investors: Could be the  Syndicate of the Foundation for  Community Work, an Institutional  investor or a philanthropist.  Service providers: TBC  Intermediary: D. Capital Partners  Performance manager:  mothers2mothers   Technical assistance providers:  Social Finance UK  Outcome evaluator: TBC Upfront capital  commitment USD 1.1  million across 2 impact  bonds (social  development and health),  the total potential outcome  payment of which could  reach USD 3.6 million.  Additional grants: USD  110,000.\n",
            "Top  8  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  9  :   Programme  manager  and  Development  Impact Bonds Adviser  PbR comparator site  Investor  Investor  Investor  Performance manager  Intermediary  Intermediary  Outcome funder  Outcome funder  Outcome funder  Knowledge partner  Knowledge partner  Outcome evaluator  Service provider  Service provider  Service provider  Technical advisor  Funder/technical advisor  Funder/technical advisor  PbR Comparator site  Implementer Investor/Implementer  Outcome funder Outcome funder Outcome funder  Outcome funder Investor  Investor  DIB performance manager  DIB performance manager  DIB researchers A157 DIBS  Village Enterprise   Village Enterprise  Village Enterprise Organisation Village Enterprise  Village Enterprise  Instiglio  Instiglio Instiglio Role  VE Kenya Country Director  VE  Director  of  Monitoring,  Evaluation  and  Learning  Director of Institutional Giving   VE Chief Operating Officer  Instiglio – Leading the Process Evaluation  CEO  –  and  designer  of  DIB;  part  of  design  process  Instiglio  Project  Manager;  managed  design  process and kept record of discussions  Instiglio- developed financial model for DIB Instiglio  The Anonymous Donor  Co-designed the DIB model  Co-designed the DIB model  DFID  Co-designed the DIB model  DFID  Lead Investor  Anon  Secondary investor  Bridge Fund          Nepal Employment Fund  PbR Comparator site  Social Finance UK Cardano Development Convenor/manager Contributed to DIB design and development as  technical advisor Service provider  Outcome funder Technical  development) and performance manager advisor design (DIB and Technical advisor (DIB design) Outcome funder (DIB advisor Technical  and  development),  sub-contractor  of  the  project  implementation agency    Technical advisor (transaction management) design Volta Technical advisor (transaction management) Convergence Commissioned a feasibility study to KOIS Social Finance UK Technical advisor (DIB design) Cameroon  Kangaroo  Mother  Care DIB  Cook  and  Clean  DIB  Educate Girls DIB  Educate Girls  (Rajasthan)  India  Maternal  and  Newborn  Health  DIB  Mozambique  Malaria DIB  Palestine  (West  Bank  and  Gaza)  Employment DIB World Bank Palladium USAID Volta Social Finance Volta South  Africa  ECD  Bond  Impact  Innovation  Fund  –  Social  Development  South  Africa  ECD  Impact  Bond  Innovation  Fund  -  Health  Syrian  Refugees  Employment DIB  Uganda  Sleeping  Sickness DIB  Sources reviewed A158 DIBs ICRC QEI Cameroon  Cataract Document PRP HIB Efficiency Improvement Measures Project Final Execution Version of PHII PBR Agreement Signed by DFID (26/7/2017) Benchmark Data (5/8/2017) Q&A with DFID Verification agreement signed between ICRC and Philanthropy associates Final Detailed presentation 20/4/17 20/4/2017 Final ICRC HIB Program Description Final  Initial  Verification  Report  by  Philanthropy  Associates  confirming  baseline SER as 33.87 HIB Social Investor Presentation ICRC SER Ratio and how it compares to number of beneficiaries PHII Summary of the transaction Email KOIS/DFID discussion (17/5/17) 1st ORCM presentation, February 2018 1st Quarterly Status Update Jul - Sept 2017  2nd PHII Quarterly Status Update Oct - Dec 2017 3rd PHII Quarterly Status Update Jan - Mar 18 Agenda meeting 2018/2/27 PHII Summary of the transaction – updated 29/11/18  4th and 5th PHII Quarterly Status Update (Apr-Jun and July-Sept 2018)  Annex to HIB Report Phase 1 Netherlands Instructions on PRP data collection (Assort MSR 2015 instruction final and  Assort MSR 2017 template 20 centres) BAT India Technical Assistance Grant Proposal (December 2017)  British Asian Trust - DIB Quarterly DFID Report (April-June 2018)  Education DIB Fund Financial model (June 2018)  Education  DIB  Performance  Management  Annual  Report  Template  (April  2018)  Education DIB performance Management Overview Document (April 2018)  Gray  Matters.  Proposal  for  Outcome  Evaluator  Education  DIB  Fund  (February 2018)  GyanShala Education DIB Proposal Revised (2018)  KEF Education DIB Proposal (2018)  SARD Education DIB Proposal (2018)  Service  Providers’  Proposals  Consolidated  Summary  Document  (March  2018)  020818 Schedule 5 ME Verification Protocol updated  CCBP Pilot Verification Report 2018 07 27  Legal Structure  ONGOING REPORT_aug2018 – quality A159 DIBs Document  OPIC Q2  2018_Cataract Loan Reporting_Final  SteerCo Cataract Bond Report - July 2018.Final  Cataract Bond 2 page Summary_FINAL  Cataract Bond presentation  Cataract Bond_FAQs_July 2016  Cataract Bond Monitoring & Evaluation Protocol  Cameroon Cataract Performance Bond Application_FINAL  CCPB HF Grant application addendum  Value for Money data compiled by Volta  Village Enterprise DIB Design Memo, Nov 2017; Instiglio  GDI Activity Proposal for Village Enterprise  Village Enterprise DIB Process Review, July 2018; Instiglio  Paying for Poverty Alleviation; Richard Sedlmayer  CSAE Working Paper, Cash-Plus Poverty Impacts of Transfer based  intervention alleviation (RCT into Village Enterprise traditional model)    Gustafsson-Wright et al. (2017). Impact Bonds in developing countries:  early learnings from the field.  Save the Children (2018). Investing in Maternal and Child Health:  Development Impact Bonds.    Cook and Clean Development Impact Bond Concept Note.\n",
            "Top  10  :   poverty  exclusion) and to services. The aim is to support  them  gain  mobility,  autonomy  and  dignity  so  that  they are able to become active  members  society.  of  Furthermore,  family  members  who  were  taking  care  of  them  will be able to work more, and  the  the  household  as  a  whole  can  increase its income.\n",
            "Top  11  :   received  a  grant from the other As  stakeholders  committed  to  the  DIB,  outcome  funders  USAID-DIV,  (DFID,  the  anonymous  and  contributed  Donor)  funds  Instiglio  to  support  to  finalisation  of the project design.\n",
            "Top  12  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  13  :   Secondary  users  of the learning generated  by  the evaluation  will  be  organisations that are  using or thinking about using impact bonds or similar approaches to financing development  local  and  national  include  outcome  programmes.  Such  organisations  governments in developing countries as well as public and private donors who want to achieve  results  for  a  given  population),  investors  (private  and  public  sector  organisations  that  are  willing to pre-finance social impact projects in developing countries and be repaid on a pay- for-success basis), and service providers (NGOs, charities, social enterprises, private sector  organisations that deliver services to achieve development outcomes). They will benefit from  the  findings  produced  by  the  evaluation,  and the  practical  recommendations  it  contains  for  using DIBs and DIB-like structures in the future. Please see governance section for how users  are represented or engaged in the evaluation.\n",
            "Top  14  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  15  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  16  :   counsel); Cameroon-based legal  counsel; in-country public health  consultant; data systems provider.  Outcome evaluator: TBC Outcome Funder: TBC  Investors: Likely to be BIX  capital. Shell Foundation and  DFID funded the DIB set-up; IFC  funds the data gathering for the  certification process with support  from the Ministry of Finance in  Japan.  Service providers: Mimi-Moto  (cookstove producer); Emerging  Cooking Solutions (ECS, seller of  Mimi-Moto cookstoves). Apart  from ECS, Cardano will select  one more social enterprise.  Intermediary: Cardano  Development.   Technical assistance providers:  Baker McKenzie (pro-bono legal  adviser).  Outcome evaluator: TBC Outcome Funder: Children  Investment Fund Foundation.   Investors: UBS Optimus  Foundation.   Service providers: Educate  Girls.\n",
            "Top  17  :   Nine impact investors,  including Delta Fund Overseas  Private  Corporation (OPIC), Netri Foundation Investment Outcome Verifier  Philanthropy Advisors  Gray Matters India  Project Manager  None None Performance  manager  Learning Partner  None None Dalberg Brookings IDinsight   Instiglio None Instiglio AEDES  Bond manager / technical advisor: Volta  Capital  None None Stakeholder Knowledge  Partner ICRC None DIB structures QEI Tata Trusts VE None Cataract Bond None The  structure  of  the  four  DIBs  under  the  scope  of  the  evaluation  were  quite  varied.  Table  3.3  categorises  the  four  DIBs  against  a  range  of  characteristics. Further detail on these characteristics are set out in Annex I.\n",
            "Top  18  :   Local governments in  Mali, DRC, and Nigeria Governments  of  Switzerland,  Belgium,    UK  and  Italy,  and  La  Caixa Foundation.   Munich  Re,  Lombard  Odier  pension  fund,  charitable  foundations  and others Outcome  Funders Investors Village Enterprise.\n",
            "Top  19  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  20  :   A156 Annex H: Consultees and Sources reviewed Consultees DIBS  ICRC QEI Cataract Bond KOIS Organisation  ICRC Role  Head of the HIB  Partner,  Principal  –  Impact  investing,  Senior  Associate  Executive Director, Capital Relief Transactions  Programme  Officer,  Federal  Department  of  Foreign  Affairs  FDFA,  Swiss  Agency  for  Development and Cooperation SDC  Government of Belgium  Advisor, Development Cooperation Government  Switzerland Munich Re of DFID World Bank GPOBA  UBSOF  UBSOF  UBSOF  Dalberg  BAT  BAT  MSDF  Comic Relief  Comic Relief  Tata Trust  Tata Trust  GMI  GyanShala  Kaivalya  SARD  EducateGirls  DFID  DFID  DFID GEC  AEF/  Foundation  The Magrabi Foundation  Conrad  Hilton  N.  Foundation  Conrad  Foundation  Sightsavers  The  Fred  Foundation  OPIC  Netri Foundation  Volta  Volta  CGD The  Magrabi Hollows Hilton N.\n",
            "\n",
            "\n",
            "\n",
            "Query:  service users\n",
            "Top  1  :   •  Thirdly, McHugh et al (2013) and Sinclair et al (2014) note that many SIB guides (Centre  for  Social  Impact  Bonds,  Audit  Commission  and  the  Cabinet  office)  recommend  outsourcing funding, service delivery and the responsibility for selecting a provider. The  rationale is that it is reasonable for investors or intermediaries to influence how the project A186 is  delivered  and  to  terminate  the  project  in  the  event  of  sustained  under-performance,  given that they are taking on the risk. The implicit assumption is that the provision of the  service should be accountable to those who pay for it rather than those who use it, which  is problematic for accountability to service users / beneficiaries.\n",
            "Top  2  :   Service  providers  have  significant  control  over  the  outcomes.  This  may  be  weaker  in  contexts  of  policy  uncertainty  and  high  risk.  Otherwise,  the  service  provider or investor may not be willing to take on this risk  if there is too much out of their control.\n",
            "Top  3  :   Stakeholder type RW2  RW3 ICRC Village Enterprise QEI Cataract Table F.1 : Proposed consultations managers Project  performance  managers  intermediaries /  / x x n/a provider:  Project x x Service  managers/service  managers/practitioners Outcome  /  donors  funders  (including  DFID  and  other  donors) x x Investors Outcomes verification agents  process  level  Project  evaluators / learning partners  National  district/local  governments and x x  x x x x  x x Instiglio (Project Manager,  Process Learning lead,  CEO, Financial Model  Developer)  Director of MEL; Kenya  and Uganda country  Director, CEO, COO DFID, USAID, the  Anonymous Donor Group of private family  foundations and SV2, via  ImpactAssets  IDInsight  Instiglio TBC Dalberg  (Performance  manager) Volta Capital (bond  manager) Gyan Shala, SARD,  Kaiyvala Education  Foundation The Magrabi  Foundation British Asian Trust,  Tata Trusts, MSDF,  Comic Relief, USB OF The Fred Hollows  Foundation, Conrad  N. Hilton Foundation  and Sightsavers  OPIC and Netri  Foundation Gray Matters India  N/A AEDES  N/A N/A Regional  governments in the  states where the  service providers are  operating PRP Lead, Director of  Finance, HIB Head, Staff at  the 3 HIB centres and  identified comparison  centres  Governments of  Switzerland, Belgium, UK  and Italy, and La Caixa  Foundation  Munich Re, Lombard Odier  pension fund, charitable  foundations and others   Philanthropy Associates  N/A Local Governments in Mali,  DRC, and Nigeria A144 Stakeholder type RW2  RW3 ICRC Village Enterprise Local  organisations  that  work  with the project  Advisors (designers) Service users / beneficiaries x x x x x x Ministry of Health in  countries of operation  KOIS TBC N/A QEI N/A Dalberg Sample  of  users  in  new  ICRC centres, and the 8 pilot  centres.\n",
            "Top  4  :   The four DIBs are operating in development/humanitarian contexts, and the service providers  are  primarily  non-governmental  organisations.  The  DIBs  are  similar  in  duration  (all  approximately five years in length) and timescale, operating between 2017-23.\n",
            "Top  5  :   2.2.3 Reporting and dissemination As part of the inception phase, we undertook an analysis of stakeholders, and identified the  three  types  of  users:  DFID  stakeholders,  stakeholders  involved  in the  pilot  DIBs  and those  interested in DIBs and/or SIBs. The reporting and communication outputs have been designed  with these stakeholders in mind. The table below maps the deliverables to the targeted users.  This is followed by a brief description of each type of deliverable.\n",
            "Top  6  :   There are some exceptions to the positive incentivisation of service providers, and the reasons  for this have been explored in evaluations.\n",
            "Top  7  :   •  Firstly, the interests of the service provider and investor overlap. Both are incentivised to  reach  the  outcome  targets,  because  they  bear  the  reputational  and  financial  risk,  respectively. Hence, service providers may focus on those easier to reach, or on short- term activities to trigger payments. Both actors may be incentivised to design easier to  achieve outcome targets. The outcome funder is a key counterbalance to these interests,  and ensure that pressure for success thresholds are ambitious and repayment conditions  are at least at the risk-return rate of funding alternative (i.e. at market level). The outcome  funder  plays  a  crucial  role  in  protecting  the  interests  of  beneficiaries.  This  may  be  problematic  in  cases  where  outcome  funders  cede  control  over  all  aspects,  including  grantee selection and evaluation of outcomes to private investors, for example, in the case  of the Peterborough SIB (Warner 2013).\n",
            "Top  8  :   Yes Using  the  performance  management  system established by Dalberg, service  providers  enter  and  report  data  on  a  variety  of  indicators  on  a  quarterly  this  basis.  Dalberg  information  course  to  correction measures.\n",
            "Top  9  :   4% on 1m)  on Presence of capital protection  measures (60%)  risk  sharing  Presence  of  arrangements  potential  –  downside for service provider 94% payment on outcomes   6%  covers  contingency  costs  on  the  DIB,  and  for  costs  including  communications evaluation 100%  payment  on  outcomes Full risk on investors.   Presence  of  risk  sharing  arrangements  –  potential upside for service provider of Full risk on investors  risk  Presence  sharing  arrangements  –  potential  upside  for  service providers Identifying and selecting stakeholders  Social  intent  of  service  providers  Social  intent  of  investors  Structuring the vehicle and developing the operating model Are the service providers /  investors  a  charity  or  company  without  explicit  social values?\n",
            "Top  10  :   Yes Yes/No TBD Comments Data  is  collected  by  service  providers  against  the  performance  management  framework  provided  by  Dalberg,  which  complements with other data collection  methods  and  independently  assesses  data.\n",
            "Top  11  :   Yes Service  providers  need  to  report  data  collection  and  analysis  procedures  in  quarterly reports provided by Dalberg.\n",
            "Top  12  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  13  :   Governments National and district governments Users of new ICRC  centres, and the 8 pilot  centres.\n",
            "Top  14  :   •  This was the first DIB that the service provider was involved with. In order to understand  the  process  fully  they  needed  both  legal  and  accounting  consultancy  support  to  help  structure the finance and set up the SPV. Stakeholders from VE said this was a lot of work  and time (over 100 hours of legal and accountancy support).\n",
            "Top  15  :   Sustainability  of  services:  It  was  theorised  that  demonstrated  impact  of  SIBs  would  lead  to  scaling of models, but no UK SIB has been continued at the end of its contract (Social Finance  2018). The strongest argument for sustainability seems to be the use of multi-year contracting,  which could provide more continuous and reliable service. However, there is little evidence in this  area at the moment (Gustafsson et al 2015).\n",
            "Top  16  :   •  Any additional costs needed to access the service (e.g. out of pocket  payments, transportation costs), or in-kind delivery on the part of  beneficiaries or local government.\n",
            "Top  17  :   N/A Yes All  users  calculated  and  not  calculated  based  on  a  sample  basis.\n",
            "Top  18  :   4.4.1.4  Comparison with other impact bonds The  finding  that  impact  bonds  enable  some,  but  not  all,  service  providers  to  take  on  PbR  contracts is a common one. In the UK there is evidence that SIBs have enabled smaller service  providers to enter PbR contracts who would not have been able to do so previously because  of  the  financial  risk.  The  CBO  Evaluation  (unpublished)  found  similar  barriers  for  service  providers entering the market - as has been seen in these four DIBs, it was found that investors  work repeatedly with trusted organisations with strong and credible management teams, and  entering into an impact bond requires a degree of capability and capacity that a large number  of smaller service providers do not have.\n",
            "Top  19  :   suggests that whilst DIBs enable some service providers to be included in PbR contracts, they  themselves still experience barriers that may limit their participation in a project.\n",
            "Top  20  :   •  Need for risk transfer too great to be borne by service providers.\n",
            "\n",
            "\n",
            "\n",
            "Query:  participants\n",
            "Top  1  :   Participants’ comments were useful in informing and refining the evaluation team’s analysis. A  brief summary of the discussion, and how they have been incorporated into the report, is set out  below.\n",
            "Top  2  :   Service  provider;   outcome  funder All  stakeholders Service  provider  Service  provider Service  provider Service  provider Service  provider Unlikely  to  be  able  to  receive  this  information Likely  to  be  largely  qualitative  data Unlikely  to  be  able  to  receive  disaggregated  information on  participants Unlikely  to  be  able  to  receive  disaggregated  information on  participants x x x x x x A146 Targeting  costs  will  be  difficult to  obtain Indicator Stakeholder ICRC VE QEI Cataract RW1   RW2  RW3   Comparison  programmes Different effects of the intervention on the  different sub-groups.\n",
            "Top  3  :   The  sampling  strategy  used  was  purposive.  There  was  a  limited  number  of  stakeholders involved in the set up phase, and random sampling was not considered  necessary or appropriate. For the DIB-level research, for the most part, the evaluation  team  contacted  all  relevant  stakeholders,  namely  investors,  service  providers,  outcome  funders,  performance  managers  and  outcome  evaluators.  All  stakeholders  involved  were invited to participate in the evaluation, but some stakeholders did not  participate in the evaluation. However, the team has tried to address this by drawing  on a range of programme documentation, and triangulating the findings and data from  the existing stakeholder interviews.\n",
            "Top  4  :   •  QEI – Poor schools were selected to participate in the programme.\n",
            "Top  5  :   Table 6.1: Project focus and measurement approach Projects focused on  Innovation  Building evidence  Replication, drawing on an established evidence  base  Scaling,  using  established,  highly  evidence- based interventions Measurement  Non-experimental  Quasi-experimental or experimental  Against a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for  participation in a DIB. It then sets out five lessons learned around the process of identifying  stakeholders and managing relationships, before concluding with lessons learned around how  the  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different  benefits.\n",
            "Top  6  :   The target audience groups for the communication activities are as follows: Phase Primary  users:  DFID  stakeholders users: Secondary  Stakeholders  involved  in  the  pilot  DIBs Tertiary users: those  interested  in  DIBs  and/or SIBs Case studies  Reports  Internal workshops  External Workshops  Annual Briefing  Learnings outputs           E.7 Involvement of stakeholders                  The evaluation has been designed and managed to meet the information and decision-making  needs of the intended users. Discussions were carried out with DFID and stakeholders of the  pilot DIBs in order to inform the approach and needs of stakeholders, as part of the inception  phase.  DFID  is  also  coordinating  the  evaluation  stakeholder  group  for  this  purpose.  Additionally, during this first research wave, the evaluation team has held bi-weekly catch up  calls  with  DFID,  to  inform  DFID  of  emerging  issues  and  to  ensure  DFID  input  in  the  implementation of the evaluation. The scope of the evaluation and individual DIB level plans,  in  terms  of  data  to  be  shared  and  consultations  to  be  undertaken  over  the  course  of  the A140 evaluation, have been discussed and agreed with the DIB level stakeholders. The individual  DIB level plans are set out in Annex E.\n",
            "Top  7  :   E.6.3.3  Communication Plan In the inception report, we undertook a stakeholder analysis, which categorised stakeholders  into  primary  users  (DFID),  secondary  users  (stakeholders  involved  in  the  pilot  DIBs)  and  tertiary users (those involved in other DIBs or SIBs or considering implementation of DIBs or  SIBs).\n",
            "Top  8  :   The  table  below  sets  out  the  stakeholders  consulted  in  research  wave  1,  and  the  areas  discussed. The precise areas discussed were tailored depending on the role of the interviewee  within the DIB and the point of progress of each DIB, and sent in advance to DIB stakeholders.\n",
            "Top  9  :   In parenthesis in this table under the ‘interviewed’ columns, we have included the number of  individuals interviewed. A full list of stakeholders interviewed is set out in Annex H. For the  most part, we sought to speak to all stakeholders, with the following exceptions: • • • In the  case  of  the  ICRC  HIB,  we  did  not  receive  responses  from  2  of  the  outcome  funders, nor from 1 investor.\n",
            "Top  10  :   received  a  grant from the other As  stakeholders  committed  to  the  DIB,  outcome  funders  USAID-DIV,  (DFID,  the  anonymous  and  contributed  Donor)  funds  Instiglio  to  support  to  finalisation  of the project design.\n",
            "Top  11  :   Table E.3: Stakeholder consultations in RW1 Stakeholder type Areas discussed: Wave 1 (Set up) Project  managers  /  performance managers /   intermediaries Service provider: Project  managers Service  Service managers provider: funders /  Outcome  donors  (including  DFID  and other donors) Investors Outcomes  agents verification Project  evaluators  partners level  process  learning / Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  View on DFID’s role in the DIB  Reasons  for  getting  involved  in  project,  including  what  they  hope  to  achieve and concerns  Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  View on DFID’s role in the DIB  Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  Reasons for getting involved in project, including what hope to achieve  and concerns  Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  View on DFID’s role in the DIB  Reasons for getting involved in project, including what hope to achieve  and concerns  Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  View on DFID’s role in the DIB  Progress and lessons learnt in setting up project; what factors affected  this progress (including the DIB); and how things could be improved for  this DIB and future DIBs  Findings from activity completed to date A125 The sampling strategy used was purposive. Given the focus on the set up phase, there was a  limited number of stakeholders involved, and random sampling was not considered necessary  or appropriate. For the DIB-level research, for the most part, the evaluation team contacted all  relevant  stakeholders,  namely  investors,  service  providers,  outcome  funders,  performance  managers  and  outcome  evaluators,  with  the  aim  of  gathering  and  comparing  different  perspectives, and trying to avoid biases. On certain occasions, the team managed to interview  more than one representative from the same organisation, as their role within the DIB differed  and this allowed us to collect more accurate information. Some stakeholders did not participate  in  the  evaluation.  However,  the  team  has  tried  to  address  this  by  drawing  on  a  range  of  programme  documentation,  and  triangulating  the  findings  and  data  from  the  existing  stakeholder interviews.\n",
            "Top  12  :   • Disadvantages of engaging with outcome funders first •  Certain stakeholders in one of the DIBs noted that engagement with multiple potential outcome funders at the same time was inefficient.\n",
            "Top  13  :   d e w e v r e n t i I Table 2.3 Stakeholders consulted ICRC QEI VE Cataract  Bond d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I 1 (3) 3 (4) 3  2  n/a 5  74  n/a 3  9  n/a 5  1  n/a 3 (4)  1 (1)  1 (2) 2 (3)  2 (2)  0 3 (5)  1 (3)  1 (2) 3 (4)  2  0 Outcome Funders  Investors  PbR Comparator  sites  Advisors /  Intermediaries /  Performance  Managers  1  Service Providers  -  Other funders  1  Outcome Evaluator  1  DIB researchers  Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of  individuals interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out  the total number of organisations within this stakeholder category.\n",
            "Top  14  :   Strong / Weak Commercial / Social Intermediated Direct (OF and service provider)  /  and  investors);  Managed  (OF  and  intermediary)   Strong / Weak (OF of Strength  performance  management  system on Lead  managing  performance  Governance arrangements and level of involvement of stakeholders:  Outcome funder  Role  of High / Low Intermediary / service provider Investor High / Low Typologies  of  structure  depending on which actor  has  the  contract  with  the  outcome funder.   How  hands  on  are  the  other  stakeholders?  Is  dedicated  a  there  performance  management function?   Who  takes  the  lead  in  performance  management?\n",
            "Top  15  :   The cost of engaging outcome funders was a cost driver, largely borne by those responsible  for fundraising. In the case of ICRC, this was ICRC, with support from the Belgian and Swiss  governments. In the case of QEI, this was BAT, as the outcome convenor. In the case of VE,  Instiglio and the anonymous donor led the identification of outcome funders.\n",
            "Top  16  :   In the case of the VE, two out of the three outcome funders were sampled. We will  speak to the third outcome funder over the next two research waves. Additionally, VE  has a consortium of investors, and a purposive sample was taken. The research team  interviewed  the  lead  investor  and  a  secondary  investor  recommended  by  the  lead  investor.\n",
            "Top  17  :   •  Outcome funder engagement: Time was spent engaging with foundations that did not  result in any commitments to provide outcomes funding. Furthermore, engaging multiple  outcome funders at different times created inefficiency.\n",
            "Top  18  :   •  Design: It was stated that negotiations lacked clear protocols for ensuring the right levels  of inclusivity, which increased the amount of time it took, and therefore staff time required,  to finalise the design of the DIB in all stakeholders.\n",
            "Top  19  :   Programme  manager  and  Development  Impact Bonds Adviser  PbR comparator site  Investor  Investor  Investor  Performance manager  Intermediary  Intermediary  Outcome funder  Outcome funder  Outcome funder  Knowledge partner  Knowledge partner  Outcome evaluator  Service provider  Service provider  Service provider  Technical advisor  Funder/technical advisor  Funder/technical advisor  PbR Comparator site  Implementer Investor/Implementer  Outcome funder Outcome funder Outcome funder  Outcome funder Investor  Investor  DIB performance manager  DIB performance manager  DIB researchers A157 DIBS  Village Enterprise   Village Enterprise  Village Enterprise Organisation Village Enterprise  Village Enterprise  Instiglio  Instiglio Instiglio Role  VE Kenya Country Director  VE  Director  of  Monitoring,  Evaluation  and  Learning  Director of Institutional Giving   VE Chief Operating Officer  Instiglio – Leading the Process Evaluation  CEO  –  and  designer  of  DIB;  part  of  design  process  Instiglio  Project  Manager;  managed  design  process and kept record of discussions  Instiglio- developed financial model for DIB Instiglio  The Anonymous Donor  Co-designed the DIB model  Co-designed the DIB model  DFID  Co-designed the DIB model  DFID  Lead Investor  Anon  Secondary investor  Bridge Fund          Nepal Employment Fund  PbR Comparator site  Social Finance UK Cardano Development Convenor/manager Contributed to DIB design and development as  technical advisor Service provider  Outcome funder Technical  development) and performance manager advisor design (DIB and Technical advisor (DIB design) Outcome funder (DIB advisor Technical  and  development),  sub-contractor  of  the  project  implementation agency    Technical advisor (transaction management) design Volta Technical advisor (transaction management) Convergence Commissioned a feasibility study to KOIS Social Finance UK Technical advisor (DIB design) Cameroon  Kangaroo  Mother  Care DIB  Cook  and  Clean  DIB  Educate Girls DIB  Educate Girls  (Rajasthan)  India  Maternal  and  Newborn  Health  DIB  Mozambique  Malaria DIB  Palestine  (West  Bank  and  Gaza)  Employment DIB World Bank Palladium USAID Volta Social Finance Volta South  Africa  ECD  Bond  Impact  Innovation  Fund  –  Social  Development  South  Africa  ECD  Impact  Bond  Innovation  Fund  -  Health  Syrian  Refugees  Employment DIB  Uganda  Sleeping  Sickness DIB  Sources reviewed A158 DIBs ICRC QEI Cameroon  Cataract Document PRP HIB Efficiency Improvement Measures Project Final Execution Version of PHII PBR Agreement Signed by DFID (26/7/2017) Benchmark Data (5/8/2017) Q&A with DFID Verification agreement signed between ICRC and Philanthropy associates Final Detailed presentation 20/4/17 20/4/2017 Final ICRC HIB Program Description Final  Initial  Verification  Report  by  Philanthropy  Associates  confirming  baseline SER as 33.87 HIB Social Investor Presentation ICRC SER Ratio and how it compares to number of beneficiaries PHII Summary of the transaction Email KOIS/DFID discussion (17/5/17) 1st ORCM presentation, February 2018 1st Quarterly Status Update Jul - Sept 2017  2nd PHII Quarterly Status Update Oct - Dec 2017 3rd PHII Quarterly Status Update Jan - Mar 18 Agenda meeting 2018/2/27 PHII Summary of the transaction – updated 29/11/18  4th and 5th PHII Quarterly Status Update (Apr-Jun and July-Sept 2018)  Annex to HIB Report Phase 1 Netherlands Instructions on PRP data collection (Assort MSR 2015 instruction final and  Assort MSR 2017 template 20 centres) BAT India Technical Assistance Grant Proposal (December 2017)  British Asian Trust - DIB Quarterly DFID Report (April-June 2018)  Education DIB Fund Financial model (June 2018)  Education  DIB  Performance  Management  Annual  Report  Template  (April  2018)  Education DIB performance Management Overview Document (April 2018)  Gray  Matters.  Proposal  for  Outcome  Evaluator  Education  DIB  Fund  (February 2018)  GyanShala Education DIB Proposal Revised (2018)  KEF Education DIB Proposal (2018)  SARD Education DIB Proposal (2018)  Service  Providers’  Proposals  Consolidated  Summary  Document  (March  2018)  020818 Schedule 5 ME Verification Protocol updated  CCBP Pilot Verification Report 2018 07 27  Legal Structure  ONGOING REPORT_aug2018 – quality A159 DIBs Document  OPIC Q2  2018_Cataract Loan Reporting_Final  SteerCo Cataract Bond Report - July 2018.Final  Cataract Bond 2 page Summary_FINAL  Cataract Bond presentation  Cataract Bond_FAQs_July 2016  Cataract Bond Monitoring & Evaluation Protocol  Cameroon Cataract Performance Bond Application_FINAL  CCPB HF Grant application addendum  Value for Money data compiled by Volta  Village Enterprise DIB Design Memo, Nov 2017; Instiglio  GDI Activity Proposal for Village Enterprise  Village Enterprise DIB Process Review, July 2018; Instiglio  Paying for Poverty Alleviation; Richard Sedlmayer  CSAE Working Paper, Cash-Plus Poverty Impacts of Transfer based  intervention alleviation (RCT into Village Enterprise traditional model)    Gustafsson-Wright et al. (2017). Impact Bonds in developing countries:  early learnings from the field.  Save the Children (2018). Investing in Maternal and Child Health:  Development Impact Bonds.    Cook and Clean Development Impact Bond Concept Note.\n",
            "Top  20  :   •  Finally,  where  there  had  been  minimum  activity  on  the  part  of  the  outcome  evaluators/verifiers,  we  decided  to  not  consult  with  them  during  this  first  research  wave.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligible population\n",
            "Top  1  :   More incentivised to focus on target populations:   Evidence from the Employment Fund in  Nepal  (Chakravarty  et  al,  2016)  suggested that  specific targets for the  hard  to reach, such  as  greater payments for disadvantaged groups discouraged cherry picking and more focus on the  hard to reach populations.\n",
            "Top  2  :   A  minimum  of  12,660  households in Kenya and  Uganda  Local  representatives  and Uganda  DFID, USAID DIV and an  anonymous donor government  in  Kenya Michael  &  Susan  Dell  Foundation,  BT, Comic Relief, Mittal Foundation.\n",
            "Top  3  :   4.  Data to build up a business case, including data on the eligible cohort and outcomes likely to be achieved.\n",
            "Top  4  :   Intermediary  / Fiduciary Target  population •  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these  represent fixed or recurrent costs.\n",
            "Top  5  :   to individuals in the bottom  two wealth quintiles of the  population  in  Cameroon  by the end of year 5.\n",
            "Top  6  :   estimating in Stratification by school size, urban/rural  location and school type ensures that all  parts  of  the  population  are  included  in  the sample.\n",
            "Top  7  :   5.4.1 Targeting strategy The approach to equity will be guided by the individual programmes’ targeting strategies, to  understand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the  effectiveness of the targeting strategy of the DIB, especially in terms of the hard to reach. The  evaluation  will  look  at  how  well  the  programmes  are  fulfilling  their  targeting  strategy  and  whether there are certain sub-groups which are not being reached. At this stage, the following  strategies are understood to be in place: • ICRC – The HIB targets disabled people in a geographic area.\n",
            "Top  8  :   poverty  exclusion) and to services. The aim is to support  them  gain  mobility,  autonomy  and  dignity  so  that  they are able to become active  members  society.  of  Furthermore,  family  members  who  were  taking  care  of  them  will be able to work more, and  the  the  household  as  a  whole  can  increase its income.\n",
            "Top  9  :   •  VE – The intervention identifies individuals who live in extreme poverty and are unable  to  provide  for  their  family’s  basic  needs.  VE  assesses  poverty  levels  through  a  community-based Poverty Wealth Ranking exercise coupled with the Progress-out-of- Poverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment  Agreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the  financial model.\n",
            "Top  10  :   ICRC  HIB:  The  outcome  metric  is  based  on  people  benefitting  from  physical  rehabilitation  services.  However,  the  M&E  data  will  include  disaggregated  data  on  gender  and  age.  Furthermore, a key limitation with the verification process is that it is limited to those within  urban  areas,  accessible  by  the  verification  firm.  Over  the  next  two  research  waves,  the  evaluation team will assess the extent to which targeting may have been affected by use of  the  HIB,  and  the  extent  to  which  the  verification  process  will  be  affected  by  the  limited  geographical coverage.\n",
            "Top  11  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  12  :   49 Voluntary, community and social enterprise (VCSE) organisations and social investors  50 Reproductive health in Pakistan (Witter et al, 2016), RBA in Ethiopia (Cambridge Education, 2015) and Rwanda  (Upper Quartile, 2015), Sierra Leone’s Budget support program.\n",
            "Top  13  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  14  :   Annex E.5  Annex E.5 Annex E.5 and E.7 Annex E.5, E.7 and  E.9 4.2 4.3 4.4 4.5 4.7 4.8 4.9 4.10 4.11 5.1 5.2 5.3 5.4 A113 Ref  EQUALs Criteria 5.5  The evaluation process was transparent enough to ensure its legitimacy.\n",
            "Top  15  :   Sample  of  participating  households  in  Kenya  and  Uganda Sample  of  people  in  treatment schools young  the Cataract N/A Volta Capital  Aravind Foundation  Sample  of  patients  in  hospital  in  Cameroon  from  middle  and  low  income backgrounds RW1   RW2  RW3   Comparison  programmes x x x All  stakeholders Stakeholder ICRC VE QEI Cataract this  is VE  have  stated  that  data  available  and  can  be shared  with us.\n",
            "Top  16  :   d e w e v r e n t i I Table 2.3 Stakeholders consulted ICRC QEI VE Cataract  Bond d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I 1 (3) 3 (4) 3  2  n/a 5  74  n/a 3  9  n/a 5  1  n/a 3 (4)  1 (1)  1 (2) 2 (3)  2 (2)  0 3 (5)  1 (3)  1 (2) 3 (4)  2  0 Outcome Funders  Investors  PbR Comparator  sites  Advisors /  Intermediaries /  Performance  Managers  1  Service Providers  -  Other funders  1  Outcome Evaluator  1  DIB researchers  Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of  individuals interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out  the total number of organisations within this stakeholder category.\n",
            "Top  17  :   In parenthesis in this table under the ‘interviewed’ columns, we have included the number of  individuals interviewed. A full list of stakeholders interviewed is set out in Annex H. For the  most part, we sought to speak to all stakeholders, with the following exceptions: • • • In the  case  of  the  ICRC  HIB,  we  did  not  receive  responses  from  2  of  the  outcome  funders, nor from 1 investor.\n",
            "Top  18  :   Table 2.6: Limitations and mitigations Limitations Mitigations Generalisability  of  findings:  The  number  of  DIBs both within this evaluation and in the wider  sector is small and very varied, limiting the ability  to  make  generalisable  conclusions  about  the  effectiveness of DIBs.  Approach  to  causal  inference:  The  effect  of  using  a  DIB  is  not  quantified.  The  use  of  experimental  or  quasi-experimental  methods  in  order  to  claim  attribution  is  not  appropriate  in  these  contexts.  It  cannot  be  assumed  that  any  differences between the DIB and non-DIB areas  can be attributed to the DIB mechanism.  Limited  availability  of  cost  data:  The  cost  analysis is limited by the limited availability of cost  data,  including  in-kind  costs  such  as  staff  time,  limited  availability  of  comparable  and  benchmark data, to assess interest rates and the  risk and return alignment.\n",
            "Top  19  :   Section 1.4, Annex  E.1, E.2, E.3 and E.4,  and Annex F  Section 1.2 3.3  The product describes the target audience(s) for the evaluation.\n",
            "Top  20  :   The sampling strategy is described, and is appropriate. Primary and  secondary data sources are appropriate, adequate and reliable. Sample  sizes are adequate.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligibility criteria\n",
            "Top  1  :   Section 4  Sections 4, 5 and 6 A114 5.6 5.7 5.8 6.3 6.4 6.5 6.6 6.7 Corresponding  Section Ref  EQUALs Criteria 8. RECOMMENDATIONS 8.1  Recommendations follow logically from the findings and evidence cited.  8.2  They are relevant to the evaluation and targeted at the intended users.  They are prioritised and clearly presented, enabling individuals or  departments to follow up on each specific recommendation.\n",
            "Top  2  :   Annex E.5  Annex E.5 Annex E.5 and E.7 Annex E.5, E.7 and  E.9 4.2 4.3 4.4 4.5 4.7 4.8 4.9 4.10 4.11 5.1 5.2 5.3 5.4 A113 Ref  EQUALs Criteria 5.5  The evaluation process was transparent enough to ensure its legitimacy.\n",
            "Top  3  :   Section 1.2 and 2.2.3 A112 3.1 3.2 Ref  EQUALs Criteria 3.4  The product justifies the timing of the evaluation.\n",
            "Top  4  :   DIB. The criteria was PbR funded interventions working in similar sector, and,  where possible, similar geographies.\n",
            "Top  5  :   The product describes and justifies which evaluation criteria are applied  (e.g. OECD DAC). This includes discussion around which criteria were not  relevant for this evaluation.  The evaluation methods are described and justified. These methods are  appropriate for addressing the evaluation questions.  The methodology is appropriate for assessing the cross-cutting issues of  gender, poverty, human rights, HIV/AIDS, environment, anti-corruption,  capacity building, and power relations.\n",
            "Top  6  :   M.1.3  Criteria This  sub-section  explores  the  main  criteria  set  out  within  the  literature  as  necessary  for  the  effective  use  of  an  impact  bond.  Echoing  some  of  the  principles  above,  they  can  broadly  be  consolidated into five criteria: Analysis  of  the  SIB  evidence  seems  to  suggest  four  necessary  criteria  for  an  impact  bond  to  launch.\n",
            "Top  7  :   6.2  The analysis is presented against the evaluation questions and criteria.\n",
            "Top  8  :   Performance of team.   Personnel  with  appropriate  level  of expertise are available across  life of requirement.\n",
            "Top  9  :   The  OECD-DAC  criteria  on  relevance,  efficiency  and  effectiveness  are  relevant  to  this  evaluation.  The  evaluation  focuses  on  the  DIB  funding  mechanism,  and  the  process  of  designing DIBs including the relevance and efficiency of the activities involved in designing,  launching and managing a project using a DIBs model for the various stakeholders in the DIB;  and assesses how the DIB model improves (if at all) the performance and effectiveness of  development  programmes  in  terms  of  achieving  results  efficiently.  The  evaluation  should  consider  how  the  DIB  model  takes  into  account  cross-cutting  areas  that  mean  some  beneficiaries are more vulnerable or harder to reach (e.g. due to disability, power relations,  environment, gender, poverty).\n",
            "Top  10  :   Evaluation framework for the evaluation The two tables below set out the evaluation framework for the evaluation, which maps the two evaluation questions (EQ1 and EQ2) to the  OECD DAC criteria and evaluation sub-questions finalised during the inception phase. All the DAC criteria are relevant and will be applied over  the  course  of  the  evaluation.  The  evaluation  sub-questions  are  then  mapped  to  the  indicators  designed  during  the  inception  phase.  The  corresponding research waves in which these sub-questions will be covered are also marked. Annex E sets out the full evaluation framework,  which links the evaluation questions and sub-questions to the corresponding data collection method.\n",
            "Top  11  :   •  QEI – Poor schools were selected to participate in the programme.\n",
            "Top  12  :   6.4.1 Identifying and selecting stakeholders and managing relationships -  necessary conditions Summary:  The  necessary  conditions  for  DIBs  are  similar  to  SIBs  developed  in  high-income  countries,  and  involve  strong  leadership,  within  and  across  organisations.  Additionally,  stakeholders also need to have sufficient capacity and skills, as well as a willingness to adapt  and learn. Finally, in this early stage of the market, stakeholders with strong a reputation and  track records are needed to lend credibility to the DIBs.\n",
            "Top  13  :   ICRC  HIB:  The  outcome  metric  is  based  on  people  benefitting  from  physical  rehabilitation  services.  However,  the  M&E  data  will  include  disaggregated  data  on  gender  and  age.  Furthermore, a key limitation with the verification process is that it is limited to those within  urban  areas,  accessible  by  the  verification  firm.  Over  the  next  two  research  waves,  the  evaluation team will assess the extent to which targeting may have been affected by use of  the  HIB,  and  the  extent  to  which  the  verification  process  will  be  affected  by  the  limited  geographical coverage.\n",
            "Top  14  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  15  :   We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We  welcome efforts by the evaluator to find savings during the life of the evaluation.\n",
            "Top  16  :   Aligning DIB with organisational requirements Stakeholders involved in the DIBs have to ensure the terms of the DIB are aligned with their  organisation requirements and different methods were used to address this.\n",
            "Top  17  :   •  The evaluation design and implementation must meet standard ethical practices.\n",
            "Top  18  :   6.1.1 Identifying appropriate interventions - necessary conditions Summary: DIBs require clear and suitable outcomes and a shared understanding of the policy  problem. Additionally, there are other practical constraints on the type of interventions suitable,  such as the timeframe of the impact bond and the level of external risk factors.\n",
            "Top  19  :   6.5.1 Structuring the vehicle and developing the operating model - necessary  conditions Summary: Given the early stage of the market, organisations and legislative frameworks are  often unable to accommodate the DIB, resulting in the need to set up SPVs or ‘work arounds’  in the terms of the contracts that can deviate from what a ‘standard DIB’ looks like.\n",
            "Top  20  :   The  number  of  schools  selected  to  collect  data  shall  ensure  adequate  accuracy  variance  components  between  and  within  schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  cohort\n",
            "Top  1  :   26 The evaluation firm will conduct two instances of data collection, through which end line data will be collected from a sample  of households from each cohort. Baseline data collected by Village Enterprise may be used for creation of covariates to be  used during the analysis. Accordingly, each group of cohorts will have its own impact estimation based on which the trustee will  pay Village Enterprise. The RCT design is an improved version of the RCT performed between 2014 and 2017 to evaluate  Village Enterprise’s intervention in Uganda. The randomization will be made at the village level. The evaluator will randomly  assign the villages to receive the Village Enterprise program.\n",
            "Top  2  :   4.4.1.8  Comparison with other impact bonds Collaboration and co-ordination is a strong theme present across impact bonds, both in terms  of bringing together new partners, and strengthening pre-existing relationships. For example,  there are good examples in the UK where SIBs have brought very different partners together  as funders all interested in achieving similar outcomes (such as the local authority, schools  and philanthropists as outcome funders in the West London Zone SIB, or different government  departments in the Youth Engagement Fund).\n",
            "Top  3  :   ix Contents Acknowledgements and disclaimer .......................................... i Executive Summary ................................................................... ii  Recommendations ............................................................................................. viii  List of Tables ......................................................................................................... 3  Table of Figures .................................................................................................... 5 1.0 2.0 3.0 4.0 5.0 Introduction ................................................................. 1  Overview of the DIBS pilot programme ............................................. 1  Objectives of the Evaluation .............................................................. 4  Scope of the Research Wave 1 Report .............................................. 5  Overview of the Evaluation Process.................................................. 5  Report structure .................................................................................. 6 Evaluation Framework and Methodology .................. 8  Evaluation framework for the evaluation .......................................... 8  Overview of the methodology .......................................................... 12  Methodological limitations ............................................................... 18 Summary of the DIBs ................................................ 20  Programme components .................................................................. 22  Stakeholders involved in the DIBs .................................................. 26  DIB structures ................................................................................... 27  Conclusion ........................................................................................ 30 Analysis and Findings – DIB Effects ........................ 31  The DIB effect indicators .................................................................. 32  Presence of the DIB effect indicators: Summary............................ 34  Risk transfer effects ......................................................................... 37  Partnership effects ........................................................................... 40  Financing and funding effects ......................................................... 43  Design effects ................................................................................... 47  Other factors influencing the DIB effect .......................................... 51  Additional effects not identified in the framework ......................... 52  Conclusions ...................................................................................... 52 Analysis and Findings – Costs of designing and  delivering DIBs .......................................................... 55  Economy ........................................................................................... 57 6.0 Efficiency ........................................................................................... 68  Effectiveness ..................................................................................... 68  Equity................................................................................................. 72  Conclusion ........................................................................................ 73 Analysis and Findings – Improving the process of  designing and agreeing DIBs ................................... 75  Identifying appropriate interventions .............................................. 77  Identifying metrics and structuring payments ................................ 81  Measuring impact ............................................................................. 85  Identifying and selecting stakeholders and managing relationships  ........................................................................................................... 87  Structuring the vehicle and developing the operating model ........ 95  Conclusion ........................................................................................ 97 7.0 Lessons .................................................................... 101 8.0 Recommendations .................................................. 104  Recommendations to all DIB stakeholders ................................... 104  Recommendations to DIB designers ............................................. 104 Annex A: Case study reports .................................................... 1 Annex B: Terms of Reference ................................................. 81  Background and Context ................................................................. 81  B.1  What do we mean by other aid mechanisms? ................................ 82  B.2  How strong is the evidence on DIBs? ............................................. 83  B.3  What is the DFID DIBs pilot programme? ....................................... 84  B.4  Users of the Evaluation .................................................................... 85  B.5  Evaluation Methodology ................................................................... 90  B.6  Data Sources ..................................................................................... 91  B.7  Evaluation Outputs and Timeframe ................................................. 93  B.8  Lighter-Touch Interim Outputs ........................................................ 95  B.9  Evaluation Management Team ......................................................... 97  B.10 Annex C: Bibliography .......................................................... 106 Annex D: EQUALs criteria mapped to report sections ....... 112 Annex E: Evaluation methodology ....................................... 116  Evaluation Framework .................................................................... 117  E.1  DIB-level research........................................................................... 124  E.2 E.3  E.4  E.5  E.6  E.7 Programme-level Research ............................................................ 135  Sector-level Research .................................................................... 135  Approach to data collection ........................................................... 136  Analysis, Reporting and Dissemination ........................................ 137  Involvement of stakeholders .......................................................... 140 Annex F: Individual DIB level plans ...................................... 144 Annex G: Data Quality Assessment ..................................... 149  ICRC ................................................................................................. 149  G.1  QEI ................................................................................................... 150  G.2  Village Enterprise............................................................................ 152  G.3   Cameroon Cataract Bond ............................................................... 154  G.4 Annex H: Consultees and Sources reviewed ...................... 157 Annex I: Framework for categorising DIBs .......................... 162 Annex J: DIBs reviewed as part of programme level consultations ........................................................... 164 Annex K: Learning workshop note ....................................... 172 Annex L: VfM Analysis – Supporting Evidence ................... 174  ICRC ................................................................................................. 174  L.1  Quality Education India DIB ........................................................... 174  L.2  VE DIB.............................................................................................. 175  L.3  Cameroon Cataract Bond ............................................................... 175  L.4 Annex M: Literature Review .................................................. 177  Hypothesised effects of DIBs ........................................................ 177  a.  Input ................................................................................................. 189  b.  Recommendations .......................................................................... 196  c.  What approaches have been used to evaluate impact bonds? What  d.  are the main challenges and solutions? ....................................... 198 Annex N: List of Acronyms ................................................... 201 List of Tables Table 2.1: Evaluation Framework – EQ1 ............................................... 8 Table 2.2: Evaluation Framework – EQ2 ............................................. 10  Table 2.3 Stakeholders consulted ....................................................... 13  Table 2.4 Comparator Sites ................................................................ 14  Table 2.5: Deliverables mapped to target audiences........................... 17  Table 2.6: Limitations and mitigations ................................................. 18  Table 3.1: Programme components .................................................... 22  Table 3.2: Key stakeholders ................................................................ 26  Table 3.3: DIBs against DIB dimensions ............................................. 27  Table 4.1: DIB effect indicators ........................................................... 33  Table 4.2: Presence of DIB effect indicators in the four DIB projects .. 35  Table 6.1: Project focus and measurement approach ......................... 87  Table 6.2: Advantages and disadvantages to different approaches to  identifying and engaging with stakeholders ......................................... 91 Table B.1: Alternative aid mechanism ................................................. 82  Table B.2 EO 1: Inception Report ....................................................... 93  Table B.3: EO2 – Evaluation Report on the Process of designing and  launching DIBs .................................................................................... 93  Table B.4: EO3 –  Mid-Term Evaluation Report on DIBs ..................... 94  Table B.5: EO4 – Final Evaluation Report on DIBs ............................. 94  Table B.6: Good Performance Indicators ............................................ 98 Table E.1: Evaluation Framework ..................................................... 117  Table E.2: DIB effects and indicators ................................................ 122  Table E.3: Stakeholder consultations in RW1 ................................... 125  Table E.4: Research Waves ............................................................. 131  Table E.5: VfM Framework ............................................................... 131  Table E.6: VfM Indicators .................................................................. 132  Table E.7: Costing Structure ............................................................. 133  Table E.8: Communication Plan ........................................................ 140 Table F.1 : Proposed consultations ................................................... 144  Table F.2: Value for Money data ....................................................... 145  Table F.3: Other data ........................................................................ 147 Table M.1: Sources consulted ........................................................... 178  Table M.2: Impact bond principles .................................................... 182  Table M.3: Categorisation of SIBs by level of innovation ................... 185  Table M.4: Challenges of designing impact bonds ............................ 196 Table of Figures Figure 1.1: DIBs pilot programme theory of change .............................. 3  Figure 4.1: DFID risk assessment of three DIBs ................................. 38 Figure M.1: Framework for synthesising evaluation evidence ........... 177  Figure M.2: Strengths and weaknesses of existing evidence and  evaluation approaches and methods related to SIBs and DIBs (Drew  and Clist 2015:27) ............................................................................. 199 1.0  Introduction Overview of the DIBS pilot programme 1.1.1 DIBs and the current stage of the market.\n",
            "Top  4  :   In terms of the lessons learned: •  Discussion focused on the importance of clearly defining roles and responsibilities at the  outset, and take advantage of the process of co-design that characterises the DIB, in terms  of shared experience and shared learning.\n",
            "Top  5  :   Stakeholder type RW2  RW3 ICRC Village Enterprise QEI Cataract Table F.1 : Proposed consultations managers Project  performance  managers  intermediaries /  / x x n/a provider:  Project x x Service  managers/service  managers/practitioners Outcome  /  donors  funders  (including  DFID  and  other  donors) x x Investors Outcomes verification agents  process  level  Project  evaluators / learning partners  National  district/local  governments and x x  x x x x  x x Instiglio (Project Manager,  Process Learning lead,  CEO, Financial Model  Developer)  Director of MEL; Kenya  and Uganda country  Director, CEO, COO DFID, USAID, the  Anonymous Donor Group of private family  foundations and SV2, via  ImpactAssets  IDInsight  Instiglio TBC Dalberg  (Performance  manager) Volta Capital (bond  manager) Gyan Shala, SARD,  Kaiyvala Education  Foundation The Magrabi  Foundation British Asian Trust,  Tata Trusts, MSDF,  Comic Relief, USB OF The Fred Hollows  Foundation, Conrad  N. Hilton Foundation  and Sightsavers  OPIC and Netri  Foundation Gray Matters India  N/A AEDES  N/A N/A Regional  governments in the  states where the  service providers are  operating PRP Lead, Director of  Finance, HIB Head, Staff at  the 3 HIB centres and  identified comparison  centres  Governments of  Switzerland, Belgium, UK  and Italy, and La Caixa  Foundation  Munich Re, Lombard Odier  pension fund, charitable  foundations and others   Philanthropy Associates  N/A Local Governments in Mali,  DRC, and Nigeria A144 Stakeholder type RW2  RW3 ICRC Village Enterprise Local  organisations  that  work  with the project  Advisors (designers) Service users / beneficiaries x x x x x x Ministry of Health in  countries of operation  KOIS TBC N/A QEI N/A Dalberg Sample  of  users  in  new  ICRC centres, and the 8 pilot  centres.\n",
            "Top  6  :   Programme  manager  and  Development  Impact Bonds Adviser  PbR comparator site  Investor  Investor  Investor  Performance manager  Intermediary  Intermediary  Outcome funder  Outcome funder  Outcome funder  Knowledge partner  Knowledge partner  Outcome evaluator  Service provider  Service provider  Service provider  Technical advisor  Funder/technical advisor  Funder/technical advisor  PbR Comparator site  Implementer Investor/Implementer  Outcome funder Outcome funder Outcome funder  Outcome funder Investor  Investor  DIB performance manager  DIB performance manager  DIB researchers A157 DIBS  Village Enterprise   Village Enterprise  Village Enterprise Organisation Village Enterprise  Village Enterprise  Instiglio  Instiglio Instiglio Role  VE Kenya Country Director  VE  Director  of  Monitoring,  Evaluation  and  Learning  Director of Institutional Giving   VE Chief Operating Officer  Instiglio – Leading the Process Evaluation  CEO  –  and  designer  of  DIB;  part  of  design  process  Instiglio  Project  Manager;  managed  design  process and kept record of discussions  Instiglio- developed financial model for DIB Instiglio  The Anonymous Donor  Co-designed the DIB model  Co-designed the DIB model  DFID  Co-designed the DIB model  DFID  Lead Investor  Anon  Secondary investor  Bridge Fund          Nepal Employment Fund  PbR Comparator site  Social Finance UK Cardano Development Convenor/manager Contributed to DIB design and development as  technical advisor Service provider  Outcome funder Technical  development) and performance manager advisor design (DIB and Technical advisor (DIB design) Outcome funder (DIB advisor Technical  and  development),  sub-contractor  of  the  project  implementation agency    Technical advisor (transaction management) design Volta Technical advisor (transaction management) Convergence Commissioned a feasibility study to KOIS Social Finance UK Technical advisor (DIB design) Cameroon  Kangaroo  Mother  Care DIB  Cook  and  Clean  DIB  Educate Girls DIB  Educate Girls  (Rajasthan)  India  Maternal  and  Newborn  Health  DIB  Mozambique  Malaria DIB  Palestine  (West  Bank  and  Gaza)  Employment DIB World Bank Palladium USAID Volta Social Finance Volta South  Africa  ECD  Bond  Impact  Innovation  Fund  –  Social  Development  South  Africa  ECD  Impact  Bond  Innovation  Fund  -  Health  Syrian  Refugees  Employment DIB  Uganda  Sleeping  Sickness DIB  Sources reviewed A158 DIBs ICRC QEI Cameroon  Cataract Document PRP HIB Efficiency Improvement Measures Project Final Execution Version of PHII PBR Agreement Signed by DFID (26/7/2017) Benchmark Data (5/8/2017) Q&A with DFID Verification agreement signed between ICRC and Philanthropy associates Final Detailed presentation 20/4/17 20/4/2017 Final ICRC HIB Program Description Final  Initial  Verification  Report  by  Philanthropy  Associates  confirming  baseline SER as 33.87 HIB Social Investor Presentation ICRC SER Ratio and how it compares to number of beneficiaries PHII Summary of the transaction Email KOIS/DFID discussion (17/5/17) 1st ORCM presentation, February 2018 1st Quarterly Status Update Jul - Sept 2017  2nd PHII Quarterly Status Update Oct - Dec 2017 3rd PHII Quarterly Status Update Jan - Mar 18 Agenda meeting 2018/2/27 PHII Summary of the transaction – updated 29/11/18  4th and 5th PHII Quarterly Status Update (Apr-Jun and July-Sept 2018)  Annex to HIB Report Phase 1 Netherlands Instructions on PRP data collection (Assort MSR 2015 instruction final and  Assort MSR 2017 template 20 centres) BAT India Technical Assistance Grant Proposal (December 2017)  British Asian Trust - DIB Quarterly DFID Report (April-June 2018)  Education DIB Fund Financial model (June 2018)  Education  DIB  Performance  Management  Annual  Report  Template  (April  2018)  Education DIB performance Management Overview Document (April 2018)  Gray  Matters.  Proposal  for  Outcome  Evaluator  Education  DIB  Fund  (February 2018)  GyanShala Education DIB Proposal Revised (2018)  KEF Education DIB Proposal (2018)  SARD Education DIB Proposal (2018)  Service  Providers’  Proposals  Consolidated  Summary  Document  (March  2018)  020818 Schedule 5 ME Verification Protocol updated  CCBP Pilot Verification Report 2018 07 27  Legal Structure  ONGOING REPORT_aug2018 – quality A159 DIBs Document  OPIC Q2  2018_Cataract Loan Reporting_Final  SteerCo Cataract Bond Report - July 2018.Final  Cataract Bond 2 page Summary_FINAL  Cataract Bond presentation  Cataract Bond_FAQs_July 2016  Cataract Bond Monitoring & Evaluation Protocol  Cameroon Cataract Performance Bond Application_FINAL  CCPB HF Grant application addendum  Value for Money data compiled by Volta  Village Enterprise DIB Design Memo, Nov 2017; Instiglio  GDI Activity Proposal for Village Enterprise  Village Enterprise DIB Process Review, July 2018; Instiglio  Paying for Poverty Alleviation; Richard Sedlmayer  CSAE Working Paper, Cash-Plus Poverty Impacts of Transfer based  intervention alleviation (RCT into Village Enterprise traditional model)    Gustafsson-Wright et al. (2017). Impact Bonds in developing countries:  early learnings from the field.  Save the Children (2018). Investing in Maternal and Child Health:  Development Impact Bonds.    Cook and Clean Development Impact Bond Concept Note.\n",
            "Top  7  :   Instiglio and the Anonymous  Donor The  Cataract  Bond  Design  Coalition,  which  is  formed  of  The  Fred  Hollows  Foundation,  the  Conrad  N.  Hilton  Foundation,  Sightsavers,  the  African  Eye Foundation and Volta Capital  Africa Eye Foundation (AEF), the not- for-profit arm of the Magrabi ICO  Cameroon Eye Institute (MICEI) 18,000  low-income  patients  and  middle-income  patients  with  cataracts  in urban and rural areas in Cameroon Service Provider ICRC Service Users Gyan Shala, Kaivalya Education  Foundation, SARD (Society for All  Round Development)     300,000  primary  school  children  in  Delhi and Gujarat.\n",
            "Top  8  :   49 Voluntary, community and social enterprise (VCSE) organisations and social investors  50 Reproductive health in Pakistan (Witter et al, 2016), RBA in Ethiopia (Cambridge Education, 2015) and Rwanda  (Upper Quartile, 2015), Sierra Leone’s Budget support program.\n",
            "Top  9  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  10  :   •  QEI – Poor schools were selected to participate in the programme.\n",
            "Top  11  :   d e w e v r e n t i I Table 2.3 Stakeholders consulted ICRC QEI VE Cataract  Bond d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I 1 (3) 3 (4) 3  2  n/a 5  74  n/a 3  9  n/a 5  1  n/a 3 (4)  1 (1)  1 (2) 2 (3)  2 (2)  0 3 (5)  1 (3)  1 (2) 3 (4)  2  0 Outcome Funders  Investors  PbR Comparator  sites  Advisors /  Intermediaries /  Performance  Managers  1  Service Providers  -  Other funders  1  Outcome Evaluator  1  DIB researchers  Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of  individuals interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out  the total number of organisations within this stakeholder category.\n",
            "Top  12  :   In parenthesis in this table under the ‘interviewed’ columns, we have included the number of  individuals interviewed. A full list of stakeholders interviewed is set out in Annex H. For the  most part, we sought to speak to all stakeholders, with the following exceptions: • • • In the  case  of  the  ICRC  HIB,  we  did  not  receive  responses  from  2  of  the  outcome  funders, nor from 1 investor.\n",
            "Top  13  :   PbR funded programme  World Bank Global Partnership  on Output-Based Aid   Girls Education Challenge Helvetas livelihood programme Due to the late engagement of the Cameroon Cataract Bond, no comparator sites have yet  been identified, although there are potential sites which are being discussed with stakeholders.  Additional work will be undertaken as part of the KiT review, in order to undertake comparative  analysis between the Cameroon Cataract Bond and its comparator sites. Further detail is set  out below.\n",
            "Top  14  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  15  :   E.6.3.3  Communication Plan In the inception report, we undertook a stakeholder analysis, which categorised stakeholders  into  primary  users  (DFID),  secondary  users  (stakeholders  involved  in  the  pilot  DIBs)  and  tertiary users (those involved in other DIBs or SIBs or considering implementation of DIBs or  SIBs).\n",
            "Top  16  :   IMC Worldwide, Ideas to Impact, DFID, USAID and Rockfeller Foundation (2017). Innovating  in development: Sharing learning, improving impact (Workshop report).\n",
            "Top  17  :   A156 Annex H: Consultees and Sources reviewed Consultees DIBS  ICRC QEI Cataract Bond KOIS Organisation  ICRC Role  Head of the HIB  Partner,  Principal  –  Impact  investing,  Senior  Associate  Executive Director, Capital Relief Transactions  Programme  Officer,  Federal  Department  of  Foreign  Affairs  FDFA,  Swiss  Agency  for  Development and Cooperation SDC  Government of Belgium  Advisor, Development Cooperation Government  Switzerland Munich Re of DFID World Bank GPOBA  UBSOF  UBSOF  UBSOF  Dalberg  BAT  BAT  MSDF  Comic Relief  Comic Relief  Tata Trust  Tata Trust  GMI  GyanShala  Kaivalya  SARD  EducateGirls  DFID  DFID  DFID GEC  AEF/  Foundation  The Magrabi Foundation  Conrad  Hilton  N.  Foundation  Conrad  Foundation  Sightsavers  The  Fred  Foundation  OPIC  Netri Foundation  Volta  Volta  CGD The  Magrabi Hollows Hilton N.\n",
            "Top  18  :   estimating in Stratification by school size, urban/rural  location and school type ensures that all  parts  of  the  population  are  included  in  the sample.\n",
            "Top  19  :   The evaluation team also undertook sub-analysis to disaggregate the data to show differences  between groups. The team examined the extent to which key findings differ between the three  DIBs,  and  whether  different  stakeholder  groups  have  different  experiences  of  the  DIB  mechanism.\n",
            "Top  20  :   o  Secondly,  we  identified  programmes  working  in  similar  sector  and  contexts,  funded under payment by results. One PbR comparator site was identified per 4 Of the seven investors, there is one cornerstone investor and one placement intermediary that identified the  other five investors. The one investor consulted represents over 50% of the total investment.\n",
            "\n",
            "\n",
            "\n",
            "Query:  clients\n",
            "Top  1  :   Foundation.  Service providers: Washington- based Population Services  International (PSI) and the  Rajasthan NGO Hindustan Latex  Family Planning Promotion Trust  (HLFPPT).  Intermediary:  Palladium.  Performance manager:  Palladium.   Technical assistance providers:  Reed Smith (Pro bono legal counsel);   Outcome evaluator:  Mathematica Policy Research  (MPR).\n",
            "Top  2  :   •  This was the first DIB that the service provider was involved with. In order to understand  the  process  fully  they  needed  both  legal  and  accounting  consultancy  support  to  help  structure the finance and set up the SPV. Stakeholders from VE said this was a lot of work  and time (over 100 hours of legal and accountancy support).\n",
            "Top  3  :   A156 Annex H: Consultees and Sources reviewed Consultees DIBS  ICRC QEI Cataract Bond KOIS Organisation  ICRC Role  Head of the HIB  Partner,  Principal  –  Impact  investing,  Senior  Associate  Executive Director, Capital Relief Transactions  Programme  Officer,  Federal  Department  of  Foreign  Affairs  FDFA,  Swiss  Agency  for  Development and Cooperation SDC  Government of Belgium  Advisor, Development Cooperation Government  Switzerland Munich Re of DFID World Bank GPOBA  UBSOF  UBSOF  UBSOF  Dalberg  BAT  BAT  MSDF  Comic Relief  Comic Relief  Tata Trust  Tata Trust  GMI  GyanShala  Kaivalya  SARD  EducateGirls  DFID  DFID  DFID GEC  AEF/  Foundation  The Magrabi Foundation  Conrad  Hilton  N.  Foundation  Conrad  Foundation  Sightsavers  The  Fred  Foundation  OPIC  Netri Foundation  Volta  Volta  CGD The  Magrabi Hollows Hilton N.\n",
            "Top  4  :   Risk Management Financial  Management Robust cost control in line with contract.    Accurate  and  forecasting and invoices.\n",
            "Top  5  :   Disclaimer This  report  has  been  prepared  by  Ecorys  for  DFID,  for  services  specified  in  the  Terms  of  Reference and contract of engagement.\n",
            "Top  6  :   Stakeholder type RW2  RW3 ICRC Village Enterprise QEI Cataract Table F.1 : Proposed consultations managers Project  performance  managers  intermediaries /  / x x n/a provider:  Project x x Service  managers/service  managers/practitioners Outcome  /  donors  funders  (including  DFID  and  other  donors) x x Investors Outcomes verification agents  process  level  Project  evaluators / learning partners  National  district/local  governments and x x  x x x x  x x Instiglio (Project Manager,  Process Learning lead,  CEO, Financial Model  Developer)  Director of MEL; Kenya  and Uganda country  Director, CEO, COO DFID, USAID, the  Anonymous Donor Group of private family  foundations and SV2, via  ImpactAssets  IDInsight  Instiglio TBC Dalberg  (Performance  manager) Volta Capital (bond  manager) Gyan Shala, SARD,  Kaiyvala Education  Foundation The Magrabi  Foundation British Asian Trust,  Tata Trusts, MSDF,  Comic Relief, USB OF The Fred Hollows  Foundation, Conrad  N. Hilton Foundation  and Sightsavers  OPIC and Netri  Foundation Gray Matters India  N/A AEDES  N/A N/A Regional  governments in the  states where the  service providers are  operating PRP Lead, Director of  Finance, HIB Head, Staff at  the 3 HIB centres and  identified comparison  centres  Governments of  Switzerland, Belgium, UK  and Italy, and La Caixa  Foundation  Munich Re, Lombard Odier  pension fund, charitable  foundations and others   Philanthropy Associates  N/A Local Governments in Mali,  DRC, and Nigeria A144 Stakeholder type RW2  RW3 ICRC Village Enterprise Local  organisations  that  work  with the project  Advisors (designers) Service users / beneficiaries x x x x x x Ministry of Health in  countries of operation  KOIS TBC N/A QEI N/A Dalberg Sample  of  users  in  new  ICRC centres, and the 8 pilot  centres.\n",
            "Top  7  :   All stakeholders confirmed there had been additional costs - either actual, in kind or pro bono  – for staff time and consultancy in designing and setting up the DIBs. These costs tend to  be incurred by outcome funders and service providers. They relate mainly to the investor  returns that will be paid (either by the outcome funder or service providers).\n",
            "Top  8  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  9  :   In the next two research waves, we envisage speaking directly with beneficiaries. We will work  closely  with  our  peer  reviewer,  our  local  researchers  and  the  service  providers,  in  order  to  ensure our research is conducted in an ethically appropriate manner.\n",
            "Top  10  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  11  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  12  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  13  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  14  :   •  Formed  of:  Representatives  of  the  stakeholders  involved  in  each  of  the  3  DIBs  –  including the service providers: ICRC and Village Enterprise; other donors e.g. USAID,  Belgium,  Switzerland,  British  Asian  Trust,  MSDF;  investors  e.g.  UBS  Optimus  Foundation;  and  involved  project  managers  such  as  Instiglio,  the  DFID  DIBs  team,  DFID PbR Advisor, and DFID Evaluation Advisor.\n",
            "Top  15  :   suggests that whilst DIBs enable some service providers to be included in PbR contracts, they  themselves still experience barriers that may limit their participation in a project.\n",
            "Top  16  :   The Fred Hollows Foundation, Conrad  N. Hilton Foundation, Sightsavers UBS  Optimus  Foundation  leads  an  investment  pool  of  multiple  private  investors.\n",
            "Top  17  :   •  Cataract DIB – The financial model works through cross sub-subsidisation, and  there is a specific equity target, to provide 40 percent of surgeries to individuals  belonging to the bottom two wealth quintiles of the population in Cameroon.\n",
            "Top  18  :   •  Need for risk transfer too great to be borne by service providers.\n",
            "Top  19  :   2.2.3 Reporting and dissemination As part of the inception phase, we undertook an analysis of stakeholders, and identified the  three  types  of  users:  DFID  stakeholders,  stakeholders  involved  in the  pilot  DIBs  and those  interested in DIBs and/or SIBs. The reporting and communication outputs have been designed  with these stakeholders in mind. The table below maps the deliverables to the targeted users.  This is followed by a brief description of each type of deliverable.\n",
            "Top  20  :   received  a  grant from the other As  stakeholders  committed  to  the  DIB,  outcome  funders  USAID-DIV,  (DFID,  the  anonymous  and  contributed  Donor)  funds  Instiglio  to  support  to  finalisation  of the project design.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  2  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  3  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  4  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  5  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  6  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  7  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Design finalisation and  stakeholder engagement )  Consultancy fees for setting up  DIB (Reaching Execution  readiness including field trip) Legal and financial advice SPV set-up direct costs Implementation Costs Not  estimated Not  estimated 158,000 158,000 - - 86,300 86,300 104,804 - 65,000 169,804 Not  estimated Not  estimated 126,000 1,986 126,000   1,986 Budgeted implementation costs relating to the use of the DIB are set out in the table below.  Contract management costs cover additional grant management, financial management and  reporting requirements relating to the use of the DIB. The verification costs excludes the USD  70,915  costs  for  the  process  evaluation,  which  is  not  an  essential  component  of  the  DIB.  Village Enterprise also have their own verification process, separate from the one delivered  by IDInsight which will incur a cost. This has not been estimated and will be revisited in the  following research waves.\n",
            "Top  9  :   •  Legal and financial advice – this was a common cost which was often provided pro- bono at least in part by professional firms 2.  Implementation costs: These include additional performance management, project  management and reporting time, on top of what would have normally been spent on a  grant-funded project. This also includes costs of verifying the outcome targets and, for  some DIBs, escrow costs. Costs can be split into the following three categories: •  Contract management including performance management, project management and reporting •  Verification costs manager costs •  Costs related to the DIB transaction, such as escrow, legal fees and transaction As implementation is still underway, our estimate of implementation costs relating to  the DIB is based on expected costs, as identified through budgets and discussions  with stakeholders.\n",
            "Top  10  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  11  :   It was not possible to estimate the time spent on these activities. The activities for which we  do have costs for are those where advisors were contracted, including advice on contract  design and legal costs. These are set out in the table below.\n",
            "Top  12  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  13  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  14  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  15  :   Cost  categories Costs (including actual, budgeted,  in-kind and pro-bono)  Otherwise,  stakeholders  described  the significant time commitment e.g.  staff time over two years.\n",
            "Top  16  :   expertise costs, and the  risk  premium  (return  to  investors,  including  interest) (Clist 2017).   This  should  cover  the  full  cost, including staff time not  charged, of all actors.   Where possible, this will be  disaggregated by ‘first time’  which  DIB  costs  hypothetically  wouldn’t  have  to  be  incurred  again  for any subsequent DIBs.38  Cost drivers to be analysed  which  to  elements of the DIB are the  most  time- intensive/expensive.   programme  Savings  costs  (including  staff  time)  as  a  result  of  the  impact  bond.\n",
            "Top  17  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  18  :   The  type  of  additional  costs  incurred  during  the  set-up  and  design  phase  can  be  described under three categories: •  Staff  time  –  this  was  provided  largely  ‘in-kind’  by  stakeholders  using  their  own  existing resources, unless staff time was covered by a separate grant (e.g. DFID  technical assistance grant for QEI and Government of the Netherlands for ICRC).\n",
            "Top  19  :   Design and set up costs Stakeholders from both VE and Instiglio commented on the increased cost at the design and  set-up stage of the DIB in the form of staff costs. VE estimated a total of 2160 hours spent on  DIB design and structuring and Outcome Payment Agreement (OPA) negotiation and 1058  hours on investment fundraising and structuring. This staff time was provided in-kind.\n",
            "Top  20  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  4  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  5  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  6  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  7  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  8  :   5.1.2.4 Additional DIB costs for Cataract DIB The maximum committed outcome funding for the Cataract DIB is USD 3.5 million, of which  USD 2.8 million relate to outcome payments.\n",
            "Top  9  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  10  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  11  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  12  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  13  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.  In one  case  performance  management  costs  are  (QEI)  co-funded  by  investor.  Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  14  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  15  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  16  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  17  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  18  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  19  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  20  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  3  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  4  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  5  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  6  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  7  :   i)  ii)  iii)  iv) i) ii) DIBs are understood by DFID as one type of payments by results (PbR), or a type of funding  whereby payments are made after the achievement of pre-agreed outcomes (DFID, 2014). In  a standard PbR contract, there are four actors: an outcome funder who funds the outcomes;   the service provider delivering the intervention;   the target population, benefiting from the services; and  a validating agency that validates the results on which the payments are based.\n",
            "Top  8  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  9  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  10  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  11  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  12  :   32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government’s  use  of  payment  by  results  https://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of- payment-by-results.pdf   33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts.  Dalberg  https://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by- results-contracts Intelligence A83 • • Impact bonds have enabled the development of strong monitoring and  evaluation systems: the impact bond mechanism incentivises evidence collection  and can therefore lead to improving outcomes for service users through identifying  interventions that work.   Impact bonds can shift the focus of government toward preventive services:  this could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and  we have to be mindful that the portfolio of SIBs projects target different outcomes, emerging  evidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve  effectiveness and efficiency of outcome delivery, and generate valuable impact evidence.\n",
            "Top  13  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  14  :   research indicates Payment  by  Results  approaches  enable  donors  to  transfer the risk/uncertainty over whether an intervention  will achieve results to the provider.   However,  that  some  providers  (particularly  those  with  smaller  balance  sheets,  or  less  access to commercial loans) would be unable pre-finance  their  intervention  and  wait  for  payment  on  delivery  of  results, or would be unwilling to take on the financial risk  associated with underperforming on a PbR contract. As a  result providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of  contracts.3233 B.3  How strong is the evidence on DIBs?\n",
            "Top  15  :   3.  Maximum payments to investor: This includes the maximum return payable to the  investor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any  interest payment.\n",
            "Top  16  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  17  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  18  :   Outcome Funder: World Bank  Group.   Investors: Five investors from  Chile, Palestine, Holland, Britain  and Switzerland, which have not  signed yet.  Service providers: 2-4  Palestinian, not-for profit service  providers to be selected per cycle, Payment terms: The  investors will form a SPV  for the flow of funds, with a  DIB manager (contracted  by the PIA) who will  manage the SPV funds on  behalf of the investors.   Outcome metric: Likely to  be a mixture of training The outcome funds are  USD 5 million. The DIB is  part of a wider World  Bank project called  Finance for Jobs, a larger  initiative to create  employment in West Bank  and Gaza.\n",
            "Top  19  :   Maximum payments to investors The maximum payments to investors is the cost which seems to be most clearly additional  compared to similar programmes, and the ones which are most clearly proportional to the size  of  the  maximum  committed  outcome  funding.  Annualised  interest  rates  provide  the  most  commonly used comparison of returns. As expected, the highest maximum return is for the  ICRC HIB, and the lowest, for the Cataract DIB, corresponding to the respective sizes of the  DIBs. The cost of the payments to investors is borne mainly by outcome funders, though there  are exceptions; in the case of underperformance in the ICRC HIB and Cataract  DIB, ICRC  and AEF are respectively liable for some of this repayment.\n",
            "Top  20  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  2  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  3  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  4  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  5  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  6  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  7  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  8  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  9  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  10  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  11  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  12  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  13  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  14  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  15  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  16  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  17  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  18  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  19  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  20  :   34 For example, input based grants and pay for service contracts or standard payment by results.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  4  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  5  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  6  :   5.1.2.4 Additional DIB costs for Cataract DIB The maximum committed outcome funding for the Cataract DIB is USD 3.5 million, of which  USD 2.8 million relate to outcome payments.\n",
            "Top  7  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  8  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  9  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  10  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  11  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  12  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  13  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  14  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  15  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  16  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  17  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  18  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  19  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  20  :   Payment terms: TBC  Outcome metric: To be  finalized. Likely to include:  a) number of hospitals  attaining quality KMC  prerequisites; b) number of  infants receiving quality  KMC services; c) number  or % of infants achieving  target nutritional  status/weight at 40 weeks  gestational age and/or at  follow-up.  Range of returns: TBC The planned operating  budget USD 2.1 million,  to be spent over three-to  four years. Total outcome  commitment of USD 2.8  million. Upfront capital  commitment: USD 3.0  million (pre-capital  recycling).  Additional grants  (covering feasibility study,  baseline data study, DIB  design and structuring,  data systems design,  legal advice) USD 1  million.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  2  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  3  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  4  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  5  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  6  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  7  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  8  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  9  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  10  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  11  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  12  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  13  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  14  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  15  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  16  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  17  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  18  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  19  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  20  :   Payment terms: TBC  Outcome metric: To be  finalized. Likely to include:  a) number of hospitals  attaining quality KMC  prerequisites; b) number of  infants receiving quality  KMC services; c) number  or % of infants achieving  target nutritional  status/weight at 40 weeks  gestational age and/or at  follow-up.  Range of returns: TBC The planned operating  budget USD 2.1 million,  to be spent over three-to  four years. Total outcome  commitment of USD 2.8  million. Upfront capital  commitment: USD 3.0  million (pre-capital  recycling).  Additional grants  (covering feasibility study,  baseline data study, DIB  design and structuring,  data systems design,  legal advice) USD 1  million.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  4  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  5  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  6  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  7  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  8  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  9  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  10  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  11  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  12  :   32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government’s  use  of  payment  by  results  https://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of- payment-by-results.pdf   33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts.  Dalberg  https://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by- results-contracts Intelligence A83 • • Impact bonds have enabled the development of strong monitoring and  evaluation systems: the impact bond mechanism incentivises evidence collection  and can therefore lead to improving outcomes for service users through identifying  interventions that work.   Impact bonds can shift the focus of government toward preventive services:  this could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and  we have to be mindful that the portfolio of SIBs projects target different outcomes, emerging  evidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve  effectiveness and efficiency of outcome delivery, and generate valuable impact evidence.\n",
            "Top  13  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  14  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  15  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond  Role  of  the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond Measuring impact Validation  impact of Payment  based  experimental/quasi- experimental  or  validated  administrative data7 on Payment  based  on  validated  administrative data.   This will include verification of  records  physical  and  verification  of  mobility  of  beneficiaries.\n",
            "Top  16  :   Outcome Funder: World Bank  Group.   Investors: Five investors from  Chile, Palestine, Holland, Britain  and Switzerland, which have not  signed yet.  Service providers: 2-4  Palestinian, not-for profit service  providers to be selected per cycle, Payment terms: The  investors will form a SPV  for the flow of funds, with a  DIB manager (contracted  by the PIA) who will  manage the SPV funds on  behalf of the investors.   Outcome metric: Likely to  be a mixture of training The outcome funds are  USD 5 million. The DIB is  part of a wider World  Bank project called  Finance for Jobs, a larger  initiative to create  employment in West Bank  and Gaza.\n",
            "Top  17  :   research indicates Payment  by  Results  approaches  enable  donors  to  transfer the risk/uncertainty over whether an intervention  will achieve results to the provider.   However,  that  some  providers  (particularly  those  with  smaller  balance  sheets,  or  less  access to commercial loans) would be unable pre-finance  their  intervention  and  wait  for  payment  on  delivery  of  results, or would be unwilling to take on the financial risk  associated with underperforming on a PbR contract. As a  result providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of  contracts.3233 B.3  How strong is the evidence on DIBs?\n",
            "Top  18  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  19  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "Top  20  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Investment  vehicle  costs  varied  depending  on  ranged  from  nothing,  to  105k.  These  costs  depend  on  the  contracting  mechanisms  used.  Loan,  legal  and  escrow  fees  seem  to  be  consistently  cost  around  USD  30,000  –  40,000.  The  highest  costs  involve  fees  payable  to  trustees.\n",
            "Top  2  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  3  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  4  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  5  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  6  :   Table 5.1: Additional DIB costs in the design, set up and implementation phases on top  of programming costs under a grant programme Costs (including actual, budgeted,  in-kind and pro-bono) Paid for by Cost  categories   Design and set up  Staff  time set  up Where  estimated,  this  ranged  from  USD  150,000  to  USD  490,000.\n",
            "Top  7  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  9  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  10  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  13  :   A173 Annex L: VfM Analysis – Supporting Evidence This sub-section provides additional detail on the value for money (VfM) analysis undertaken on  the four DIBs, in terms of detail on the cost drivers across the four DIBs and the extent to which  costs can be considered ‘first DIB’ costs.\n",
            "Top  14  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "Top  15  :   Investment  vehicle  related  costs  e.g.  Escrow  legal  and  fees  Maximum payments to investors  Maximum  payments Note: Conversions done based on the exchange rate on 5 May 2019.\n",
            "Top  16  :   Do the extra costs represent value for money - to what extent do they lead to  additional results, impacts and benefits?\n",
            "Top  17  :   Do  the  extra  costs  represent  value  for  money  -  to  what  extent  do  they  lead  to  additional  results,  impacts  and benefits?\n",
            "Top  18  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "Top  19  :   Table  5.1  presents  the  ranges  of  cost  estimates  under  these  categories  and  which  stakeholders paid for these additional costs.\n",
            "Top  20  :   The types of costs incurred, and who paid for these costs, are discussed below. It must be  borne in mind that across several DIBs, it was acknowledged that estimates were incomplete.  Hence, it is useful to review these costs as the types and minimum level of costs required to  launch and implement a DIB at this stage of the market.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   and activities Supplier  demonstrates  how  evaluation  chosen  approach  represent value for money across life of  contract.    Including  proactive  identification  of  efficiencies  and  savings  –  e.g.  where  opportunities arise that enable evaluator  learning  synergies  and  to  remove duplicative activities.\n",
            "Top  2  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  3  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  4  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  5  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  6  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  7  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  9  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  10  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   Commercial Strong Strong Social 100%  payment  on  outcomes  (though  the  achievement  of  the  outcomes  only  affects  interest payable) Presence  of  capital  protection  measures (Full protection)  Presence  of  arrangements  upside  and  downside  service provider sharing  potential  for risk  – Strong Social and Commercial Type  contract6 of Typologies  of  structure  depending on which actor  has  the  contract  with  the  outcome funder.\n",
            "Top  13  :   Risk Management Financial  Management Robust cost control in line with contract.    Accurate  and  forecasting and invoices.\n",
            "Top  14  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  15  :   This contract will require the Supplier to operate in conflict-affected areas and parts of it are  highly insecure. The security situation is volatile and subject to change at short notice. The  Supplier  should  be  comfortable working  in such  an  environment  and  should  be  capable  of  deploying to any areas required within the region in order to deliver the Contract.\n",
            "Top  16  :   The maximum budget available for this evaluation is GBP 300,000 (exclusive of VAT) Documents / References •  DIBs Pilot Business Case  •  DIBs Pilot Business Case Addendum  •  DIBs Pilot programme Logframe  •  Village Enterprise DIB – Instiglio’s Learning/Process Review document (giving more info on their approach) B.10.5 Duty of Care The  Supplier  is  responsible  for  the  safety  and  well-being  of  their  Personnel  (as  defined  in  Section  2  of  the  Contract)  and Third  Parties affected  by  their  activities  under  this  contract,  including appropriate security arrangements. They will also be responsible for the provision of  suitable security arrangements for their domestic and business property.\n",
            "Top  17  :   Contract Duration, Contact Adaptability and Break Points The evaluation should get underway as soon as possible, with the ideal start date being 1 April  2018, and will last until March 2023 to allow all outputs to be produced and quality assurance  to be completed.    DFID reserves the option to break the contract after each of the Evaluation Report outputs is  completed.  Continuation  of  the  services  after  each  output  is  produced  will  be  based  on  agreement  of  the  deliverables  and  on  satisfactory  performance  and  the  progress  of  the  Supplier against the specified outputs.\n",
            "Top  18  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  19  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  20  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  2  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  3  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  4  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  5  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  6  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  7  :   Contract Duration, Contact Adaptability and Break Points The evaluation should get underway as soon as possible, with the ideal start date being 1 April  2018, and will last until March 2023 to allow all outputs to be produced and quality assurance  to be completed.    DFID reserves the option to break the contract after each of the Evaluation Report outputs is  completed.  Continuation  of  the  services  after  each  output  is  produced  will  be  based  on  agreement  of  the  deliverables  and  on  satisfactory  performance  and  the  progress  of  the  Supplier against the specified outputs.\n",
            "Top  8  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  9  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  10  :   Risk Management Financial  Management Robust cost control in line with contract.    Accurate  and  forecasting and invoices.\n",
            "Top  11  :   This contract will require the Supplier to operate in conflict-affected areas and parts of it are  highly insecure. The security situation is volatile and subject to change at short notice. The  Supplier  should  be  comfortable working  in such  an  environment  and  should  be  capable  of  deploying to any areas required within the region in order to deliver the Contract.\n",
            "Top  12  :   The maximum budget available for this evaluation is GBP 300,000 (exclusive of VAT) Documents / References •  DIBs Pilot Business Case  •  DIBs Pilot Business Case Addendum  •  DIBs Pilot programme Logframe  •  Village Enterprise DIB – Instiglio’s Learning/Process Review document (giving more info on their approach) B.10.5 Duty of Care The  Supplier  is  responsible  for  the  safety  and  well-being  of  their  Personnel  (as  defined  in  Section  2  of  the  Contract)  and Third  Parties affected  by  their  activities  under  this  contract,  including appropriate security arrangements. They will also be responsible for the provision of  suitable security arrangements for their domestic and business property.\n",
            "Top  13  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  14  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  15  :   •  Force majeure clauses also enable investors to terminate the contract in the event of non-delivery.\n",
            "Top  16  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  17  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  18  :   Disclaimer This  report  has  been  prepared  by  Ecorys  for  DFID,  for  services  specified  in  the  Terms  of  Reference and contract of engagement.\n",
            "Top  19  :   Structuring and developing the operating model •  The larger number of stakeholders involved in the DIBs to date, and the often diverse legislative frameworks, increase the transaction costs of this stage of the DIB  development. The optimal solution would be to amend the legislative frameworks to  accommodate DIBs. Where this is not possible, other potential solutions include: limiting the number of stakeholders involved; o  o  using pooled funding structures;  o  using other ways to minimise the number of contracts involved; and  o  standardising deals.\n",
            "Top  20  :   Effort  should  not  be  easily  observed,  otherwise  the  contract could be based on this instead.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Identifying metrics and structuring payments 3.  Building a database of impact bond returns, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the DIBs market.  However, context specificity may limit the usefulness of standardisation and caution is also  advised in terms of developing rate cards, due to the early stage of the market and limited  data available.\n",
            "Top  2  :   Identifying metrics and structuring payments 15. Building a database of impact bond returns, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the DIBs market.  However, context specificity may limit the usefulness of standardisation and caution is also  advised in terms of developing rate cards, due to the early stage of the market and limited  data available.\n",
            "Top  3  :   • It  is  important  to  developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders and linked to other metrics within the sector/country.\n",
            "Top  4  :   •  Develop outcome metrics and rate cards that are understood by all stakeholders and  linked  to  other  metrics  used  in  the  sector  or  country,  to  increase  the  value  of  the  learning generated, minimise the costs of data collection and facilitate the broader DIB  market and/or potential transition to a SIB.\n",
            "Top  5  :   •  Develop outcome metrics and rate cards that are understood by all stakeholders and  linked to other metrics used in the sector / country, to increase the value of the learning  generated, minimise the costs of data collection and facilitate the broader DIB market  and/or potential transition to a SIB.\n",
            "Top  6  :   4.  Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  7  :   16. Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  8  :   Identifying metrics A few stakeholders across the DIB suggested that developing templates for outcome metrics  and  rate  cards  could  reduce  the  transaction  costs  of  setting  up  outcome  metrics.  This  is  particularly true in the case of DIBs in the same sector, and the QEI DIB was able to build on  the  work  done  in  the  Educate  Girls  DIB.  Both  the  QEI  and  VE  DIBs  have  the  ambition  to  generate  lessons  and  grow  the  DIB  market  in  their  respective  sector,  and  the  metrics  are  priced  per  outcome,  which facilitates  transferring  the  outcome  metric  to  other  interventions  and providers.\n",
            "Top  9  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  10  :   Identifying metrics and structuring payments •  Building  a  database  on  interest  rates,  outcome  metrics  and  rate  cards  and  drawing  on  private sector expertise on pricing risk would facilitate the growing of the DIBs market.\n",
            "Top  11  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  12  :   6.2.2.2  Comparison with other impact bonds Stakeholders  interviewed  who  are  involved  in  the  wider  impact  bond  sector  echoed  the  argument that standardised templates could help reduce transaction costs. However, one key  stakeholder also argued that outcome design and pricing still involves a substantial degree of  customisation, depending on the specific context, partners involved, intervention and related  risk and requirements. Therefore the extent to which standardisation can apply is limited, and  templates  will  always  need  to  be  somewhat  bespoke.  For  example,  the  debate  around  standard rate cards is still ongoing in the UK SIB market, a much more mature impact bond market, which suggests it may take some time before this will materialise in the more nascent  use of impact bonds in middle income and developing countries.\n",
            "Top  13  :   ix Contents Acknowledgements and disclaimer .......................................... i Executive Summary ................................................................... ii  Recommendations ............................................................................................. viii  List of Tables ......................................................................................................... 3  Table of Figures .................................................................................................... 5 1.0 2.0 3.0 4.0 5.0 Introduction ................................................................. 1  Overview of the DIBS pilot programme ............................................. 1  Objectives of the Evaluation .............................................................. 4  Scope of the Research Wave 1 Report .............................................. 5  Overview of the Evaluation Process.................................................. 5  Report structure .................................................................................. 6 Evaluation Framework and Methodology .................. 8  Evaluation framework for the evaluation .......................................... 8  Overview of the methodology .......................................................... 12  Methodological limitations ............................................................... 18 Summary of the DIBs ................................................ 20  Programme components .................................................................. 22  Stakeholders involved in the DIBs .................................................. 26  DIB structures ................................................................................... 27  Conclusion ........................................................................................ 30 Analysis and Findings – DIB Effects ........................ 31  The DIB effect indicators .................................................................. 32  Presence of the DIB effect indicators: Summary............................ 34  Risk transfer effects ......................................................................... 37  Partnership effects ........................................................................... 40  Financing and funding effects ......................................................... 43  Design effects ................................................................................... 47  Other factors influencing the DIB effect .......................................... 51  Additional effects not identified in the framework ......................... 52  Conclusions ...................................................................................... 52 Analysis and Findings – Costs of designing and  delivering DIBs .......................................................... 55  Economy ........................................................................................... 57 6.0 Efficiency ........................................................................................... 68  Effectiveness ..................................................................................... 68  Equity................................................................................................. 72  Conclusion ........................................................................................ 73 Analysis and Findings – Improving the process of  designing and agreeing DIBs ................................... 75  Identifying appropriate interventions .............................................. 77  Identifying metrics and structuring payments ................................ 81  Measuring impact ............................................................................. 85  Identifying and selecting stakeholders and managing relationships  ........................................................................................................... 87  Structuring the vehicle and developing the operating model ........ 95  Conclusion ........................................................................................ 97 7.0 Lessons .................................................................... 101 8.0 Recommendations .................................................. 104  Recommendations to all DIB stakeholders ................................... 104  Recommendations to DIB designers ............................................. 104 Annex A: Case study reports .................................................... 1 Annex B: Terms of Reference ................................................. 81  Background and Context ................................................................. 81  B.1  What do we mean by other aid mechanisms? ................................ 82  B.2  How strong is the evidence on DIBs? ............................................. 83  B.3  What is the DFID DIBs pilot programme? ....................................... 84  B.4  Users of the Evaluation .................................................................... 85  B.5  Evaluation Methodology ................................................................... 90  B.6  Data Sources ..................................................................................... 91  B.7  Evaluation Outputs and Timeframe ................................................. 93  B.8  Lighter-Touch Interim Outputs ........................................................ 95  B.9  Evaluation Management Team ......................................................... 97  B.10 Annex C: Bibliography .......................................................... 106 Annex D: EQUALs criteria mapped to report sections ....... 112 Annex E: Evaluation methodology ....................................... 116  Evaluation Framework .................................................................... 117  E.1  DIB-level research........................................................................... 124  E.2 E.3  E.4  E.5  E.6  E.7 Programme-level Research ............................................................ 135  Sector-level Research .................................................................... 135  Approach to data collection ........................................................... 136  Analysis, Reporting and Dissemination ........................................ 137  Involvement of stakeholders .......................................................... 140 Annex F: Individual DIB level plans ...................................... 144 Annex G: Data Quality Assessment ..................................... 149  ICRC ................................................................................................. 149  G.1  QEI ................................................................................................... 150  G.2  Village Enterprise............................................................................ 152  G.3   Cameroon Cataract Bond ............................................................... 154  G.4 Annex H: Consultees and Sources reviewed ...................... 157 Annex I: Framework for categorising DIBs .......................... 162 Annex J: DIBs reviewed as part of programme level consultations ........................................................... 164 Annex K: Learning workshop note ....................................... 172 Annex L: VfM Analysis – Supporting Evidence ................... 174  ICRC ................................................................................................. 174  L.1  Quality Education India DIB ........................................................... 174  L.2  VE DIB.............................................................................................. 175  L.3  Cameroon Cataract Bond ............................................................... 175  L.4 Annex M: Literature Review .................................................. 177  Hypothesised effects of DIBs ........................................................ 177  a.  Input ................................................................................................. 189  b.  Recommendations .......................................................................... 196  c.  What approaches have been used to evaluate impact bonds? What  d.  are the main challenges and solutions? ....................................... 198 Annex N: List of Acronyms ................................................... 201 List of Tables Table 2.1: Evaluation Framework – EQ1 ............................................... 8 Table 2.2: Evaluation Framework – EQ2 ............................................. 10  Table 2.3 Stakeholders consulted ....................................................... 13  Table 2.4 Comparator Sites ................................................................ 14  Table 2.5: Deliverables mapped to target audiences........................... 17  Table 2.6: Limitations and mitigations ................................................. 18  Table 3.1: Programme components .................................................... 22  Table 3.2: Key stakeholders ................................................................ 26  Table 3.3: DIBs against DIB dimensions ............................................. 27  Table 4.1: DIB effect indicators ........................................................... 33  Table 4.2: Presence of DIB effect indicators in the four DIB projects .. 35  Table 6.1: Project focus and measurement approach ......................... 87  Table 6.2: Advantages and disadvantages to different approaches to  identifying and engaging with stakeholders ......................................... 91 Table B.1: Alternative aid mechanism ................................................. 82  Table B.2 EO 1: Inception Report ....................................................... 93  Table B.3: EO2 – Evaluation Report on the Process of designing and  launching DIBs .................................................................................... 93  Table B.4: EO3 –  Mid-Term Evaluation Report on DIBs ..................... 94  Table B.5: EO4 – Final Evaluation Report on DIBs ............................. 94  Table B.6: Good Performance Indicators ............................................ 98 Table E.1: Evaluation Framework ..................................................... 117  Table E.2: DIB effects and indicators ................................................ 122  Table E.3: Stakeholder consultations in RW1 ................................... 125  Table E.4: Research Waves ............................................................. 131  Table E.5: VfM Framework ............................................................... 131  Table E.6: VfM Indicators .................................................................. 132  Table E.7: Costing Structure ............................................................. 133  Table E.8: Communication Plan ........................................................ 140 Table F.1 : Proposed consultations ................................................... 144  Table F.2: Value for Money data ....................................................... 145  Table F.3: Other data ........................................................................ 147 Table M.1: Sources consulted ........................................................... 178  Table M.2: Impact bond principles .................................................... 182  Table M.3: Categorisation of SIBs by level of innovation ................... 185  Table M.4: Challenges of designing impact bonds ............................ 196 Table of Figures Figure 1.1: DIBs pilot programme theory of change .............................. 3  Figure 4.1: DFID risk assessment of three DIBs ................................. 38 Figure M.1: Framework for synthesising evaluation evidence ........... 177  Figure M.2: Strengths and weaknesses of existing evidence and  evaluation approaches and methods related to SIBs and DIBs (Drew  and Clist 2015:27) ............................................................................. 199 1.0  Introduction Overview of the DIBS pilot programme 1.1.1 DIBs and the current stage of the market.\n",
            "Top  14  :   5.3.2 Summary of available benchmarked interest rates Based on the impact bonds to date (see the table below) 30% seems to be the upper end of  expected financial return. At the lower end, a negative return could be expected, should targets  not be met.\n",
            "Top  15  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  16  :   and VE DIB  None Cataract  DIB 100% 8% p.a. if performance  targets met; 4% p.a. if not  met (OPIC); 0% p.a. if  targets not met (Netri).\n",
            "Top  17  :   counsel); Cameroon-based legal  counsel; in-country public health  consultant; data systems provider.  Outcome evaluator: TBC Outcome Funder: TBC  Investors: Likely to be BIX  capital. Shell Foundation and  DFID funded the DIB set-up; IFC  funds the data gathering for the  certification process with support  from the Ministry of Finance in  Japan.  Service providers: Mimi-Moto  (cookstove producer); Emerging  Cooking Solutions (ECS, seller of  Mimi-Moto cookstoves). Apart  from ECS, Cardano will select  one more social enterprise.  Intermediary: Cardano  Development.   Technical assistance providers:  Baker McKenzie (pro-bono legal  adviser).  Outcome evaluator: TBC Outcome Funder: Children  Investment Fund Foundation.   Investors: UBS Optimus  Foundation.   Service providers: Educate  Girls.\n",
            "Top  18  :   India  (Rajasthan ) Maternal  and  Newborn  Health DIB The bond is intended to  improve and standardize  the quality of maternal care  in Rajasthan’s private  healthcare facilities. The  DIB implementing partners  will guide the targeted  private healthcare facilities Outcome Funder: USAID and  Merck for Mothers. MOU with the  Rajasthan State Ministry of Health  to invest in, and scale-up, the  partnership if the pilot program is  deemed successful by the  independent evaluator.  Investors: UBS Optimus Payment terms: Six- monthly payment to  investors, with USD 4,500  for each facility at  progressive stage and  remainder USD 13,500 for  facilities that reach Joint  Quality Standard (JQS) Projected total investment  of USD 9 million, USD 1  million of which is set  aside for results  verification. UBS Optimus  Foundation will provide  80% of the USD 4 million  upfront working capital A166 Development  Stage No DIB Objective Stakeholders involved Structure Value through quality  improvements and the  application process to be  accredited through the  government-approved  healthcare facility  certification process.\n",
            "Top  19  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  20  :   •  Where  possible, this  will  be  disaggregated  by  ‘first  time’  DIB  costs  which  hypothetically  wouldn’t  have  to  be  incurred  again  for  any  subsequent DIBs.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "Top  3  :   As set out in the alignment principle of PbR, PbR may be only beneficial when incentives were  not initially aligned: •  Holden and Patch (2017) found that GEC staff were already very motivated to achieve  outcomes before the introduction of the payment incentive. Similarly, Rwanda was already  focused  on  increasing  enrolment  before  the  introduction  of  the  RBA  (Upper  Quartile,  2015).\n",
            "Top  4  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  5  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  6  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  7  :   In addition, Ecorys’s evaluations have seen some evidence of the ‘perverse incentives’. These  are  often  associated  with  outcomes  based  commissioning,  primarily  ‘cherry  picking’  (where  services target beneficiaries easiest to reach/turn around as opposed to the hardest to reach) and  ‘parking’ (where beneficiaries are left on programmes but not supported, either because it is clear  they will not achieve any outcomes or because the provider gets paid for having beneficiaries on  the programme).\n",
            "Top  8  :   One hypothesis is that measures can fail to incentivise recipients if they are too complex relative  to the incentive size. This seems to be the case for certain Health Results Innovation Trust Fund  (HRITF) PbR agreements (Kandpal 2016), NGOs (Holden and Patch, 2017) and governments  (Cambridge Education, 2015 and Upper Quartile, 2015). Measures can also fail to incentivise if  the incentives are too low, agreements too short or outside of the recipient’s control (such that  the recipient has no incentive to try). Clist (2017) notes that a common theme for projects with A191 poor  performance  is  incentives  which  are  insufficient,  in  comparison  to  the  programme’s  complexity and duration, and perverse incentives to prioritise the short term over the long term.\n",
            "Top  9  :   risk transferred  needs The  amount  of  to  be  commensurate  with  the  risk  premium  paid.  Different  actors will have different levels of risk aversion, and this  may affect the risk premium and the pool of interested  actors.  Determining  the  appropriate  risk  and  reward  structure  (pricing  and  outcomes)  to  get  the  incentives  right can be difficult.\n",
            "Top  10  :   This seems to be supported by the success stories as well. Where PbR worked best and provided  the  strongest  evidence  of  success  was  where  incentives  were  also  largest,  including  HRITF’s  programme in the Misiones province (where incentives were largest); Employment Fund in Nepal  where  organisations  responded  to  the  incentive  to  increase  employment,  not  just  training;  the  Uganda RBF health project, where incentivised quality of care increased.\n",
            "Top  11  :   VE:  The  targeting  strategy  addresses  equity  concerns,  and  at  the  moment,  there  are  no  particular  risks  identified  with  the  outcome  target  or  verification  process  potentially  driving  perverse incentives. This will be monitored over the next two research waves.\n",
            "Top  12  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  13  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  14  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  15  :   3.  Maximum payments to investor: This includes the maximum return payable to the  investor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any  interest payment.\n",
            "Top  16  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  17  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  18  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  19  :   Payment  based  on  quasi-experimental  methods Payment  based  on  experimental methods Payment  based  on  validated  administrative data.\n",
            "Top  20  :   B.10.4 Budget and Payments tied to Outputs The Evaluator is expected to tie payments to delivery of the four main Evaluation Outputs –  the Evaluation Reports – with each payment commensurate to the work involved in that stage.  The  payments  will  be  made  when  each  output  is  accepted  by  DFID  as  being  of  good  or  excellent quality, where the requirements have been met with no shortcomings.\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Investment  vehicle  costs  varied  depending  on  ranged  from  nothing,  to  105k.  These  costs  depend  on  the  contracting  mechanisms  used.  Loan,  legal  and  escrow  fees  seem  to  be  consistently  cost  around  USD  30,000  –  40,000.  The  highest  costs  involve  fees  payable  to  trustees.\n",
            "Top  2  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  3  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  4  :   Table 5.1: Additional DIB costs in the design, set up and implementation phases on top  of programming costs under a grant programme Costs (including actual, budgeted,  in-kind and pro-bono) Paid for by Cost  categories   Design and set up  Staff  time set  up Where  estimated,  this  ranged  from  USD  150,000  to  USD  490,000.\n",
            "Top  5  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  6  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  7  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  8  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  9  :   The types of costs incurred, and who paid for these costs, are discussed below. It must be  borne in mind that across several DIBs, it was acknowledged that estimates were incomplete.  Hence, it is useful to review these costs as the types and minimum level of costs required to  launch and implement a DIB at this stage of the market.\n",
            "Top  10  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  11  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  12  :   This section is set out as follows: •  Section 5.1.1 introduces the categories of costs used •  Section 5.1.2 discusses the costs incurred per DIB in more detail.\n",
            "Top  13  :   Cost by Activity (USD) Cost per stakeholder (USD) Design and Set-up  Staff time spent on setting up contracts  External advice on contract design  (including Technical adviser Volta &  Legal counsel Linklaters) Implementation Costs Outcome  Funders Intermediary Total 100,000 66,213 166,213 255,450 - 255,450 14 This additional cost for intermediaries was calculated by taking the sum of Volta's invoiced fees during the design phase  (USD 225,250) and multiplying by 25% to represent the additional time Volta staff spent working on the Bond that was not  reported on or compensated. An additional USD 9,900 was estimated for the additional time from Linklaters. This was  calculated by multiplying their fee by 0.33 to represent their additional time above their compensated rate.  15 The calculation assumes a rate of USD 1000 a day for a senior staffer The  table  below  sets  out  the  estimated  additional  costs  of  implementation,  compared  to  a  traditional grant funded project. Based on The Fred Hollow Foundation’s previous experience,  it is estimated that the additional cost of performance management, project management and  reporting is approximately 30% of Volta’s USD 175,000 fee, hence approximately USD 52,500.  Similarly, should a traditional grant be used, The Fred Hollow Foundation would engage an  evaluation  consultant  to  undertake  a  mid-term  and  end  of  project  reviewer,  and  a  data  validation  approach  using  spot  checks  and  internal  audit.  Hence,  the  ‘additional’  cost  of  verification is based on an estimate of 40% of AEDES verification fee.\n",
            "Top  14  :   Additional costs need to be offset by other benefits, such  as  increased  outcomes  or  efficiency  gains  (including  reduced staff time or transaction costs).\n",
            "Top  15  :   Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly found in the design or delivery stages?\n",
            "Top  16  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "Top  17  :   Table  5.1  presents  the  ranges  of  cost  estimates  under  these  categories  and  which  stakeholders paid for these additional costs.\n",
            "Top  18  :   Do the extra costs represent value for money - to what extent do they lead to  additional results, impacts and benefits?\n",
            "Top  19  :   Do  the  extra  costs  represent  value  for  money  -  to  what  extent  do  they  lead  to  additional  results,  impacts  and benefits?\n",
            "Top  20  :   Investment  vehicle  related  costs  e.g.  Escrow  legal  and  fees  Maximum payments to investors  Maximum  payments Note: Conversions done based on the exchange rate on 5 May 2019.\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   Identification of individuals who live on less than  USD 1.90 per day Creation  of  Business  Savings  Groups  (BSG),  which are self-governing councils of businesses Local  mentors  deliver  a  four-month  training  program  the  necessary knowledge to run a business to  equip  participants  with Seed capital  is granted to  each group of three  participants,  to  enable  them  to  start  their  business Mentors  provide  continuous  guidance  to  the  participants  for  one  year,  coaching  them  in  choosing the focus of their business, as well as  how  to  grow  and  manage  their  business  and  finances, including saving in Business Savings  Groups.\n",
            "Top  2  :   excellent good/ all by reporting Qualitative  Evaluator       Value of savings generated.\n",
            "Top  3  :   Cashable  savings:  A  review  delivered  by  Azemati  et  al  (2013)  found  that,  based  on  the  SIB  experience in the US, there was little evidence that interventions truly pay for themselves. This  could be related to the fact that PbR projects seem to generally be subject to expectations of both  being innovative and following? standard procedures for traditional aid modalities. (Clist 2017) Impact Bonds Market which increases competition and drives down costs: There is limited  evidence on this point, as the impact bonds market is still nascent.\n",
            "Top  4  :   Efficiency Effectiveness Equity It is too early to draw conclusions on the efficiency of the DIBs.  No savings have yet been  realised, though opportunities for efficiency savings have been identified and these will be  reviewed during subsequent research waves.\n",
            "Top  5  :   We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We  welcome efforts by the evaluator to find savings during the life of the evaluation.\n",
            "Top  6  :   •  The report should also include an updated financial plan for  the  evaluation  –  including  highlighting  any  savings  that  are  possible  following  detailed  design  phase  and  engagement  with project level learning providers.\n",
            "Top  7  :   •  Financial risk is reduced for investors, through the use of capital protection, coupon payments and earlier repayments to investors.\n",
            "Top  8  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  9  :   Financial  ranging  return  from -100% to maximum of  9.9%  IRR,  depending  on  final number of households  reached  VE  performance.\n",
            "Top  10  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   o  Who pays for these additional costs and to what extent do they see the benefits?\n",
            "Top  13  :   4  Level  of  returns  and  profit  made  by  the investors.\n",
            "Top  14  :   Conclusion 29 One investor noted that indemnities can work in a PbR set up, due to service providers’ limited assets, but that it would be  very unlikely for investors to be willing to provide this indemnity.   30 Pooled funds are funds from many individual investors that are aggregated for the purposes of investment, as in the case of  a mutual or pension fund. Pooled funds are also used within the humanitarian and development sector to aggregate funding  from multiple donors.\n",
            "Top  15  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "Top  16  :   At this early stage of the evaluation, it is too soon to draw firm conclusions for the evaluation  questions, so we present here thematic learnings and observations from the data in relation  to the evaluation questions and across the 4E value for money framework.  We have started  to build up a picture of what the additional costs of a DIB are. This is with a view to exploring  whether the additional costs of a DIB provide additional benefits, as well as the cost drivers to  developing DIBs. Section 6 sets out the early learning in terms of how these cost drivers can  be managed to reduce transaction costs. We have also started to detail the risk and return  expected  for  each  DIB  with  a  view  to  exploring  whether  the  amount  of  risk  transferred  is  commensurate  with  the  return  to  investor.  This  builds  on  the  analysis  of  risk  transfer  undertaken in section 5.\n",
            "Top  17  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  18  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  19  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  20  :   •  Where  possible, this  will  be  disaggregated  by  ‘first  time’  DIB  costs  which  hypothetically  wouldn’t  have  to  be  incurred  again  for  any  subsequent DIBs.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What outcomes were achieved?\n",
            "Top  1  :   Secondary  outcomes  resulting  from  improved  incomes,  such  as  wellbeing,  diets,  access  to  education and healthcare are achieved.\n",
            "Top  2  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  3  :   Expansion  of  the  existing  programme  of  a  service  provider  and  implementation  of  a  programme  already  proven  successful  in  new schools (using new methods) of Expansion  the  existing programme of  a service provider Implementation of a  programme already proven  successful but in a new  context Contract focuses on achievement of specific  outcomes – intervention defined but subject  to  change  and  adaptation  depending  on  needs Contract  focuses  on  achievement  of  specific  outcomes  –  defined  intervention Contract  involves  a  specific  and well-defined intervention Characteristic  Description   provider,  specific  or  specific  outcomes  which  enables  intervention  service  to  providers  defined   organise  work  as  they  prefer.   Identifying metrics and structuring payments Nature  payment  outcomes of Were  payments  made  squarely  for  outcomes  or  was some payment made  for inputs or activities?\n",
            "Top  4  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  5  :   “The joint  awareness and  wealth of  foundation  knowledge  that  came into  play on  this,  you  can’t  underestimate  you  know…  part  of  the  beauty  of  this  piece  in  international  development…it’s about the data that’s sitting within foundations, especially deep technical  foundations like MSDF…to share and be open minded, and move a bit…. And to say right  we’re willing to put that out there to be tested.” (Representative from British Asian Trust, QEI  DIB. Comment made during case study consultations) One outcome funder felt this greater collaboration between outcome funders was because the  focus on outcomes brought about by the DIB mechanism ensured they were united towards  “one common outcome” and although views on how to achieve this outcome varied, it was still  described  as  ‘a  single  united  goal’.  Interviewees  valued  these  new  partnerships  and  collaborative ways of working; they felt the partnerships brought together a merged expertise,  whilst the willingness to share information and data enabled organisations to gain a greater  understanding of the context.\n",
            "Top  6  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  7  :   Other  intended  outcomes  as  set  out  in  the M&E framework.\n",
            "Top  8  :   • identifying  outcome funders  which  took  longer  than  expected.  The  number  of  outcome  funders lengthened the time it took to negotiate outcome metrics and pricing.\n",
            "Top  9  :   Table 2.1: Evaluation Framework – EQ1 Key  evaluation  questions Effectiveness and sustainability sub-questions Indicators EQ1: Assess  how the DIB  model affects  the design,  delivery, To  what  extent  were  the  three  DIB  projects  successful  in  realising  their  aims,  outputs, outcomes and impacts?   To what extent was the level of success and failure due to the DIB model - was  the DIB model a small, medium or large driver of success and was it at all critical  to the projects’ overall performance?\n",
            "Top  10  :   Outcome Funder: Goodbye  Malaria, underwritten by Nandos  and other corporates.   Investors: not defined.   Service providers: Lubombo  Spatial Development Initiative  (LDSI) II.   Intermediary:  D. Capital  Partners.  Technical assistance providers:  the University of Pretoria, the  Medical Research Council, and  the National Malaria Control  Programme within the Ministry of  Health (Mozambique).  Outcome evaluator: TBC Payment terms: The  payment would have been  made as a bullet payment  at the end of the third year  of bond implementation,  based on the achievement  of expected outcomes.  Outcome metric: 60%  reduction in the prevalence  and incidence of malaria  cases, compared to  baseline rates at year 1.  Incidence of malaria cases  based on prevalence  testing done at sentinel  sites in each district.    Range of returns: The  maximum potential loss of  investment for investors  would have been 30%, and  the maximum return 0.05%.\n",
            "Top  11  :   Wave 3 April  2022  March  2023 the  DIBs  and the  Legacy  of  programme, including the extent to  which  outcomes  and  DIBs  were  sustained.\n",
            "Top  12  :   QEI: The three NGO organisations are all expansions of programmes with existing evidence  of their effectiveness (improvements in outcomes compared to a counterfactual). Therefore,  we worked with the service provider to compare data on performance, and qualitatively explore  the differences in how the project was set-up and developed, at what cost, and what was the  working relationship between the stakeholders involved, in areas affected by DIB and non-DIB  contracts.\n",
            "Top  13  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  14  :   •  Finally,  where  there  had  been  minimum  activity  on  the  part  of  the  outcome  evaluators/verifiers,  we  decided  to  not  consult  with  them  during  this  first  research  wave.\n",
            "Top  15  :   A171 N/A Payment terms: N/A  Outcome metric:  Audited  delivery of the mass  treatment intervention and  statistically significant  reductions in the T. brucei  s.l. parasite among the  cattle population in target  areas.  Range of returns: N/A Annex K: Learning workshop note The learning workshop offered the evaluation team the opportunity to test emerging findings with  the key evaluation stakeholders. A complete list of the participants is set out in the table below.\n",
            "Top  16  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  17  :   Ibidem; Pereira, J; Villota, C. 2013. Hitting the target? Evaluating the effectiveness of results- based approaches to aid. Brussels: EURODAD.\n",
            "Top  18  :   2.2.2 Analysis The data collection generated a variety of qualitative and quantitative evidence, which enabled  the triangulation  of  different  data  sources set  out  above. The  data from  the  transcripts  and  field notes were summarised and synthesised under the headings and sub headings within  the  Evaluation  Framework.  Findings  from  different  data  sources  were  triangulated.  Where  findings between the data sets contradicted each other, each data set was further interrogated  to examine possible explanations. Analysis took place at three levels, focusing firstly on the  individual DIBs; bringing this together to analyse progress at a programme level; and finally  considering the implications for the wider DIB sector. We also held debriefings with all team  members, including the external experts, to support in this analysis stage.\n",
            "Top  19  :   A  more  socially  cohesive  and  to  a  stable  society  actively  larger  workforce  contributing  to  the  country’s  prosperity.\n",
            "Top  20  :   Effectiveness and sustainability sub-questions Indicators Did the DIB model provide added value in relation to the cross-cutting issues of  gender, poverty, human rights, HIV/AIDs, environment, anti-corruption, capacity  building and power relations?  Where was the DIB model most effective - was its greatest value in terms of the  design, delivery, relationship development, cost effectiveness, time efficiency or  impact on beneficiaries?  Comparisons  To what extent does the effectiveness vary across the three projects and why?  How does the effectiveness compare to other DIBs and funding mechanisms and  why?  Spillovers To what extent did stakeholders involved in the DIB use any of the working  practices of the model in their other work? To what extent did good practice  within the DIBs spread to other interventions or organisations?\n",
            "\n",
            "\n",
            "\n",
            "Query:  What impact was achieved?\n",
            "Top  1  :   M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited  number of evaluations seeking to identify the instrument effect and the challenge of establishing  comparative baselines.\n",
            "Top  2  :   Impact Expected outcomes are produced…more effectively than with other approaches…more efficiently  than with other approaches… •  With the focus on results and not inputs, this also enables a market for impact bonds, for  example  through  outcome  funds,  which  can  be  used  to  increase  competition  in  the  delivery of target outcomes and drive down costs.\n",
            "Top  3  :   •  An effect highlighted but not included in the presentation was the signalling effect that can  be produced by successful DIBs. This can signal to the government the feasibility of using  impact  bonds  to  solve  this  social  problem,  and  creates  the  potential  for  the  DIB  transitioning to a SIB. This is not an effect that can be expected to emerge during the set  up phase, and will be reviewed in the following research waves.\n",
            "Top  4  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  5  :   Measuring impact This sub-section explores the balance between reducing the transaction costs and increasing  the  benefits  of  the  impact  measurement  stage  in  terms  of  accuracy  and  usability.  The  necessary conditions for measuring impact largely relate to having clear outcomes that can  be attributable to the intervention, and the robust metrics to capture the targeted outcomes,  which are discussed in sections 6.0 and 6.2.1 respectively.\n",
            "Top  6  :   Other objectives sought by members with the design of impact bonds included: i) to create  better models for diaspora philanthropy; ii) to create a platform that allows a bridge for low- income/transition countries to go from aid-dependent economies to investment-partnership  opportunities; and iii) to advance the robustness and fidelity of impacts of poverty alleviation  programming at scale.\n",
            "Top  7  :   South  Africa ECD  Impact The DIB aims to improve  health, nutrition and  developmental outcomes of Late-stage  design Outcome Funder: Provincial  Department of Health and  Discovery Fund.\n",
            "Top  8  :   Efficiency This section looks at whether there have been any positive or negative changes to efficiency  as a result of the impact bond.  Stakeholders were interviewed to confirm whether there  have been any savings in programming costs as a result of the impact bond.\n",
            "Top  9  :   investors, including Any positive or negative changes to equity as  a result of the impact bond.\n",
            "Top  10  :   Effectiveness  Any  positive  or  negative  changes  to  effectiveness  as  a  result  of  the  impact  bond.   Any  positive  or  negative  changes  to  equity  as  a  result of the impact bond.\n",
            "Top  11  :   Additional costs of the  impact bond,  disaggregated where  possible by:   • stage (design, set-up,  delivery, learning);   • actor who incurs this  cost; and  • type of cost (staff time,  consultancy and  expertise costs, and the  risk premium (return to  investors, including  interest)   Savings in programme  costs (including staff  time) as a result of the  impact bond.   How effectively has risk  been transferred - what  is the alignment of Research Wave DIBs level research Programme  level research Wider impact  bond sector w e i v e r   t n e m u c o D s n o i t a t l u s n o c B D I s e t i s r o t a r a p m o C i s s y a n a l a t a D 1 W R 2 W R 3 W R i w e v e r   t n e m u c o d e m m a r g o r P w e i v e r e r u t a r e t i L s n o i t a t l u s n o c l r e d o h e k a t S s p o h s k r o w i g n n r a e L Methods s n o i t a t l u s n o c D F D I s i l s y a n a t s o C x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x A119 transferred risks with  return?\n",
            "Top  12  :   Perhaps two of the most significant landmarks in these projects is that they have demonstrated  that private investors are willing to take on sizeable levels of risk in impact bonds (i.e. in the  ICRC Humanitarian Impact Bond (HIB), which includes private investment), and it is possible  to launch impact bonds at a larger scale (i.e. the Quality Education India Development Impact  Bond (QEI DIB), which builds on the Educate Girls DIB).\n",
            "Top  13  :   Two of the most significant landmarks in these projects is that they have demonstrated that  private investors are willing to take on sizeable levels of risk in impact bonds (i.e. in the ICRC  HIB, which includes private investment), and it is possible to launch impact bonds at a larger  scale (i.e. the QEI DIB, which builds on the Educate Girls DIB).\n",
            "Top  14  :   Research  Wave 2  3 x x x x x x x x x x x x x x x x x x x x x the where impact  bond,  by:   possible  (design,  set-up,  delivery,  and Additional  costs  of  disaggregated  •  stage  learning);   •  actor  who  this  cost;  and  incurs  •  type  of  cost  (staff  time,  consultancy  and  expertise costs, and the risk premium (return  interest)   to  Savings in programme costs (including staff  time)  as  a  result  of  the  impact  bond.  How  effectively  has  risk  been  transferred  -  what is the alignment of transferred risks with  return?\n",
            "Top  15  :   Effectiveness and sustainability sub-questions Indicators Did the DIB model provide added value in relation to the cross-cutting issues of  gender, poverty, human rights, HIV/AIDs, environment, anti-corruption, capacity  building and power relations?  Where was the DIB model most effective - was its greatest value in terms of the  design, delivery, relationship development, cost effectiveness, time efficiency or  impact on beneficiaries?  Comparisons  To what extent does the effectiveness vary across the three projects and why?  How does the effectiveness compare to other DIBs and funding mechanisms and  why?  Spillovers To what extent did stakeholders involved in the DIB use any of the working  practices of the model in their other work? To what extent did good practice  within the DIBs spread to other interventions or organisations?\n",
            "Top  16  :   (2018).  Paying USAID Investing for Impact (n.d.) 6 ways in which an impact bond adds value.\n",
            "Top  17  :   4.6.1.2  Main finding: Extent to which DIB effect present across four projects Summary:  The  DIB  did  not  enable  radical  innovation,  but  all  four  projects  had  elements  of  incremental innovation within them, which was possible because of the DIB funding.\n",
            "Top  18  :   Conclusions The focus of this section has been on how the DIB mechanism has affected the design and  set up of these four projects, examining in particular the extent to which the purported ‘DIB  effects’ set out in DfID’s Theory of Change and other literature materialised. The evidence so  far  would  suggest  that  the  majority  of  the  DIB  effects  have  materialised,  albeit  to  different  degrees and with some nuances.\n",
            "Top  19  :   A  more  socially  cohesive  and  to  a  stable  society  actively  larger  workforce  contributing  to  the  country’s  prosperity.\n",
            "Top  20  :   4.4.1.6  Main finding: Extent to which DIB effect present across four projects Summary:  The  DIB  enabled  coordination  and  collaboration  between  new  actors,  and  also  strengthened this for actors that had previously worked together.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What were the results of the intervention?\n",
            "Top  1  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  2  :   This report presents the evaluation’s initial findings against these questions. Given the stage  of the interventions funded by the DIBs, findings are focused on the design stage. The effects  of the DIB in terms of the intervention quality and outcomes remain to be seen. Furthermore,  it is important to note that DIBs are still in a pilot phase, and the lessons learned draw on a  small number of ‘test cases’. These findings will continue to be refined and developed based  on additional evidence over the remainder of the evaluation.\n",
            "Top  3  :   •  Sufficient  evidence  base  for  the  proposed intervention:  Stakeholders  across  all  four DIBs noted the importance of having sufficient evidence for the effectiveness of  the  intervention.  In  all  four  DIBs  the  interventions  had  a  strong  evidence  base  that  offered  reassurances  to  investors  and  also  enabled  stakeholders  to  build  strong  business cases. The evidence base was less strong in the Cataract Bond (as there is  limited evidence to demonstrate the effectiveness of the intervention in Sub-Saharan  Africa),  but  stakeholders  were  comfortable  that  there  was  sufficient  evidence  from  previous interventions delivered by the service providers, in other contexts.\n",
            "Top  4  :   At this stage of the evaluation, no financial savings have been reported through changes in  efficiency  in  the  intervention,  as  a  result  of  use  of  the  impact  bond  funding  mechanism.  However, certain opportunities for efficiency gains during implementation were identified, and  will be followed up over the next two research waves. These are briefly summarised in the  table below: Table 5.4 Expected efficiencies DIB  ICRC HIB Expected efficiencies   ICRC expects some efficiencies with having a 5-year grant compared with having an  annual funding cycle.  For example, the ability to roll funds over from one year to the  next would save time in having to renegotiate these agreements.    One of the ICRC outcome funders reported they expect savings over the course of  the programme in terms of management time required.  No opportunities for efficiency savings related to the intervention have been identified  yet.   VE  has  used  the  DIB  as  an  opportunity  to  improve  its  adaptive  management  practices, monitoring and verification processes. It is expected that this will lead to  stronger performance management of staff, and consequently, increased efficiency.  Cataract DIB  Adaptive  management  techniques  have  been  adopted  to  support  the  reaching  of QEI DIB    VE DIB targets.\n",
            "Top  5  :   2.4 2.3 2.2  The product describes the intervention logic and/or theory of change.  The product provides a relevant and sufficient description of the local,  national and/or international development context within which the  intervention was operating.  The product identifies key linkages between the evaluated intervention and  other relevant projects / programmes / donors. If no linkages are identified,  the product justifies why other projects / programmes / donors were not  relevant to the evaluation.  There is an assessment of the policy context for the intervention and this  includes reference to poverty reduction strategies, gender equality,  environmental protection, and human rights.  The product describes the extent to which the intervention has been  managed and delivered against Paris Declaration principles.\n",
            "Top  6  :   In terms of the design of the intervention itself, there was some evidence that the DIB led to  some  improvements,  but  this  effect  was  not  as  strong  as  the  impact  on  the  monitoring  procedures. In the Cataract Bond the introduction of the equity target is leading to a greater  focus on outreach, targets for it, and an understanding of whether the people reached through  outreach  are  the  poorest.  There  has  also  been  a greater  focus  on  quality  –  the  hospital  is  doing a much deeper dive to understand quality of outcomes and introduce interventions, such  as regular management and staff sessions, to understand and strengthen quality, something  that stakeholders noted seems to be less consistently done as part of grant-funded projects.  In the VE DIB, the service provider did make substantial improvements to the design of the  intervention, but it is not clear whether this is due to the DIB mechanism or the involvement of  DFID as a funder: “Because of DfID there was a lot of detail required on the design but I would not say it would  uniformly  be  the  case  across  the  DIB  sector”  (Village  Enterprise  representative.  Comment  made during case study consultations) In  the  ICRC  HIB  the  intervention  design  was  no  different  to  how  it  been  implemented  previously.\n",
            "Top  7  :   However, whilst the interventions themselves have been funded before, the nature of three of  the projects is different in the DIBs (VE, ICRC and QEI), and it is the DIB element that enabled  them to be delivered in a different guise. As described in Effect 1: Transfer of financial risk,  each project had a new element that was deemed risky by the outcome funders, and in three  of the DIBs outcome funders reported they would not have funded the projects in their current  guise because they deemed them to be too risky.\n",
            "Top  8  :   Emerging findings in terms of how the DIB affected the interventions funded by the DIBs were  discussed. The DIB effects table set out in the inception report was used to frame findings. The  key lessons learned were also presented.\n",
            "Top  9  :   The aim of the DIB is to  prevent two deadly strains  of Sleeping Sickness from  overlapping in Northern  Uganda. A successful pilot  was implemented in  2014/15, in which 20,000  cattle were treated for  Sleeping Sickness. The  intervention model also  includes a behaviour  change component to  ensure that farmers spray  their cattle effectively to  prevent the spread of  Sleeping Sickness and  improve cattle health.\n",
            "Top  10  :   Expansion  of  the  existing  programme  of  a  service  provider  and  implementation  of  a  programme  already  proven  successful  in  new schools (using new methods) of Expansion  the  existing programme of  a service provider Implementation of a  programme already proven  successful but in a new  context Contract focuses on achievement of specific  outcomes – intervention defined but subject  to  change  and  adaptation  depending  on  needs Contract  focuses  on  achievement  of  specific  outcomes  –  defined  intervention Contract  involves  a  specific  and well-defined intervention Characteristic  Description   provider,  specific  or  specific  outcomes  which  enables  intervention  service  to  providers  defined   organise  work  as  they  prefer.   Identifying metrics and structuring payments Nature  payment  outcomes of Were  payments  made  squarely  for  outcomes  or  was some payment made  for inputs or activities?\n",
            "Top  11  :   is The  the  features  of  intervention,  and  whether  it  totally  new,  an  expansion  of  an  existing  programme  or  involves  a  whose  programme  underpinning  principles  have already been tested  the  Extent  contract  a  specific  and  well-defined  intervention  and  service to  which  involves Expansion  of  the  existing  programme  of  a  service  provider.  Implementation  of  a  programme proven successful  improvement  (efficiency  measures  testing)  and  new  programme  (Digital  Centre  Management System).   Contract  involves  a  specific  and  well-defined  intervention,  though  there  is  room  to  test  and adapt Level  innovation of of Level  outcome  orientation  and  flexibility  versus the Grant received.   DFID  provided  a  grant  to  BAT  to  cover  a  proportion  of  operational,  design  and  contracting  costs;  remainder  was  covered  by  UBSOF.  Other  actors  covered  their own costs.  Instiglio  the  anonymous donor for initial design work and  stakeholder engagement.\n",
            "Top  12  :   A171 N/A Payment terms: N/A  Outcome metric:  Audited  delivery of the mass  treatment intervention and  statistically significant  reductions in the T. brucei  s.l. parasite among the  cattle population in target  areas.  Range of returns: N/A Annex K: Learning workshop note The learning workshop offered the evaluation team the opportunity to test emerging findings with  the key evaluation stakeholders. A complete list of the participants is set out in the table below.\n",
            "Top  13  :   3.  Shared understanding of the policy ‘problem’ and sufficient evidence for the intervention so that it is credible or knowledge-based.\n",
            "Top  14  :   This would appear to have led to degree of risk aversion in these DIBs. This affected both the  selection of service providers and interventions; with careful selection to ensure both service  providers and the interventions had established track records. The evidence would suggest  this risk aversion limited the extent to which other DIB effects materialised, as we describe  further in this section.\n",
            "Top  15  :   6.1.3 Identifying appropriate interventions - increasing the model’s benefits Summary:  The  benefits  of  using  the  DIB  model  are  the  strongest  when  there  is  a  value  proposition  to  the  use  of  the  DIB,  whereby  they  resolve  a  specific  challenge  that  cannot  be  Box 1: Value propositions of the  four DIBs  addressed by other funding mechanisms.\n",
            "Top  16  :   The DIB did not enable completely new interventions to be funded, as all four had been funded  previously in a different guise, in some instances by the same funders. For example, DFID,  the  Belgian  and  Swiss  governments  all  provide  core  funding  to  ICRC;  the  same  outcome  funders  in  the  QEI  DIB  have  funded  the  same  service  providers  to  deliver  very  similar  interventions; the Magrabi Foundation had already raised USD 10m of the USD 12m required  to fund the hospital; and the Village Enterprise intervention is already operating in other parts  of Africa. Therefore, the DIB has not enabled completely new interventions to be funded, which  would not have received funding otherwise.\n",
            "Top  17  :   Section 4 presents the analysis and findings of the evaluation in relation to EQ1, assessing how  the DIB model affects the design and set up phase of development interventions.\n",
            "Top  18  :   4.5.1.2  Main finding: Extent to which DIB effect present across four projects DIB Effect  Funding  projects  which  would  not  have  been  funded  otherwise,  or  not in  the same guise ICRC QEI VE Yes Yes Yes Cataract Bond  No, likely project  could have been  funded without DIB 4.5.1.3 Analysis from four projects Summary: All four interventions had been funded previously, and so the DIB did not enable  completely new interventions to be funded. However, in three of the DIBs the interventions had  alterations and outcome funders reported that would not have funded these without the DIB;  therefore the DIB did alter the guise of the interventions, including enabling projects to operate  at larger scale or with innovative elements.\n",
            "Top  19  :   A100 Annex 1 – Initial Country Risk Assessment by DFID The programme under evaluation involves activities in multiple countries. DFID has provided  an overall initial risk assessment for the programme locations as shown below: A101 DFID Overall Initial Project/Intervention Summary Risk Assessment Matrix Dec-17Read in conjunction with the FCO Travel Advisory on each countryCountryHIGH RISK LOCATIONSMEDIUM RISK LOCATIONSDate ConductedThemeDFID Risk ScoreDFID Risk ScoreOverall Rating5 - VERY HIGH RISK3 - MEDIUM RISKFCO Travel Advice52Host Nation Travel AdviceN/AN/ATransportation55Security[*]53Civil Unrest53Violence/crime53Terrorism*54War41Hurricane13Earthquake****13Flood*****23Medical Services**53Nature of Project Intervention32Mean (ignoring nature of project)43Mode (ignoring nature of project)5312345Very Low RiskLow RiskMedium RiskHigh RiskVery High RiskMedium*The FCO travel advice for Uganda, Kenya, Nigeria and Mali advises that there is a general threat from terrorism**Medical facilities outside of Capital Cities, and particularly away from cities are limited***FCO advise against all travel to Borno State. There is also a  High Risk (4) threat of kidnapping across Nigeria and Maiduguri in particular**** Earthquake risk is (3) on Indian border with Pakistan and in Delhi***** Flash flooding can occur during the wet season in Nigeria; Eastern Uganda; and monsoon in North India.High RiskFor example: Abuja and Borno State in Nigeria; Mali; Kinshasa in DRC; parts of Kenya, including Nairobi; and the immediacte vicinity of the India-Pakistan border.For example, other project locations incl: Uganda (excluding Karamoja, which is not relevant to this project); Gujarat, Rajasthan, and Delhi in India (with exception of the area in immediate vicinity of the border between India and Pakistan where the Supplier is not required to travel).Dec-17LocationLow SUPPLEMENTARY ANNEXES Annex A1: DFID Theory of Change for DIBs A102 Annex A2: Initial Framework for Assessing Theory of Change for DIBs Initial framework for assessing the Theory of Change behind DIBs, developed during DFID evaluability assessment A103 Were(cid:9)(cid:9)/(cid:9)can(cid:9)deal-breakers(cid:9)/(cid:9)critical(cid:9)success(cid:9)factors(cid:9)identified(cid:9)early?(cid:9)What(cid:9)were(cid:9)they?Costs(cid:9)and(cid:9)cost(cid:9)drivers:(cid:9)(cid:9)What(cid:9)were(cid:9)the(cid:9)duration(cid:9)and(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stages?(cid:9)(cid:9)How(cid:9)were(cid:9)costs(cid:9)divided(cid:9)across(cid:9)the(cid:9)different(cid:9)participants?(cid:9)What(cid:9)factors(cid:9)drove(cid:9)the(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stakeholders?(cid:9)Which(cid:9)costs(cid:9)show(cid:9)potential(cid:9)to(cid:9)decrease(cid:9)in(cid:9)future(cid:9)deals?(cid:9)(cid:9)What(cid:9)steps(cid:9)can(cid:9)be(cid:9)taken(cid:9)to(cid:9)reduce(cid:9)future(cid:9)costs?Comparison(cid:9)with(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)How(cid:9)do(cid:9)costs(cid:9)compare(cid:9)(higher(cid:9)or(cid:9)lower)(cid:9)with(cid:9)alternative(cid:9)funding(cid:9)mechanisms(cid:9)(for(cid:9)both(cid:9)provider(cid:9)and(cid:9)for(cid:9)funder/(cid:9)payors)?(cid:9)For(cid:9)which(cid:9)stages(cid:9)did(cid:9)the(cid:9)costs(cid:9)differ?Cost-effectiveness:How(cid:9)does(cid:9)the(cid:9)effectiveness(cid:9)of(cid:9)the(cid:9)DIB(cid:9)funded(cid:9)projects(cid:9)(ie,(cid:9)impact(cid:9)/(cid:9)cost)(cid:9)compare(cid:9)with(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)by(cid:9)different(cid:9)mechanisms?(cid:9)Additionality(cid:9)of(cid:9)funding:(cid:9)Was(cid:9)the(cid:9)funding(cid:9)for(cid:9)the(cid:9)DIB(cid:9)net(cid:9)new(cid:9)to(cid:9)development?(cid:9)Or(cid:9)does(cid:9)DIB(cid:9)funding(cid:9)shift(cid:9)existing(cid:9)resources(cid:9)to(cid:9)more(cid:9)effective(cid:9)uses?(cid:9)How(cid:9)was(cid:9)this(cid:9)judged?InputsProcessesOutputs(cid:9)/(cid:9)impactCost-effectiveness1.(cid:9)FeasibilityAppropriate(cid:9)projects:(cid:9)What(cid:9)are(cid:9)their(cid:9)attributes(cid:9)(eg,(cid:9)sector,(cid:9)problems(cid:9)/(cid:9)opportunities(cid:9)addressed,(cid:9)innovative(cid:9)or(cid:9)scaling(cid:9)up(cid:9)mature(cid:9)interventions,(cid:9)preventive,(cid:9)measurable(cid:9)baselines(cid:9)etc)?(cid:9)Funders(cid:9)/(cid:9)payors:(cid:9)(cid:9)How(cid:9)many?What(cid:9)are(cid:9)their(cid:9)goals(cid:9)and(cid:9)motivations?(cid:9)Was(cid:9)perceived(cid:9)transfer(cid:9)of(cid:9)risk(cid:9)a(cid:9)motivation?(cid:9)Were(cid:9)they(cid:9)easy(cid:9)/(cid:9)difficult(cid:9)to(cid:9)find(cid:9)/(cid:9)engage?(cid:9)Why?Providers:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)are(cid:9)they(cid:9)resource(cid:9)&(cid:9)capital(cid:9)constrained,(cid:9)are(cid:9)they(cid:9)used(cid:9)to(cid:9)PbR(cid:9)contracts,(cid:9)do(cid:9)they(cid:9)already(cid:9)have(cid:9)an(cid:9)appropriate(cid:9)monitoring(cid:9)system(cid:9)etc.)?Investors:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)commercial(cid:9)or(cid:9)foundations,(cid:9)established(cid:9)or(cid:9)new(cid:9)to(cid:9)development,(cid:9)how(cid:9)many)?Intermediaries:(cid:9)Which(cid:9)intermediaries(cid:9)are(cid:9)involved(cid:9)What(cid:9)roles(cid:9)do(cid:9)they(cid:9)play?(cid:9)Who(cid:9)do(cid:9)they(cid:9)represent?(cid:9)How(cid:9)were(cid:9)they(cid:9)funded?Capacity-building:(cid:9)What,(cid:9)if(cid:9)any,(cid:9)support(cid:9)has(cid:9)been(cid:9)provided(cid:9)to(cid:9)help(cid:9)stakeholders(cid:9)prepare(cid:9)for(cid:9)the(cid:9)DIB?(cid:9)Has(cid:9)it(cid:9)been(cid:9)useful?Context:(cid:9)What(cid:9)contextual(cid:9)factors(cid:9)significantly(cid:9)influenced(cid:9)the(cid:9)development(cid:9)of(cid:9)the(cid:9)DIB?Estimates(cid:9)of(cid:9)impact:Was(cid:9)the(cid:9)intervention(cid:9)successful?(cid:9)Does(cid:9)it(cid:9)seem(cid:9)that(cid:9)the(cid:9)funding(cid:9)instrument(cid:9)played(cid:9)a(cid:9)role(cid:9)in(cid:9)whether(cid:9)or(cid:9)not(cid:9)it(cid:9)was(cid:9)(ie,(cid:9)via(cid:9)the(cid:9)mechanisms(cid:9)in(cid:9)(cid:9)3.(cid:9)Implementation)?Comparability(cid:9)to(cid:9)impact(cid:9)from(cid:9)using(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)Were(cid:9)the(cid:9)results(cid:9)different(cid:9)to(cid:9)past(cid:9)/(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)using(cid:9)other(cid:9)instruments?Unintended(cid:9)outcomes:(cid:9)Were(cid:9)there(cid:9)any(cid:9)unintended(cid:9)outcomes,(cid:9)positive(cid:9)or(cid:9)negative?Engagement(cid:9)with(cid:9)beneficiaries:(cid:9)Did(cid:9)the(cid:9)DIBs(cid:9)create(cid:9)more(cid:9)or(cid:9)less(cid:9)engagement(cid:9)between(cid:9)beneficiaries(cid:9)and(cid:9)service(cid:9)providers?Sustainability:(cid:9)Are(cid:9)there(cid:9)reasons(cid:9)to(cid:9)believe(cid:9)any(cid:9)outcomes(cid:9)/(cid:9)impact(cid:9)achieved(cid:9)will(cid:9)be(cid:9)more(cid:9)or(cid:9)less(cid:9)sustainable(cid:9)than(cid:9)those(cid:9)achieved(cid:9)using(cid:9)other(cid:9)instruments?Repeatability:(cid:9)Would(cid:9)the(cid:9)various(cid:9)stakeholders(cid:9)participate(cid:9)in(cid:9)a(cid:9)similar(cid:9)instrument(cid:9)in(cid:9)the(cid:9)future?(cid:9)Under(cid:9)what(cid:9)conditions?What(cid:9)factor,(cid:9)if(cid:9)any,(cid:9)drove(cid:9)improvement?1)(cid:9)change(cid:9)in(cid:9)incentives(cid:9)(mgmt.(cid:9)and/or(cid:9)front-line)2)(cid:9)increased(cid:9)flexibility(cid:9)/(cid:9)autonomy3)(cid:9)support(cid:9)from(cid:9)active(cid:9)investorDid(cid:9)these(cid:9)or(cid:9)other(cid:9)factors(cid:9)increase(cid:9)focus(cid:9)on(cid:9)outcomes(cid:9)and(cid:9)delivery?Were(cid:9)investors(cid:9)&(cid:9)funders(cid:9)/(cid:9)payorsactive(cid:9)or(cid:9)passive(cid:9)in(cid:9)this(cid:9)stage?(cid:9)If(cid:9)active,(cid:9)did(cid:9)they(cid:9)add(cid:9)value?(cid:9)(cid:9)What(cid:9)were(cid:9)challenges?(cid:9)Were(cid:9)they(cid:9)overcome?(cid:9)If(cid:9)so,(cid:9)how?What(cid:9)factors(cid:9)were(cid:9)important(cid:9)for(cid:9)projects(cid:9)that(cid:9)did(cid:9)/(cid:9)did(cid:9)not(cid:9)proceed?(cid:9)2.(cid:9)Structuring(cid:9)the(cid:9)deal3.(cid:9)Implementation4.(cid:9)Evaluation(cid:9)and(cid:9)paymentsWhat(cid:9)measures(cid:9)&(cid:9)method(cid:9)were(cid:9)used(cid:9)to(cid:9)estimate(cid:9)impact?(cid:9)Were(cid:9)these(cid:9)appropriate(cid:9)(eg,(cid:9)were(cid:9)the(cid:9)measures(cid:9)good(cid:9)predictors(cid:9)of(cid:9)positive(cid:9)effects)?What(cid:9)were(cid:9)the(cid:9)timings(cid:9)of(cid:9)the(cid:9)payments(cid:9)(and(cid:9)investments)?(cid:9)Were(cid:9)outcome(cid:9)payments(cid:9)recycled(cid:9)as(cid:9)operating(cid:9)costs?What(cid:9)were(cid:9)challenges(cid:9)in(cid:9)validating(cid:9)the(cid:9)outcome(cid:9)measures(cid:9)(eg,(cid:9)data(cid:9)quality,(cid:9)collection(cid:9)capacity(cid:9)etc.)?How(cid:9)were(cid:9)external(cid:9)factors(cid:9)that(cid:9)influence(cid:9)outcomes(cid:9)addressed?Were(cid:9)repayment(cid:9)terms(cid:9)renegotiated?(cid:9)If(cid:9)so,(cid:9)why?\fAnnex D – DFID Indicative Programme Gantt Chart (subject to change) A104 DIBs Pilot Programme timelineJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDProgrammeBusiness CaseApproval of BCXProject Appraisal , Diligence, Approval (ICRC)Project Appraisal, Diligence, Approval (VE)DFID Annual ReviewsProject Completion ReviewDFID commissioned Evaluation Tentative Timeline for OutputsIssue TenderxSuppliers BiddingxBid evaluation & contractingxEvaluation Inception (4 weeks)xDIBs Design Phase Learning Report (QA)xxxxXMid-Term Evaluation Report (QA)XFinal Evaluation Report (QA)XAnnual Evidence/Learning ReportQuality Assurance of ToR, Design, OutputsICRCDesign (largely complete b4 DFID engaged)PbR Agreement negotiation/finalisationImplementationBuilding of new centres, training staff, testing efficiency measures in 8 centresOperationalisation of the new centresProject Progress ReportsLa Caixa Outcomes Payment (~£0.88m on completion of building of centres)◊SER Outcomes Measurement & Payment (verification activities)NB: ICRC will produce monthly SER reports◊Learning Activities (no internal activities planned)VE DIBDesign Fnalisation & Contract negotiationOutcomes Verifier tender & designImplementationCohort 1dark red = targetting; light red = training and mentoringCash transfer verification & payment◊◊green shows verification of initial seed transfer (larger portion); and second smaller supplementary seed transfer; with ◊ showing donor payment $1 for every $ transferred.Cohort 2Cash transfer verification & payment◊◊Cohort 3Cash transfer verification & payment◊◊Cohort 4Cash transfer verification & payment◊◊Endline Outcomes Measurement & Payment cohorts 1-4◊Cohort 5Cash transfer verification & payment◊◊Cohort 6Cash transfer verification & payment◊◊Cohort 7Cash transfer verification & payment◊◊Endline Outcomes Measurement (cohorts 5-7) & Payment (pooled result cohorts 1-7)◊◊Learning Activities and Reports produced (✓)✓✓✓✓BAT Education DIBDesign of Education DIB IndiaxxxOutcome measurement instrument to be piloted in june/july, and baselines done in july or september)Implementation of Education DIB in IndiaOutcomes Measurement & PaymentsNB: We expect annual outcomes verification and annual results payments, but timing isn't confirmed◊◊◊◊BAT Learning ActivitiesNB: Timing of learning activities & outputs are estimated, and will be confirmed later this yearResearch Report on BAT Education DIB✓✓Selection of areas of feasibility study◊Feasibility Reports for South Asia◊Proof of Concept Reports for South Asia◊DIBs Expansion - Design?Stage 1Stage 2KeyPayments◊Reports Produced✓20232022201620172018201920202021We assume sustained service provision at centres, with maintained or increasing SER and replicated across ICRC PR programmeSome service providers will continue to deliver interventions in the schools after end of the programme.School year runs Sept -July.4 Years of schooling starting Sept 2018\fEnd of ToR Changes to the Terms of Reference Changes to the Terms of Reference were agreed during the inception phase, and set out in the inception report. No other changes have been  made during this research wave.\n",
            "Top  20  :   Development  Stage Early-stage  design Outcome Funder: TBC  Investors: TBC  Service providers: Two service  providers have been shortlisted,  delivering employment and  entrepreneurship interventions.  Four potential service providers  have been identified and are  following detailed due diligence.  Intermediary: KOIS Invest  (feasibility study)   Technical assistance providers:  TBC Outcome Funder: N/A  Investors: N/A  Service providers: N/A  Intermediary:  Social Finance UK  Technical assistance providers:  N/A Late-stage  design – failed  to launch,  pending  availability of  outcome  funding Syrian  Refugee  Employme nt DIB 10  Uganda Sleeping  Sickness  DIB The multi-country DIB  intends to improve the  welfare of Syrian refugees  and vulnerable host  populations by funding job  market integration and  access to livelihoods  interventions in the Middle  East. This includes  employment and  entrepreneurship  interventions.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What was the impact of the intervention?\n",
            "Top  1  :   In terms of the design of the intervention itself, there was some evidence that the DIB led to  some  improvements,  but  this  effect  was  not  as  strong  as  the  impact  on  the  monitoring  procedures. In the Cataract Bond the introduction of the equity target is leading to a greater  focus on outreach, targets for it, and an understanding of whether the people reached through  outreach  are  the  poorest.  There  has  also  been  a greater  focus  on  quality  –  the  hospital  is  doing a much deeper dive to understand quality of outcomes and introduce interventions, such  as regular management and staff sessions, to understand and strengthen quality, something  that stakeholders noted seems to be less consistently done as part of grant-funded projects.  In the VE DIB, the service provider did make substantial improvements to the design of the  intervention, but it is not clear whether this is due to the DIB mechanism or the involvement of  DFID as a funder: “Because of DfID there was a lot of detail required on the design but I would not say it would  uniformly  be  the  case  across  the  DIB  sector”  (Village  Enterprise  representative.  Comment  made during case study consultations) In  the  ICRC  HIB  the  intervention  design  was  no  different  to  how  it  been  implemented  previously.\n",
            "Top  2  :   This report presents the evaluation’s initial findings against these questions. Given the stage  of the interventions funded by the DIBs, findings are focused on the design stage. The effects  of the DIB in terms of the intervention quality and outcomes remain to be seen. Furthermore,  it is important to note that DIBs are still in a pilot phase, and the lessons learned draw on a  small number of ‘test cases’. These findings will continue to be refined and developed based  on additional evidence over the remainder of the evaluation.\n",
            "Top  3  :   Finally, an important part of understanding the effects of using DIBs includes consideration of  the sustainability of the intervention and mechanisms, and the extent to which there may be  potential for take up by the national government. This will be a focus of research waves 2 and  3, and we have planned for extensive consultations with the relevant government officials.\n",
            "Top  4  :   Emerging findings in terms of how the DIB affected the interventions funded by the DIBs were  discussed. The DIB effects table set out in the inception report was used to frame findings. The  key lessons learned were also presented.\n",
            "Top  5  :   The future research waves will explore how the DIB affects the delivery and performance of  the intervention.\n",
            "Top  6  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  7  :   This would appear to have led to degree of risk aversion in these DIBs. This affected both the  selection of service providers and interventions; with careful selection to ensure both service  providers and the interventions had established track records. The evidence would suggest  this risk aversion limited the extent to which other DIB effects materialised, as we describe  further in this section.\n",
            "Top  8  :   2.4 2.3 2.2  The product describes the intervention logic and/or theory of change.  The product provides a relevant and sufficient description of the local,  national and/or international development context within which the  intervention was operating.  The product identifies key linkages between the evaluated intervention and  other relevant projects / programmes / donors. If no linkages are identified,  the product justifies why other projects / programmes / donors were not  relevant to the evaluation.  There is an assessment of the policy context for the intervention and this  includes reference to poverty reduction strategies, gender equality,  environmental protection, and human rights.  The product describes the extent to which the intervention has been  managed and delivered against Paris Declaration principles.\n",
            "Top  9  :   4.5.1.2  Main finding: Extent to which DIB effect present across four projects DIB Effect  Funding  projects  which  would  not  have  been  funded  otherwise,  or  not in  the same guise ICRC QEI VE Yes Yes Yes Cataract Bond  No, likely project  could have been  funded without DIB 4.5.1.3 Analysis from four projects Summary: All four interventions had been funded previously, and so the DIB did not enable  completely new interventions to be funded. However, in three of the DIBs the interventions had  alterations and outcome funders reported that would not have funded these without the DIB;  therefore the DIB did alter the guise of the interventions, including enabling projects to operate  at larger scale or with innovative elements.\n",
            "Top  10  :   It is also interesting to note that the reputational risk was seen as having upsides as well as  downsides.  For  many  of  the  service  providers  across the  DIBs the  spotlight  the  DIB  would  create  on  their  organisation  was  a  motivating  factor  for  joining  the  DIB.  Furthermore,  the  backing  of  an  external  investor  signals  confidence  in  both  the  intervention  and  the  service  provider, and that the outcome targets can be met.\n",
            "Top  11  :   3.  Shared understanding of the policy ‘problem’ and sufficient evidence for the intervention so that it is credible or knowledge-based.\n",
            "Top  12  :   In some instances stakeholders did not agree on the presence of some of the effects – we have  made this clear in the relevant sections.\n",
            "Top  13  :   Section 4 presents the analysis and findings of the evaluation in relation to EQ1, assessing how  the DIB model affects the design and set up phase of development interventions.\n",
            "Top  14  :   The process took a considerable amount of staff time and capacity.\n",
            "Top  15  :   Effectiveness  Any  positive  or  negative  changes  to  effectiveness  as  a  result  of  the  impact  bond.   Any  positive  or  negative  changes  to  equity  as  a  result of the impact bond.\n",
            "Top  16  :   Spillovers To what extent did stakeholders  involved in the DIB use any of the  working practices of the model in  their other work? To what extent did  good practice within the DIBs  spread to other interventions or  organisations?\n",
            "Top  17  :   investors, including Any positive or negative changes to equity as  a result of the impact bond.\n",
            "Top  18  :   It should also be noted that, whilst the evaluation includes a set of indicators to measure these  effects, evaluator judgement was necessary to judge the extent to which the effects were present,  and  the  extent  to  which  these  can  be  attributed  to  the  DIB.  Where  evaluator  judgement  was  applied, we have tried to make this clear in the description of the presence of the effects below.  These  judgements  were  tested  and  verified  with  stakeholders  during  the  internal  learning  workshop. We also shared a draft copy of this report with stakeholders and asked for comments.\n",
            "Top  19  :   A100 Annex 1 – Initial Country Risk Assessment by DFID The programme under evaluation involves activities in multiple countries. DFID has provided  an overall initial risk assessment for the programme locations as shown below: A101 DFID Overall Initial Project/Intervention Summary Risk Assessment Matrix Dec-17Read in conjunction with the FCO Travel Advisory on each countryCountryHIGH RISK LOCATIONSMEDIUM RISK LOCATIONSDate ConductedThemeDFID Risk ScoreDFID Risk ScoreOverall Rating5 - VERY HIGH RISK3 - MEDIUM RISKFCO Travel Advice52Host Nation Travel AdviceN/AN/ATransportation55Security[*]53Civil Unrest53Violence/crime53Terrorism*54War41Hurricane13Earthquake****13Flood*****23Medical Services**53Nature of Project Intervention32Mean (ignoring nature of project)43Mode (ignoring nature of project)5312345Very Low RiskLow RiskMedium RiskHigh RiskVery High RiskMedium*The FCO travel advice for Uganda, Kenya, Nigeria and Mali advises that there is a general threat from terrorism**Medical facilities outside of Capital Cities, and particularly away from cities are limited***FCO advise against all travel to Borno State. There is also a  High Risk (4) threat of kidnapping across Nigeria and Maiduguri in particular**** Earthquake risk is (3) on Indian border with Pakistan and in Delhi***** Flash flooding can occur during the wet season in Nigeria; Eastern Uganda; and monsoon in North India.High RiskFor example: Abuja and Borno State in Nigeria; Mali; Kinshasa in DRC; parts of Kenya, including Nairobi; and the immediacte vicinity of the India-Pakistan border.For example, other project locations incl: Uganda (excluding Karamoja, which is not relevant to this project); Gujarat, Rajasthan, and Delhi in India (with exception of the area in immediate vicinity of the border between India and Pakistan where the Supplier is not required to travel).Dec-17LocationLow SUPPLEMENTARY ANNEXES Annex A1: DFID Theory of Change for DIBs A102 Annex A2: Initial Framework for Assessing Theory of Change for DIBs Initial framework for assessing the Theory of Change behind DIBs, developed during DFID evaluability assessment A103 Were(cid:9)(cid:9)/(cid:9)can(cid:9)deal-breakers(cid:9)/(cid:9)critical(cid:9)success(cid:9)factors(cid:9)identified(cid:9)early?(cid:9)What(cid:9)were(cid:9)they?Costs(cid:9)and(cid:9)cost(cid:9)drivers:(cid:9)(cid:9)What(cid:9)were(cid:9)the(cid:9)duration(cid:9)and(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stages?(cid:9)(cid:9)How(cid:9)were(cid:9)costs(cid:9)divided(cid:9)across(cid:9)the(cid:9)different(cid:9)participants?(cid:9)What(cid:9)factors(cid:9)drove(cid:9)the(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stakeholders?(cid:9)Which(cid:9)costs(cid:9)show(cid:9)potential(cid:9)to(cid:9)decrease(cid:9)in(cid:9)future(cid:9)deals?(cid:9)(cid:9)What(cid:9)steps(cid:9)can(cid:9)be(cid:9)taken(cid:9)to(cid:9)reduce(cid:9)future(cid:9)costs?Comparison(cid:9)with(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)How(cid:9)do(cid:9)costs(cid:9)compare(cid:9)(higher(cid:9)or(cid:9)lower)(cid:9)with(cid:9)alternative(cid:9)funding(cid:9)mechanisms(cid:9)(for(cid:9)both(cid:9)provider(cid:9)and(cid:9)for(cid:9)funder/(cid:9)payors)?(cid:9)For(cid:9)which(cid:9)stages(cid:9)did(cid:9)the(cid:9)costs(cid:9)differ?Cost-effectiveness:How(cid:9)does(cid:9)the(cid:9)effectiveness(cid:9)of(cid:9)the(cid:9)DIB(cid:9)funded(cid:9)projects(cid:9)(ie,(cid:9)impact(cid:9)/(cid:9)cost)(cid:9)compare(cid:9)with(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)by(cid:9)different(cid:9)mechanisms?(cid:9)Additionality(cid:9)of(cid:9)funding:(cid:9)Was(cid:9)the(cid:9)funding(cid:9)for(cid:9)the(cid:9)DIB(cid:9)net(cid:9)new(cid:9)to(cid:9)development?(cid:9)Or(cid:9)does(cid:9)DIB(cid:9)funding(cid:9)shift(cid:9)existing(cid:9)resources(cid:9)to(cid:9)more(cid:9)effective(cid:9)uses?(cid:9)How(cid:9)was(cid:9)this(cid:9)judged?InputsProcessesOutputs(cid:9)/(cid:9)impactCost-effectiveness1.(cid:9)FeasibilityAppropriate(cid:9)projects:(cid:9)What(cid:9)are(cid:9)their(cid:9)attributes(cid:9)(eg,(cid:9)sector,(cid:9)problems(cid:9)/(cid:9)opportunities(cid:9)addressed,(cid:9)innovative(cid:9)or(cid:9)scaling(cid:9)up(cid:9)mature(cid:9)interventions,(cid:9)preventive,(cid:9)measurable(cid:9)baselines(cid:9)etc)?(cid:9)Funders(cid:9)/(cid:9)payors:(cid:9)(cid:9)How(cid:9)many?What(cid:9)are(cid:9)their(cid:9)goals(cid:9)and(cid:9)motivations?(cid:9)Was(cid:9)perceived(cid:9)transfer(cid:9)of(cid:9)risk(cid:9)a(cid:9)motivation?(cid:9)Were(cid:9)they(cid:9)easy(cid:9)/(cid:9)difficult(cid:9)to(cid:9)find(cid:9)/(cid:9)engage?(cid:9)Why?Providers:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)are(cid:9)they(cid:9)resource(cid:9)&(cid:9)capital(cid:9)constrained,(cid:9)are(cid:9)they(cid:9)used(cid:9)to(cid:9)PbR(cid:9)contracts,(cid:9)do(cid:9)they(cid:9)already(cid:9)have(cid:9)an(cid:9)appropriate(cid:9)monitoring(cid:9)system(cid:9)etc.)?Investors:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)commercial(cid:9)or(cid:9)foundations,(cid:9)established(cid:9)or(cid:9)new(cid:9)to(cid:9)development,(cid:9)how(cid:9)many)?Intermediaries:(cid:9)Which(cid:9)intermediaries(cid:9)are(cid:9)involved(cid:9)What(cid:9)roles(cid:9)do(cid:9)they(cid:9)play?(cid:9)Who(cid:9)do(cid:9)they(cid:9)represent?(cid:9)How(cid:9)were(cid:9)they(cid:9)funded?Capacity-building:(cid:9)What,(cid:9)if(cid:9)any,(cid:9)support(cid:9)has(cid:9)been(cid:9)provided(cid:9)to(cid:9)help(cid:9)stakeholders(cid:9)prepare(cid:9)for(cid:9)the(cid:9)DIB?(cid:9)Has(cid:9)it(cid:9)been(cid:9)useful?Context:(cid:9)What(cid:9)contextual(cid:9)factors(cid:9)significantly(cid:9)influenced(cid:9)the(cid:9)development(cid:9)of(cid:9)the(cid:9)DIB?Estimates(cid:9)of(cid:9)impact:Was(cid:9)the(cid:9)intervention(cid:9)successful?(cid:9)Does(cid:9)it(cid:9)seem(cid:9)that(cid:9)the(cid:9)funding(cid:9)instrument(cid:9)played(cid:9)a(cid:9)role(cid:9)in(cid:9)whether(cid:9)or(cid:9)not(cid:9)it(cid:9)was(cid:9)(ie,(cid:9)via(cid:9)the(cid:9)mechanisms(cid:9)in(cid:9)(cid:9)3.(cid:9)Implementation)?Comparability(cid:9)to(cid:9)impact(cid:9)from(cid:9)using(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)Were(cid:9)the(cid:9)results(cid:9)different(cid:9)to(cid:9)past(cid:9)/(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)using(cid:9)other(cid:9)instruments?Unintended(cid:9)outcomes:(cid:9)Were(cid:9)there(cid:9)any(cid:9)unintended(cid:9)outcomes,(cid:9)positive(cid:9)or(cid:9)negative?Engagement(cid:9)with(cid:9)beneficiaries:(cid:9)Did(cid:9)the(cid:9)DIBs(cid:9)create(cid:9)more(cid:9)or(cid:9)less(cid:9)engagement(cid:9)between(cid:9)beneficiaries(cid:9)and(cid:9)service(cid:9)providers?Sustainability:(cid:9)Are(cid:9)there(cid:9)reasons(cid:9)to(cid:9)believe(cid:9)any(cid:9)outcomes(cid:9)/(cid:9)impact(cid:9)achieved(cid:9)will(cid:9)be(cid:9)more(cid:9)or(cid:9)less(cid:9)sustainable(cid:9)than(cid:9)those(cid:9)achieved(cid:9)using(cid:9)other(cid:9)instruments?Repeatability:(cid:9)Would(cid:9)the(cid:9)various(cid:9)stakeholders(cid:9)participate(cid:9)in(cid:9)a(cid:9)similar(cid:9)instrument(cid:9)in(cid:9)the(cid:9)future?(cid:9)Under(cid:9)what(cid:9)conditions?What(cid:9)factor,(cid:9)if(cid:9)any,(cid:9)drove(cid:9)improvement?1)(cid:9)change(cid:9)in(cid:9)incentives(cid:9)(mgmt.(cid:9)and/or(cid:9)front-line)2)(cid:9)increased(cid:9)flexibility(cid:9)/(cid:9)autonomy3)(cid:9)support(cid:9)from(cid:9)active(cid:9)investorDid(cid:9)these(cid:9)or(cid:9)other(cid:9)factors(cid:9)increase(cid:9)focus(cid:9)on(cid:9)outcomes(cid:9)and(cid:9)delivery?Were(cid:9)investors(cid:9)&(cid:9)funders(cid:9)/(cid:9)payorsactive(cid:9)or(cid:9)passive(cid:9)in(cid:9)this(cid:9)stage?(cid:9)If(cid:9)active,(cid:9)did(cid:9)they(cid:9)add(cid:9)value?(cid:9)(cid:9)What(cid:9)were(cid:9)challenges?(cid:9)Were(cid:9)they(cid:9)overcome?(cid:9)If(cid:9)so,(cid:9)how?What(cid:9)factors(cid:9)were(cid:9)important(cid:9)for(cid:9)projects(cid:9)that(cid:9)did(cid:9)/(cid:9)did(cid:9)not(cid:9)proceed?(cid:9)2.(cid:9)Structuring(cid:9)the(cid:9)deal3.(cid:9)Implementation4.(cid:9)Evaluation(cid:9)and(cid:9)paymentsWhat(cid:9)measures(cid:9)&(cid:9)method(cid:9)were(cid:9)used(cid:9)to(cid:9)estimate(cid:9)impact?(cid:9)Were(cid:9)these(cid:9)appropriate(cid:9)(eg,(cid:9)were(cid:9)the(cid:9)measures(cid:9)good(cid:9)predictors(cid:9)of(cid:9)positive(cid:9)effects)?What(cid:9)were(cid:9)the(cid:9)timings(cid:9)of(cid:9)the(cid:9)payments(cid:9)(and(cid:9)investments)?(cid:9)Were(cid:9)outcome(cid:9)payments(cid:9)recycled(cid:9)as(cid:9)operating(cid:9)costs?What(cid:9)were(cid:9)challenges(cid:9)in(cid:9)validating(cid:9)the(cid:9)outcome(cid:9)measures(cid:9)(eg,(cid:9)data(cid:9)quality,(cid:9)collection(cid:9)capacity(cid:9)etc.)?How(cid:9)were(cid:9)external(cid:9)factors(cid:9)that(cid:9)influence(cid:9)outcomes(cid:9)addressed?Were(cid:9)repayment(cid:9)terms(cid:9)renegotiated?(cid:9)If(cid:9)so,(cid:9)why?\fAnnex D – DFID Indicative Programme Gantt Chart (subject to change) A104 DIBs Pilot Programme timelineJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDProgrammeBusiness CaseApproval of BCXProject Appraisal , Diligence, Approval (ICRC)Project Appraisal, Diligence, Approval (VE)DFID Annual ReviewsProject Completion ReviewDFID commissioned Evaluation Tentative Timeline for OutputsIssue TenderxSuppliers BiddingxBid evaluation & contractingxEvaluation Inception (4 weeks)xDIBs Design Phase Learning Report (QA)xxxxXMid-Term Evaluation Report (QA)XFinal Evaluation Report (QA)XAnnual Evidence/Learning ReportQuality Assurance of ToR, Design, OutputsICRCDesign (largely complete b4 DFID engaged)PbR Agreement negotiation/finalisationImplementationBuilding of new centres, training staff, testing efficiency measures in 8 centresOperationalisation of the new centresProject Progress ReportsLa Caixa Outcomes Payment (~£0.88m on completion of building of centres)◊SER Outcomes Measurement & Payment (verification activities)NB: ICRC will produce monthly SER reports◊Learning Activities (no internal activities planned)VE DIBDesign Fnalisation & Contract negotiationOutcomes Verifier tender & designImplementationCohort 1dark red = targetting; light red = training and mentoringCash transfer verification & payment◊◊green shows verification of initial seed transfer (larger portion); and second smaller supplementary seed transfer; with ◊ showing donor payment $1 for every $ transferred.Cohort 2Cash transfer verification & payment◊◊Cohort 3Cash transfer verification & payment◊◊Cohort 4Cash transfer verification & payment◊◊Endline Outcomes Measurement & Payment cohorts 1-4◊Cohort 5Cash transfer verification & payment◊◊Cohort 6Cash transfer verification & payment◊◊Cohort 7Cash transfer verification & payment◊◊Endline Outcomes Measurement (cohorts 5-7) & Payment (pooled result cohorts 1-7)◊◊Learning Activities and Reports produced (✓)✓✓✓✓BAT Education DIBDesign of Education DIB IndiaxxxOutcome measurement instrument to be piloted in june/july, and baselines done in july or september)Implementation of Education DIB in IndiaOutcomes Measurement & PaymentsNB: We expect annual outcomes verification and annual results payments, but timing isn't confirmed◊◊◊◊BAT Learning ActivitiesNB: Timing of learning activities & outputs are estimated, and will be confirmed later this yearResearch Report on BAT Education DIB✓✓Selection of areas of feasibility study◊Feasibility Reports for South Asia◊Proof of Concept Reports for South Asia◊DIBs Expansion - Design?Stage 1Stage 2KeyPayments◊Reports Produced✓20232022201620172018201920202021We assume sustained service provision at centres, with maintained or increasing SER and replicated across ICRC PR programmeSome service providers will continue to deliver interventions in the schools after end of the programme.School year runs Sept -July.4 Years of schooling starting Sept 2018\fEnd of ToR Changes to the Terms of Reference Changes to the Terms of Reference were agreed during the inception phase, and set out in the inception report. No other changes have been  made during this research wave.\n",
            "Top  20  :   4.3.2.2  Main finding: Extent to which DIB effect present across four projects Summary: The DIB mechanism increased the reputational risk in three of the projects.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Were the contracted outcomes achieved?\n",
            "Top  1  :   •  Contracting: There was a poor understanding of outcome funders’ procurement burden which delayed the start of the contracting process •  Financier  engagement:  Given  VE’s  limited  experience  and  infrastructure  to  do  this,  it took considerably longer.\n",
            "Top  2  :   Expansion  of  the  existing  programme  of  a  service  provider  and  implementation  of  a  programme  already  proven  successful  in  new schools (using new methods) of Expansion  the  existing programme of  a service provider Implementation of a  programme already proven  successful but in a new  context Contract focuses on achievement of specific  outcomes – intervention defined but subject  to  change  and  adaptation  depending  on  needs Contract  focuses  on  achievement  of  specific  outcomes  –  defined  intervention Contract  involves  a  specific  and well-defined intervention Characteristic  Description   provider,  specific  or  specific  outcomes  which  enables  intervention  service  to  providers  defined   organise  work  as  they  prefer.   Identifying metrics and structuring payments Nature  payment  outcomes of Were  payments  made  squarely  for  outcomes  or  was some payment made  for inputs or activities?\n",
            "Top  3  :   research indicates Payment  by  Results  approaches  enable  donors  to  transfer the risk/uncertainty over whether an intervention  will achieve results to the provider.   However,  that  some  providers  (particularly  those  with  smaller  balance  sheets,  or  less  access to commercial loans) would be unable pre-finance  their  intervention  and  wait  for  payment  on  delivery  of  results, or would be unwilling to take on the financial risk  associated with underperforming on a PbR contract. As a  result providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of  contracts.3233 B.3  How strong is the evidence on DIBs?\n",
            "Top  4  :   Identifying and selecting stakeholders and managing relationships 18. Across three of the DIBs, it was challenging to engage outcome funders. There is a benefit  to  identifying  outcome  funders  interested  in  using  outcome  based  contracting,  and  the  types  of  interventions  they  are  interested  in  earlier  on,  and  recognising  that  outcome  funders  need  to  be  involved  in  the  design  of  the  DIB.  Identifying  outcome  funders  first  could also enable a competitive process for selecting service providers. On the other hand,  outcome  funders  are  concerned  about  the  risks  of  getting  involved  with  a  new  funding  mechanism, and it can be easier for outcome funders to get involved at a later stage, when  the other stakeholders have been identified and the terms are more developed.\n",
            "Top  5  :   Identifying and selecting stakeholders and managing relationships 6.  Across three of the DIBs, it was challenging to engage outcome funders. There is a benefit  to  identifying  outcome  funders  interested  in  using  outcome  based  contracting,  and  the  types  of  interventions  they  are  interested  in  earlier  on,  and  recognising  that  outcome  funders  need  to  be  involved  in  the  design  of  the  DIB.  Identifying  outcome  funders  first  could also enable a competitive process for selecting service providers. On the other hand,  outcome  funders  are  concerned  about  the  risks  of  getting  involved  with  a  new  funding  mechanism,  and  it  can  be  more  efficient  for  outcome  funders  to  get  involved  at  a  later  stage,  when  the  other  stakeholders  have  been  identified  and  the  terms  are  more  developed.\n",
            "Top  6  :   •  Clear and measurable outcomes: The interventions’ target outcomes also needed to  be clear and measurable, in order to enable the development of outcome metrics (see  Section 6.2.2 for further discussion on identifying metrics and structuring payments) •  Feasible  timeframe  for  achieving  outcomes:  The  target  outcomes  of  the  intervention need to be feasible within the duration of the impact bond. Stakeholders  in the QEI DIB thought that the education sector was especially appropriate, given the  fact  that  outcomes  are  attached  to  the  academic  year  and  possible  in  a  shorter  timeframe, which enables outcome targets to be based on actual outcomes instead of  proxies. Similarly, ICRC commented that certain centres within its programmes would  not have been suitable, given the difficulty of operationalising the centres within five  years. Finally, the VE DIB and Cataract Bond outcomes were tailored to the duration  of the impact bond.\n",
            "Top  7  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  8  :   QEI: The three NGO organisations are all expansions of programmes with existing evidence  of their effectiveness (improvements in outcomes compared to a counterfactual). Therefore,  we worked with the service provider to compare data on performance, and qualitatively explore  the differences in how the project was set-up and developed, at what cost, and what was the  working relationship between the stakeholders involved, in areas affected by DIB and non-DIB  contracts.\n",
            "Top  9  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  10  :   4.6.2.2  Main finding: Extent to which DIB effect present across four projects Summary:  In  general  attaching  outcomes  to  payments  led  to  more  rigorous  design  of  monitoring procedures than fee-for-service contracts or grants, but appeared no different than  other forms or PbR. The DIB had mixed impact on the design of the intervention.\n",
            "Top  11  :   “The joint  awareness and  wealth of  foundation  knowledge  that  came into  play on  this,  you  can’t  underestimate  you  know…  part  of  the  beauty  of  this  piece  in  international  development…it’s about the data that’s sitting within foundations, especially deep technical  foundations like MSDF…to share and be open minded, and move a bit…. And to say right  we’re willing to put that out there to be tested.” (Representative from British Asian Trust, QEI  DIB. Comment made during case study consultations) One outcome funder felt this greater collaboration between outcome funders was because the  focus on outcomes brought about by the DIB mechanism ensured they were united towards  “one common outcome” and although views on how to achieve this outcome varied, it was still  described  as  ‘a  single  united  goal’.  Interviewees  valued  these  new  partnerships  and  collaborative ways of working; they felt the partnerships brought together a merged expertise,  whilst the willingness to share information and data enabled organisations to gain a greater  understanding of the context.\n",
            "Top  12  :   Transparency and accountability: There is limited evidence to date that beneficiaries and other  stakeholders have used the verified outcome data in order to demand better services and drive  accountability. However, the extent to which verified outcome data has been shared and validated  with beneficiaries will be important to explore.\n",
            "Top  13  :   •  Outcome funder engagement: Time was spent engaging with foundations that did not  result in any commitments to provide outcomes funding. Furthermore, engaging multiple  outcome funders at different times created inefficiency.\n",
            "Top  14  :   •  Outcome verification can lead to greater transparency around the impact of the funding and the service providers’ work, and correspondingly, improved accountability.\n",
            "Top  15  :   “The  collaboration  it  has  been  remarkable from  the  UBSOF  and  Dalberg.”  (Representative  from service provider, QEI DIB. Comment made during case study consultations) This strengthening was mainly due to the alignment of interests between outcome funders and  service  providers  to  ensure  the  projects  were  designed  robustly  with  accurate  and  clear  outcome measures. It was also a consequence of the complexities in designing the DIB, which  required strong levels of communication between the different parties.\n",
            "Top  16  :   4.0  Analysis and Findings – DIB Effects Summary The DIB mechanism has made it possible to implement Payment by Results (PbR) contracts  in contexts where, previously, this would not have been possible because the projects were  too  risky  or  too  large.  This  is  primarily  due  to  the  new  partnerships  created  between  governments,  donors,  delivery  partners  and  (to  a  degree)  the  private  sector,  in  which  the  financial risk is shared between these groups. The DIB has fostered new working relationships  between  stakeholders  and  has  led  to  greater  levels  of  collaboration  than  is  normally  seen,  primarily  because  the  DIB  aligns  all  stakeholders’  interests  but  also  because  the  intensive  design stage forces closer partnership working. A large amount of work has been done in all  four  DIBs  to  build  a  stronger  performance management  infrastructure,  including  investing in  new  monitoring  systems  and  working  closely  with  the  service  providers  to  embed  adaptive  management systems.\n",
            "Top  17  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  18  :   There appears to be a tension between testing a ‘pure’ DIB, and tailoring the DIB to meet  the  objectives  of  stakeholders.  For  example,  certain  outcome  funders  in  the  Cataract  Performance Loan and the ICRC HIB were disappointed in some of the terms offered. One of  the outcome funders felt that the final terms dampened the most important element of the DIB  to  them,  namely  testing  the  integrity  of  the  DIB  model,  and  especially  the  aspect  of  risk  sharing.31 Similarly, an outcome funder in the ICRC HIB expressed disappointment that a risk  guarantee  was  included,  as  well  as  the  fact  that  there  was  a  payment  attached  to  the  construction of the centres. However, other actors felt that the changes were a promising step  forward in terms of enabling an investor to participate. It should also be borne in mind that a  key concern set out by Arena (2016) and echoed by Palladium and USAID (2016) is that during  this  phase  of  the  market,  as  outcome  funders  and  other  actors  are  still  building  up  the  architecture  to  supports  DIBs,  too  much  tailoring  and  ‘work  arounds’  can  introduce  complications and make it difficult to standardise processes to reduce transaction costs, thus  potentially limiting the model’s benefits.\n",
            "Top  19  :   Outcome Funder: Goodbye  Malaria, underwritten by Nandos  and other corporates.   Investors: not defined.   Service providers: Lubombo  Spatial Development Initiative  (LDSI) II.   Intermediary:  D. Capital  Partners.  Technical assistance providers:  the University of Pretoria, the  Medical Research Council, and  the National Malaria Control  Programme within the Ministry of  Health (Mozambique).  Outcome evaluator: TBC Payment terms: The  payment would have been  made as a bullet payment  at the end of the third year  of bond implementation,  based on the achievement  of expected outcomes.  Outcome metric: 60%  reduction in the prevalence  and incidence of malaria  cases, compared to  baseline rates at year 1.  Incidence of malaria cases  based on prevalence  testing done at sentinel  sites in each district.    Range of returns: The  maximum potential loss of  investment for investors  would have been 30%, and  the maximum return 0.05%.\n",
            "Top  20  :   In the Cataract DIB outcomes funders felt the reporting was more efficient and transparent  compared  to  other  projects  they  have  been  involved  in,  and  more  work  was  done  with the  hospital  to  ensure they understood  the targets  set.  It  was also  strongly  apparent  in the  VE DIB; for example as a consequence of the DIB they have implemented enhanced cost tracking  to examine spending in the DIB context versus other programs in order to improve efficiency.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results outcomes achieved impact\n",
            "Top  1  :   M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited  number of evaluations seeking to identify the instrument effect and the challenge of establishing  comparative baselines.\n",
            "Top  2  :   Impact Expected outcomes are produced…more effectively than with other approaches…more efficiently  than with other approaches… •  With the focus on results and not inputs, this also enables a market for impact bonds, for  example  through  outcome  funds,  which  can  be  used  to  increase  competition  in  the  delivery of target outcomes and drive down costs.\n",
            "Top  3  :   •  Secondly, use of impact bonds and the requirement of a measurable outcome metric may  promote  narrow  conceptions  of  programme  design,  constraining  possible,  fundable  solutions  to  those  that  generate  high  returns,  which  can  be  captured  in  a  performance  management framework. The move to a narrow conception of outcomes means that that  impact bonds undermine systemic issues. For example, Cooper et al (2016) note that a  SIB working on homelessness failed to address systemic issues, and instead relied on an  understanding of a homeless person as a failed individual. This more narrow view also  has implications for the sustainability of results. Also, benefits achieved in one area may  be transferred as costs to another area, outside the scope of what is covered by the SIB  outcome metrics (Warner 2013).\n",
            "Top  4  :   Secondary  users  of the learning generated  by  the evaluation  will  be  organisations that are  using or thinking about using impact bonds or similar approaches to financing development  local  and  national  include  outcome  programmes.  Such  organisations  governments in developing countries as well as public and private donors who want to achieve  results  for  a  given  population),  investors  (private  and  public  sector  organisations  that  are  willing to pre-finance social impact projects in developing countries and be repaid on a pay- for-success basis), and service providers (NGOs, charities, social enterprises, private sector  organisations that deliver services to achieve development outcomes). They will benefit from  the  findings  produced  by  the  evaluation,  and the  practical  recommendations  it  contains  for  using DIBs and DIB-like structures in the future. Please see governance section for how users  are represented or engaged in the evaluation.\n",
            "Top  5  :   Secondary  outcomes  resulting  from  improved  incomes,  such  as  wellbeing,  diets,  access  to  education and healthcare are achieved.\n",
            "Top  6  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond  Role  of  the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond Measuring impact Validation  impact of Payment  based  experimental/quasi- experimental  or  validated  administrative data7 on Payment  based  on  validated  administrative data.   This will include verification of  records  physical  and  verification  of  mobility  of  beneficiaries.\n",
            "Top  7  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the SIB Carter  et  al  (2016) Gustafsson- Wright  et  al  (2017) Carter  et  al  2018 Gustafsson- Wright  et  al  (2017) Arena  et  al  (2016) Measuring impact  Validation  of  outcome metrics used Methodology  to  estimate  the  outcome  of  the programme Validated  administrative  data  /  quasi- experimental  experimental methods or Gustafsson- Wright  et  al  (2017) A163 Annex J: DIBs reviewed as part of programme level consultations As part of our programme level data collection, the evaluation team interviewed a range of stakeholders, involved in other DIBs in various  stages of development, and reviewed a range of deal and proposal documentation. Our interviews were undertaken in the second half of  2018, and the table summarises the information we have for these 9 DIBs, in terms of the objective of the DIB, the stage of development,  the stakeholders involved, the structure of the DIB and the value of the DIB. It must be noted that the majority of these DIBs are under  negotiation, and the table below reflects the information provided to the evaluation team during the consultation, and may be already out of  date by the time of this report’s publication.\n",
            "Top  8  :   •  Outcome verification can lead to greater transparency around the impact of the funding and the service providers’ work, and correspondingly, improved accountability.\n",
            "Top  9  :   32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government’s  use  of  payment  by  results  https://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of- payment-by-results.pdf   33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts.  Dalberg  https://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by- results-contracts Intelligence A83 • • Impact bonds have enabled the development of strong monitoring and  evaluation systems: the impact bond mechanism incentivises evidence collection  and can therefore lead to improving outcomes for service users through identifying  interventions that work.   Impact bonds can shift the focus of government toward preventive services:  this could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and  we have to be mindful that the portfolio of SIBs projects target different outcomes, emerging  evidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve  effectiveness and efficiency of outcome delivery, and generate valuable impact evidence.\n",
            "Top  10  :   Measuring impact This sub-section explores the balance between reducing the transaction costs and increasing  the  benefits  of  the  impact  measurement  stage  in  terms  of  accuracy  and  usability.  The  necessary conditions for measuring impact largely relate to having clear outcomes that can  be attributable to the intervention, and the robust metrics to capture the targeted outcomes,  which are discussed in sections 6.0 and 6.2.1 respectively.\n",
            "Top  11  :   Equity Detail  Any savings in programming costs as a result  of  the  impact  bond.  i.e.  lower  reporting/audit  costs.     How effectively are the risks being transferred,  and how well is this aligned with risk?   What  are  the  effects  on  outcomes  (including  those not captured by the outcome measure)   How  well  are  the  programmes  fulfilling  their  targeting  strategy?  Are  there  certain  sub- groups  that  are  not  being  reached?  The  approach  to  equity  will  be  guided  by  the  individual programmes’ targeting strategies, to  understand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the  effectiveness  of  the  targeting  strategy  of  the  DIB, especially in terms of the hard to reach.\n",
            "Top  12  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  13  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  14  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  15  :   Upper Quartile. 2015. Evaluation of Results Based Aid in Rwandan Education, available at  http://iati.dfid.gov.uk/iati_documents/5549076.pdf the  Results-Based  Financing  Programme Valadez, J., Jeffrey, C, Brant, T. Vargas, W and Pagano, M. 2015. Final Impact Assessment  of  for  Northern  Uganda,  available  at  https://www.gov.uk/governemnt/uplaods/system/uplaods/attachment_data/file/607579/Evalu ation-of-Results-Based -Financing-Programme-for-Norther-Uganda.pdf Witter, S., Zaman, R., Scott, M. and Mistry, R. 2016. Evaluation of Delivering Reproductive  Health  Results  (DRHR)  through  non  state  providers,  MSI/PSI  Impact  Evaluation  Report,  at:  Available  https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/533669/Delive ring-Reproductive-Health_Results-Non-State-Providers_Pakistan1.pdf A111 Annex D: EQUALs criteria mapped to report sections Ref  EQUALs Criteria 1. STRUCTURE AND CLARITY 1.1 1.2 1.3 1.4 The product is accessible to the intended audience (e.g. free of jargon,  written in plain English, logical use of sections, appropriate use of tables,  graphs and diagrams).  It is clear who has carried out the evaluation.  An executive summary is included, and it can stand alone as an accurate  summary of the main product.  The annexes contain – at the least – the original TORs, the evaluation  framework, a bibliography and a list of consultees.\n",
            "Top  16  :   Center  for  Global  Development  and  Social  Finance  (2013).  Investing  in  Social  Outcomes:  DIBs:  The  Report  of  the  Development  Impact  Bond  Working  Group.  Center  for  Global A106 Development and Social Finance. https://www.cgdev.org/sites/default/files/investing-in-social- outcomes-development-impact-bonds.pdf Center for Global Development and Social Finance (2014). DIBs Briefing Note.\n",
            "Top  17  :   Table 2.1: Evaluation Framework – EQ1 Key  evaluation  questions Effectiveness and sustainability sub-questions Indicators EQ1: Assess  how the DIB  model affects  the design,  delivery, To  what  extent  were  the  three  DIB  projects  successful  in  realising  their  aims,  outputs, outcomes and impacts?   To what extent was the level of success and failure due to the DIB model - was  the DIB model a small, medium or large driver of success and was it at all critical  to the projects’ overall performance?\n",
            "Top  18  :   Outcome  verification  data  will  be  used  to  the  understand  returns  payable.  The  data  can  also  be  compared against the  other  outcome  data,  to  understand  the  extent  to  which these  are  correlated  (improvement  in  the  target  outcome  but  worsening  across  other  outcomes  may A147 Outcome  Verification x x x Service  provider Verification  reports a  quarterly  report  from  The  Magrabi  Foundati on that  includes  all the  metrics  for the  hospital  Volta  has  already  provided Village  Enterprise  (quarterly  report)  includes all the  metrics for the  project IDInsight has  outcome  verification  report for each  cohort (the first  of these  reports has  already been  received) BAT /  DfID  (a  quarterly  report  that  includes  all the  metrics  for the  project) Gray  Matters  India  (we  have  already  received  the  baseline  report) Data type Examples  of relevant  reports How this data will  be used Comp arison  progra mmes Learning  Activities Internal  and  external  learning  reports Progress  reports Investment  returns   Outcome  payments perverse suggest  incentives).   be  Learning  will  across  compared  DIBs  and  contextualised  within  the  from  learning  other impact bonds.    To  understand  how  the  DIB  performs  against targets.\n",
            "Top  19  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  20  :   Effectiveness  Any  positive  or  negative  changes  to  effectiveness  as  a  result  of  the  impact  bond.   Any  positive  or  negative  changes  to  equity  as  a  result of the impact bond.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results\n",
            "Top  1  :   Pearson, M. 2011. Results based aid and results based financing: What are they? Have they  delivered results? London: HLSP.\n",
            "Top  2  :   Ibidem; Pereira, J; Villota, C. 2013. Hitting the target? Evaluating the effectiveness of results- based approaches to aid. Brussels: EURODAD.\n",
            "Top  3  :   G.1 ICRC Question Yes/No Comments Validity – Data should clearly and adequate represent the intended results Indicator and collection methods  are clear and sensible.\n",
            "Top  4  :   DFID. 2016. Project Completion Report of Pilot Project of Results Based Aid in the Education  Sector in Ethiopia, available at: http://iati.dfid.gov.uk/iati_documents/5419380.odt DFID. 2016b. Annual Review of WASH Results Programme 2016 Annual Review, available  at: http://iati.dfid.gov.uk/iati_documetns/5498698.odt DFID  (2017).  Economic  Development  Strategy:  prosperity,  poverty  and  meeting  global  challenges.\n",
            "Top  5  :   M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited  number of evaluations seeking to identify the instrument effect and the challenge of establishing  comparative baselines.\n",
            "Top  6  :   Kandpal,  E.  2016.  Completed  Impact  Evaluations  and  Emerging  Lessons  from  the  Health  Results  at:  https://www.rbfhealth.org/sites/rbf/files/IE%20and%20emerging%20lessons_Eeshani%20Ka ndpal.pdf Innovation available Portfolio, Learning Trust Fund Khatib-Othman, H. 2016. Country Programmes: Strategic Issues, Report to the [GAVI] Board  7-8  December  2016,  Appendix  B,  available  at  http://www.gavi.org/about/governance/gavi- board/minutes/2016/7-dec/minutes/07a---country-programmes---strategic-issues/ King, J. 2017. Using Economic Methods Evaluatively. American Journal of Evaluation, Vol 38,  issue 1, March 2017 King and OPM. 2018. The OPM Approach to Assessing value for Money: A Guide. Oxford  Policy Management Ltd.\n",
            "Top  7  :   Upper Quartile. 2015. Evaluation of Results Based Aid in Rwandan Education, available at  http://iati.dfid.gov.uk/iati_documents/5549076.pdf the  Results-Based  Financing  Programme Valadez, J., Jeffrey, C, Brant, T. Vargas, W and Pagano, M. 2015. Final Impact Assessment  of  for  Northern  Uganda,  available  at  https://www.gov.uk/governemnt/uplaods/system/uplaods/attachment_data/file/607579/Evalu ation-of-Results-Based -Financing-Programme-for-Norther-Uganda.pdf Witter, S., Zaman, R., Scott, M. and Mistry, R. 2016. Evaluation of Delivering Reproductive  Health  Results  (DRHR)  through  non  state  providers,  MSI/PSI  Impact  Evaluation  Report,  at:  Available  https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/533669/Delive ring-Reproductive-Health_Results-Non-State-Providers_Pakistan1.pdf A111 Annex D: EQUALs criteria mapped to report sections Ref  EQUALs Criteria 1. STRUCTURE AND CLARITY 1.1 1.2 1.3 1.4 The product is accessible to the intended audience (e.g. free of jargon,  written in plain English, logical use of sections, appropriate use of tables,  graphs and diagrams).  It is clear who has carried out the evaluation.  An executive summary is included, and it can stand alone as an accurate  summary of the main product.  The annexes contain – at the least – the original TORs, the evaluation  framework, a bibliography and a list of consultees.\n",
            "Top  8  :   G.2 QEI Question Yes/No Comments Validity – Data should clearly and adequate represent the intended results Does the information collected measure what  it is supposed to measure?\n",
            "Top  9  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  10  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  11  :   Sections 4, 5 and 6 Sections 4, 5 and 6  Annexes C and H Sections 4, 5 and 6 Sections 4, 5 and 6 Sections 4, 5 and 6 Sections 4, 5 and 6 7. FINDINGS 7.1  Findings follow logically from the analysis.  7.2  Findings address the evaluation questions and criteria.  7.3  The relevance of the context (e.g. developmental, policy, institutional) is Sections 4, 5 and 6  Sections 4, 5 and 6  Sections 3, 4, 5 and 6 7.4  The evidence is clear and sufficiently triangulated.  7.5  Findings are useful and they are presented in ways that are accessible to Sections 4, 5 and 6  Sections 4, 5 and 6 7.6  Findings reflect diverse views and interests. If not, there is adequate Sections 4, 5 and 6 taken into account.\n",
            "Top  12  :   The  findings  from  the  qualitative  analysis  were  triangulated  with  the  findings  from  the  quantitative  analysis,  which  was  described  above.  The  two  sets  of  data  were  examined  to 39 Ritchie, J., and Lewis, J. (2013) Qualitative Research Practice, SAGE, Sections 8-9.\n",
            "Top  13  :   transparent  and  open answer 100% of outputs are delivered on  time,  agreed  evaluation  questions  and  are  rated  by  EQUALS.\n",
            "Top  14  :   ix Contents Acknowledgements and disclaimer .......................................... i Executive Summary ................................................................... ii  Recommendations ............................................................................................. viii  List of Tables ......................................................................................................... 3  Table of Figures .................................................................................................... 5 1.0 2.0 3.0 4.0 5.0 Introduction ................................................................. 1  Overview of the DIBS pilot programme ............................................. 1  Objectives of the Evaluation .............................................................. 4  Scope of the Research Wave 1 Report .............................................. 5  Overview of the Evaluation Process.................................................. 5  Report structure .................................................................................. 6 Evaluation Framework and Methodology .................. 8  Evaluation framework for the evaluation .......................................... 8  Overview of the methodology .......................................................... 12  Methodological limitations ............................................................... 18 Summary of the DIBs ................................................ 20  Programme components .................................................................. 22  Stakeholders involved in the DIBs .................................................. 26  DIB structures ................................................................................... 27  Conclusion ........................................................................................ 30 Analysis and Findings – DIB Effects ........................ 31  The DIB effect indicators .................................................................. 32  Presence of the DIB effect indicators: Summary............................ 34  Risk transfer effects ......................................................................... 37  Partnership effects ........................................................................... 40  Financing and funding effects ......................................................... 43  Design effects ................................................................................... 47  Other factors influencing the DIB effect .......................................... 51  Additional effects not identified in the framework ......................... 52  Conclusions ...................................................................................... 52 Analysis and Findings – Costs of designing and  delivering DIBs .......................................................... 55  Economy ........................................................................................... 57 6.0 Efficiency ........................................................................................... 68  Effectiveness ..................................................................................... 68  Equity................................................................................................. 72  Conclusion ........................................................................................ 73 Analysis and Findings – Improving the process of  designing and agreeing DIBs ................................... 75  Identifying appropriate interventions .............................................. 77  Identifying metrics and structuring payments ................................ 81  Measuring impact ............................................................................. 85  Identifying and selecting stakeholders and managing relationships  ........................................................................................................... 87  Structuring the vehicle and developing the operating model ........ 95  Conclusion ........................................................................................ 97 7.0 Lessons .................................................................... 101 8.0 Recommendations .................................................. 104  Recommendations to all DIB stakeholders ................................... 104  Recommendations to DIB designers ............................................. 104 Annex A: Case study reports .................................................... 1 Annex B: Terms of Reference ................................................. 81  Background and Context ................................................................. 81  B.1  What do we mean by other aid mechanisms? ................................ 82  B.2  How strong is the evidence on DIBs? ............................................. 83  B.3  What is the DFID DIBs pilot programme? ....................................... 84  B.4  Users of the Evaluation .................................................................... 85  B.5  Evaluation Methodology ................................................................... 90  B.6  Data Sources ..................................................................................... 91  B.7  Evaluation Outputs and Timeframe ................................................. 93  B.8  Lighter-Touch Interim Outputs ........................................................ 95  B.9  Evaluation Management Team ......................................................... 97  B.10 Annex C: Bibliography .......................................................... 106 Annex D: EQUALs criteria mapped to report sections ....... 112 Annex E: Evaluation methodology ....................................... 116  Evaluation Framework .................................................................... 117  E.1  DIB-level research........................................................................... 124  E.2 E.3  E.4  E.5  E.6  E.7 Programme-level Research ............................................................ 135  Sector-level Research .................................................................... 135  Approach to data collection ........................................................... 136  Analysis, Reporting and Dissemination ........................................ 137  Involvement of stakeholders .......................................................... 140 Annex F: Individual DIB level plans ...................................... 144 Annex G: Data Quality Assessment ..................................... 149  ICRC ................................................................................................. 149  G.1  QEI ................................................................................................... 150  G.2  Village Enterprise............................................................................ 152  G.3   Cameroon Cataract Bond ............................................................... 154  G.4 Annex H: Consultees and Sources reviewed ...................... 157 Annex I: Framework for categorising DIBs .......................... 162 Annex J: DIBs reviewed as part of programme level consultations ........................................................... 164 Annex K: Learning workshop note ....................................... 172 Annex L: VfM Analysis – Supporting Evidence ................... 174  ICRC ................................................................................................. 174  L.1  Quality Education India DIB ........................................................... 174  L.2  VE DIB.............................................................................................. 175  L.3  Cameroon Cataract Bond ............................................................... 175  L.4 Annex M: Literature Review .................................................. 177  Hypothesised effects of DIBs ........................................................ 177  a.  Input ................................................................................................. 189  b.  Recommendations .......................................................................... 196  c.  What approaches have been used to evaluate impact bonds? What  d.  are the main challenges and solutions? ....................................... 198 Annex N: List of Acronyms ................................................... 201 List of Tables Table 2.1: Evaluation Framework – EQ1 ............................................... 8 Table 2.2: Evaluation Framework – EQ2 ............................................. 10  Table 2.3 Stakeholders consulted ....................................................... 13  Table 2.4 Comparator Sites ................................................................ 14  Table 2.5: Deliverables mapped to target audiences........................... 17  Table 2.6: Limitations and mitigations ................................................. 18  Table 3.1: Programme components .................................................... 22  Table 3.2: Key stakeholders ................................................................ 26  Table 3.3: DIBs against DIB dimensions ............................................. 27  Table 4.1: DIB effect indicators ........................................................... 33  Table 4.2: Presence of DIB effect indicators in the four DIB projects .. 35  Table 6.1: Project focus and measurement approach ......................... 87  Table 6.2: Advantages and disadvantages to different approaches to  identifying and engaging with stakeholders ......................................... 91 Table B.1: Alternative aid mechanism ................................................. 82  Table B.2 EO 1: Inception Report ....................................................... 93  Table B.3: EO2 – Evaluation Report on the Process of designing and  launching DIBs .................................................................................... 93  Table B.4: EO3 –  Mid-Term Evaluation Report on DIBs ..................... 94  Table B.5: EO4 – Final Evaluation Report on DIBs ............................. 94  Table B.6: Good Performance Indicators ............................................ 98 Table E.1: Evaluation Framework ..................................................... 117  Table E.2: DIB effects and indicators ................................................ 122  Table E.3: Stakeholder consultations in RW1 ................................... 125  Table E.4: Research Waves ............................................................. 131  Table E.5: VfM Framework ............................................................... 131  Table E.6: VfM Indicators .................................................................. 132  Table E.7: Costing Structure ............................................................. 133  Table E.8: Communication Plan ........................................................ 140 Table F.1 : Proposed consultations ................................................... 144  Table F.2: Value for Money data ....................................................... 145  Table F.3: Other data ........................................................................ 147 Table M.1: Sources consulted ........................................................... 178  Table M.2: Impact bond principles .................................................... 182  Table M.3: Categorisation of SIBs by level of innovation ................... 185  Table M.4: Challenges of designing impact bonds ............................ 196 Table of Figures Figure 1.1: DIBs pilot programme theory of change .............................. 3  Figure 4.1: DFID risk assessment of three DIBs ................................. 38 Figure M.1: Framework for synthesising evaluation evidence ........... 177  Figure M.2: Strengths and weaknesses of existing evidence and  evaluation approaches and methods related to SIBs and DIBs (Drew  and Clist 2015:27) ............................................................................. 199 1.0  Introduction Overview of the DIBS pilot programme 1.1.1 DIBs and the current stage of the market.\n",
            "Top  15  :   11.3 11.4 The report indicates whether the evaluation team was able to work freely  and without interference.  Information sources and their contributions were independent of other  parties with an interest in the evaluation.\n",
            "Top  16  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  17  :   Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon  Cataract d e w e v r e n t i I d e w e v r e n t i I l a t o T l a t o T l a t o T l a t o T d e w e v r e n t i I d e w e v r e n t i I A126 Outcome Funders  Investors 3 (4)  1 (1) 5  2 3 (5)  1 (3) 5  1 2 (3)  2 (2) 3  - 3 (4)  2 3  2 ICRC QEI VE PbR Comparator sites  Advisors / Intermediaries /  Performance Managers  Service Providers  Other funders  Outcome Evaluator  DIB researchers 1 (2)  1 (3) 1 (2)  0  0  - n/a  1 1  1  1  - 1 (2)  3 (4) 3 (3)  1 (2)  1 (1)  - n/a  3 3  1  1  - 0  1 (4) 1 (4)  -  0  - Cameroon  Cataract n/a  1 0  1(2) n/a  1 1  -  1  - 1 (2)  -  0  1 (1) 1  -  1  1 Notes: The “interviewed” column sets out the number of organisations interviewed, and in parenthesis, the number of individuals  interviewed (in certain organisations, we interviewed more than one individual). The “total” column sets out the total number of  organisations within this stakeholder category.\n",
            "Top  18  :   6.2  The analysis is presented against the evaluation questions and criteria.\n",
            "Top  19  :   N/A Yes All  users  calculated  and  not  calculated  based  on  a  sample  basis.\n",
            "Top  20  :   When reading the table below, please see the Evaluation Outputs Section for the proposed  content of each ‘Evaluation Output (EO)’ referenced in the table.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes achieved\n",
            "Top  1  :   Secondary  outcomes  resulting  from  improved  incomes,  such  as  wellbeing,  diets,  access  to  education and healthcare are achieved.\n",
            "Top  2  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  3  :   Methods: Qualitative   Sources: As above + access to  the data used to verify if the  desired programme outcomes  have been achieved. See Data  Annex for which outcomes will  have been measured by  expected Mid Term and Final  Evaluation Report dates.\n",
            "Top  4  :   M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited  number of evaluations seeking to identify the instrument effect and the challenge of establishing  comparative baselines.\n",
            "Top  5  :   3.  Maximum payments to investor: This includes the maximum return payable to the  investor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any  interest payment.\n",
            "Top  6  :   Impact Expected outcomes are produced…more effectively than with other approaches…more efficiently  than with other approaches… •  With the focus on results and not inputs, this also enables a market for impact bonds, for  example  through  outcome  funds,  which  can  be  used  to  increase  competition  in  the  delivery of target outcomes and drive down costs.\n",
            "Top  7  :   2.  Clear  outcomes  –  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).\n",
            "Top  8  :   •  Outcome funder engagement: Time was spent engaging with foundations that did not  result in any commitments to provide outcomes funding. Furthermore, engaging multiple  outcome funders at different times created inefficiency.\n",
            "Top  9  :   Effectiveness37 To  what  extent  were  the  three  DIB  projects successful in realising their  aims,  outputs,  outcomes  and  impacts?\n",
            "Top  10  :   outcomes, and once a year  with relation to all the other  outcomes.   Outcome metric:  Recruitment; mother child  unit: antenatal care (ANC)  access, reduction in  maternal alcohol  consumption (RMAC),  prevention of mother  to child transmission of  HIV, birth-weight (BW); 0-1  years: exclusive breast  feeding  (EBF), weight for age,  prevention of HIV  transmission, prevention  and treatment of TB; 1-2  years: height for age,  immunization, prevention  and treatment of TB,  primary caregiver  assessment.  Range of returns:  Investment rate of return is  capped at 16%.\n",
            "Top  11  :   16. Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  12  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  13  :   4.  Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  14  :   •  Outcome verification can lead to greater transparency around the impact of the funding and the service providers’ work, and correspondingly, improved accountability.\n",
            "Top  15  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  16  :   5.4.2 Outcome metric and verification strategy The use of an outcome metric has implications for equity. There is a danger of cherry picking  certain  sub-populations.  This  sub-section  considers  how  the  outcome  metric,  verification  strategy  and  reporting  have  been  designed  with  equity  in  mind.  All  DIBs  have  considered  issues  related  to  equity  during  the  design  process,  and  introduced  mechanisms  to  monitor  how well equity issues are being addressed. This will be reviewed over the new two research  waves.\n",
            "Top  17  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the SIB Carter  et  al  (2016) Gustafsson- Wright  et  al  (2017) Carter  et  al  2018 Gustafsson- Wright  et  al  (2017) Arena  et  al  (2016) Measuring impact  Validation  of  outcome metrics used Methodology  to  estimate  the  outcome  of  the programme Validated  administrative  data  /  quasi- experimental  experimental methods or Gustafsson- Wright  et  al  (2017) A163 Annex J: DIBs reviewed as part of programme level consultations As part of our programme level data collection, the evaluation team interviewed a range of stakeholders, involved in other DIBs in various  stages of development, and reviewed a range of deal and proposal documentation. Our interviews were undertaken in the second half of  2018, and the table summarises the information we have for these 9 DIBs, in terms of the objective of the DIB, the stage of development,  the stakeholders involved, the structure of the DIB and the value of the DIB. It must be noted that the majority of these DIBs are under  negotiation, and the table below reflects the information provided to the evaluation team during the consultation, and may be already out of  date by the time of this report’s publication.\n",
            "Top  18  :   Other  intended  outcomes  as  set  out  in  the M&E framework.\n",
            "Top  19  :   Carter, E., FitzGerald, C., Dixon, R., Economy, C., Hameed, T., and Airoldi, M. (2018) Building  the tools for public services to secure better outcomes: Collaboration, Prevention, Innovation,  Government Outcomes Lab, University of Oxford, Blavatnik School of Government.\n",
            "Top  20  :   Outcome  verification  data  will  be  used  to  the  understand  returns  payable.  The  data  can  also  be  compared against the  other  outcome  data,  to  understand  the  extent  to  which these  are  correlated  (improvement  in  the  target  outcome  but  worsening  across  other  outcomes  may A147 Outcome  Verification x x x Service  provider Verification  reports a  quarterly  report  from  The  Magrabi  Foundati on that  includes  all the  metrics  for the  hospital  Volta  has  already  provided Village  Enterprise  (quarterly  report)  includes all the  metrics for the  project IDInsight has  outcome  verification  report for each  cohort (the first  of these  reports has  already been  received) BAT /  DfID  (a  quarterly  report  that  includes  all the  metrics  for the  project) Gray  Matters  India  (we  have  already  received  the  baseline  report) Data type Examples  of relevant  reports How this data will  be used Comp arison  progra mmes Learning  Activities Internal  and  external  learning  reports Progress  reports Investment  returns   Outcome  payments perverse suggest  incentives).   be  Learning  will  across  compared  DIBs  and  contextualised  within  the  from  learning  other impact bonds.    To  understand  how  the  DIB  performs  against targets.\n",
            "\n",
            "\n",
            "\n",
            "Query:  impact\n",
            "Top  1  :   Measuring impact This sub-section explores the balance between reducing the transaction costs and increasing  the  benefits  of  the  impact  measurement  stage  in  terms  of  accuracy  and  usability.  The  necessary conditions for measuring impact largely relate to having clear outcomes that can  be attributable to the intervention, and the robust metrics to capture the targeted outcomes,  which are discussed in sections 6.0 and 6.2.1 respectively.\n",
            "Top  2  :   (2018).  Paying USAID Investing for Impact (n.d.) 6 ways in which an impact bond adds value.\n",
            "Top  3  :   Measuring impact The  validation  process  should  be  designed  to  balance  costs  with  the  evidence  requirements  of  stakeholders. Experimental and quasi-experimental methods are significantly more expensive than  validation of administrative data, and may not be necessary in all cases to measure impact.\n",
            "Top  4  :   Impact Expected outcomes are produced…more effectively than with other approaches…more efficiently  than with other approaches… •  With the focus on results and not inputs, this also enables a market for impact bonds, for  example  through  outcome  funds,  which  can  be  used  to  increase  competition  in  the  delivery of target outcomes and drive down costs.\n",
            "Top  5  :   IMC Worldwide, Ideas to Impact, DFID, USAID and Rockfeller Foundation (2017). Innovating  in development: Sharing learning, improving impact (Workshop report).\n",
            "Top  6  :   M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited  number of evaluations seeking to identify the instrument effect and the challenge of establishing  comparative baselines.\n",
            "Top  7  :   •  The impact bond mechanism stimulates collaboration: this applies to all parties involved in impact bonds.\n",
            "Top  8  :   investors, including Any positive or negative changes to equity as  a result of the impact bond.\n",
            "Top  9  :   Gustafsson-Wright  et  al  2015.  Impact  bonds  in  developing countries: Early learning from the field.\n",
            "Top  10  :   43 http://www.instiglio.org/en/impact-bonds/ A178 Inputs Below, the main ways DIBs are hypothesised to affect programmes are set out against the input  and process elements of the framework set out in Figure M.1 above.\n",
            "Top  11  :   Measuring impact 5.  The validation process should be designed to meet the needs of stakeholders. Different  considerations may apply to different contexts. We note that there can be an automatic  preference to use experimental approaches or quasi-experimental approaches. However,  where  an  intervention  or  certain  causal  links  are  sufficiently  backed  by  evidence,  there  may  be  less  value  in  using  experimental  or  quasi-experimental  methods  compared  to  validated administrative data.\n",
            "Top  12  :   4.3.2.4  Comparison with other impact bonds Our finding that impact bonds increase other risks is consistent with the broader literature. For  example, both Social Finance (2018) and Gustafsson-Wright et al (2015) note that while the  funder’s risk has been reduced to some degree as payments are only made if it works, the  funder  is  subject  to  new  risks  through  increased  exposure,  risk  of  demonstrated  failure  or  paying too much.\n",
            "Top  13  :   Measuring impact 17. The validation process should be designed to meet the needs of stakeholders. Different  considerations may apply to different contexts. We note that there can be an automatic  preference to use experimental approaches or quasi-experimental approaches. However,  where  an  intervention  or  certain  causal  links  are  sufficiently  backed  by  evidence,  there  may  be  less  value  in  using  experimental  or  quasi-experimental  methods  compared  to  validated administrative data.\n",
            "Top  14  :   Effectiveness  Any  positive  or  negative  changes  to  effectiveness  as  a  result  of  the  impact  bond.   Any  positive  or  negative  changes  to  equity  as  a  result of the impact bond.\n",
            "Top  15  :   Hindi, and Math by funding  Educate Girls’ intervention  in Rajasthan, India. The  impact bond structure is  used because of the focus  of results and related  flexibility of the intervention,  and because of the  possibility to unlock new  funding streams.\n",
            "Top  16  :   4.6.1.4  Comparison with other impact bonds According to Gustafsson-Wright et al., 2015, so far impact bonds have not supported many  radically  innovative  interventions,  but  some  have  supported  interventions  that  are  being  delivered  in  different  ways  or  to  different  populations,  as  is  the  case  with  these  four  DIBs.\n",
            "Top  17  :   Center  for  Global  Development  and  Social  Finance.  2013.  Investing  in  Social  Outcomes:  DIBs Supplier  Access  to  Prefinance  in  PbR  (Chinfatt  and Carson 2017) Three  key  ways  in  which  the  impact  bond  is  expected to lead change 10 claimed benefits of impact bonds The  ‘Deal  Book’  categorising  all  impact  bonds  in  middle  and  low  income  countries.  Each  DIB  is  assessed against a list of justifications for using the  DIB / reason(s) existing financing was/is inadequate 6 case studies presented, including where DIB can  add value.\n",
            "Top  18  :   Additional costs of the  impact bond,  disaggregated where  possible by:   • stage (design, set-up,  delivery, learning);   • actor who incurs this  cost; and  • type of cost (staff time,  consultancy and  expertise costs, and the  risk premium (return to  investors, including  interest)   Savings in programme  costs (including staff  time) as a result of the  impact bond.   How effectively has risk  been transferred - what  is the alignment of Research Wave DIBs level research Programme  level research Wider impact  bond sector w e i v e r   t n e m u c o D s n o i t a t l u s n o c B D I s e t i s r o t a r a p m o C i s s y a n a l a t a D 1 W R 2 W R 3 W R i w e v e r   t n e m u c o d e m m a r g o r P w e i v e r e r u t a r e t i L s n o i t a t l u s n o c l r e d o h e k a t S s p o h s k r o w i g n n r a e L Methods s n o i t a t l u s n o c D F D I s i l s y a n a t s o C x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x A119 transferred risks with  return?\n",
            "Top  19  :   •  Section 5 concludes with a summary of the challenges to evaluating impact bonds, and approaches that have been used.\n",
            "Top  20  :   Research  Wave 2  3 x x x x x x x x x x x x x x x x x x x x x the where impact  bond,  by:   possible  (design,  set-up,  delivery,  and Additional  costs  of  disaggregated  •  stage  learning);   •  actor  who  this  cost;  and  incurs  •  type  of  cost  (staff  time,  consultancy  and  expertise costs, and the risk premium (return  interest)   to  Savings in programme costs (including staff  time)  as  a  result  of  the  impact  bond.  How  effectively  has  risk  been  transferred  -  what is the alignment of transferred risks with  return?\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  2  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  3  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  4  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  5  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  6  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  7  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Design finalisation and  stakeholder engagement )  Consultancy fees for setting up  DIB (Reaching Execution  readiness including field trip) Legal and financial advice SPV set-up direct costs Implementation Costs Not  estimated Not  estimated 158,000 158,000 - - 86,300 86,300 104,804 - 65,000 169,804 Not  estimated Not  estimated 126,000 1,986 126,000   1,986 Budgeted implementation costs relating to the use of the DIB are set out in the table below.  Contract management costs cover additional grant management, financial management and  reporting requirements relating to the use of the DIB. The verification costs excludes the USD  70,915  costs  for  the  process  evaluation,  which  is  not  an  essential  component  of  the  DIB.  Village Enterprise also have their own verification process, separate from the one delivered  by IDInsight which will incur a cost. This has not been estimated and will be revisited in the  following research waves.\n",
            "Top  9  :   •  Legal and financial advice – this was a common cost which was often provided pro- bono at least in part by professional firms 2.  Implementation costs: These include additional performance management, project  management and reporting time, on top of what would have normally been spent on a  grant-funded project. This also includes costs of verifying the outcome targets and, for  some DIBs, escrow costs. Costs can be split into the following three categories: •  Contract management including performance management, project management and reporting •  Verification costs manager costs •  Costs related to the DIB transaction, such as escrow, legal fees and transaction As implementation is still underway, our estimate of implementation costs relating to  the DIB is based on expected costs, as identified through budgets and discussions  with stakeholders.\n",
            "Top  10  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  11  :   It was not possible to estimate the time spent on these activities. The activities for which we  do have costs for are those where advisors were contracted, including advice on contract  design and legal costs. These are set out in the table below.\n",
            "Top  12  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  13  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  14  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  15  :   Cost  categories Costs (including actual, budgeted,  in-kind and pro-bono)  Otherwise,  stakeholders  described  the significant time commitment e.g.  staff time over two years.\n",
            "Top  16  :   expertise costs, and the  risk  premium  (return  to  investors,  including  interest) (Clist 2017).   This  should  cover  the  full  cost, including staff time not  charged, of all actors.   Where possible, this will be  disaggregated by ‘first time’  which  DIB  costs  hypothetically  wouldn’t  have  to  be  incurred  again  for any subsequent DIBs.38  Cost drivers to be analysed  which  to  elements of the DIB are the  most  time- intensive/expensive.   programme  Savings  costs  (including  staff  time)  as  a  result  of  the  impact  bond.\n",
            "Top  17  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  18  :   The  type  of  additional  costs  incurred  during  the  set-up  and  design  phase  can  be  described under three categories: •  Staff  time  –  this  was  provided  largely  ‘in-kind’  by  stakeholders  using  their  own  existing resources, unless staff time was covered by a separate grant (e.g. DFID  technical assistance grant for QEI and Government of the Netherlands for ICRC).\n",
            "Top  19  :   Design and set up costs Stakeholders from both VE and Instiglio commented on the increased cost at the design and  set-up stage of the DIB in the form of staff costs. VE estimated a total of 2160 hours spent on  DIB design and structuring and Outcome Payment Agreement (OPA) negotiation and 1058  hours on investment fundraising and structuring. This staff time was provided in-kind.\n",
            "Top  20  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  4  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  5  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  6  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  7  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  8  :   5.1.2.4 Additional DIB costs for Cataract DIB The maximum committed outcome funding for the Cataract DIB is USD 3.5 million, of which  USD 2.8 million relate to outcome payments.\n",
            "Top  9  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  10  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  11  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  12  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  13  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.  In one  case  performance  management  costs  are  (QEI)  co-funded  by  investor.  Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  14  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  15  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  16  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  17  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  18  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  19  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  20  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  3  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  4  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  5  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  6  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  7  :   i)  ii)  iii)  iv) i) ii) DIBs are understood by DFID as one type of payments by results (PbR), or a type of funding  whereby payments are made after the achievement of pre-agreed outcomes (DFID, 2014). In  a standard PbR contract, there are four actors: an outcome funder who funds the outcomes;   the service provider delivering the intervention;   the target population, benefiting from the services; and  a validating agency that validates the results on which the payments are based.\n",
            "Top  8  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  9  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  10  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  11  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  12  :   32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government’s  use  of  payment  by  results  https://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of- payment-by-results.pdf   33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts.  Dalberg  https://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by- results-contracts Intelligence A83 • • Impact bonds have enabled the development of strong monitoring and  evaluation systems: the impact bond mechanism incentivises evidence collection  and can therefore lead to improving outcomes for service users through identifying  interventions that work.   Impact bonds can shift the focus of government toward preventive services:  this could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and  we have to be mindful that the portfolio of SIBs projects target different outcomes, emerging  evidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve  effectiveness and efficiency of outcome delivery, and generate valuable impact evidence.\n",
            "Top  13  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  14  :   research indicates Payment  by  Results  approaches  enable  donors  to  transfer the risk/uncertainty over whether an intervention  will achieve results to the provider.   However,  that  some  providers  (particularly  those  with  smaller  balance  sheets,  or  less  access to commercial loans) would be unable pre-finance  their  intervention  and  wait  for  payment  on  delivery  of  results, or would be unwilling to take on the financial risk  associated with underperforming on a PbR contract. As a  result providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of  contracts.3233 B.3  How strong is the evidence on DIBs?\n",
            "Top  15  :   3.  Maximum payments to investor: This includes the maximum return payable to the  investor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any  interest payment.\n",
            "Top  16  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  17  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  18  :   Outcome Funder: World Bank  Group.   Investors: Five investors from  Chile, Palestine, Holland, Britain  and Switzerland, which have not  signed yet.  Service providers: 2-4  Palestinian, not-for profit service  providers to be selected per cycle, Payment terms: The  investors will form a SPV  for the flow of funds, with a  DIB manager (contracted  by the PIA) who will  manage the SPV funds on  behalf of the investors.   Outcome metric: Likely to  be a mixture of training The outcome funds are  USD 5 million. The DIB is  part of a wider World  Bank project called  Finance for Jobs, a larger  initiative to create  employment in West Bank  and Gaza.\n",
            "Top  19  :   Maximum payments to investors The maximum payments to investors is the cost which seems to be most clearly additional  compared to similar programmes, and the ones which are most clearly proportional to the size  of  the  maximum  committed  outcome  funding.  Annualised  interest  rates  provide  the  most  commonly used comparison of returns. As expected, the highest maximum return is for the  ICRC HIB, and the lowest, for the Cataract DIB, corresponding to the respective sizes of the  DIBs. The cost of the payments to investors is borne mainly by outcome funders, though there  are exceptions; in the case of underperformance in the ICRC HIB and Cataract  DIB, ICRC  and AEF are respectively liable for some of this repayment.\n",
            "Top  20  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  2  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  3  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  4  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  5  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  6  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  7  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  8  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  9  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  10  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  11  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  12  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  13  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  14  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  15  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  16  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  17  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  18  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  19  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  20  :   34 For example, input based grants and pay for service contracts or standard payment by results.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  4  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  5  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  6  :   5.1.2.4 Additional DIB costs for Cataract DIB The maximum committed outcome funding for the Cataract DIB is USD 3.5 million, of which  USD 2.8 million relate to outcome payments.\n",
            "Top  7  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  8  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  9  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  10  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  11  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  12  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  13  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  14  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  15  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  16  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  17  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  18  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  19  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  20  :   Payment terms: TBC  Outcome metric: To be  finalized. Likely to include:  a) number of hospitals  attaining quality KMC  prerequisites; b) number of  infants receiving quality  KMC services; c) number  or % of infants achieving  target nutritional  status/weight at 40 weeks  gestational age and/or at  follow-up.  Range of returns: TBC The planned operating  budget USD 2.1 million,  to be spent over three-to  four years. Total outcome  commitment of USD 2.8  million. Upfront capital  commitment: USD 3.0  million (pre-capital  recycling).  Additional grants  (covering feasibility study,  baseline data study, DIB  design and structuring,  data systems design,  legal advice) USD 1  million.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  2  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  3  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  4  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  5  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  6  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  7  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  8  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  9  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  10  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  11  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  12  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  13  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  14  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  15  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  16  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  17  :   Paid  for  by  the  outcome  funder  or  funded by a separate grant.\n",
            "Top  18  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Implementation Performance management,  Project management, Reporting Not  estimated Not  estimated 670,000                         40,000   -                           40,000   - 670,000                          40,000                          40,000 -                      - Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4  million.\n",
            "Top  19  :   Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome  Funder Investor Service  Provider Total Implementation  Performance management  (Dalberg) Project management Reporting Verification (Outcomes  Evaluation by Gray Matters  India) Return to investors 254,263 392,137                        - 646,400 Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated Not  estimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.\n",
            "Top  20  :   Payment terms: TBC  Outcome metric: To be  finalized. Likely to include:  a) number of hospitals  attaining quality KMC  prerequisites; b) number of  infants receiving quality  KMC services; c) number  or % of infants achieving  target nutritional  status/weight at 40 weeks  gestational age and/or at  follow-up.  Range of returns: TBC The planned operating  budget USD 2.1 million,  to be spent over three-to  four years. Total outcome  commitment of USD 2.8  million. Upfront capital  commitment: USD 3.0  million (pre-capital  recycling).  Additional grants  (covering feasibility study,  baseline data study, DIB  design and structuring,  data systems design,  legal advice) USD 1  million.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  3  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  4  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  5  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  6  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  7  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  8  :   Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: •  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.\n",
            "Top  9  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  10  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  11  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  12  :   32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government’s  use  of  payment  by  results  https://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of- payment-by-results.pdf   33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts.  Dalberg  https://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by- results-contracts Intelligence A83 • • Impact bonds have enabled the development of strong monitoring and  evaluation systems: the impact bond mechanism incentivises evidence collection  and can therefore lead to improving outcomes for service users through identifying  interventions that work.   Impact bonds can shift the focus of government toward preventive services:  this could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and  we have to be mindful that the portfolio of SIBs projects target different outcomes, emerging  evidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve  effectiveness and efficiency of outcome delivery, and generate valuable impact evidence.\n",
            "Top  13  :   A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC  Outcome metric:  TBC  Range of returns: TBC Outcome funds USD 10- 30 million (anticipated).\n",
            "Top  14  :   to Outcome funders paid for the  technical assistance. All other  actors covered their own  costs.\n",
            "Top  15  :   the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond  Role  of  the  outcome  funder  /  investor  toward  service  providers  and  its  level  of  control  over  the  organisations  involved  in  the impact bond Measuring impact Validation  impact of Payment  based  experimental/quasi- experimental  or  validated  administrative data7 on Payment  based  on  validated  administrative data.   This will include verification of  records  physical  and  verification  of  mobility  of  beneficiaries.\n",
            "Top  16  :   Outcome Funder: World Bank  Group.   Investors: Five investors from  Chile, Palestine, Holland, Britain  and Switzerland, which have not  signed yet.  Service providers: 2-4  Palestinian, not-for profit service  providers to be selected per cycle, Payment terms: The  investors will form a SPV  for the flow of funds, with a  DIB manager (contracted  by the PIA) who will  manage the SPV funds on  behalf of the investors.   Outcome metric: Likely to  be a mixture of training The outcome funds are  USD 5 million. The DIB is  part of a wider World  Bank project called  Finance for Jobs, a larger  initiative to create  employment in West Bank  and Gaza.\n",
            "Top  17  :   research indicates Payment  by  Results  approaches  enable  donors  to  transfer the risk/uncertainty over whether an intervention  will achieve results to the provider.   However,  that  some  providers  (particularly  those  with  smaller  balance  sheets,  or  less  access to commercial loans) would be unable pre-finance  their  intervention  and  wait  for  payment  on  delivery  of  results, or would be unwilling to take on the financial risk  associated with underperforming on a PbR contract. As a  result providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of  contracts.3233 B.3  How strong is the evidence on DIBs?\n",
            "Top  18  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  19  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "Top  20  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Investment  vehicle  costs  varied  depending  on  ranged  from  nothing,  to  105k.  These  costs  depend  on  the  contracting  mechanisms  used.  Loan,  legal  and  escrow  fees  seem  to  be  consistently  cost  around  USD  30,000  –  40,000.  The  highest  costs  involve  fees  payable  to  trustees.\n",
            "Top  2  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  3  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  4  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  5  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  6  :   Table 5.1: Additional DIB costs in the design, set up and implementation phases on top  of programming costs under a grant programme Costs (including actual, budgeted,  in-kind and pro-bono) Paid for by Cost  categories   Design and set up  Staff  time set  up Where  estimated,  this  ranged  from  USD  150,000  to  USD  490,000.\n",
            "Top  7  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  9  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  10  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  13  :   A173 Annex L: VfM Analysis – Supporting Evidence This sub-section provides additional detail on the value for money (VfM) analysis undertaken on  the four DIBs, in terms of detail on the cost drivers across the four DIBs and the extent to which  costs can be considered ‘first DIB’ costs.\n",
            "Top  14  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "Top  15  :   Investment  vehicle  related  costs  e.g.  Escrow  legal  and  fees  Maximum payments to investors  Maximum  payments Note: Conversions done based on the exchange rate on 5 May 2019.\n",
            "Top  16  :   Do the extra costs represent value for money - to what extent do they lead to  additional results, impacts and benefits?\n",
            "Top  17  :   Do  the  extra  costs  represent  value  for  money  -  to  what  extent  do  they  lead  to  additional  results,  impacts  and benefits?\n",
            "Top  18  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "Top  19  :   Table  5.1  presents  the  ranges  of  cost  estimates  under  these  categories  and  which  stakeholders paid for these additional costs.\n",
            "Top  20  :   The types of costs incurred, and who paid for these costs, are discussed below. It must be  borne in mind that across several DIBs, it was acknowledged that estimates were incomplete.  Hence, it is useful to review these costs as the types and minimum level of costs required to  launch and implement a DIB at this stage of the market.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   and activities Supplier  demonstrates  how  evaluation  chosen  approach  represent value for money across life of  contract.    Including  proactive  identification  of  efficiencies  and  savings  –  e.g.  where  opportunities arise that enable evaluator  learning  synergies  and  to  remove duplicative activities.\n",
            "Top  2  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  3  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  4  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  5  :   External advice on contract design and setting up the DIB was provided by Instiglio, funded  by outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804  respectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168  hours) for both the OPA agreement negotiation and investments structuring/negotiation and  special purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV.  The table below provides further detail.\n",
            "Top  6  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  7  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  8  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  9  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  10  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   Commercial Strong Strong Social 100%  payment  on  outcomes  (though  the  achievement  of  the  outcomes  only  affects  interest payable) Presence  of  capital  protection  measures (Full protection)  Presence  of  arrangements  upside  and  downside  service provider sharing  potential  for risk  – Strong Social and Commercial Type  contract6 of Typologies  of  structure  depending on which actor  has  the  contract  with  the  outcome funder.\n",
            "Top  13  :   Risk Management Financial  Management Robust cost control in line with contract.    Accurate  and  forecasting and invoices.\n",
            "Top  14  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  15  :   This contract will require the Supplier to operate in conflict-affected areas and parts of it are  highly insecure. The security situation is volatile and subject to change at short notice. The  Supplier  should  be  comfortable working  in such  an  environment  and  should  be  capable  of  deploying to any areas required within the region in order to deliver the Contract.\n",
            "Top  16  :   The maximum budget available for this evaluation is GBP 300,000 (exclusive of VAT) Documents / References •  DIBs Pilot Business Case  •  DIBs Pilot Business Case Addendum  •  DIBs Pilot programme Logframe  •  Village Enterprise DIB – Instiglio’s Learning/Process Review document (giving more info on their approach) B.10.5 Duty of Care The  Supplier  is  responsible  for  the  safety  and  well-being  of  their  Personnel  (as  defined  in  Section  2  of  the  Contract)  and Third  Parties affected  by  their  activities  under  this  contract,  including appropriate security arrangements. They will also be responsible for the provision of  suitable security arrangements for their domestic and business property.\n",
            "Top  17  :   Contract Duration, Contact Adaptability and Break Points The evaluation should get underway as soon as possible, with the ideal start date being 1 April  2018, and will last until March 2023 to allow all outputs to be produced and quality assurance  to be completed.    DFID reserves the option to break the contract after each of the Evaluation Report outputs is  completed.  Continuation  of  the  services  after  each  output  is  produced  will  be  based  on  agreement  of  the  deliverables  and  on  satisfactory  performance  and  the  progress  of  the  Supplier against the specified outputs.\n",
            "Top  18  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  19  :   These  ranged  from  USD  650k  to  USD 6.4m Paid for by the outcome funder.\n",
            "Top  20  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  2  :   This contract is provided under the GEFA contract. In line with the terms and conditions of the  GEFA contract, all intellectual property rights in all material (including but not limited to reports,  data, designs whether or not electronically stored) produced by the Supplier or the Supplier's  Personnel pursuant to the performance of the Services (\"the Material\") shall be the property  of the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID  a perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material.  DFID will be the final owner of the findings of the evaluation.\n",
            "Top  3  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  4  :   Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome  Funder Investor Service  Provider Total Design and Set up  Staff time setting up contract,  negotiations, meetings feasibility  study External advice on contract design  (KOIS)  External advice on legal and  financial aspects of contract (pro  bono) Implementation Costs Not  estimated 40,500 457,739 498,239 - 698,767   - 698,767 - >50,000   - >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be  reviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to  verification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional  management and reporting requirements, which would not have been necessary should this  have been a traditional grant.\n",
            "Top  5  :   External  advice  contract  design   Legal  financial  advice on and Three out of the four DIBs estimated  to be just over USD 250,000, while  one  DIB  estimated  this  to  be  USD  687,000.   Not all these costs were included in  budgets.    Where  costs  had  been  captured, these ranged from >USD  50,000  to  USD  120,000.  However,  in  most  cases  this  underestimated  the full cost as not all the pro-bono  hours had been recorded.\n",
            "Top  6  :   •  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded  through a grant, paid for by the lead on the impact bond or provided pro-bono by  the advisors, and often through a combination of the above.\n",
            "Top  7  :   Contract Duration, Contact Adaptability and Break Points The evaluation should get underway as soon as possible, with the ideal start date being 1 April  2018, and will last until March 2023 to allow all outputs to be produced and quality assurance  to be completed.    DFID reserves the option to break the contract after each of the Evaluation Report outputs is  completed.  Continuation  of  the  services  after  each  output  is  produced  will  be  based  on  agreement  of  the  deliverables  and  on  satisfactory  performance  and  the  progress  of  the  Supplier against the specified outputs.\n",
            "Top  8  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  9  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  10  :   Risk Management Financial  Management Robust cost control in line with contract.    Accurate  and  forecasting and invoices.\n",
            "Top  11  :   This contract will require the Supplier to operate in conflict-affected areas and parts of it are  highly insecure. The security situation is volatile and subject to change at short notice. The  Supplier  should  be  comfortable working  in such  an  environment  and  should  be  capable  of  deploying to any areas required within the region in order to deliver the Contract.\n",
            "Top  12  :   The maximum budget available for this evaluation is GBP 300,000 (exclusive of VAT) Documents / References •  DIBs Pilot Business Case  •  DIBs Pilot Business Case Addendum  •  DIBs Pilot programme Logframe  •  Village Enterprise DIB – Instiglio’s Learning/Process Review document (giving more info on their approach) B.10.5 Duty of Care The  Supplier  is  responsible  for  the  safety  and  well-being  of  their  Personnel  (as  defined  in  Section  2  of  the  Contract)  and Third  Parties affected  by  their  activities  under  this  contract,  including appropriate security arrangements. They will also be responsible for the provision of  suitable security arrangements for their domestic and business property.\n",
            "Top  13  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  14  :   Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and  contracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters)  contributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided  approximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders’  costs  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each  month of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as  a part contribution to the cost of their due diligence.\n",
            "Top  15  :   •  Force majeure clauses also enable investors to terminate the contract in the event of non-delivery.\n",
            "Top  16  :   Potential approaches which could bring together funding from multiple actors and create scale  include outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on  the same areas. Outcomes rate cards would allow the outcome funder to set prices for certain  outcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al.,  2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a  broad spectrum of actors is incentivised (Clist 2017).\n",
            "Top  17  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  18  :   Disclaimer This  report  has  been  prepared  by  Ecorys  for  DFID,  for  services  specified  in  the  Terms  of  Reference and contract of engagement.\n",
            "Top  19  :   Structuring and developing the operating model •  The larger number of stakeholders involved in the DIBs to date, and the often diverse legislative frameworks, increase the transaction costs of this stage of the DIB  development. The optimal solution would be to amend the legislative frameworks to  accommodate DIBs. Where this is not possible, other potential solutions include: limiting the number of stakeholders involved; o  o  using pooled funding structures;  o  using other ways to minimise the number of contracts involved; and  o  standardising deals.\n",
            "Top  20  :   Effort  should  not  be  easily  observed,  otherwise  the  contract could be based on this instead.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Identifying metrics and structuring payments 3.  Building a database of impact bond returns, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the DIBs market.  However, context specificity may limit the usefulness of standardisation and caution is also  advised in terms of developing rate cards, due to the early stage of the market and limited  data available.\n",
            "Top  2  :   Identifying metrics and structuring payments 15. Building a database of impact bond returns, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the DIBs market.  However, context specificity may limit the usefulness of standardisation and caution is also  advised in terms of developing rate cards, due to the early stage of the market and limited  data available.\n",
            "Top  3  :   • It  is  important  to  developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders and linked to other metrics within the sector/country.\n",
            "Top  4  :   •  Develop outcome metrics and rate cards that are understood by all stakeholders and  linked  to  other  metrics  used  in  the  sector  or  country,  to  increase  the  value  of  the  learning generated, minimise the costs of data collection and facilitate the broader DIB  market and/or potential transition to a SIB.\n",
            "Top  5  :   •  Develop outcome metrics and rate cards that are understood by all stakeholders and  linked to other metrics used in the sector / country, to increase the value of the learning  generated, minimise the costs of data collection and facilitate the broader DIB market  and/or potential transition to a SIB.\n",
            "Top  6  :   4.  Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  7  :   16. Outcome metrics and targets work best when returns to investors and outcome funders,  and respective incentives, are aligned. Developing outcome metrics and rate cards that  are understood by all stakeholders and linked to other metrics within the sector/country  can increase the value of the learning generated, and also facilitate the broader DIB market  and/or potential transition to a SIB. It is noted that there can be a tension between using a  robust model and using a less robust model that is aligned with measures used by others  in the sector.\n",
            "Top  8  :   Identifying metrics A few stakeholders across the DIB suggested that developing templates for outcome metrics  and  rate  cards  could  reduce  the  transaction  costs  of  setting  up  outcome  metrics.  This  is  particularly true in the case of DIBs in the same sector, and the QEI DIB was able to build on  the  work  done  in  the  Educate  Girls  DIB.  Both  the  QEI  and  VE  DIBs  have  the  ambition  to  generate  lessons  and  grow  the  DIB  market  in  their  respective  sector,  and  the  metrics  are  priced  per  outcome,  which facilitates  transferring  the  outcome  metric  to  other  interventions  and providers.\n",
            "Top  9  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  10  :   Identifying metrics and structuring payments •  Building  a  database  on  interest  rates,  outcome  metrics  and  rate  cards  and  drawing  on  private sector expertise on pricing risk would facilitate the growing of the DIBs market.\n",
            "Top  11  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  12  :   6.2.2.2  Comparison with other impact bonds Stakeholders  interviewed  who  are  involved  in  the  wider  impact  bond  sector  echoed  the  argument that standardised templates could help reduce transaction costs. However, one key  stakeholder also argued that outcome design and pricing still involves a substantial degree of  customisation, depending on the specific context, partners involved, intervention and related  risk and requirements. Therefore the extent to which standardisation can apply is limited, and  templates  will  always  need  to  be  somewhat  bespoke.  For  example,  the  debate  around  standard rate cards is still ongoing in the UK SIB market, a much more mature impact bond market, which suggests it may take some time before this will materialise in the more nascent  use of impact bonds in middle income and developing countries.\n",
            "Top  13  :   ix Contents Acknowledgements and disclaimer .......................................... i Executive Summary ................................................................... ii  Recommendations ............................................................................................. viii  List of Tables ......................................................................................................... 3  Table of Figures .................................................................................................... 5 1.0 2.0 3.0 4.0 5.0 Introduction ................................................................. 1  Overview of the DIBS pilot programme ............................................. 1  Objectives of the Evaluation .............................................................. 4  Scope of the Research Wave 1 Report .............................................. 5  Overview of the Evaluation Process.................................................. 5  Report structure .................................................................................. 6 Evaluation Framework and Methodology .................. 8  Evaluation framework for the evaluation .......................................... 8  Overview of the methodology .......................................................... 12  Methodological limitations ............................................................... 18 Summary of the DIBs ................................................ 20  Programme components .................................................................. 22  Stakeholders involved in the DIBs .................................................. 26  DIB structures ................................................................................... 27  Conclusion ........................................................................................ 30 Analysis and Findings – DIB Effects ........................ 31  The DIB effect indicators .................................................................. 32  Presence of the DIB effect indicators: Summary............................ 34  Risk transfer effects ......................................................................... 37  Partnership effects ........................................................................... 40  Financing and funding effects ......................................................... 43  Design effects ................................................................................... 47  Other factors influencing the DIB effect .......................................... 51  Additional effects not identified in the framework ......................... 52  Conclusions ...................................................................................... 52 Analysis and Findings – Costs of designing and  delivering DIBs .......................................................... 55  Economy ........................................................................................... 57 6.0 Efficiency ........................................................................................... 68  Effectiveness ..................................................................................... 68  Equity................................................................................................. 72  Conclusion ........................................................................................ 73 Analysis and Findings – Improving the process of  designing and agreeing DIBs ................................... 75  Identifying appropriate interventions .............................................. 77  Identifying metrics and structuring payments ................................ 81  Measuring impact ............................................................................. 85  Identifying and selecting stakeholders and managing relationships  ........................................................................................................... 87  Structuring the vehicle and developing the operating model ........ 95  Conclusion ........................................................................................ 97 7.0 Lessons .................................................................... 101 8.0 Recommendations .................................................. 104  Recommendations to all DIB stakeholders ................................... 104  Recommendations to DIB designers ............................................. 104 Annex A: Case study reports .................................................... 1 Annex B: Terms of Reference ................................................. 81  Background and Context ................................................................. 81  B.1  What do we mean by other aid mechanisms? ................................ 82  B.2  How strong is the evidence on DIBs? ............................................. 83  B.3  What is the DFID DIBs pilot programme? ....................................... 84  B.4  Users of the Evaluation .................................................................... 85  B.5  Evaluation Methodology ................................................................... 90  B.6  Data Sources ..................................................................................... 91  B.7  Evaluation Outputs and Timeframe ................................................. 93  B.8  Lighter-Touch Interim Outputs ........................................................ 95  B.9  Evaluation Management Team ......................................................... 97  B.10 Annex C: Bibliography .......................................................... 106 Annex D: EQUALs criteria mapped to report sections ....... 112 Annex E: Evaluation methodology ....................................... 116  Evaluation Framework .................................................................... 117  E.1  DIB-level research........................................................................... 124  E.2 E.3  E.4  E.5  E.6  E.7 Programme-level Research ............................................................ 135  Sector-level Research .................................................................... 135  Approach to data collection ........................................................... 136  Analysis, Reporting and Dissemination ........................................ 137  Involvement of stakeholders .......................................................... 140 Annex F: Individual DIB level plans ...................................... 144 Annex G: Data Quality Assessment ..................................... 149  ICRC ................................................................................................. 149  G.1  QEI ................................................................................................... 150  G.2  Village Enterprise............................................................................ 152  G.3   Cameroon Cataract Bond ............................................................... 154  G.4 Annex H: Consultees and Sources reviewed ...................... 157 Annex I: Framework for categorising DIBs .......................... 162 Annex J: DIBs reviewed as part of programme level consultations ........................................................... 164 Annex K: Learning workshop note ....................................... 172 Annex L: VfM Analysis – Supporting Evidence ................... 174  ICRC ................................................................................................. 174  L.1  Quality Education India DIB ........................................................... 174  L.2  VE DIB.............................................................................................. 175  L.3  Cameroon Cataract Bond ............................................................... 175  L.4 Annex M: Literature Review .................................................. 177  Hypothesised effects of DIBs ........................................................ 177  a.  Input ................................................................................................. 189  b.  Recommendations .......................................................................... 196  c.  What approaches have been used to evaluate impact bonds? What  d.  are the main challenges and solutions? ....................................... 198 Annex N: List of Acronyms ................................................... 201 List of Tables Table 2.1: Evaluation Framework – EQ1 ............................................... 8 Table 2.2: Evaluation Framework – EQ2 ............................................. 10  Table 2.3 Stakeholders consulted ....................................................... 13  Table 2.4 Comparator Sites ................................................................ 14  Table 2.5: Deliverables mapped to target audiences........................... 17  Table 2.6: Limitations and mitigations ................................................. 18  Table 3.1: Programme components .................................................... 22  Table 3.2: Key stakeholders ................................................................ 26  Table 3.3: DIBs against DIB dimensions ............................................. 27  Table 4.1: DIB effect indicators ........................................................... 33  Table 4.2: Presence of DIB effect indicators in the four DIB projects .. 35  Table 6.1: Project focus and measurement approach ......................... 87  Table 6.2: Advantages and disadvantages to different approaches to  identifying and engaging with stakeholders ......................................... 91 Table B.1: Alternative aid mechanism ................................................. 82  Table B.2 EO 1: Inception Report ....................................................... 93  Table B.3: EO2 – Evaluation Report on the Process of designing and  launching DIBs .................................................................................... 93  Table B.4: EO3 –  Mid-Term Evaluation Report on DIBs ..................... 94  Table B.5: EO4 – Final Evaluation Report on DIBs ............................. 94  Table B.6: Good Performance Indicators ............................................ 98 Table E.1: Evaluation Framework ..................................................... 117  Table E.2: DIB effects and indicators ................................................ 122  Table E.3: Stakeholder consultations in RW1 ................................... 125  Table E.4: Research Waves ............................................................. 131  Table E.5: VfM Framework ............................................................... 131  Table E.6: VfM Indicators .................................................................. 132  Table E.7: Costing Structure ............................................................. 133  Table E.8: Communication Plan ........................................................ 140 Table F.1 : Proposed consultations ................................................... 144  Table F.2: Value for Money data ....................................................... 145  Table F.3: Other data ........................................................................ 147 Table M.1: Sources consulted ........................................................... 178  Table M.2: Impact bond principles .................................................... 182  Table M.3: Categorisation of SIBs by level of innovation ................... 185  Table M.4: Challenges of designing impact bonds ............................ 196 Table of Figures Figure 1.1: DIBs pilot programme theory of change .............................. 3  Figure 4.1: DFID risk assessment of three DIBs ................................. 38 Figure M.1: Framework for synthesising evaluation evidence ........... 177  Figure M.2: Strengths and weaknesses of existing evidence and  evaluation approaches and methods related to SIBs and DIBs (Drew  and Clist 2015:27) ............................................................................. 199 1.0  Introduction Overview of the DIBS pilot programme 1.1.1 DIBs and the current stage of the market.\n",
            "Top  14  :   5.3.2 Summary of available benchmarked interest rates Based on the impact bonds to date (see the table below) 30% seems to be the upper end of  expected financial return. At the lower end, a negative return could be expected, should targets  not be met.\n",
            "Top  15  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  16  :   and VE DIB  None Cataract  DIB 100% 8% p.a. if performance  targets met; 4% p.a. if not  met (OPIC); 0% p.a. if  targets not met (Netri).\n",
            "Top  17  :   counsel); Cameroon-based legal  counsel; in-country public health  consultant; data systems provider.  Outcome evaluator: TBC Outcome Funder: TBC  Investors: Likely to be BIX  capital. Shell Foundation and  DFID funded the DIB set-up; IFC  funds the data gathering for the  certification process with support  from the Ministry of Finance in  Japan.  Service providers: Mimi-Moto  (cookstove producer); Emerging  Cooking Solutions (ECS, seller of  Mimi-Moto cookstoves). Apart  from ECS, Cardano will select  one more social enterprise.  Intermediary: Cardano  Development.   Technical assistance providers:  Baker McKenzie (pro-bono legal  adviser).  Outcome evaluator: TBC Outcome Funder: Children  Investment Fund Foundation.   Investors: UBS Optimus  Foundation.   Service providers: Educate  Girls.\n",
            "Top  18  :   India  (Rajasthan ) Maternal  and  Newborn  Health DIB The bond is intended to  improve and standardize  the quality of maternal care  in Rajasthan’s private  healthcare facilities. The  DIB implementing partners  will guide the targeted  private healthcare facilities Outcome Funder: USAID and  Merck for Mothers. MOU with the  Rajasthan State Ministry of Health  to invest in, and scale-up, the  partnership if the pilot program is  deemed successful by the  independent evaluator.  Investors: UBS Optimus Payment terms: Six- monthly payment to  investors, with USD 4,500  for each facility at  progressive stage and  remainder USD 13,500 for  facilities that reach Joint  Quality Standard (JQS) Projected total investment  of USD 9 million, USD 1  million of which is set  aside for results  verification. UBS Optimus  Foundation will provide  80% of the USD 4 million  upfront working capital A166 Development  Stage No DIB Objective Stakeholders involved Structure Value through quality  improvements and the  application process to be  accredited through the  government-approved  healthcare facility  certification process.\n",
            "Top  19  :   1 (2)  -  0  1 (1) 3 (3)  1 (2)  1 (1)  - 1 (2)  0  0  - 1 (4)  -  0  - 3  1  1  - 1  -  1  - 1  1  1  - 1 (4) 1(2) A full list of consultations is set out in Annex H.\n",
            "Top  20  :   •  Where  possible, this  will  be  disaggregated  by  ‘first  time’  DIB  costs  which  hypothetically  wouldn’t  have  to  be  incurred  again  for  any  subsequent DIBs.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in  repayment of principal and interest to lenders and USD 0.12m in incentive payments  to  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract  surgeries, quality of surgery and financial sustainability of the hospital). The outcome  funders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows  Foundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment  Corporation (OPIC) and the Netri Foundation.\n",
            "Top  2  :   There  can  be  incomplete  alignment  between  outcome  funders and service providers in terms of incentives and  goals.  If  the  service  provider  is  always  incentivised  to  deliver  the  target  outcomes,  the  payments  by  results  would not change incentives, and as such there would  be no expected gains in efficiency or effectiveness. For  improved  performance,  the  incentive  needs  to  lead  to  better alignment of incentives and aims, and the service  provider needs to be able to effect changes. The service  provider  also  needs  i)  a  level  of  autonomy,  and  ii)  the  capacity and skills to improve delivery.\n",
            "Top  3  :   As set out in the alignment principle of PbR, PbR may be only beneficial when incentives were  not initially aligned: •  Holden and Patch (2017) found that GEC staff were already very motivated to achieve  outcomes before the introduction of the payment incentive. Similarly, Rwanda was already  focused  on  increasing  enrolment  before  the  introduction  of  the  RBA  (Upper  Quartile,  2015).\n",
            "Top  4  :   Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders, and correspondingly, incentives, are aligned. This was cited by the majority of  respondents as a key ambition in the design of the outcome metrics and payment structure.\n",
            "Top  5  :   6.2.3 Identifying metrics and structuring payments - increasing the model’s  benefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome  funders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate  cards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the  sector/country can increase the value of the learning generated and also facilitate the broader  DIB market and/or potential transition to a SIB.\n",
            "Top  6  :   outcome payment of USD  0–USD 412,000 to UBSOF  in 2018  Outcome metric: 1)  Enrolment outcomes (20%  of outcome payment):  number of girls on school  rosters in grades 2-8 in the  treatment group over 3  years; 2) Learning  outcomes (80% of outcome  payment): Annual Status of  Education Report (ASER)  measures basic literacy in  Hindi, basic literacy in  English, and basic  numeracy.  Range of returns: Target  IRR = 10%, max IRR =  15%; UBSOF pays  incentive to Educate Girls  equal to 32% of its payment  above principal Implementatio n.\n",
            "Top  7  :   In addition, Ecorys’s evaluations have seen some evidence of the ‘perverse incentives’. These  are  often  associated  with  outcomes  based  commissioning,  primarily  ‘cherry  picking’  (where  services target beneficiaries easiest to reach/turn around as opposed to the hardest to reach) and  ‘parking’ (where beneficiaries are left on programmes but not supported, either because it is clear  they will not achieve any outcomes or because the provider gets paid for having beneficiaries on  the programme).\n",
            "Top  8  :   One hypothesis is that measures can fail to incentivise recipients if they are too complex relative  to the incentive size. This seems to be the case for certain Health Results Innovation Trust Fund  (HRITF) PbR agreements (Kandpal 2016), NGOs (Holden and Patch, 2017) and governments  (Cambridge Education, 2015 and Upper Quartile, 2015). Measures can also fail to incentivise if  the incentives are too low, agreements too short or outside of the recipient’s control (such that  the recipient has no incentive to try). Clist (2017) notes that a common theme for projects with A191 poor  performance  is  incentives  which  are  insufficient,  in  comparison  to  the  programme’s  complexity and duration, and perverse incentives to prioritise the short term over the long term.\n",
            "Top  9  :   risk transferred  needs The  amount  of  to  be  commensurate  with  the  risk  premium  paid.  Different  actors will have different levels of risk aversion, and this  may affect the risk premium and the pool of interested  actors.  Determining  the  appropriate  risk  and  reward  structure  (pricing  and  outcomes)  to  get  the  incentives  right can be difficult.\n",
            "Top  10  :   This seems to be supported by the success stories as well. Where PbR worked best and provided  the  strongest  evidence  of  success  was  where  incentives  were  also  largest,  including  HRITF’s  programme in the Misiones province (where incentives were largest); Employment Fund in Nepal  where  organisations  responded  to  the  incentive  to  increase  employment,  not  just  training;  the  Uganda RBF health project, where incentivised quality of care increased.\n",
            "Top  11  :   VE:  The  targeting  strategy  addresses  equity  concerns,  and  at  the  moment,  there  are  no  particular  risks  identified  with  the  outcome  target  or  verification  process  potentially  driving  perverse incentives. This will be monitored over the next two research waves.\n",
            "Top  12  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  13  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  14  :   The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  649,333.\n",
            "Top  15  :   3.  Maximum payments to investor: This includes the maximum return payable to the  investor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any  interest payment.\n",
            "Top  16  :   Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on  improvements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five  outcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead  outcome funder. The UBS Optimus Foundation raised the investment from donations.\n",
            "Top  17  :   Up to CHF 26.09 million of outcome payments will be made based on improvements  in the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated  by the number of beneficiaries having regained mobility thanks to a mobility device,  divided by the number of local rehabilitation professionals. The outcome funders are  the  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The  cornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company),  alongside six other investors.\n",
            "Top  18  :   Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in  household income. The outcome funders are DfID, USAID and an anonymous donor.  This  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead  investor.\n",
            "Top  19  :   Payment  based  on  quasi-experimental  methods Payment  based  on  experimental methods Payment  based  on  validated  administrative data.\n",
            "Top  20  :   B.10.4 Budget and Payments tied to Outputs The Evaluator is expected to tie payments to delivery of the four main Evaluation Outputs –  the Evaluation Reports – with each payment commensurate to the work involved in that stage.  The  payments  will  be  made  when  each  output  is  accepted  by  DFID  as  being  of  good  or  excellent quality, where the requirements have been met with no shortcomings.\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Investment  vehicle  costs  varied  depending  on  ranged  from  nothing,  to  105k.  These  costs  depend  on  the  contracting  mechanisms  used.  Loan,  legal  and  escrow  fees  seem  to  be  consistently  cost  around  USD  30,000  –  40,000.  The  highest  costs  involve  fees  payable  to  trustees.\n",
            "Top  2  :   Implementation  Contract  management  costs These  costs  were  in  budgets  and  ranged  from  between  USD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with  third  parties  but  varied  in  size  with  two  validated  administration  data  having  lower  verification costs e.g.  around USD  larger  costs  50k  and  around  USD  500-600k  (involving  experimental/quasi-experimental  approaches).  They  types  of  costs  under  this  category  varied  between  DIBs  depending  on  how  they have been  set  up.  Total  costs  under  this  category  range  from  USD  30k  to  USD 105k.\n",
            "Top  3  :   Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD  670,000. This will be further reviewed as part of the next research wave. Verification costs  were around USD 50k for the two DIBs using validated administrative data, and between USD  500-600k for the two DIBs using experimental/quasi-experimental approaches.\n",
            "Top  4  :   Table 5.1: Additional DIB costs in the design, set up and implementation phases on top  of programming costs under a grant programme Costs (including actual, budgeted,  in-kind and pro-bono) Paid for by Cost  categories   Design and set up  Staff  time set  up Where  estimated,  this  ranged  from  USD  150,000  to  USD  490,000.\n",
            "Top  5  :   Cost by Activity (USD) Cost per stakeholder (USD) Outcome  Funder Intermediary Total Implementation  Performance management,  project management, reporting   (transaction manager costs  Volta)  Verification (AEDES)  Loan fees  (OPIC maintenance  fees)  Legal fees (process agent fees -  OPIC requirement) Return to investors 52,500           64,454 30,000 1,325 -   - - - 52,500           64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.\n",
            "Top  6  :   A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For  example, market day rates were used in the estimates.\n",
            "Top  7  :   Cost by Activity (GBP) Outcome  Funder Cost by stakeholder (GBP)   Service  Provider Investor GBP Total Design and Set up  Staff time spent on setting up  contracts  External advice on contract  design (Dalberg UK) Not  estimated Not  estimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP  254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set  up phase. Additional costs are expected for project management and reporting, and these will  be captured in the next research waves. The verification costs are expected to be USD 494k.\n",
            "Top  8  :   5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which  USD 4.3 million represents the maximum committed outcome funding.\n",
            "Top  9  :   The types of costs incurred, and who paid for these costs, are discussed below. It must be  borne in mind that across several DIBs, it was acknowledged that estimates were incomplete.  Hence, it is useful to review these costs as the types and minimum level of costs required to  launch and implement a DIB at this stage of the market.\n",
            "Top  10  :   Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an  emerging finding is that design and set up phase costs are not proportional to the size of the  DIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff  time spent on the design and set up was reported, involving thousands of hours of staff times,  over multiple months and years. Across all DIBs, external advice was needed on design of the  impact bond, financial and legal advice. External advice on contract design cost around USD  250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and  financial advice varied, but a number of DIBs reported that figures were likely to be under- reported, as not all pro-bono hours had been recorded.\n",
            "Top  11  :   Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome  Funder Investor Service  Provider Total Implementation Project management Reporting Verification (RCT and Process  Evaluation)  Trustee fees (including  Escrow) Return to investors 118,585 35,958 478,162 105,300 -                  -                            -                  - Not  -     estimated                          -                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD  755,000.\n",
            "Top  12  :   This section is set out as follows: •  Section 5.1.1 introduces the categories of costs used •  Section 5.1.2 discusses the costs incurred per DIB in more detail.\n",
            "Top  13  :   Cost by Activity (USD) Cost per stakeholder (USD) Design and Set-up  Staff time spent on setting up contracts  External advice on contract design  (including Technical adviser Volta &  Legal counsel Linklaters) Implementation Costs Outcome  Funders Intermediary Total 100,000 66,213 166,213 255,450 - 255,450 14 This additional cost for intermediaries was calculated by taking the sum of Volta's invoiced fees during the design phase  (USD 225,250) and multiplying by 25% to represent the additional time Volta staff spent working on the Bond that was not  reported on or compensated. An additional USD 9,900 was estimated for the additional time from Linklaters. This was  calculated by multiplying their fee by 0.33 to represent their additional time above their compensated rate.  15 The calculation assumes a rate of USD 1000 a day for a senior staffer The  table  below  sets  out  the  estimated  additional  costs  of  implementation,  compared  to  a  traditional grant funded project. Based on The Fred Hollow Foundation’s previous experience,  it is estimated that the additional cost of performance management, project management and  reporting is approximately 30% of Volta’s USD 175,000 fee, hence approximately USD 52,500.  Similarly, should a traditional grant be used, The Fred Hollow Foundation would engage an  evaluation  consultant  to  undertake  a  mid-term  and  end  of  project  reviewer,  and  a  data  validation  approach  using  spot  checks  and  internal  audit.  Hence,  the  ‘additional’  cost  of  verification is based on an estimate of 40% of AEDES verification fee.\n",
            "Top  14  :   Additional costs need to be offset by other benefits, such  as  increased  outcomes  or  efficiency  gains  (including  reduced staff time or transaction costs).\n",
            "Top  15  :   Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly found in the design or delivery stages?\n",
            "Top  16  :   other Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff,  monitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs  mainly  found  in  the  design or delivery stages?\n",
            "Top  17  :   Table  5.1  presents  the  ranges  of  cost  estimates  under  these  categories  and  which  stakeholders paid for these additional costs.\n",
            "Top  18  :   Do the extra costs represent value for money - to what extent do they lead to  additional results, impacts and benefits?\n",
            "Top  19  :   Do  the  extra  costs  represent  value  for  money  -  to  what  extent  do  they  lead  to  additional  results,  impacts  and benefits?\n",
            "Top  20  :   Investment  vehicle  related  costs  e.g.  Escrow  legal  and  fees  Maximum payments to investors  Maximum  payments Note: Conversions done based on the exchange rate on 5 May 2019.\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   Identification of individuals who live on less than  USD 1.90 per day Creation  of  Business  Savings  Groups  (BSG),  which are self-governing councils of businesses Local  mentors  deliver  a  four-month  training  program  the  necessary knowledge to run a business to  equip  participants  with Seed capital  is granted to  each group of three  participants,  to  enable  them  to  start  their  business Mentors  provide  continuous  guidance  to  the  participants  for  one  year,  coaching  them  in  choosing the focus of their business, as well as  how  to  grow  and  manage  their  business  and  finances, including saving in Business Savings  Groups.\n",
            "Top  2  :   excellent good/ all by reporting Qualitative  Evaluator       Value of savings generated.\n",
            "Top  3  :   Cashable  savings:  A  review  delivered  by  Azemati  et  al  (2013)  found  that,  based  on  the  SIB  experience in the US, there was little evidence that interventions truly pay for themselves. This  could be related to the fact that PbR projects seem to generally be subject to expectations of both  being innovative and following? standard procedures for traditional aid modalities. (Clist 2017) Impact Bonds Market which increases competition and drives down costs: There is limited  evidence on this point, as the impact bonds market is still nascent.\n",
            "Top  4  :   Efficiency Effectiveness Equity It is too early to draw conclusions on the efficiency of the DIBs.  No savings have yet been  realised, though opportunities for efficiency savings have been identified and these will be  reviewed during subsequent research waves.\n",
            "Top  5  :   We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We  welcome efforts by the evaluator to find savings during the life of the evaluation.\n",
            "Top  6  :   •  The report should also include an updated financial plan for  the  evaluation  –  including  highlighting  any  savings  that  are  possible  following  detailed  design  phase  and  engagement  with project level learning providers.\n",
            "Top  7  :   •  Financial risk is reduced for investors, through the use of capital protection, coupon payments and earlier repayments to investors.\n",
            "Top  8  :   Payment terms: Payment  is realised every six months  with relation to recruitment Outcome funds USD 1.38  million, split between the  two outcome funders.\n",
            "Top  9  :   Financial  ranging  return  from -100% to maximum of  9.9%  IRR,  depending  on  final number of households  reached  VE  performance.\n",
            "Top  10  :   Payment terms: Payment  attached to recruitment and  retention targets: every 6  months.  Payment attached to  attendance and  development assessment:  once a year.  Outcome metric:  Recruitment and retention,  attendance, development  assessment score  Range of returns: The  maximum return on  investment is capped at  16% The outcome funds are  USD 2.2 million, split  between the two outcome  funders. The upfront  capital commitment is of  USD 1.1 million across  two impact bonds (social  development and health,  see below). The total  potential outcome  payment could reach USD  3.6 million. Additional  grants accrue to USD  111,000.\n",
            "Top  11  :   Upfront capital  commitment was USD 4  million, first close at USD  2 million.\n",
            "Top  12  :   o  Who pays for these additional costs and to what extent do they see the benefits?\n",
            "Top  13  :   4  Level  of  returns  and  profit  made  by  the investors.\n",
            "Top  14  :   Conclusion 29 One investor noted that indemnities can work in a PbR set up, due to service providers’ limited assets, but that it would be  very unlikely for investors to be willing to provide this indemnity.   30 Pooled funds are funds from many individual investors that are aggregated for the purposes of investment, as in the case of  a mutual or pension fund. Pooled funds are also used within the humanitarian and development sector to aggregate funding  from multiple donors.\n",
            "Top  15  :   Possibility  to  get  the  from  data  Volta  but  might  not  be  very  detailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:   • (design,  set-up,  delivery, stage  learning); •  actor who incurs this cost; and  • type of cost (staff time, consultancy  and  expertise  costs,  and  the  risk  premium  investors,  (return  including interest).\n",
            "Top  16  :   At this early stage of the evaluation, it is too soon to draw firm conclusions for the evaluation  questions, so we present here thematic learnings and observations from the data in relation  to the evaluation questions and across the 4E value for money framework.  We have started  to build up a picture of what the additional costs of a DIB are. This is with a view to exploring  whether the additional costs of a DIB provide additional benefits, as well as the cost drivers to  developing DIBs. Section 6 sets out the early learning in terms of how these cost drivers can  be managed to reduce transaction costs. We have also started to detail the risk and return  expected  for  each  DIB  with  a  view  to  exploring  whether  the  amount  of  risk  transferred  is  commensurate  with  the  return  to  investor.  This  builds  on  the  analysis  of  risk  transfer  undertaken in section 5.\n",
            "Top  17  :   6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing  on private sector expertise on pricing risk would facilitate the growing of the impact bond market.  Extensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on  standardised templates.\n",
            "Top  18  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  19  :   •  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.\n",
            "Top  20  :   •  Where  possible, this  will  be  disaggregated  by  ‘first  time’  DIB  costs  which  hypothetically  wouldn’t  have  to  be  incurred  again  for  any  subsequent DIBs.\n",
            "\n",
            "\n",
            "\n",
            "Begin experiment for key  #17192\n",
            "Query:  What is the study design?\n",
            "Top  1  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  2  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  5  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  6  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  7  :   The remaining students from  the Comparison pool who were matched will become the No Pre-K  Comparison group for the remainder of the study. Comparison group students will receive a  frequency  weight equal to the number of times they were matched. Note that as a result, the  Comparison group should contain approximately two times as many unique individuals as the  Treatment group.\n",
            "Top  8  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  9  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  10  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  11  :   The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm  the cause of the irregularity, which could include handchecking code, checking for  continued  balance in the treatment and comparison groups, and looking for policy changes within  specific  schools or system-wide that could have affected  outcomes.\n",
            "Top  12  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  13  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  14  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  15  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  16  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  17  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  18  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  19  :   A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a  cohort timing chart).\n",
            "Top  20  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the research method?\n",
            "Top  1  :   Matching Methodology  Remedies  In the event that the Evaluator deems that the propensity  score matching algorithm has produced  an inadequate match, the Evaluator may make modifications  to the matching methodology. This  could include introducing a caliper to ensure that certain variables are matched to within a  narrow range (or matched exactly), adding or subtracting additional covariates, increasing or  decreasing the number of matches, or other techniques deemed rigorous and appropriate by the  Evaluator.\n",
            "Top  2  :   9 The Evaluator  may revise the methodology  for  averaging  the mobility  rate  if they  determine  that the  current  methodology  includes  a grade  breakpoint  year that could  result  in abnormally  high  mobility  out of the district.  This  methodology  must be finalized  before  the first  cohort  reaches  6th grade.\n",
            "Top  3  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  4  :   The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm  the cause of the irregularity, which could include handchecking code, checking for  continued  balance in the treatment and comparison groups, and looking for policy changes within  specific  schools or system-wide that could have affected  outcomes.\n",
            "Top  5  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  6  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  7  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  8  :   The Evaluator will use a nearest-neighbor matching algorithm  to identify  the two closest  matches based on propensity  score for each Treatment group observation, with replacement.\n",
            "Top  9  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  10  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  12  :   In reporting the extent to which the CPC program has been successful at preparing children for kindergarten, comparisons may be instructive with respect to the degree our research findings agree with what we would expect from one year of preschool.\n",
            "Top  13  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  14  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  15  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "Top  16  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  17  :   A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a  cohort timing chart).\n",
            "Top  18  :     Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  19  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  20  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "\n",
            "\n",
            "\n",
            "Query:  How was data collected and analysed?\n",
            "Top  1  :   Data will be collected on an annual basis on the based on the last school day in June which is  reported for accuracy  in the beginning of July. This may be adjusted  based on discussions  between the Evaluator and CPS to reflect the earliest date that all the necessary data would be  available.\n",
            "Top  2  :   To create the Treatment Group in school year r, the Evaluator will receive the data collected on  the last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in  school year t up to the date of the data collection. The data collected  and shared will contain all  the student data elements listed above. After  screening for eligibility as described above and  removing ineligible students from  the sample, the Evaluator will use students' ZIP codes to  merge on neighborhood data, and students'  school IDs to merge on school characteristics.  Neighborhood data will be collected from a reliable source such as Chapin Hall. This will create  a de-identified  student-level file that contains student-level characteristics, characteristics of that  student's neighborhood of residence, and characteristics of that student's  school.\n",
            "Top  3  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  4  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  5  :   DATA  COLLECTION  a.  Student data  b.  Neighborhood  data  c.  School data  d.  Data security VI.\n",
            "Top  6  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  7  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  8  :   The TS GOLD™ Spring 2015 data were missing for three11 of the 328 children, resulting in a final analytic sample for this outcome of 325 children (99% of the 328 children), which we used to calculate kindergarten readiness.\n",
            "Top  9  :   Neighborhood  data  The Evaluator will pull neighborhood  data from  publicly available census data, such as the  American Community  Survey 5-year  averages, which break out characteristics by zip code.  Neighborhood  data include: Neighborhood % of population in poverty  Neighborhood  % of population that are single mothers  Neighborhood  % of population that is Black  Neighborhood  % of population that is Hispanic  Neighborhood  % of population  employed  Neighborhood  crime statistics  Neighborhood  health  indicators The Evaluator will update the neighborhood  data file when creating a new cohort of matched  groups.\n",
            "Top  10  :   Data Security 6 Crime stats and health  indicators subject  to availability  of data. It may be possible to pull data from a Chapin  Hall  neighborhood  analysis. These covariates may be omitted  if it proves too difficult  or costly to obtain them.\n",
            "Top  11  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  13  :   School data  Data on school level characteristics will be provided by CPS, including: •  CPS School ID  •  Total student body population  •  % Free/RP lunch  •  % Black  •  % Hispanic  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating (Levels  1, 2, or 3) from the 2013/14 school year7 These data, except for attendance and the school rating, will be updated annually. Attendance  and rating data from  SY2013/14 (or the closest assessment prior to SY2013/14) will remain fixed  to reflect  the fact that the presence of a CPC may improve attendance and the school rating over  time, which could affect  the matching algorithm for later cohorts. The Evaluator may adjust  this  protocol  if extraneous events such as school closures, new leadership, or expansive new  programs are added at individual  schools or system wide that could contribute to imbalanced  matches.\n",
            "Top  14  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  15  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "Top  16  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 across various subgroups of children 3 to 5 years old. Next, Lambert, Kim, and Burts (2014b) established the external validity of the instrument by examining whether teacher ratings of child development and learning were associated with child demographic characteristics in expected directions. For example, children with identified disabilities started behind their typically developing peers and developed at a slower rate.\n",
            "Top  18  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  19  :   To create the matched No Pre-K Comparison group, the Evaluator will append the Treatment  Group dataset and the No Pre-K Comparison pool dataset, creating an indicator to identify  which  children are members of the Treatment group. The Evaluator will then run a probit model using  the treatment indicator as the dependent variable and the following  variables as  independent  variables: •  Race binary  indicators  •  Ethnicity binary  indicators •  Gender (\"Male\" binary  indicator)  •  Parental education (subject to availability)  •  Language spoken at home binaries  •  Neighborhood  % poverty  •  Neighborhood  % single mothers  •  Neighborhood  % by race  •  Neighborhood  % by ethnicity  •  Neighborhood  % employed  •  Neighborhood  crime rates (subject to availability)  •  Neighborhood health indicators (subject to availability)  •  Total student population of school currently  attending  •  % Free/RP lunch at school currently  attending  •  Racial composition of school currently  attending  •  Ethnicity composition of school currently  attending  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating binaries from  the 2013/14 school year Using the results of this model, the Evaluator will predict a propensity score based on a student's  observed characteristics. This score effectively  represents the likelihood that a child, given his  individual, neighborhood,  and school level characteristics, would be in the Treatment group.\n",
            "Top  20  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design method methodology data collection research design\n",
            "Top  1  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  2  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  3  :   DATA  COLLECTION  a.  Student data  b.  Neighborhood  data  c.  School data  d.  Data security VI.\n",
            "Top  4  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  5  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  6  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  7  :   9 The Evaluator  may revise the methodology  for  averaging  the mobility  rate  if they  determine  that the  current  methodology  includes  a grade  breakpoint  year that could  result  in abnormally  high  mobility  out of the district.  This  methodology  must be finalized  before  the first  cohort  reaches  6th grade.\n",
            "Top  8  :   To create the Treatment Group in school year r, the Evaluator will receive the data collected on  the last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in  school year t up to the date of the data collection. The data collected  and shared will contain all  the student data elements listed above. After  screening for eligibility as described above and  removing ineligible students from  the sample, the Evaluator will use students' ZIP codes to  merge on neighborhood data, and students'  school IDs to merge on school characteristics.  Neighborhood data will be collected from a reliable source such as Chapin Hall. This will create  a de-identified  student-level file that contains student-level characteristics, characteristics of that  student's neighborhood of residence, and characteristics of that student's  school.\n",
            "Top  9  :   Data will be collected on an annual basis on the based on the last school day in June which is  reported for accuracy  in the beginning of July. This may be adjusted  based on discussions  between the Evaluator and CPS to reflect the earliest date that all the necessary data would be  available.\n",
            "Top  10  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  12  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  13  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  14  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  15  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  16  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  17  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  18  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  19  :   Data Security 6 Crime stats and health  indicators subject  to availability  of data. It may be possible to pull data from a Chapin  Hall  neighborhood  analysis. These covariates may be omitted  if it proves too difficult  or costly to obtain them.\n",
            "Top  20  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design\n",
            "Top  1  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  2  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  3  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  4  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  5  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  6  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  7  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  8  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  9  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  10  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  12  :   The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm  the cause of the irregularity, which could include handchecking code, checking for  continued  balance in the treatment and comparison groups, and looking for policy changes within  specific  schools or system-wide that could have affected  outcomes.\n",
            "Top  13  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  14  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  15  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  16  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  17  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  18  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  19  :   Calculating effect  size for Special Education  utilization  To calculate the impact on Special Education utilization, the Evaluator will calculate the Average  Effect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the  Lenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the  data available, increasing sample sizes and precision of estimates.\n",
            "Top  20  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "\n",
            "\n",
            "\n",
            "Query:  method\n",
            "Top  1  :   Matching Methodology  Remedies  In the event that the Evaluator deems that the propensity  score matching algorithm has produced  an inadequate match, the Evaluator may make modifications  to the matching methodology. This  could include introducing a caliper to ensure that certain variables are matched to within a  narrow range (or matched exactly), adding or subtracting additional covariates, increasing or  decreasing the number of matches, or other techniques deemed rigorous and appropriate by the  Evaluator.\n",
            "Top  2  :   9 The Evaluator  may revise the methodology  for  averaging  the mobility  rate  if they  determine  that the  current  methodology  includes  a grade  breakpoint  year that could  result  in abnormally  high  mobility  out of the district.  This  methodology  must be finalized  before  the first  cohort  reaches  6th grade.\n",
            "Top  3  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  4  :   The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm  the cause of the irregularity, which could include handchecking code, checking for  continued  balance in the treatment and comparison groups, and looking for policy changes within  specific  schools or system-wide that could have affected  outcomes.\n",
            "Top  5  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  6  :   The Evaluator will use a nearest-neighbor matching algorithm  to identify  the two closest  matches based on propensity  score for each Treatment group observation, with replacement.\n",
            "Top  7  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  8  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  9  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  10  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  12  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  13  :   The same protocol will be used to identify  the Other CPS Pre-K Comparison group, replacing the  No CPS Pre-K Comparison pool with the Other CPS Pre-K Comparison pool.\n",
            "Top  14  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  15  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  16  :   To create the Treatment Group in school year r, the Evaluator will receive the data collected on  the last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in  school year t up to the date of the data collection. The data collected  and shared will contain all  the student data elements listed above. After  screening for eligibility as described above and  removing ineligible students from  the sample, the Evaluator will use students' ZIP codes to  merge on neighborhood data, and students'  school IDs to merge on school characteristics.  Neighborhood data will be collected from a reliable source such as Chapin Hall. This will create  a de-identified  student-level file that contains student-level characteristics, characteristics of that  student's neighborhood of residence, and characteristics of that student's  school.\n",
            "Top  17  :     Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  18  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  19  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  20  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "\n",
            "\n",
            "\n",
            "Query:  methodology\n",
            "Top  1  :   9 The Evaluator  may revise the methodology  for  averaging  the mobility  rate  if they  determine  that the  current  methodology  includes  a grade  breakpoint  year that could  result  in abnormally  high  mobility  out of the district.  This  methodology  must be finalized  before  the first  cohort  reaches  6th grade.\n",
            "Top  2  :   Matching Methodology  Remedies  In the event that the Evaluator deems that the propensity  score matching algorithm has produced  an inadequate match, the Evaluator may make modifications  to the matching methodology. This  could include introducing a caliper to ensure that certain variables are matched to within a  narrow range (or matched exactly), adding or subtracting additional covariates, increasing or  decreasing the number of matches, or other techniques deemed rigorous and appropriate by the  Evaluator.\n",
            "Top  3  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  4  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  7  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  8  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  9  :   The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm  the cause of the irregularity, which could include handchecking code, checking for  continued  balance in the treatment and comparison groups, and looking for policy changes within  specific  schools or system-wide that could have affected  outcomes.\n",
            "Top  10  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  11  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  12  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  13  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 across various subgroups of children 3 to 5 years old. Next, Lambert, Kim, and Burts (2014b) established the external validity of the instrument by examining whether teacher ratings of child development and learning were associated with child demographic characteristics in expected directions. For example, children with identified disabilities started behind their typically developing peers and developed at a slower rate.\n",
            "Top  14  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  15  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  16  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "Top  17  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  18  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  19  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  20  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "\n",
            "\n",
            "\n",
            "Query:  data collection\n",
            "Top  1  :   DATA  COLLECTION  a.  Student data  b.  Neighborhood  data  c.  School data  d.  Data security VI.\n",
            "Top  2  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  3  :   Data will be collected on an annual basis on the based on the last school day in June which is  reported for accuracy  in the beginning of July. This may be adjusted  based on discussions  between the Evaluator and CPS to reflect the earliest date that all the necessary data would be  available.\n",
            "Top  4  :   To create the Treatment Group in school year r, the Evaluator will receive the data collected on  the last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in  school year t up to the date of the data collection. The data collected  and shared will contain all  the student data elements listed above. After  screening for eligibility as described above and  removing ineligible students from  the sample, the Evaluator will use students' ZIP codes to  merge on neighborhood data, and students'  school IDs to merge on school characteristics.  Neighborhood data will be collected from a reliable source such as Chapin Hall. This will create  a de-identified  student-level file that contains student-level characteristics, characteristics of that  student's neighborhood of residence, and characteristics of that student's  school.\n",
            "Top  5  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  6  :   Neighborhood  data  The Evaluator will pull neighborhood  data from  publicly available census data, such as the  American Community  Survey 5-year  averages, which break out characteristics by zip code.  Neighborhood  data include: Neighborhood % of population in poverty  Neighborhood  % of population that are single mothers  Neighborhood  % of population that is Black  Neighborhood  % of population that is Hispanic  Neighborhood  % of population  employed  Neighborhood  crime statistics  Neighborhood  health  indicators The Evaluator will update the neighborhood  data file when creating a new cohort of matched  groups.\n",
            "Top  7  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  8  :   Data Security 6 Crime stats and health  indicators subject  to availability  of data. It may be possible to pull data from a Chapin  Hall  neighborhood  analysis. These covariates may be omitted  if it proves too difficult  or costly to obtain them.\n",
            "Top  9  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  10  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  12  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  13  :   School data  Data on school level characteristics will be provided by CPS, including: •  CPS School ID  •  Total student body population  •  % Free/RP lunch  •  % Black  •  % Hispanic  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating (Levels  1, 2, or 3) from the 2013/14 school year7 These data, except for attendance and the school rating, will be updated annually. Attendance  and rating data from  SY2013/14 (or the closest assessment prior to SY2013/14) will remain fixed  to reflect  the fact that the presence of a CPC may improve attendance and the school rating over  time, which could affect  the matching algorithm for later cohorts. The Evaluator may adjust  this  protocol  if extraneous events such as school closures, new leadership, or expansive new  programs are added at individual  schools or system wide that could contribute to imbalanced  matches.\n",
            "Top  14  :   The TS GOLD™ Spring 2015 data were missing for three11 of the 328 children, resulting in a final analytic sample for this outcome of 325 children (99% of the 328 children), which we used to calculate kindergarten readiness.\n",
            "Top  15  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  16  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 across various subgroups of children 3 to 5 years old. Next, Lambert, Kim, and Burts (2014b) established the external validity of the instrument by examining whether teacher ratings of child development and learning were associated with child demographic characteristics in expected directions. For example, children with identified disabilities started behind their typically developing peers and developed at a slower rate.\n",
            "Top  18  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  19  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  20  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "\n",
            "\n",
            "\n",
            "Query:  research design\n",
            "Top  1  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  2  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  5  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  6  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  7  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  8  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  9  :     Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  10  :   •  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the  Program, which is completed by all Program staff where appropriate.\n",
            "Top  11  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  12  :   A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a  cohort timing chart).\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  15  :   •  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on- site visits to monitor quality and effectiveness  to the Program.\n",
            "Top  16  :     Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  17  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  18  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  19  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  20  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the target population?\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  3  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  4  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  5  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  6  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  7  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  8  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  9  :   This “no Pre-K” comparison group will be identified when the children are in kindergarten for each of the four Cohorts. Specifically, the evaluators will create a no Pre-K comparison group for each cohort of intervention children using propensity score matching processes.\n",
            "Top  10  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "Top  11  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  12  :   The remaining students from  the Comparison pool who were matched will become the No Pre-K  Comparison group for the remainder of the study. Comparison group students will receive a  frequency  weight equal to the number of times they were matched. Note that as a result, the  Comparison group should contain approximately two times as many unique individuals as the  Treatment group.\n",
            "Top  13  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  14  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  15  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  16  :   (2)  What is the rate of third-grade literacy as defined by performance in meeting or exceeding grade-level performance on the state or district-administered third-grade assessment in reading?\n",
            "Top  17  :   1 The inlention is to identify  children  in the \"age cycle four\" year -  the year prior to when they are planning to attend Kindergarten. At the time of  the drafting  of this document, this was defined  by CPS as attaining age four on or before  September  1st. This age identification  protocol  may be  adapted  as necessary to capture these children.\n",
            "Top  18  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  19  :   Similarly, for  the No  Pre-K  Comparison  group, we will  limit the primary  analysis  sample  to  eligible No  Pre-K  children  who  attend  at least  66% of school  days  in a given  school  year.  If a  child  at any  point during  the  Kindergarten  year  attends  a school  operating  a CPC program,  that  child  will be omitted  from  primary  analyses.\n",
            "Top  20  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who are the intended beneficiaries of the service?\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  3  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  4  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  7  :   In the second year (2015-16) of the SIB-CPC project, two additional sites, identified by CPS and approved by the city of Chicago, were added to the six 2014-15 SIB- CPC sites. The project anticipates that four cohorts of children will be served across the eight sites, identified by the school year in which children begin preschool (cohort 1: 2014-15, cohort 2: 2015-16, cohort 3: 2016-17, cohort 4: 2017-18) (see Appendix B for grade levels of children in the four cohorts across years.) Evaluation Design SIB and PFS initiatives typically involve an independent evaluator to help determine whether the outcomes have been achieved. Because government only pays when outcomes are achieved rather than for activities, the focus of the evaluation is on measuring the outcomes of the individuals participating in the initiative.\n",
            "Top  8  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  9  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  12  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  13  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  14  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  15  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "Top  16  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  17  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  18  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  19  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  20  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who does the service try to help?\n",
            "Top  1  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  2  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  3  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  4  :     CPS and, most specifically, the Office of Early Childhood Education provides  meaningful professional development and ongoing coaching and feedback for  teachers, aides, and other staff members that facilitates high-quality instructional  practices.\n",
            "Top  5  :   •  CPS and, most specifically,  the Office  of Early Childhood  Education provides meaningful  professional  development and ongoing coaching and feedback  for teachers,  aides, and other staff members that facilitates  high-quality  instructional practices.\n",
            "Top  6  :     Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  7  :   Schools continue to enroll  students  throughout the school year as slots  open up due to attrition, new  funding,  etc. Staff conduct additional  outreach  in communities with lower than  expected enrollment to help fill all the  slots. This includes additional ad  spots, flyers, and community events.  These children will only be included  for evaluation purposes if they meet  the dosage and eligibility  requirements  outlined in this document.\n",
            "Top  8  :   •  Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the  primary grades.  Provide a part-time Kindergarten  aide when funding  is available to support the transition  into Kindergarten.\n",
            "Top  9  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  10  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  11  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  12  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  13  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  14  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  15  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "Top  16  :     Program staff meet with parents over the course of each school year to review their child’s progress and discuss parent program opportunities with the Parent  Resource Teacher (PRT).\n",
            "Top  17  :   •  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on- site visits to monitor quality and effectiveness  to the Program.\n",
            "Top  18  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "Top  19  :     Offer and engage families in monthly activities. PRTs create and distribute a monthly parent involvement calendar, and conduct parent/teacher conferences  over the year to review progress in the parent program.\n",
            "Top  20  :     Provide a resource room dedicated to parent and family activities through Kindergarten when possible.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who was eligible for inclusion in the intervention?\n",
            "Top  1  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  2  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  3  :   Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.\n",
            "Top  4  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  5  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  6  :   Additionally, a child needed to have attended a CPC pre-K classroom for at least 66% of the days (not consecutively) in a given school year—a percentage considered a sufficient amount or “dose” of the intervention to affect child outcomes.\n",
            "Top  7  :   All four-year-olds  at CPC SIB sites, including children attending full-day  classes, will be  included in the treatment group, subject to the exclusions listed below.\n",
            "Top  8  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  9  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  10  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  11  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  12  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  13  :   This “no Pre-K” comparison group will be identified when the children are in kindergarten for each of the four Cohorts. Specifically, the evaluators will create a no Pre-K comparison group for each cohort of intervention children using propensity score matching processes.\n",
            "Top  14  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  15  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  16  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  18  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  19  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  20  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population beneficiaries service users participants eligible population eligibility criteria cohort clients\n",
            "Top  1  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  2  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  3  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  4  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  5  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  6  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  7  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  8  :   A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a  cohort timing chart).\n",
            "Top  9  :   The cohort used to determine kindergarten readiness included children from the five sites that were already providing the CPC model to three- and four-year olds.\n",
            "Top  10  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  11  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  12  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  13  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  14  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  15  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  16  :   This “no Pre-K” comparison group will be identified when the children are in kindergarten for each of the four Cohorts. Specifically, the evaluators will create a no Pre-K comparison group for each cohort of intervention children using propensity score matching processes.\n",
            "Top  17  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  18  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  19  :   Participants in the CPC program (the Treatment Group) will be compared to groups of matched  comparison  students who did not have a CPC experience through the use of a propensity  score  matching algorithm. One comparison group will consist of children who did not attend any  form  of CPS Pre-K (No Pre-K comparison group). Another group will consist of children  who  attended some other type of CPS pre-K program, such as Head Start or Pre-School  for All (Other  Pre-K comparison group).\n",
            "Top  20  :   June through  September  2014 Registration September 2014 Enrollment September 2014 onward Rolling enrollment the CPS SIM IMPACT system.\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  3  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  4  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  5  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  6  :   Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains Number of domains meeting or  exceeding the 50th percentile Percent Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain Domain Percent Cognitive Language Literacy Math Physical Social-emotional 11% 3% 7% 11% 9% 10% 49% 80% 64% 72% 78% 58% 77% Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Discussion Socio-demographic risk factors—the most extensively studied of which is poverty— are associated with variability in skill development, as well as differential growth in later academic achievement. Early childhood programs potentially mitigate the risks endemic to children from disadvantaged backgrounds, with studies showing that the strongest positive short- and long-term outcomes result from intensive and comprehensive programs targeting low-income children (Burger, 2010; Institute for Research on Poverty, 1997; Reynolds et al., 2010). Prior studies highlight early childhood as a critical and sensitive period for the development of brain architecture and neurochemistry (e.g., Knudsen, Heckman, Cameron, & Shonkoff, 2006) and subsequent academic and socio-emotional well-being (Shonkoff & Phillips, 2000).\n",
            "Top  7  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  8  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  9  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  10  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  11  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  12  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  13  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  14  :   The remaining students from  the Comparison pool who were matched will become the No Pre-K  Comparison group for the remainder of the study. Comparison group students will receive a  frequency  weight equal to the number of times they were matched. Note that as a result, the  Comparison group should contain approximately two times as many unique individuals as the  Treatment group.\n",
            "Top  15  :   STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol  Comparison group students will be selected using a propensity  score matching technique.  Individuals from the treatment group will be matched to up to two individuals from  the No Pre-K  Comparison group and up to two individuals from  the Other CPS Pre-K Comparison group.  Matching will be conducted with replacement to allow comparison individuals to be matched  more than once.\n",
            "Top  16  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  17  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  18  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  19  :   1 The inlention is to identify  children  in the \"age cycle four\" year -  the year prior to when they are planning to attend Kindergarten. At the time of  the drafting  of this document, this was defined  by CPS as attaining age four on or before  September  1st. This age identification  protocol  may be  adapted  as necessary to capture these children.\n",
            "Top  20  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "\n",
            "\n",
            "\n",
            "Query:  beneficiaries\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  3  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  4  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  5  :     Two-thirds of the children (68%) were identified as Hispanic and one-third (29%) were identified as African-American. Fewer than 2% of the children were identified as Caucasian and the remaining 2% were identified as Asian or multiracial.\n",
            "Top  6  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  7  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  8  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  9  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :   Children are enrolled upon attendance  on the first  day of school.\n",
            "Top  12  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  13  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  14  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  15  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  16  :   The Evaluator will use a nearest-neighbor matching algorithm  to identify  the two closest  matches based on propensity  score for each Treatment group observation, with replacement.\n",
            "Top  17  :   Appendices Appendix A:  Chicago Child-Parent Center Social Impact Bond Evaluation Plan Appendix B:  Timing of Cohorts Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Appendix A: Chicago Child-Parent Center Social  Impact Bond Evaluation Plan Social Impact Bond Report November 2015 A-1 Chicago Child-Parent  Center  Social Impact  Bond  Evaluation  Plan December 2, 2014 Table of Contents I.\n",
            "Top  18  :   To create the Treatment Group in school year r, the Evaluator will receive the data collected on  the last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in  school year t up to the date of the data collection. The data collected  and shared will contain all  the student data elements listed above. After  screening for eligibility as described above and  removing ineligible students from  the sample, the Evaluator will use students' ZIP codes to  merge on neighborhood data, and students'  school IDs to merge on school characteristics.  Neighborhood data will be collected from a reliable source such as Chapin Hall. This will create  a de-identified  student-level file that contains student-level characteristics, characteristics of that  student's neighborhood of residence, and characteristics of that student's  school.\n",
            "Top  19  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  20  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "\n",
            "\n",
            "\n",
            "Query:  service users\n",
            "Top  1  :   Indeed, the CPCs emphasize the provision of comprehensive services and parental involvement—program features that are considered to be strongly associated with program quality (Reynolds & Hayakawa, 2011; Reynolds, Magnuson, & Ou, 2010). A typical CPC site includes the components listed in Exhibit 1.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  4  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  5  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  6  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  7  :   •  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on- site visits to monitor quality and effectiveness  to the Program.\n",
            "Top  8  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "Top  9  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  10  :     CPS and, most specifically, the Office of Early Childhood Education provides  meaningful professional development and ongoing coaching and feedback for  teachers, aides, and other staff members that facilitates high-quality instructional  practices.\n",
            "Top  11  :   To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to  the average of a binary indicator of Special Education utilization among the No CPS Pre-K  Comparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of  Special Education utilization  among the Treatment group for cohort / in year t. At the discretion  of the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the  Approval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help  account for any differences  in covariates between the Treatment group and the Comparison  group.\n",
            "Top  12  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  13  :   •  CPS and, most specifically,  the Office  of Early Childhood  Education provides meaningful  professional  development and ongoing coaching and feedback  for teachers,  aides, and other staff members that facilitates  high-quality  instructional practices.\n",
            "Top  14  :   Kim, Lambert, and Burts (2013) recently published data that provide empirical evidence supporting the validity for the TS GOLD™ domains and learning objectives for typically developing children, as well as English-language learners and for those children identified with special needs or disabilities. In other words, this observation- based teacher rating evaluation measures the construct domains in the same way                                                               12 ECLS-B and ECLS-K are two contemporary longitudinal datasets that draw from a nationally  representative sample; both collected direct assessments of children’s skills at kindergarten entry (cf.  Hair, Halle, Terry-Humen, Lavelle, & Calkins, 2006; Lee, Zhai, Brooks-Gunn, Han, & Waldfogel, 2014).\n",
            "Top  15  :     Meet regularly, under the direction of the Principal, to discuss operations and best practices within the CPC.\n",
            "Top  16  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  17  :     Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  18  :   June through  September  2014 Registration September 2014 Enrollment September 2014 onward Rolling enrollment the CPS SIM IMPACT system.\n",
            "Top  19  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  20  :   •  Meet regularly, under the direction  of the Principal to discuss operations and best practices within the CPC.\n",
            "\n",
            "\n",
            "\n",
            "Query:  participants\n",
            "Top  1  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  2  :   Parent Involvement and Engagement •  Engage a PRT and School-Community  Representative  (SCR) to work closely with the  Head Teacher and Liaisons to maintain a consistently  supportive parent program.  •  Encourage parents to sign a CPC school-home agreement at the start of the school year  outlining a plan for fostering  learning at home and participating  in CPC activities.  •  Offer  and engage families  in monthly  activities. PRTs create and distribute a monthly  parent involvement calendar, and conduct parent/teacher  conferences  over the year to  review progress in the parent program.\n",
            "Top  3  :   Collaborative Leadership Team   Engage a Program leadership team that includes the Head Teacher, Parent Resource Teacher, and School-Community Representative.\n",
            "Top  4  :   Parent Involvement and Engagement   Engage a PRT and School-Community Representative (SCR) to work closely with  the Head Teacher and Liaisons to maintain a consistently supportive parent  program.\n",
            "Top  5  :   Collaborative Leadership  Team •  Engage a Program leadership team that includes the Head Teacher, Parent Resource Teacher, and School-Community  Representative.\n",
            "Top  6  :     About one-tenth (11%) of the children attending the five sites had an identified mild developmental delay or disability or an identified 504 plan that described modifications and accommodations (e.g., an extra set of textbooks, home instruction, a tape recorder or keyboard for taking notes) that they needed to perform at the same level as their peers.\n",
            "Top  7  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  8  :     Offer and engage families in monthly activities. PRTs create and distribute a monthly parent involvement calendar, and conduct parent/teacher conferences  over the year to review progress in the parent program.\n",
            "Top  9  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  10  :   •  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on- site visits to monitor quality and effectiveness  to the Program.\n",
            "Top  11  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  12  :     Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the primary grades.\n",
            "Top  13  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "Top  14  :     Collaborate with the PRT and classroom teachers to ensure that opportunities to  engage families in student learning are available, appropriate, and aligned to the  program and parents’ needs.\n",
            "Top  15  :   The Evaluator  may  add  additional  criteria based  on an analysis  of enrollment  and  attendance  data with the approval  of  CPS, the  City,  and  the  Project  Coordinator  and  Approval  of the  Lender  Committee.\n",
            "Top  16  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  17  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  18  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "Top  19  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  20  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligible population\n",
            "Top  1  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  2  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  3  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  4  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  6  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  7  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  8  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  9  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  10  :     Two-thirds of the children (68%) were identified as Hispanic and one-third (29%) were identified as African-American. Fewer than 2% of the children were identified as Caucasian and the remaining 2% were identified as Asian or multiracial.\n",
            "Top  11  :   Similarly, for  the No  Pre-K  Comparison  group, we will  limit the primary  analysis  sample  to  eligible No  Pre-K  children  who  attend  at least  66% of school  days  in a given  school  year.  If a  child  at any  point during  the  Kindergarten  year  attends  a school  operating  a CPC program,  that  child  will be omitted  from  primary  analyses.\n",
            "Top  12  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  13  :   Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains Number of domains meeting or  exceeding the 50th percentile Percent Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain Domain Percent Cognitive Language Literacy Math Physical Social-emotional 11% 3% 7% 11% 9% 10% 49% 80% 64% 72% 78% 58% 77% Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Discussion Socio-demographic risk factors—the most extensively studied of which is poverty— are associated with variability in skill development, as well as differential growth in later academic achievement. Early childhood programs potentially mitigate the risks endemic to children from disadvantaged backgrounds, with studies showing that the strongest positive short- and long-term outcomes result from intensive and comprehensive programs targeting low-income children (Burger, 2010; Institute for Research on Poverty, 1997; Reynolds et al., 2010). Prior studies highlight early childhood as a critical and sensitive period for the development of brain architecture and neurochemistry (e.g., Knudsen, Heckman, Cameron, & Shonkoff, 2006) and subsequent academic and socio-emotional well-being (Shonkoff & Phillips, 2000).\n",
            "Top  14  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  15  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  16  :   More recently, Reynolds and colleagues (2014) published data in a peer-reviewed journal showing that 80.9% of children attending full-day CPC classrooms (n = 409) and 58.7% of children attending part-day CPC classrooms (n = 573) were considered kindergarten-ready when kindergarten-readiness was defined as meeting the national norm on four of the six TS GOLD™ subdomains. Additionally, full-day participants demonstrated higher average levels of skill mastery in the subdomains of language, mathematics, socio-emotional development, and physical health (but not for literacy and cognitive development). Reynolds and colleagues (2014) report a higher proportion of children who are kindergarten ready, but use a less stringent standard for “readiness,” i.e., a threshold of four compared with five; five was the standard for the current evaluation. If we had used that standard of 4 of 6 domains, an additional 9% would meet that kindergarten readiness criteria, for a total of 68% (Exhibit 3). The independent evaluator decided prior to the analysis to use the more stringent standard of 5 of 6 domains to represent kindergarten readiness.\n",
            "Top  17  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  18  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  19  :   The TS GOLD™ Spring 2015 data were missing for three11 of the 328 children, resulting in a final analytic sample for this outcome of 325 children (99% of the 328 children), which we used to calculate kindergarten readiness.\n",
            "Top  20  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligibility criteria\n",
            "Top  1  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).\n",
            "Top  3  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  4  :   Eligible Population -  No Pre-K Comparison Group  The No Pre-K Comparison Group in this study will be identified  via a propensity  score matching  algorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible  No Pre-K children will include all children who meet the following  criteria: •  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are five years of age as of September 1st  •  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten  •  Did not attend a Head Start program  funded  through the City of Chicago  •  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or  more of a city funded  pre-school program, or any days at any CPC site over the course of the  school year. Days need not have been attended  consecutively.\n",
            "Top  5  :   Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score  matching algorithm that pulls from  a pool of eligible children who attended other forms of CPS  pre-K within the district. The pool of eligible Other CPS Pre-K children will include children  who meet the following  criteria: •  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools  o  Schools currently operating a CPC, as part of the SIB program or otherwise  o  Magnet and Selective Enrollment  Schools  o  Schools that serve exclusively  a special education  population •  Are four years of age as of September 1st.  •  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched  Treatment cohort begins pre-school to ensure that children within both groups are on the same  age cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations  The hypothesis is that the CPC program will have the biggest impact on children who are  deemed at risk for poor school performance  and achievement, but who lack a severe or  significant  disability. Without additional  support, many of these children may end up being  diagnosed with a mild learning disability, emotional  disturbance, or developmental  delay  (including speech/language  impairment). For these children, additional  support in the classroom  and at home can help ensure that they stay on track developmentally  with their peers, avoiding  the need for years of special education  services.\n",
            "Top  6  :   A child may enter the program based on CPS age elibility criteria. For the 2014/15 school year,  this entailed being age 4 as of September 1st.\n",
            "Top  7  :   STUDY  POPULATION  a.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group  c.  Eligible population -  Other CPS Pre-K Comparison group  d.  Exclusions RECRUITMENT  PROCEDURES  a.  CPS Pre-K recruitment process III.\n",
            "Top  8  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  9  :   The Evaluator  may  add  additional  criteria based  on an analysis  of enrollment  and  attendance  data with the approval  of  CPS, the  City,  and  the  Project  Coordinator  and  Approval  of the  Lender  Committee.\n",
            "Top  10  :   Similarly, for  the No  Pre-K  Comparison  group, we will  limit the primary  analysis  sample  to  eligible No  Pre-K  children  who  attend  at least  66% of school  days  in a given  school  year.  If a  child  at any  point during  the  Kindergarten  year  attends  a school  operating  a CPC program,  that  child  will be omitted  from  primary  analyses.\n",
            "Top  11  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  12  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  13  :   More recently, Reynolds and colleagues (2014) published data in a peer-reviewed journal showing that 80.9% of children attending full-day CPC classrooms (n = 409) and 58.7% of children attending part-day CPC classrooms (n = 573) were considered kindergarten-ready when kindergarten-readiness was defined as meeting the national norm on four of the six TS GOLD™ subdomains. Additionally, full-day participants demonstrated higher average levels of skill mastery in the subdomains of language, mathematics, socio-emotional development, and physical health (but not for literacy and cognitive development). Reynolds and colleagues (2014) report a higher proportion of children who are kindergarten ready, but use a less stringent standard for “readiness,” i.e., a threshold of four compared with five; five was the standard for the current evaluation. If we had used that standard of 4 of 6 domains, an additional 9% would meet that kindergarten readiness criteria, for a total of 68% (Exhibit 3). The independent evaluator decided prior to the analysis to use the more stringent standard of 5 of 6 domains to represent kindergarten readiness.\n",
            "Top  14  :     Provide highly qualified educational staff that will provide the classroom instruction and parent engagement activities. For example, classroom teachers are certified  with a bachelor’s degree (or higher). Overall, program staff must adhere to the  requirements set forth by the CPS Talent office, in accordance with collective  bargaining unit agreements, and state regulations. Any changes in CPS education  and certification requirements will be complied with.\n",
            "Top  15  :   •  Provide highly qualified  educational  staff that will provide the classroom instruction and parent engagement activities. For example, classroom teachers are certified  with a  bachelor's degree (or higher). Overall, program staff must adhere to the requirements set  forth by the CPS Talent office,  in accordance with collective bargaining unit agreements,  and state regulations.  Any changes in CPS education and certification  requirements will  be complied with.\n",
            "Top  16  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  17  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  18  :   Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.\n",
            "Top  19  :   We categorized children as kindergarten ready on each domain by the criterion of meeting or exceeding the 50% percentile on the standard score for that domain using scores from the most recently published technical manual (Lambert, Kim, & Burts, 2014a). Then, we calculated the percentage of children who met this criterion on five of six domains.10 CALCULATING IMPACT ON KINDERGARTEN READINESS Every child who scored “at” or “above” the national norm on at least five of the six domains in the spring of their preschool year was categorized as “kindergarten ready.” Results This section discusses the results for the first cohort of SIB-CPC children (Cohort 1).\n",
            "Top  20  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "\n",
            "\n",
            "\n",
            "Query:  cohort\n",
            "Top  1  :   The cohort used to determine kindergarten readiness included children from the five sites that were already providing the CPC model to three- and four-year olds.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  4  :   children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics:   Half of the children were male (51%).\n",
            "Top  5  :   A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a  cohort timing chart).\n",
            "Top  6  :   Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC  in grade PK One‐fifth (20%) did not attend at least 66% of  days 3% had severe disability or were in a  separate classroom for special education  students Less than 1% were too young (i.e., under 4  years old in September 2014) A small percentage (2%) were not eligible for  free‐ or reduced‐price lunch or were denied  because of insufficient documentation A small percentage (2%) were removed for  two or more of the above reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the  year. As children left a site, new children were enrolled. The 449 includes all children ever enrolled  during the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old  children were enrolled at the five sites at the end of the year.\n",
            "Top  7  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  8  :   The No Pre-K Comparison  group will be identified  the year that their matched Treatment cohort  begins Kindergarten to ensure that children within both groups are on the same age cycle.\n",
            "Top  9  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  10  :   This “no Pre-K” comparison group will be identified when the children are in kindergarten for each of the four Cohorts. Specifically, the evaluators will create a no Pre-K comparison group for each cohort of intervention children using propensity score matching processes.\n",
            "Top  11  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  12  :   The year 2 report will include kindergarten readiness outcomes for children participating in Cohort 2. It will also include data examining special education placement rates in kindergarten for Cohort 1 compared with rates of special education placement in a matched-comparison sample of children who did not attend any preschool in CPS.\n",
            "Top  13  :   Appendices Appendix A:  Chicago Child-Parent Center Social Impact Bond Evaluation Plan Appendix B:  Timing of Cohorts Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Appendix A: Chicago Child-Parent Center Social  Impact Bond Evaluation Plan Social Impact Bond Report November 2015 A-1 Chicago Child-Parent  Center  Social Impact  Bond  Evaluation  Plan December 2, 2014 Table of Contents I.\n",
            "Top  14  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  15  :   To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year  /, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of  all five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the  data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility as described above and removing ineligible students from  the sample, the  Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on  school characteristics.\n",
            "Top  16  :   To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school  year t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS  of all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to  the date of the data dump. The data dump will contain all the student data elements listed above.  After  screening for eligibility  as described above and removing ineligible students from  the  sample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data  to merge on school  characteristics.\n",
            "Top  17  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  18  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "Top  19  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  20  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "\n",
            "\n",
            "\n",
            "Query:  clients\n",
            "Top  1  :     Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make  frequent on-site visits to monitor quality and effectiveness to the Program.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   •  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on- site visits to monitor quality and effectiveness  to the Program.\n",
            "Top  4  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  5  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  6  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  7  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "Top  8  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  9  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  10  :     Meet regularly, under the direction of the Principal, to discuss operations and best practices within the CPC.\n",
            "Top  11  :   •  Meet regularly, under the direction  of the Principal to discuss operations and best practices within the CPC.\n",
            "Top  12  :   •  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  15  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  16  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "Top  17  :   •  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage  families  in student learning are available, appropriate and aligned to the program and  parents' needs.\n",
            "Top  18  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  19  :   STUDY  POPULATION Eligible Population -  Treatment  Group  The Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any  of the CPC SIB sites, in full  day or half day programs, who at any point during the school year  are eligible for the National  School Lunch Program (NSLP).\n",
            "Top  20  :   DATA  COLLECTION  a.  Student data  b.  Neighborhood  data  c.  School data  d.  Data security VI.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  4  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  5  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  6  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  7  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  10  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  11  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  14  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  15  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  16  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  17  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  18  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  19  :   Children are enrolled upon attendance  on the first  day of school.\n",
            "Top  20  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  2  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  3  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  4  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  5  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  6  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  7  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  8  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  9  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  12  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  13  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  14  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  15  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  16  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  17  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  18  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  19  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  20  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  2  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  3  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  4  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  7  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  8  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  9  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  10  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  11  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  12  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  13  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  17  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  18  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  19  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  20  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  5  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  6  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  7  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  10  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  11  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  12  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  15  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  16  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  17  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  18  :   Children are enrolled upon attendance  on the first  day of school.\n",
            "Top  19  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  20  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  3  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  4  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  7  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  8  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  9  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  10  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  11  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  14  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  15  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  16  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  17  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  18  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  19  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  20  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  3  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  4  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  7  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  8  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  9  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  10  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  11  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  12  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  13  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  16  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  17  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  18  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  19  :   CPC Program Model CPC Model Description CPC programs seek to promote school readiness, parent involvement, and early learning that, in turn, will translate into long-term benefits with regards to academic achievement, higher graduation rates, and career success. The CPC model is unique Child-Parent Center Evaluation: Report for 2014-15                  April 2016 in that it is designed to (1) provide full- or part-time high-quality preschool experiences for three- and four-year old children, and (2) combine those educational experiences with family support services and parent engagement activities. The services for children and families offered by CPC sites are intended to be delivered in a coordinated and synergistic way across the preschool to third grade continuum.\n",
            "Top  20  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  2  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  3  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  4  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  5  :   SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children’s performance on the measure could be used to identify whether they were performing at or above 3 SRI’s involvement in the evaluation is currently scheduled to end in Fall 2020.\n",
            "Top  6  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  7  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  8  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  9  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  10  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  11  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  12  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  13  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  14  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  15  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  17  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  18  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  19  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  20  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  7  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  8  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  9  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  10  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  11  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "Top  13  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  14  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  15  :     Provide a resource room dedicated to parent and family activities through Kindergarten when possible.\n",
            "Top  16  :   Kindergarten  Readiness  CPS  uses the  Teaching  Strategies  Gold  (TS  Gold)  instrument  in all their  Pre-K  classrooms  to  track the development  of children.  Based  on  teacher  observations,  TS  Gold  measures  the  progress  of children  in domains  such  as socio-emotional,  physical,  language,  literacy,  and  cognitive  development.\n",
            "Top  17  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "Top  18  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  19  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  20  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  4  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  5  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  6  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  7  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  8  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  9  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  10  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  11  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  12  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  13  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  14  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  15  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  16  :     Encourage parents to sign a CPC school-home agreement at the start of the school year outlining a plan for fostering learning at home and participating in CPC  activities.\n",
            "Top  17  :   Appendices Appendix A:  Chicago Child-Parent Center Social Impact Bond Evaluation Plan Appendix B:  Timing of Cohorts Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Appendix A: Chicago Child-Parent Center Social  Impact Bond Evaluation Plan Social Impact Bond Report November 2015 A-1 Chicago Child-Parent  Center  Social Impact  Bond  Evaluation  Plan December 2, 2014 Table of Contents I.\n",
            "Top  18  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  19  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 and in later grades.\n",
            "Top  20  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  3  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  4  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  5  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  6  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  7  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  8  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  9  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  10  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  11  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  12  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  13  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  16  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  17  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  18  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  19  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  20  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  4  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  5  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  6  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  7  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  8  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  9  :   (1)  What is the rate of kindergarten readiness in children participating in the SIB- CPC sites as defined by performance on the Teaching Strategies (TS) Gold instrument (completed by teachers in the spring of preschool before a child enters kindergarten)?\n",
            "Top  10  :   School data  Data on school level characteristics will be provided by CPS, including: •  CPS School ID  •  Total student body population  •  % Free/RP lunch  •  % Black  •  % Hispanic  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating (Levels  1, 2, or 3) from the 2013/14 school year7 These data, except for attendance and the school rating, will be updated annually. Attendance  and rating data from  SY2013/14 (or the closest assessment prior to SY2013/14) will remain fixed  to reflect  the fact that the presence of a CPC may improve attendance and the school rating over  time, which could affect  the matching algorithm for later cohorts. The Evaluator may adjust  this  protocol  if extraneous events such as school closures, new leadership, or expansive new  programs are added at individual  schools or system wide that could contribute to imbalanced  matches.\n",
            "Top  11  :   To create the matched No Pre-K Comparison group, the Evaluator will append the Treatment  Group dataset and the No Pre-K Comparison pool dataset, creating an indicator to identify  which  children are members of the Treatment group. The Evaluator will then run a probit model using  the treatment indicator as the dependent variable and the following  variables as  independent  variables: •  Race binary  indicators  •  Ethnicity binary  indicators •  Gender (\"Male\" binary  indicator)  •  Parental education (subject to availability)  •  Language spoken at home binaries  •  Neighborhood  % poverty  •  Neighborhood  % single mothers  •  Neighborhood  % by race  •  Neighborhood  % by ethnicity  •  Neighborhood  % employed  •  Neighborhood  crime rates (subject to availability)  •  Neighborhood health indicators (subject to availability)  •  Total student population of school currently  attending  •  % Free/RP lunch at school currently  attending  •  Racial composition of school currently  attending  •  Ethnicity composition of school currently  attending  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating binaries from  the 2013/14 school year Using the results of this model, the Evaluator will predict a propensity score based on a student's  observed characteristics. This score effectively  represents the likelihood that a child, given his  individual, neighborhood,  and school level characteristics, would be in the Treatment group.\n",
            "Top  12  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  13  :   7 All these data are publicly available online at http://www.cps.edu/schools/find  a  school/pages/findaschool.aspx.  School rating is based on the CPS Performance  Policy which  is used to rate CPS schools. A Level  1  rating is  \"excellent\",  a Level 2 rating is \"good\" and a Level 3 rating  is \"low\".\n",
            "Top  14  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  15  :   (2)  What is the rate of third-grade literacy as defined by performance in meeting or exceeding grade-level performance on the state or district-administered third-grade assessment in reading?\n",
            "Top  16  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "Top  17  :   To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to  the average of a binary indicator of Special Education utilization among the No CPS Pre-K  Comparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of  Special Education utilization  among the Treatment group for cohort / in year t. At the discretion  of the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the  Approval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help  account for any differences  in covariates between the Treatment group and the Comparison  group.\n",
            "Top  18  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  19  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  20  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  6  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  7  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  8  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  9  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  10  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  11  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  14  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  15  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  16  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  17  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  18  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  19  :   In the second year (2015-16) of the SIB-CPC project, two additional sites, identified by CPS and approved by the city of Chicago, were added to the six 2014-15 SIB- CPC sites. The project anticipates that four cohorts of children will be served across the eight sites, identified by the school year in which children begin preschool (cohort 1: 2014-15, cohort 2: 2015-16, cohort 3: 2016-17, cohort 4: 2017-18) (see Appendix B for grade levels of children in the four cohorts across years.) Evaluation Design SIB and PFS initiatives typically involve an independent evaluator to help determine whether the outcomes have been achieved. Because government only pays when outcomes are achieved rather than for activities, the focus of the evaluation is on measuring the outcomes of the individuals participating in the initiative.\n",
            "Top  20  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  6  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  7  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  12  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  15  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  16  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "Top  18  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  19  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  20  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  2  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  3  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  4  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  7  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  8  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  9  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  10  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  11  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  12  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  13  :   To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to  the average of a binary indicator of Special Education utilization among the No CPS Pre-K  Comparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of  Special Education utilization  among the Treatment group for cohort / in year t. At the discretion  of the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the  Approval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help  account for any differences  in covariates between the Treatment group and the Comparison  group.\n",
            "Top  14  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  15  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  16  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  17  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  18  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  19  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  20  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What outcomes were achieved?\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  3  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  4  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  5  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  6  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  7  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  8  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  9  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  10  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  11  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  12  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  13  :   children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics:   Half of the children were male (51%).\n",
            "Top  14  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  15  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  17  :   In reporting the extent to which the CPC program has been successful at preparing children for kindergarten, comparisons may be instructive with respect to the degree our research findings agree with what we would expect from one year of preschool.\n",
            "Top  18  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  19  :     Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.\n",
            "Top  20  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "\n",
            "\n",
            "\n",
            "Query:  What impact was achieved?\n",
            "Top  1  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  2  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  3  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  4  :   IMPACT ON THIRD-GRADE READING AND LITERACY The Chicago Longitudinal Study (CLS) followed children over time using administrative records to examine attendance, achievement, and graduation rates in CPC participants compared with children who did not attend CPC preschool. One study found a significant positive impact on third-grade reading achievement for pre-K to third-grade participants (.53 standard deviation) compared with participants who attended CPC only for pre-K and kindergarten (Reynolds, 1994). Smaller studies of high-quality preschool interventions have found similar impacts on later school achievement compared with a no-preschool control group (e.g., Abecedarian study: Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Campbell, Raey, Pungello, Sparling, & Miller-Johnson, 2002; Perry preschool project: Belfield, Nores, Barnett, & Schweinhart, 2006).\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  7  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  8  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  9  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  10  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  11  :   3.  What is the impact of the CPC program on Third Grade literacy  as defined  by performance  on the CPS 3r  grade assessment?\n",
            "Top  12  :   Calculating effect  size for Special Education  utilization  To calculate the impact on Special Education utilization, the Evaluator will calculate the Average  Effect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the  Lenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the  data available, increasing sample sizes and precision of estimates.\n",
            "Top  13  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  14  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  15  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  16  :   Parents accept or decline placement.  Schools notify  parents of registration  dates and times.  Schools indicate parents'  acceptance  or decline of placement in Program  Management and move registered  children into the classroom  Homerooms for  IMPACT.  Teachers complete the registration  packet with families  for all new  students.  Clerks enter identifying  additional  information  into the IMPACT system.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  18  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  19  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  20  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What were the results of the intervention?\n",
            "Top  1  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  2  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  3  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  4  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  5  :   Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.\n",
            "Top  6  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  7  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  8  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  9  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  10  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  11  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  12  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  13  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  14  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  15  :   Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.\n",
            "Top  16  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  17  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  18  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  19  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  20  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "\n",
            "\n",
            "\n",
            "Query:  What was the impact of the intervention?\n",
            "Top  1  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  2  :   IMPACT ON THIRD-GRADE READING AND LITERACY The Chicago Longitudinal Study (CLS) followed children over time using administrative records to examine attendance, achievement, and graduation rates in CPC participants compared with children who did not attend CPC preschool. One study found a significant positive impact on third-grade reading achievement for pre-K to third-grade participants (.53 standard deviation) compared with participants who attended CPC only for pre-K and kindergarten (Reynolds, 1994). Smaller studies of high-quality preschool interventions have found similar impacts on later school achievement compared with a no-preschool control group (e.g., Abecedarian study: Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Campbell, Raey, Pungello, Sparling, & Miller-Johnson, 2002; Perry preschool project: Belfield, Nores, Barnett, & Schweinhart, 2006).\n",
            "Top  3  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  4  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  5  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  6  :   Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.\n",
            "Top  7  :   Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.\n",
            "Top  8  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  9  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  10  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  11  :   3.  What is the impact of the CPC program on Third Grade literacy  as defined  by performance  on the CPS 3r  grade assessment?\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  13  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  14  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  15  :   Calculating effect  size for Special Education  utilization  To calculate the impact on Special Education utilization, the Evaluator will calculate the Average  Effect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the  Lenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the  data available, increasing sample sizes and precision of estimates.\n",
            "Top  16  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  18  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  19  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  20  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Were the contracted outcomes achieved?\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   In the second year (2015-16) of the SIB-CPC project, two additional sites, identified by CPS and approved by the city of Chicago, were added to the six 2014-15 SIB- CPC sites. The project anticipates that four cohorts of children will be served across the eight sites, identified by the school year in which children begin preschool (cohort 1: 2014-15, cohort 2: 2015-16, cohort 3: 2016-17, cohort 4: 2017-18) (see Appendix B for grade levels of children in the four cohorts across years.) Evaluation Design SIB and PFS initiatives typically involve an independent evaluator to help determine whether the outcomes have been achieved. Because government only pays when outcomes are achieved rather than for activities, the focus of the evaluation is on measuring the outcomes of the individuals participating in the initiative.\n",
            "Top  3  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  4  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  5  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  6  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  7  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  8  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  9  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  10  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  11  :   SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children’s performance on the measure could be used to identify whether they were performing at or above 3 SRI’s involvement in the evaluation is currently scheduled to end in Fall 2020.\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  13  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  14  :   Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.\n",
            "Top  15  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  16  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  17  :   children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics:   Half of the children were male (51%).\n",
            "Top  18  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  19  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  20  :   In reporting the extent to which the CPC program has been successful at preparing children for kindergarten, comparisons may be instructive with respect to the degree our research findings agree with what we would expect from one year of preschool.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results outcomes achieved impact\n",
            "Top  1  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  2  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  3  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  4  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  5  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  6  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  7  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  8  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  9  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  10  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  11  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  12  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  13  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  14  :   SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children’s performance on the measure could be used to identify whether they were performing at or above 3 SRI’s involvement in the evaluation is currently scheduled to end in Fall 2020.\n",
            "Top  15  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  16  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  17  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  18  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  19  :   Calculating effect  size for Special Education  utilization  To calculate the impact on Special Education utilization, the Evaluator will calculate the Average  Effect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the  Lenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the  data available, increasing sample sizes and precision of estimates.\n",
            "Top  20  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  3  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  4  :   Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on  covariates, the Evaluator will present the match results, describing any changes that were made  to the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator  and Approved  by the Lender Committee. The Evaluator should endeavor to use a similar  matching protocol from year to year.\n",
            "Top  5  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  6  :   The TS GOLD™ Spring 2015 data were missing for three11 of the 328 children, resulting in a final analytic sample for this outcome of 325 children (99% of the 328 children), which we used to calculate kindergarten readiness.\n",
            "Top  7  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  8  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  9  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  10  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  11  :   This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  13  :   The remaining students from  the Comparison pool who were matched will become the No Pre-K  Comparison group for the remainder of the study. Comparison group students will receive a  frequency  weight equal to the number of times they were matched. Note that as a result, the  Comparison group should contain approximately two times as many unique individuals as the  Treatment group.\n",
            "Top  14  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  15  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 and in later grades.\n",
            "Top  17  :   children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics:   Half of the children were male (51%).\n",
            "Top  18  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  19  :   Data Security 6 Crime stats and health  indicators subject  to availability  of data. It may be possible to pull data from a Chapin  Hall  neighborhood  analysis. These covariates may be omitted  if it proves too difficult  or costly to obtain them.\n",
            "Top  20  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes achieved\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  3  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  4  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  5  :   Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.\n",
            "Top  6  :   SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children’s performance on the measure could be used to identify whether they were performing at or above 3 SRI’s involvement in the evaluation is currently scheduled to end in Fall 2020.\n",
            "Top  7  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  8  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  9  :   The TS Gold  instrument  is utilized  nationally  in Head  Start programs  and  some  publicly-funded  preschool  programs. The primary  outcome  metric  for  Kindergarten  Readiness  will  be the  share  of children  which  are performing  at or  above  the national  trends  across  at least  five  out  of the  following  six domains: Literacy,  Language,  Math,  Cognitive  Development,  Socio-Emotional,  Physical  health.\n",
            "Top  10  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  11  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  12  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  13  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  14  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  15  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  16  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  17  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  18  :   5.  How successful  is the CPC model at engaging parents? What strategies are the most  effective  at encouraging parental engagement? What strategies appear to have the  greatest impact on children's outcomes?\n",
            "Top  19  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  20  :   children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics:   Half of the children were male (51%).\n",
            "\n",
            "\n",
            "\n",
            "Query:  impact\n",
            "Top  1  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  2  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  3  :   Parents accept or decline placement.  Schools notify  parents of registration  dates and times.  Schools indicate parents'  acceptance  or decline of placement in Program  Management and move registered  children into the classroom  Homerooms for  IMPACT.  Teachers complete the registration  packet with families  for all new  students.  Clerks enter identifying  additional  information  into the IMPACT system.\n",
            "Top  4  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  5  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  6  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  7  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  8  :   June through  September  2014 Registration September 2014 Enrollment September 2014 onward Rolling enrollment the CPS SIM IMPACT system.\n",
            "Top  9  :   Calculating effect  size for Special Education  utilization  To calculate the impact on Special Education utilization, the Evaluator will calculate the Average  Effect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the  Lenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the  data available, increasing sample sizes and precision of estimates.\n",
            "Top  10  :   Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children’s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).\n",
            "Top  11  :   The same impact is not expected for children with severe disabilities (identified  in preschool or  at a later date), and it is also not expected that a preschool  intervention would meet the needs of  the child without the benefit  special education  services, nor would that be appropriate or within  the parameters of a child's right to a free  and appropriate education. To ensure that children  have  access to the supports they need based on a clinical evaluation, if a child at any point during the  course of the study is diagnosed with a severe disability, he or she will be removed  from  the  study group during the year that the disability is added to the child's IEP onward. The  preliminary  list of severe disabilities, with input from the Independent Evaluator, may be as  follows:  •  •  deaf-blindness  •  deafness  •  hearing impairment  •  orthopedic  impairment  •  other health  impairment autism traumatic brain  injury •  •  visual  impairment  •  multiply  disabled\"  •  • intellectual  disability  students placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the  Project Coordinator, and the Approval of the Lender Committee.\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  13  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  14  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  15  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  16  :   3.  What is the impact of the CPC program on Third Grade literacy  as defined  by performance  on the CPS 3r  grade assessment?\n",
            "Top  17  :   Source: Chicago Child-Parent Center Social Impact Bond Evaluation Plan, dated December 2, 2014,  included in Appendix A, pp. 9-11.\n",
            "Top  18  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  19  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  20  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  4  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  5  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  6  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  7  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  10  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  11  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  14  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  15  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  16  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  17  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  18  :   •  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.\n",
            "Top  19  :   Children are enrolled upon attendance  on the first  day of school.\n",
            "Top  20  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  2  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  3  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  4  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  5  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  6  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  7  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  8  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  9  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  12  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  13  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  14  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  15  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  16  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  17  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  18  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  19  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  20  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  2  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  3  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  4  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  7  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  8  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  9  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  10  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  11  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  12  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  13  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  17  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  18  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  19  :   This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children’s school readiness and school achievement.\n",
            "Top  20  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  5  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  6  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  7  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  10  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  11  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  12  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  15  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  16  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  17  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  18  :   Children are enrolled upon attendance  on the first  day of school.\n",
            "Top  19  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  20  :   To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine  what share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the  student IDs of the original group. CPS will match these IDs to their current enrollment  database  to determine which students were enrolled in a CPS school at any point in that school year. CPS  will then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS  school that year. The Mobility Factor will be defined  as: /—# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15  cohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the  evaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that  year would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student  IDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school.  The cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  2  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  3  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  4  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  7  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  8  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  9  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  10  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  11  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  14  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  15  :   We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD™ (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?\n",
            "Top  16  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  17  :   Defining  Performance  Improvement  Questions  The details of these questions will be developed  in conjunction  with CPS and other partners over  the 2014/15 school year. These analyses will be specified  in full  prior to the start of any data  collection or analyses. These analyses will not affect  the methodology  or results of the primary  impact outcomes, and will only be pursued subject to additional philanthropic or other  funding.\n",
            "Top  18  :   4.  How successful  is the CPC program at improving social-emotional  learning outcomes  (defined  by the social-emotional  components of the TS Gold instrument) compared to  children enrolled in other CPS pre-K programs?\n",
            "Top  19  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  20  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  3  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  4  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  5  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  6  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  7  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  8  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  9  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  10  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  11  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  12  :   STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol  b.  Checking for covariate balance between  groups  c.  Matching methodology  remedies  d.  Calculating mobility  e.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness  g.  Calculating effect  size for Third Grade literacy  h.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of  the Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact  outcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy.  This document also describes additional research questions that the Evaluator will seek to  explore in collaboration  with CPS to help the CPCs improve their performance.  This  methodology  will be developed  in conjunction  with CPS and other experts in the early  education  field.\n",
            "Top  13  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  16  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  17  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  18  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  19  :   CPC Program Model CPC Model Description CPC programs seek to promote school readiness, parent involvement, and early learning that, in turn, will translate into long-term benefits with regards to academic achievement, higher graduation rates, and career success. The CPC model is unique Child-Parent Center Evaluation: Report for 2014-15                  April 2016 in that it is designed to (1) provide full- or part-time high-quality preschool experiences for three- and four-year old children, and (2) combine those educational experiences with family support services and parent engagement activities. The services for children and families offered by CPC sites are intended to be delivered in a coordinated and synergistic way across the preschool to third grade continuum.\n",
            "Top  20  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  2  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  3  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  4  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  5  :   SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children’s performance on the measure could be used to identify whether they were performing at or above 3 SRI’s involvement in the evaluation is currently scheduled to end in Fall 2020.\n",
            "Top  6  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  7  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  8  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  9  :   Defining  Primary  Impact  Outcomes Special Education  Utilization  The primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of  whether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year.  This  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As  described  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student  will  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected  annually  ever year  Kindergarten  through  6 th  grade.\n",
            "Top  10  :   INTERVENTION  AND  OUTCOMES  a.  Defining  the intervention  b.  Defining  compliance with the treatment  c.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric  ii.  Kindergarten Readiness outcome metric  iii.  Third Grade Literacy outcome outcomes  d.  Defining  Performance  Improvement  Questions i.  Attendance  ii.  Dosage  iii.  Social/Emotional  learning  iv.  Transition to Kindergarten  v.  Subgroup analyses V.\n",
            "Top  11  :   SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended                                                               1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant  and two had been providing CPC services since 2013 when the original sites from the 1970s were  merged with the current site.   2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218  three- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not  open for long enough to provide adequate dosage of CPC preschool.\n",
            "Top  12  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  13  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  14  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  15  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  16  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  17  :   The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP?  2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?\n",
            "Top  18  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  19  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  20  :   For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  7  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  8  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  9  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  10  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  11  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  12  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "Top  13  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  14  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  15  :     Provide a resource room dedicated to parent and family activities through Kindergarten when possible.\n",
            "Top  16  :   Kindergarten  Readiness  CPS  uses the  Teaching  Strategies  Gold  (TS  Gold)  instrument  in all their  Pre-K  classrooms  to  track the development  of children.  Based  on  teacher  observations,  TS  Gold  measures  the  progress  of children  in domains  such  as socio-emotional,  physical,  language,  literacy,  and  cognitive  development.\n",
            "Top  17  :   •  Provide a resource room dedicated to parent and family  activities through  Kindergarten •  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.\n",
            "Top  18  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  19  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.\n",
            "Top  20  :   Individuals from  either the Treatment group or Comparison pool who are not matched will be  dropped.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  4  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  5  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  6  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  7  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  8  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  9  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  10  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  11  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  12  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  13  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  14  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  15  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  16  :     Encourage parents to sign a CPC school-home agreement at the start of the school year outlining a plan for fostering learning at home and participating in CPC  activities.\n",
            "Top  17  :   Appendices Appendix A:  Chicago Child-Parent Center Social Impact Bond Evaluation Plan Appendix B:  Timing of Cohorts Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Appendix A: Chicago Child-Parent Center Social  Impact Bond Evaluation Plan Social Impact Bond Report November 2015 A-1 Chicago Child-Parent  Center  Social Impact  Bond  Evaluation  Plan December 2, 2014 Table of Contents I.\n",
            "Top  18  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  19  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 and in later grades.\n",
            "Top  20  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  2  :   Year  1  contingency  for CPC Treatment  Group  Due to the timing of the contracting, some of the new classrooms to be added in the 2014/15  school year will not be ready to serve children until the school year has already begun. Five of  the Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody,  Peck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they  have an established  leadership team, trained and experienced teachers, and fully  outfitted  classrooms.\n",
            "Top  3  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  4  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  5  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  6  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  7  :   The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child’s education through a parent resource teacher (PRT) provided at the child’s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI’s evaluation has been designed to measure the impact of the preschool components on children’s short- and long-term outcomes.\n",
            "Top  8  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  9  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  10  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  11  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  12  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  13  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  14  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  15  :   A data sharing agreement between CPS and the Independent Evaluator will define  the  parameters for sharing data required under this agreement.\n",
            "Top  16  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  17  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  18  :   INTERVENTION  AND  OUTCOMES Defining  the Intervention  The CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC  SIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences •  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions  of  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if  available, will be limited to 20 children per session.\n",
            "Top  19  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  20  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  2  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  3  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  4  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  5  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  6  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  7  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  8  :   For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the  incremental annual increase in the Mobility Factor over the last three years.  Every year, the  Evaluator will impute a new Mobility Factor based on the average imputed marginal mobility  rate. See Appendix B for a full  example using hypothetical  data.\n",
            "Top  9  :   (1)  What is the rate of kindergarten readiness in children participating in the SIB- CPC sites as defined by performance on the Teaching Strategies (TS) Gold instrument (completed by teachers in the spring of preschool before a child enters kindergarten)?\n",
            "Top  10  :   School data  Data on school level characteristics will be provided by CPS, including: •  CPS School ID  •  Total student body population  •  % Free/RP lunch  •  % Black  •  % Hispanic  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating (Levels  1, 2, or 3) from the 2013/14 school year7 These data, except for attendance and the school rating, will be updated annually. Attendance  and rating data from  SY2013/14 (or the closest assessment prior to SY2013/14) will remain fixed  to reflect  the fact that the presence of a CPC may improve attendance and the school rating over  time, which could affect  the matching algorithm for later cohorts. The Evaluator may adjust  this  protocol  if extraneous events such as school closures, new leadership, or expansive new  programs are added at individual  schools or system wide that could contribute to imbalanced  matches.\n",
            "Top  11  :   To create the matched No Pre-K Comparison group, the Evaluator will append the Treatment  Group dataset and the No Pre-K Comparison pool dataset, creating an indicator to identify  which  children are members of the Treatment group. The Evaluator will then run a probit model using  the treatment indicator as the dependent variable and the following  variables as  independent  variables: •  Race binary  indicators  •  Ethnicity binary  indicators •  Gender (\"Male\" binary  indicator)  •  Parental education (subject to availability)  •  Language spoken at home binaries  •  Neighborhood  % poverty  •  Neighborhood  % single mothers  •  Neighborhood  % by race  •  Neighborhood  % by ethnicity  •  Neighborhood  % employed  •  Neighborhood  crime rates (subject to availability)  •  Neighborhood health indicators (subject to availability)  •  Total student population of school currently  attending  •  % Free/RP lunch at school currently  attending  •  Racial composition of school currently  attending  •  Ethnicity composition of school currently  attending  •  School-wide attendance rate from  the 2013/14 school year  •  School Rating binaries from  the 2013/14 school year Using the results of this model, the Evaluator will predict a propensity score based on a student's  observed characteristics. This score effectively  represents the likelihood that a child, given his  individual, neighborhood,  and school level characteristics, would be in the Treatment group.\n",
            "Top  12  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  13  :   7 All these data are publicly available online at http://www.cps.edu/schools/find  a  school/pages/findaschool.aspx.  School rating is based on the CPS Performance  Policy which  is used to rate CPS schools. A Level  1  rating is  \"excellent\",  a Level 2 rating is \"good\" and a Level 3 rating  is \"low\".\n",
            "Top  14  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  15  :   (2)  What is the rate of third-grade literacy as defined by performance in meeting or exceeding grade-level performance on the state or district-administered third-grade assessment in reading?\n",
            "Top  16  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "Top  17  :   To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to  the average of a binary indicator of Special Education utilization among the No CPS Pre-K  Comparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of  Special Education utilization  among the Treatment group for cohort / in year t. At the discretion  of the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the  Approval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help  account for any differences  in covariates between the Treatment group and the Comparison  group.\n",
            "Top  18  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  19  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  20  :   DATA  COLLECTION Student data  Student data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3,  CPS will strip sensitive individual identifiers  and replace them with an anonymous student ID.  The key variables CPS will provide are: •  Student ID  •  CPS School ID of school currently enrolled in  •  Date of Birth (or birth month & year)  •  Days attended to date  IEP status  •  •  IEP diagnoses  •  Reported  race  •  Reported  ethnicity  •  Free/reduced price lunch  eligibility  •  ZIP code of residence  •  Fall and Spring TS Gold scores (if applicable)  •  Any available variables on parental  education  •  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between  CPS and the Evaluator.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  3  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  6  :   Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using  outcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing  other research questions not related to payment triggers.\n",
            "Top  7  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  8  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  9  :   Investigating Highly Unexpected  Outcomes  The results of this evaluation will govern the flow of millions of dollars of payments. While it is  the full  intention of all parties to accept the results of the evaluation, in the event that a highly  irregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm  that they are due to the impact of the program, and not a flaw  in the analysis or evaluation  design. The Evaluator will have complete discretion to decide if and when a validation of the  findings may be necessary, but the following  events will serve as guiding principles that could  suggest that a validation may be warranted: •  The difference  in Special  Education Utilization rates between the Treatment group and  No Pre-K comparison group is negative or not statistically  different  from  zero (p-value  <.05) for any cohort in any year after  Kindergarten •  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year  after  Kindergarten •  An irregular pattern from  one year to the next in Special  Education utilization  for a given  group, defined  as utilization  shrinking by more than  two percentage points for a given  group, or increasing by more than seven percentage points •  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.\n",
            "Top  10  :   To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program,  for Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in  one of these five established  CPC SIB sites, in a classroom that was already  established as of  September 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the  new classrooms once all contractual  issues have been resolved, but the children who are enrolled  in those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be  included in the outcome calculations for the purposes of determining payments. This will allow  CPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes  (or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year  are different  from those of their peers who enrolled at the start of the year. The outcomes of these  late-enrollees can be used as a unique sub-group, but will not factor  into any calculations that  determine payment amounts.\n",
            "Top  11  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  12  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  13  :   It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing  CPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments  this number will be scaled to reflect the actual number of slots funded  by the Lenders as part of  this initiative.\n",
            "Top  14  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  15  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  16  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  17  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  18  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  19  :   In the second year (2015-16) of the SIB-CPC project, two additional sites, identified by CPS and approved by the city of Chicago, were added to the six 2014-15 SIB- CPC sites. The project anticipates that four cohorts of children will be served across the eight sites, identified by the school year in which children begin preschool (cohort 1: 2014-15, cohort 2: 2015-16, cohort 3: 2016-17, cohort 4: 2017-18) (see Appendix B for grade levels of children in the four cohorts across years.) Evaluation Design SIB and PFS initiatives typically involve an independent evaluator to help determine whether the outcomes have been achieved. Because government only pays when outcomes are achieved rather than for activities, the focus of the evaluation is on measuring the outcomes of the individuals participating in the initiative.\n",
            "Top  20  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  2  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  3  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  4  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  5  :   Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.\n",
            "Top  6  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  7  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  8  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  9  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  10  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  11  :   Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.\n",
            "Top  12  :   Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for:  IFF Pay For Success I, LLC  333 S. Wabash Avenue, Suite 2800  Chicago, Illinois 60604  Attention: Matthew J. Roth, Chief Operating Officer  E-mail: mroth@iff.org Copy to:  DLA Piper LLP (US)  203 N. LaSalle Street, Suite 1900  Chicago, Illinois 60601  Attention: Richard F. Klawiter, Esq.  E-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International  Erika Gaylor  Traci Kutaka  Kate Ferguson   Cyndi Williamson  Xin Wei  Donna Spiker Revised June 2019 to correct an error on p. 9          Suggested citation:  Gaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.\n",
            "Top  13  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  14  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  15  :   In addition to these impact outcome questions, this evaluation will also seek to answer  qualitative research questions that will help improve the performance  of the program going  forward  unrelated to the Pay for  Success calculations. These research questions will be  developed more fully  in conjunction  with CPS and other experts in the early education field, and  will only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race,  prior pre-school attendance, English language learner status, and potentially  other  subgroups?\n",
            "Top  16  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  17  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences   Offer Pre-K classes that are limited to 34 children for half-day classrooms (two  sessions of 17 children each) and have a minimum of 2 teaching staff. Full day  classrooms, if available, will be limited to 20 children per session.\n",
            "Top  18  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "Top  19  :   See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.\n",
            "Top  20  :   (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   The base cohort sizes are based on the number of seats actually  funded  by investors. It is  anticipated that the base cohort sizes will be as follows10: Cohort  Year Base Cohort  Size 2014/15 2015/16 2016/17 2017/18 Year Savings  Rate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual  Savings Rate to determine the Special Education Payments owed for a given cohort in a given  year. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of  $9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.\n",
            "Top  2  :   Calculating mobility  factor  The theory behind the financing  component of the SIB project  is that providing the  upfront  intervention of high quality Pre-K can produce savings to CPS downstream through reduced  Special Education  utilization among the students served. For CPS to realize these savings,  however, those students must remain in the CPS school district. If a student leaves the district,  CPS would realize no savings from  the fact that the intervention may have helped that that  student catch up to his peers and prevented him from  acquiring an IEP.\n",
            "Top  3  :   In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.\n",
            "Top  4  :   As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the  share of the original cohort that is still enrolled in a CPS school in a given year. This will be used  to adjust  the payment amounts to better reflect  savings realized by CPS.\n",
            "Top  5  :   Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3  Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by   Exclusion Criteria ........................................................................................................................ 10  Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13  Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children’s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.\n",
            "Top  6  :   2015  2016  2017  2018  2019  2020  2021  2022  2023  2024  2025  2026  2027  2028  2029  2030 $9,100 $9,191  $9,283  $9,376 $9,469  $9,564 $9,660  $9,756  $9,854 $9,953  $10,052  $10,153  $10,254  $10,357  $10,460  $10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the  Total Special Education Payment owed by CPS for that year. These calculations will be reported  to the Project Coordinator for the purposes of triggering payments to the Project Coordinator to  be used to repay the lenders.\n",
            "Top  7  :   The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the “SIB-CPC project”. The project anticipates serving four cohorts of preschool children across the eight sites over four school years— Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.\n",
            "Top  8  :   3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original  Enrollment Students still enrolled at a CPS school Cumulative  Mobility Marginal  Mobility School  Year 2014  2015  2016  2017  2018  2019  2020  2021 School  Year  2022  2023 2024  2025  2026  2027 PK K 1st 2nd 3rd 4th 5th 6th Grade  7th  8th 9th  10th  11th  12th 500  500  500  500  500  500  500  500 500  500  500  500 460  440  415  405  390  378  365  353 316  304  291  279 .08  .12  .17  .19  .22  .244  .27  .294 Imputed  Cumulative  Mobility  .319  .343 .368  .393  .417  .442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed  Marginal  Mobility Imputed average marginal mobility for future  calculations: Original  Enrollment  500  500 Imputed Students still enrolled at a  CPS school  341  328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.\n",
            "Top  9  :   To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the  Treatment group students deemed to be reading \"at or above grade level\". The Evaluator will  then multiply this number by the base cohort size, multiplied by cumulative mobility  from  the  Third Grade year of a given cohort. This will determine the Total Number of Third Grade  Children Reading at Grade Level for a given cohort. The Evaluator will then multiply this  number by the payment rate of $750 to determine the total Third Grade Literacy payments owed  by the City for that cohort.\n",
            "Top  10  :   Calculating payments for  Special Education utilization  To determine the size of Special Education payments owed in a given year for a given treatment  group cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person  for such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for  that year. This will determine the Total Number of Special Education  Slots Avoided  for a given  cohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort  size for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.\n",
            "Top  11  :   Special Education outcomes will be calculated annually every year Kindergarten through 61  grade. Outcomes will be calculated  separately for each cohort. Based  on conversations with  special education experts and reviewing existing CPS data, we believe that the vast majority  of  children who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th  grade effect  size has been calculated, we will average the effect  size over the last three years (4th,  5  and 6  grades) and lock in that average rate for the purposes of calculating payments in  grades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort.  The Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator  determines that this methodology produces skewed results. Any modifications  must be approved  by CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.\n",
            "Top  12  :   Every child who scores at or above the national norm on at least five of the six subcategories in  spring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate  the Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment  group students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the  base cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given  cohort. This will determine the Total Number of Kindergarten Ready  Children for a given  cohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine  the total Kindergarten  Readiness payments owed by the City for that cohort.\n",
            "Top  13  :   To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to  the average of a binary indicator of Special Education utilization among the No CPS Pre-K  Comparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of  Special Education utilization  among the Treatment group for cohort / in year t. At the discretion  of the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the  Approval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help  account for any differences  in covariates between the Treatment group and the Comparison  group.\n",
            "Top  14  :   For the purposes of calculating payments owed as part of the SIB transaction, impacts will  estimated using the total population  of eligible students at SIB CPC sites, and then scaled to  reflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors  annually to reflect  observed mobility trends.\n",
            "Top  15  :   This document will  serve as a template for how the evaluation will be conducted. The Evaluator  will draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with  Approval of the Lender Committee (such term being defined  herein as such term is defined  in the  Loan Documents of the Lenders)  using this document as a framework.  No changes to payment  terms or payment terminology  will be made.\n",
            "Top  16  :   Payments for  Special Education will be made every year K -  12th for each Treatment cohort.\n",
            "Top  17  :   APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'■  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and  enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED &  Mobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.\n",
            "Top  18  :   If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If  the Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation  plan to mitigate the inconsistency  in future  years. However, the results will not be recalculated  for that year or any other past year. Changes to the plan must be approved by CPS, the City, and  the Project Coordinator, and Approved by the Lender Committee.\n",
            "Top  19  :   Payments based on Special Education utilization for the SIB project will be calculated using the  difference  in outcomes between the Treatment group and the No Pre-K comparison group.\n",
            "Top  20  :     Provide culturally responsive learning opportunities for families that provide flexibility for families’ needs and schedules.\n",
            "\n",
            "\n",
            "\n",
            "Begin experiment for key  #17725\n",
            "Query:  What is the study design?\n",
            "Top  1  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  2  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  5  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  6  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  7  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  8  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  11  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  12  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  15  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  16  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  17  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  18  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  19  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  20  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the research method?\n",
            "Top  1  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  2  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  3  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  9  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  10  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  11  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  12  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  13  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  14  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  15  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  16  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  17  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  18  :   Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how  many years students in the treatment group were potentially exposed to EG programming.  Gray cells indicate when the cohort was assessed by IDinsight.\n",
            "Top  19  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  20  :   28 If available, we use the most recent assessment of these children for the calculation of learning gains.\n",
            "\n",
            "\n",
            "\n",
            "Query:  How was data collected and analysed?\n",
            "Top  1  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  2  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  5  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  6  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  7  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  8  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  9  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  10  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  11  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  12  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  13  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  14  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  15  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  16  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  17  :   •  26  children  present  at  Baseline  subsequently  dropped  out  of  school  and  were  later  enrolled by EG. We shifted these students from Type I-III to the Newly Enrolled Girls  category.  Since  100%  of  Newly  Enrolled  Girls  were  sampled,  their sampling  weight  was changed to 1. The remaining Type I-III students in the cohorts from which these  students were removed kept their original sampling weights.\n",
            "Top  18  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  19  :   Figure 6: Enrollment Verification Process To validate enrollment each year, IDinsight surveyors visited each school in which a girl was  reported enrolled and presented the headmaster with a form that included the girl’s name,  caste,  age,  and  father’s  name.  Headmasters  were  requested  to  verify  this  information  by  signing the IDinsight form as well as by showing surveyors the register.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design method methodology data collection research design\n",
            "Top  1  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  2  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  5  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  6  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  7  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  8  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  9  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  10  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  11  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  12  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  13  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  14  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  15  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  16  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  17  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  18  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  19  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  20  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "\n",
            "\n",
            "\n",
            "Query:  study design\n",
            "Top  1  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  2  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  3  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  4  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  5  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  6  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  7  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  8  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  9  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  10  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  11  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  12  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  13  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  14  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  17  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  18  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  19  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  method\n",
            "Top  1  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  2  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  3  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  6  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  7  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  8  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  9  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  10  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  11  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  12  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  13  :   Figure 6: Enrollment Verification Process To validate enrollment each year, IDinsight surveyors visited each school in which a girl was  reported enrolled and presented the headmaster with a form that included the girl’s name,  caste,  age,  and  father’s  name.  Headmasters  were  requested  to  verify  this  information  by  signing the IDinsight form as well as by showing surveyors the register.\n",
            "Top  14  :   Table 2: Learning Levels as Measured by ASER Level  1  2  3  4  5  6 Hindi  Beginner  Letters  Words  Paragraph  Story 1  Story Plus Math  Beginner  Numbers 1-10  Numbers 11-99  Subtraction  Division  — English  Beginner  Capital letters  Lowercase letters  Words  Sentences  — Calculating Learning Gains  The change in learning levels for each student is calculated by subtracting his or her total score  at Baseline from his or her total score at Endline,9 with the following caveats: •  Baseline scores for students in treatment and control schools who were not present at  Baseline are imputed to be the lowest score possible (a score of 3) and any additional  learning levels achieved by those students at Endline are assumed to be gains.\n",
            "Top  15  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  16  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  17  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  18  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  19  :   28 If available, we use the most recent assessment of these children for the calculation of learning gains.\n",
            "Top  20  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  methodology\n",
            "Top  1  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  4  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  8  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  9  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  10  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  11  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  12  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  13  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  14  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  15  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  16  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  17  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  18  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  19  :   Table 2: Learning Levels as Measured by ASER Level  1  2  3  4  5  6 Hindi  Beginner  Letters  Words  Paragraph  Story 1  Story Plus Math  Beginner  Numbers 1-10  Numbers 11-99  Subtraction  Division  — English  Beginner  Capital letters  Lowercase letters  Words  Sentences  — Calculating Learning Gains  The change in learning levels for each student is calculated by subtracting his or her total score  at Baseline from his or her total score at Endline,9 with the following caveats: •  Baseline scores for students in treatment and control schools who were not present at  Baseline are imputed to be the lowest score possible (a score of 3) and any additional  learning levels achieved by those students at Endline are assumed to be gains.\n",
            "Top  20  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "\n",
            "\n",
            "\n",
            "Query:  data collection\n",
            "Top  1  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  2  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  3  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  4  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  8  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  9  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  10  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  11  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  12  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  13  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  14  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  17  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  18  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  19  :   •  During the third round of student assessments, we identified 64 students who were  listed  twice  on  our  student  lists.  While  none  of  them  have  been  assessed  twice,  removing these duplicates affects sampling weights.\n",
            "Top  20  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  research design\n",
            "Top  1  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  2  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  5  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  9  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  10  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  11  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  12  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  15  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  17  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  18  :   Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students  at the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those  cohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during  Grade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average  treatment effect for that cohort since the yearly average treatment effects do not account for students who have  dropped out or have been retained.\n",
            "Top  19  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  20  :   Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how  many years students in the treatment group were potentially exposed to EG programming.  Gray cells indicate when the cohort was assessed by IDinsight.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the target population?\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of  eligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible  girls.\n",
            "Top  3  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  4  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  5  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  6  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  7  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  8  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  13  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  14  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  15  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  16  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  17  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  18  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  19  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who are the intended beneficiaries of the service?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  4  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  5  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  6  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  7  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  8  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  9  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  10  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  11  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  12  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  13  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  14  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  15  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  16  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  17  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  18  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  19  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  20  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who does the service try to help?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix Appendix 1: Description of Educate Girls’ Intervention Enrollment  Educate Girls delivers a comprehensive community intervention to enroll girls into school.  This intervention includes identification of out-of-school girls through door-to-door surveys,  explanation of the value of schooling to their parents and to the community, and multi- channel engagement with households with unenrolled girls. Educate Girls also uses multiple  interventions to improve school attendance and prevent drop-outs, such as frequent parent  counselling sessions and working with School Management Committees to improve school  infrastructure. It also identifies girls who have dropped out and works with the community  to re-enroll them into school.     Learning  Educate Girls trained volunteers to deliver a child-centric curriculum one to five times a week  to boys and girls in Grades 3-5. Volunteers were often drawn from the villages in which they  worked.  They  were  incentivized  with  a  small  number  of  skill  and  career  development  opportunities,  such  as  free  English  classes  and  the  possibility  of  being  hired  by  EG  in  the  future.    In Year 3, EG rolled out a new curriculum called “Gyan Ka Pitara” (“Knowledge Box”). As  part  of  this  new  curriculum,  EG  increased  the  number  of  teaching  sessions  per  day  and  conducted  home  visits  to  reach  students  who  were  frequently  absent  from  school  or  who  needed  remedial  tutoring.  In  addition  to  the  thrice  yearly  rounds  of  student  assessments  conducted  previously  in  Years  1  and  2,  EG  conducted  three  additional  rounds  of  ASER  assessments in Year 3. These additional assessments led EG to identify areas of improvement,  which  informed  adjustments  to  the  clustering  of  schools  for  program  implementation,  the  training  of  volunteers,  and  the  content  of  remedial  classes.  School  teachers  were  also  more  involved in programming in Year 3 through school meetings and block review meetings.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  5  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  6  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  7  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  8  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  9  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  10  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  13  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  14  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  15  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  16  :   A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of  students  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1  through 5. In each subsequent Endline, we assessed students who were then in grades 3, 4,  and 5 (the target grades for Educate Girls’ programming). Since a student’s grade changes year  to year, student cohort labels can be ambiguous; for instance, “Grade 3” could refer to three  different cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year  2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student  cohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the  “Y1” suffix to grade labels to remind the reader of this convention. For instance, “2Y1” refers  to students in grade 2 during the first year of the evaluation, who had progressed to grade 3  in Year 2 and grade 4 in Year 3.\n",
            "Top  17  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  18  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  19  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  20  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Who was eligible for inclusion in the intervention?\n",
            "Top  1  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  2  :   Appendix Appendix 1: Description of Educate Girls’ Intervention Enrollment  Educate Girls delivers a comprehensive community intervention to enroll girls into school.  This intervention includes identification of out-of-school girls through door-to-door surveys,  explanation of the value of schooling to their parents and to the community, and multi- channel engagement with households with unenrolled girls. Educate Girls also uses multiple  interventions to improve school attendance and prevent drop-outs, such as frequent parent  counselling sessions and working with School Management Committees to improve school  infrastructure. It also identifies girls who have dropped out and works with the community  to re-enroll them into school.     Learning  Educate Girls trained volunteers to deliver a child-centric curriculum one to five times a week  to boys and girls in Grades 3-5. Volunteers were often drawn from the villages in which they  worked.  They  were  incentivized  with  a  small  number  of  skill  and  career  development  opportunities,  such  as  free  English  classes  and  the  possibility  of  being  hired  by  EG  in  the  future.    In Year 3, EG rolled out a new curriculum called “Gyan Ka Pitara” (“Knowledge Box”). As  part  of  this  new  curriculum,  EG  increased  the  number  of  teaching  sessions  per  day  and  conducted  home  visits  to  reach  students  who  were  frequently  absent  from  school  or  who  needed  remedial  tutoring.  In  addition  to  the  thrice  yearly  rounds  of  student  assessments  conducted  previously  in  Years  1  and  2,  EG  conducted  three  additional  rounds  of  ASER  assessments in Year 3. These additional assessments led EG to identify areas of improvement,  which  informed  adjustments  to  the  clustering  of  schools  for  program  implementation,  the  training  of  volunteers,  and  the  content  of  remedial  classes.  School  teachers  were  also  more  involved in programming in Year 3 through school meetings and block review meetings.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  5  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  6  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  7  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  8  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  9  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  10  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  11  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  12  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  13  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  14  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  15  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  16  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  17  :   19 Appendix 7 shows treatment effects separately for students present at Baseline and absent at Baseline.\n",
            "Top  18  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  19  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  20  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population beneficiaries service users participants eligible population eligibility criteria cohort clients\n",
            "Top  1  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  2  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  3  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  4  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  5  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  6  :   Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of  eligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible  girls.\n",
            "Top  7  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  8  :   Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how  many years students in the treatment group were potentially exposed to EG programming.  Gray cells indicate when the cohort was assessed by IDinsight.\n",
            "Top  9  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  10  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  11  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  12  :   A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of  students  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1  through 5. In each subsequent Endline, we assessed students who were then in grades 3, 4,  and 5 (the target grades for Educate Girls’ programming). Since a student’s grade changes year  to year, student cohort labels can be ambiguous; for instance, “Grade 3” could refer to three  different cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year  2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student  cohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the  “Y1” suffix to grade labels to remind the reader of this convention. For instance, “2Y1” refers  to students in grade 2 during the first year of the evaluation, who had progressed to grade 3  in Year 2 and grade 4 in Year 3.\n",
            "Top  13  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  14  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  15  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  16  :   Figure 3: Average Learning Levels by Cohort Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Lines omit students absent at baseline (since they do not have a baseline  score), though average treatment effects (ATEs) include all students. ATEs denote the difference in average learning  gains between students in program schools and students in control schools.\n",
            "Top  17  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  18  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  19  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  target population\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  3  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  4  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  7  :   Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of  eligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible  girls.\n",
            "Top  8  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  9  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  10  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  13  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  14  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  15  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  16  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  17  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  18  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  19  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  20  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "\n",
            "\n",
            "\n",
            "Query:  beneficiaries\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  6  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  7  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  8  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  9  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  10  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  11  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  12  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  13  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  14  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  15  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  16  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  17  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  18  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  19  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  20  :   As in previous years, average treatment effects were larger for students in Bijoliya block than  for students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs.  +1.04).\n",
            "\n",
            "\n",
            "\n",
            "Query:  service users\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  3  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  4  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  5  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  6  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  7  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  8  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  9  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  10  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  11  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  12  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  13  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  14  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  15  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  16  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  17  :   Appendix 18c: ASER Testing Tool for English in Year 3 Endline ENGLISH ASSESSMENT: LEVELS 0-4 ENGLISH ASSESSMENT: LEVELS 0-4 D U G i t x M          R a           y hen old What is your name?\n",
            "Top  18  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  19  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  20  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "\n",
            "\n",
            "\n",
            "Query:  participants\n",
            "Top  1  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  2  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  3  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  6  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  7  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  8  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  9  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  10  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  11  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  12  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  15  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  16  :   Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how  many years students in the treatment group were potentially exposed to EG programming.  Gray cells indicate when the cohort was assessed by IDinsight.\n",
            "Top  17  :   •  26  children  present  at  Baseline  subsequently  dropped  out  of  school  and  were  later  enrolled by EG. We shifted these students from Type I-III to the Newly Enrolled Girls  category.  Since  100%  of  Newly  Enrolled  Girls  were  sampled,  their sampling  weight  was changed to 1. The remaining Type I-III students in the cohorts from which these  students were removed kept their original sampling weights.\n",
            "Top  18  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  19  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  20  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligible population\n",
            "Top  1  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  2  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  5  :   Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of  eligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible  girls.\n",
            "Top  6  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  9  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  10  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  11  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  12  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  13  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  14  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  15  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  16  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  17  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  18  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  19  :   Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full  population of students assessed in the evaluation. The interpretation of student types slightly  deviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant  throughout the three Endline data collection exercises and analyses.\n",
            "Top  20  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "\n",
            "\n",
            "\n",
            "Query:  eligibility criteria\n",
            "Top  1  :   Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise  indicated. ** Students are considered assessed if at least one Endline score is available.\n",
            "Top  2  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  3  :   Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of  eligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible  girls.\n",
            "Top  4  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  5  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  6  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  7  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  8  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  9  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  10  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  15  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  16  :   Figure 6: Enrollment Verification Process To validate enrollment each year, IDinsight surveyors visited each school in which a girl was  reported enrolled and presented the headmaster with a form that included the girl’s name,  caste,  age,  and  father’s  name.  Headmasters  were  requested  to  verify  this  information  by  signing the IDinsight form as well as by showing surveyors the register.\n",
            "Top  17  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  18  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  19  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  20  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "\n",
            "\n",
            "\n",
            "Query:  cohort\n",
            "Top  1  :   A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of  students  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1  through 5. In each subsequent Endline, we assessed students who were then in grades 3, 4,  and 5 (the target grades for Educate Girls’ programming). Since a student’s grade changes year  to year, student cohort labels can be ambiguous; for instance, “Grade 3” could refer to three  different cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year  2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student  cohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the  “Y1” suffix to grade labels to remind the reader of this convention. For instance, “2Y1” refers  to students in grade 2 during the first year of the evaluation, who had progressed to grade 3  in Year 2 and grade 4 in Year 3.\n",
            "Top  2  :   Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how  many years students in the treatment group were potentially exposed to EG programming.  Gray cells indicate when the cohort was assessed by IDinsight.\n",
            "Top  3  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  4  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  5  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  6  :   Figure 3: Average Learning Levels by Cohort Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Lines omit students absent at baseline (since they do not have a baseline  score), though average treatment effects (ATEs) include all students. ATEs denote the difference in average learning  gains between students in program schools and students in control schools.\n",
            "Top  7  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  8  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  9  :   •  26  children  present  at  Baseline  subsequently  dropped  out  of  school  and  were  later  enrolled by EG. We shifted these students from Type I-III to the Newly Enrolled Girls  category.  Since  100%  of  Newly  Enrolled  Girls  were  sampled,  their sampling  weight  was changed to 1. The remaining Type I-III students in the cohorts from which these  students were removed kept their original sampling weights.\n",
            "Top  10  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  11  :   •  Students in grades 4 and 5 at Baseline were expected to progress to grades 6 and 7 by  Year 3. However, 32 students from Baseline grades 4 and 5 were still in grades 3-5 at  the time of the Year 3 Endline, and thus assessed this year. Likewise, two students from  Baseline grade 5 were still in grade 5 during the Year 2 Endline. We included these  assessments in the final calculation of learning gains, leading to changes in the learning  gains of students in grades 4Y1 and 5Y1 despite these cohorts generally not being part of  the Year 3 student assessments.\n",
            "Top  12  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  13  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  14  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  15  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  16  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  17  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  18  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  19  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  20  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "\n",
            "\n",
            "\n",
            "Query:  clients\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  5  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  6  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  7  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  8  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  9  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  10  :   As in previous years, average treatment effects were larger for students in Bijoliya block than  for students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs.  +1.04).\n",
            "Top  11  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  12  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  13  :   because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for  surveyors to reach.28 •  Children were presented with paper copies of the ASER assessment and their answers  were recorded on smartphones via the SurveyCTO electronic data collection interface  used  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school  infrastructure and staffing was collected from the headmaster or head teacher in each  school or by direct observation.\n",
            "Top  14  :   Appendix 18c: ASER Testing Tool for English in Year 3 Endline ENGLISH ASSESSMENT: LEVELS 0-4 ENGLISH ASSESSMENT: LEVELS 0-4 D U G i t x M          R a           y hen old What is your name?\n",
            "Top  15  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  16  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  17  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  18  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  19  :   Figure 6: Enrollment Verification Process To validate enrollment each year, IDinsight surveyors visited each school in which a girl was  reported enrolled and presented the headmaster with a form that included the girl’s name,  caste,  age,  and  father’s  name.  Headmasters  were  requested  to  verify  this  information  by  signing the IDinsight form as well as by showing surveyors the register.\n",
            "Top  20  :   S         Z          L n         h          c fox I have a sister.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  7  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  8  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  9  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  10  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  11  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "Top  12  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  15  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  16  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  17  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  18  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  19  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  20  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  3  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  4  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  5  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  8  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  11  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  13  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  14  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  16  :   Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students  at the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those  cohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during  Grade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average  treatment effect for that cohort since the yearly average treatment effects do not account for students who have  dropped out or have been retained.\n",
            "Top  17  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  18  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  19  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  2  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  3  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  4  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  5  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  6  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  7  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  8  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  9  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  10  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "Top  15  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  16  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  17  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  18  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  19  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  20  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  4  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  7  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  8  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  9  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  10  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  11  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  12  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  13  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  16  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  17  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  18  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  19  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  20  :   Table 2: Learning Levels as Measured by ASER Level  1  2  3  4  5  6 Hindi  Beginner  Letters  Words  Paragraph  Story 1  Story Plus Math  Beginner  Numbers 1-10  Numbers 11-99  Subtraction  Division  — English  Beginner  Capital letters  Lowercase letters  Words  Sentences  — Calculating Learning Gains  The change in learning levels for each student is calculated by subtracting his or her total score  at Baseline from his or her total score at Endline,9 with the following caveats: •  Baseline scores for students in treatment and control schools who were not present at  Baseline are imputed to be the lowest score possible (a score of 3) and any additional  learning levels achieved by those students at Endline are assumed to be gains.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  5  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  6  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  9  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  10  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  11  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  17  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  18  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  19  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  20  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  3  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  8  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  9  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  10  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  11  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  12  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  13  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  14  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  15  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  16  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  17  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  18  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  19  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  3  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  6  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  7  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  8  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  11  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  12  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  13  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  14  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  15  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  16  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  17  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  18  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  19  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  20  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  5  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  6  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  7  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  8  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  9  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  10  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  11  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  12  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  13  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students  at the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those  cohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during  Grade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average  treatment effect for that cohort since the yearly average treatment effects do not account for students who have  dropped out or have been retained.\n",
            "Top  17  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  18  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  19  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  20  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  6  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  7  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  8  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  9  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  10  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  11  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  12  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  13  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  17  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  18  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  19  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  20  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  8  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  9  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  10  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  11  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  12  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  13  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  14  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  15  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  16  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  17  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  18  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  19  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  4  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  5  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  6  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  7  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  8  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  9  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  10  :   A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of  students  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1  through 5. In each subsequent Endline, we assessed students who were then in grades 3, 4,  and 5 (the target grades for Educate Girls’ programming). Since a student’s grade changes year  to year, student cohort labels can be ambiguous; for instance, “Grade 3” could refer to three  different cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year  2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student  cohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the  “Y1” suffix to grade labels to remind the reader of this convention. For instance, “2Y1” refers  to students in grade 2 during the first year of the evaluation, who had progressed to grade 3  in Year 2 and grade 4 in Year 3.\n",
            "Top  11  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  12  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  13  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  14  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  15  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  16  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  17  :   Figure 3: Average Learning Levels by Cohort Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Lines omit students absent at baseline (since they do not have a baseline  score), though average treatment effects (ATEs) include all students. ATEs denote the difference in average learning  gains between students in program schools and students in control schools.\n",
            "Top  18  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  19  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  20  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  5  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  6  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  9  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  10  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  11  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  12  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  13  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  14  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  15  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  16  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  17  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  18  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  19  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  20  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  5  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  8  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  9  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  10  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  11  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  17  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  18  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  19  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  5  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  8  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  9  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  10  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  11  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  12  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  13  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  14  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  17  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  18  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  19  :   •  Students in grades 4 and 5 at Baseline were expected to progress to grades 6 and 7 by  Year 3. However, 32 students from Baseline grades 4 and 5 were still in grades 3-5 at  the time of the Year 3 Endline, and thus assessed this year. Likewise, two students from  Baseline grade 5 were still in grade 5 during the Year 2 Endline. We included these  assessments in the final calculation of learning gains, leading to changes in the learning  gains of students in grades 4Y1 and 5Y1 despite these cohorts generally not being part of  the Year 3 student assessments.\n",
            "Top  20  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What outcomes were achieved?\n",
            "Top  1  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  6  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  7  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  8  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  9  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  10  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  11  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  12  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  13  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  14  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  15  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  16  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  17  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  18  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  19  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  20  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What impact was achieved?\n",
            "Top  1  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  2  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  3  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  4  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  5  :   Across  all  grades,  the  one-year  effects  of  the  program  in  Year  3  far  exceed  the  effects  in  previous years. The difference is greatest for students in Grade 3: whereas the program did  not have  a statistically significant  effect  on  learning  gains  for  Grade  3  students  in  previous  years, in the final year of the program Grade 3 students made gains comparable to older peers.\n",
            "Top  6  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  7  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  8  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  9  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  10  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  11  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  12  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  13  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  14  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  15  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  16  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  17  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  18  :   As in previous years, average treatment effects were larger for students in Bijoliya block than  for students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs.  +1.04).\n",
            "Top  19  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  20  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What were the results of the intervention?\n",
            "Top  1  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  4  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  5  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  6  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  7  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  8  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  9  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  10  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  11  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  12  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  13  :   19 Appendix 7 shows treatment effects separately for students present at Baseline and absent at Baseline.\n",
            "Top  14  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  15  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  16  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  17  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  18  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  19  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  20  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What was the impact of the intervention?\n",
            "Top  1  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  2  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  3  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  4  :   Appendix Appendix 1: Description of Educate Girls’ Intervention Enrollment  Educate Girls delivers a comprehensive community intervention to enroll girls into school.  This intervention includes identification of out-of-school girls through door-to-door surveys,  explanation of the value of schooling to their parents and to the community, and multi- channel engagement with households with unenrolled girls. Educate Girls also uses multiple  interventions to improve school attendance and prevent drop-outs, such as frequent parent  counselling sessions and working with School Management Committees to improve school  infrastructure. It also identifies girls who have dropped out and works with the community  to re-enroll them into school.     Learning  Educate Girls trained volunteers to deliver a child-centric curriculum one to five times a week  to boys and girls in Grades 3-5. Volunteers were often drawn from the villages in which they  worked.  They  were  incentivized  with  a  small  number  of  skill  and  career  development  opportunities,  such  as  free  English  classes  and  the  possibility  of  being  hired  by  EG  in  the  future.    In Year 3, EG rolled out a new curriculum called “Gyan Ka Pitara” (“Knowledge Box”). As  part  of  this  new  curriculum,  EG  increased  the  number  of  teaching  sessions  per  day  and  conducted  home  visits  to  reach  students  who  were  frequently  absent  from  school  or  who  needed  remedial  tutoring.  In  addition  to  the  thrice  yearly  rounds  of  student  assessments  conducted  previously  in  Years  1  and  2,  EG  conducted  three  additional  rounds  of  ASER  assessments in Year 3. These additional assessments led EG to identify areas of improvement,  which  informed  adjustments  to  the  clustering  of  schools  for  program  implementation,  the  training  of  volunteers,  and  the  content  of  remedial  classes.  School  teachers  were  also  more  involved in programming in Year 3 through school meetings and block review meetings.\n",
            "Top  5  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  6  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  7  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  8  :   Across  all  grades,  the  one-year  effects  of  the  program  in  Year  3  far  exceed  the  effects  in  previous years. The difference is greatest for students in Grade 3: whereas the program did  not have  a statistically significant  effect  on  learning  gains  for  Grade  3  students  in  previous  years, in the final year of the program Grade 3 students made gains comparable to older peers.\n",
            "Top  9  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  10  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  11  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  12  :   As in previous years, average treatment effects were larger for students in Bijoliya block than  for students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs.  +1.04).\n",
            "Top  13  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  14  :   19 Appendix 7 shows treatment effects separately for students present at Baseline and absent at Baseline.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  17  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  18  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  19  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  20  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "\n",
            "\n",
            "\n",
            "Query:  Were the contracted outcomes achieved?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  5  :   3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages,  which  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to  budgetary constraints, the DIB Working Group decided not to conduct a parallel census of  out-of-school girls in control villages. As a result, we cannot rule out the possibility that other  factors besides the Educate Girls program influenced enrollment in treatment villages.\n",
            "Top  6  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  7  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  8  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  9  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  10  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  11  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  12  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  13  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  14  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  15  :   By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls,  representing 92% of all identified out-of-school school girls eligible for enrollment. Educate  Girls thus exceeded the enrollment target of 79% by 16%.\n",
            "Top  16  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  17  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  18  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  19  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  20  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results outcomes achieved impact\n",
            "Top  1  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  2  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  3  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  4  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  5  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  6  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  7  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  8  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  9  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  10  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  11  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  12  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  13  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  14  :   Across  all  grades,  the  one-year  effects  of  the  program  in  Year  3  far  exceed  the  effects  in  previous years. The difference is greatest for students in Grade 3: whereas the program did  not have  a statistically significant  effect  on  learning  gains  for  Grade  3  students  in  previous  years, in the final year of the program Grade 3 students made gains comparable to older peers.\n",
            "Top  15  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  16  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  17  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  18  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  19  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  20  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "\n",
            "\n",
            "\n",
            "Query:  results\n",
            "Top  1  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  2  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  3  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  4  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  5  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  6  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  7  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  8  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  9  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  10  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  11  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  12  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  13  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  14  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  17  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  18  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  19  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes achieved\n",
            "Top  1  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  6  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  7  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  8  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  11  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  12  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  13  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  14  :   Learning gains were higher for treatment students than for control students across all grades  and subjects, with relatively higher gains in Math and English than in Hindi and relatively  larger treatment effects among students who were exposed to the program for more years.  EG’s program in Year 3 was particularly effective in increasing test scores.\n",
            "Top  15  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  16  :   Table 2: Learning Levels as Measured by ASER Level  1  2  3  4  5  6 Hindi  Beginner  Letters  Words  Paragraph  Story 1  Story Plus Math  Beginner  Numbers 1-10  Numbers 11-99  Subtraction  Division  — English  Beginner  Capital letters  Lowercase letters  Words  Sentences  — Calculating Learning Gains  The change in learning levels for each student is calculated by subtracting his or her total score  at Baseline from his or her total score at Endline,9 with the following caveats: •  Baseline scores for students in treatment and control schools who were not present at  Baseline are imputed to be the lowest score possible (a score of 3) and any additional  learning levels achieved by those students at Endline are assumed to be gains.\n",
            "Top  17  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  18  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  19  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  20  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "\n",
            "\n",
            "\n",
            "Query:  impact\n",
            "Top  1  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  2  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  3  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  4  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  5  :   Across  all  grades,  the  one-year  effects  of  the  program  in  Year  3  far  exceed  the  effects  in  previous years. The difference is greatest for students in Grade 3: whereas the program did  not have  a statistically significant  effect  on  learning  gains  for  Grade  3  students  in  previous  years, in the final year of the program Grade 3 students made gains comparable to older peers.\n",
            "Top  6  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  7  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  8  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  9  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  10  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  11  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  12  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  13  :   As in previous years, average treatment effects were larger for students in Bijoliya block than  for students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs.  +1.04).\n",
            "Top  14  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  15  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  16  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  17  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  18  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  19  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  20  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the costs of the contract?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  7  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  8  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  9  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  10  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  11  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "Top  12  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  15  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  16  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  17  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  18  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  19  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  20  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "\n",
            "\n",
            "\n",
            "Query:  How much is paid for outcomes?\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  3  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  4  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  5  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  8  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  11  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  13  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  14  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  16  :   Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students  at the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those  cohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during  Grade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average  treatment effect for that cohort since the yearly average treatment effects do not account for students who have  dropped out or have been retained.\n",
            "Top  17  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  18  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  19  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What are the outcomes payments?\n",
            "Top  1  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  2  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  3  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  4  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  5  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  6  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  7  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  8  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  9  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  10  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  11  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  12  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "Top  15  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  16  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  17  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  18  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  19  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  20  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the total contract value?\n",
            "Top  1  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  4  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  5  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  6  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  7  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  8  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  9  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  10  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  11  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  12  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  13  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  16  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  17  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  18  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  19  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  20  :   Table 2: Learning Levels as Measured by ASER Level  1  2  3  4  5  6 Hindi  Beginner  Letters  Words  Paragraph  Story 1  Story Plus Math  Beginner  Numbers 1-10  Numbers 11-99  Subtraction  Division  — English  Beginner  Capital letters  Lowercase letters  Words  Sentences  — Calculating Learning Gains  The change in learning levels for each student is calculated by subtracting his or her total score  at Baseline from his or her total score at Endline,9 with the following caveats: •  Baseline scores for students in treatment and control schools who were not present at  Baseline are imputed to be the lowest score possible (a score of 3) and any additional  learning levels achieved by those students at Endline are assumed to be gains.\n",
            "\n",
            "\n",
            "\n",
            "Query:  What is the price per outcome?\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  5  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  6  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  9  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  10  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  11  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  13  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  14  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  17  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  18  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  19  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  20  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment price contract value contract cap rate card incentive payment costs savings\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  3  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  6  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  7  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  8  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "Top  9  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  10  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  11  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  12  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  13  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  14  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  15  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  16  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  17  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  18  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  19  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  20  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "\n",
            "\n",
            "\n",
            "Query:  outcomes payment\n",
            "Top  1  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  2  :   Appendix 17: Merged Schools Treatment-Control Merge Cases  As per the Working Group’s decision from 2017, in cases where a treatment school closed and  merged with a control school or a control school closed and merged with a treatment school,  students have not been assessed after the school merge occurred. However, all learning gains  that  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome  payments.\n",
            "Top  3  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  6  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  7  :   2. Outcome I: Learning Gains Methodology  IDinsight conducted a three-year randomized controlled trial, clustered at the village level, to  estimate learning gains attributable to EG’s program.5 Sampling and Randomization  The evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were  selected according to the process outlined in Figure 1, below.\n",
            "Top  8  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "Top  9  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  10  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  11  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  12  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  13  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  14  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  15  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  16  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  17  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  18  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  19  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  20  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "\n",
            "\n",
            "\n",
            "Query:  price\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  5  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  6  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  7  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  8  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  9  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  10  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  11  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  12  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  13  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students  at the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those  cohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during  Grade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average  treatment effect for that cohort since the yearly average treatment effects do not account for students who have  dropped out or have been retained.\n",
            "Top  17  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  18  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  19  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  20  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract value\n",
            "Top  1  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  5  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  6  :   Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at  Baseline Years of exposure  to EG program Average learning gains Treatment  students Control  students Difference Difference  (std effects) p-Value 1  2  3  4  5 1  2  3  2  1 Total 5.97  6.76  6.13  3.59  1.32 4.96 4.59  5.40  4.43  3.06  0.84 3.88 1.38  1.35  1.71  0.52  0.48 1.08 0.46  0.41  0.50  0.16  0.28 0.31 1.36  1.23  1.72  0.39  – 1.26 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 <0.01  <0.01  <0.01  0.36  – <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized  differences are calculated by subtracting the control mean and dividing by the control standard deviation for each  grade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of  learning gains and are useful for benchmarking treatment effects against impact estimates from outside programs.  The p-value indicates the likelihood of the difference in means between treatment and control being this large (or  larger) by random chance if the treatment effect was zero.\n",
            "Top  7  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  8  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  9  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  10  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  11  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  12  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  13  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  17  :   7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline  versus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent  sample throughout the three-year evaluation and are thus comparable between treatment and control schools. On  the other hand, students who were absent at Baseline are composed of both students who were absent but enrolled  at  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG’s programming  includes  enrollment  activities,  students who were absent at Baseline are not directly comparable between treatment and control schools, limiting  our ability to make causal claims about their learning gains.  8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the  day of the assessment and had graduated out of the program after Year 1. Per the Working Group’s decision in  Year 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who  were present on the day of the Baseline assessment.  9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.\n",
            "Top  18  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  19  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  20  :   Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break  down across grade and student type.\n",
            "\n",
            "\n",
            "\n",
            "Query:  contract cap\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  4  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  5  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  8  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  9  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  10  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  11  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  12  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  13  :   18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention  throughout the three-year project.\n",
            "Top  14  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  15  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  16  :   The third and final Endline was conducted between February 2 and February 28, 2018 and is  described in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on  data collection in those years.\n",
            "Top  17  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  18  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  19  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  rate card\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  3  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  4  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  5  :   •  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 •  74% of students were assessed at the school while 26% were assessed at their home. In  the majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline  score is not available) rather than students who dropped out of school.   23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily  migrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the  child was ill or the child or family did not consent to being assessed).   24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we  include their latest available score in the calculation of learning gains.  25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the  Working Group’s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a  total of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with  another treatment school or control school closed and merged with control school), which reduced the number of  schools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found  and surveyed the affected students at home or at their new school. For more information on how school merge  cases were dealt with, please refer to Appendix 16.  26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were  in most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time  and surveyor effects.  27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to  assess them.\n",
            "Top  6  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  7  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  8  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  9  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  10  :   A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of  students  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1  through 5. In each subsequent Endline, we assessed students who were then in grades 3, 4,  and 5 (the target grades for Educate Girls’ programming). Since a student’s grade changes year  to year, student cohort labels can be ambiguous; for instance, “Grade 3” could refer to three  different cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year  2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student  cohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the  “Y1” suffix to grade labels to remind the reader of this convention. For instance, “2Y1” refers  to students in grade 2 during the first year of the evaluation, who had progressed to grade 3  in Year 2 and grade 4 in Year 3.\n",
            "Top  11  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  12  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  13  :   Consolidated  Student Group Student  Type Status at Baseline Status at Endline Students  Present at  Baseline Students  Absent at  Baseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or  unenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school,  assessed24 Newly  Enrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018,  according to the following protocol: • IDinsight visited a total of 32525 schools.26 •  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully  assessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment,  93% in control).\n",
            "Top  14  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  15  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  16  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  17  :   Figure 3: Average Learning Levels by Cohort Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Lines omit students absent at baseline (since they do not have a baseline  score), though average treatment effects (ATEs) include all students. ATEs denote the difference in average learning  gains between students in program schools and students in control schools.\n",
            "Top  18  :   •  For students who were assessed during multiple Endlines (for example, students who  were in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5  during Endline Year 5), only the final Endline score is counted.10 •  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.\n",
            "Top  19  :   S         Z          L n         h          c fox I have a sister.\n",
            "Top  20  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "\n",
            "\n",
            "\n",
            "Query:  incentive payment\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Table 1: Summary of EG’s performance against DIB targets Outcome Methodology Target Final Result Aggregate  learning gains for  all students in  grades 3-5 Clustered  (village-level)  randomized  controlled trial +5,592 ASER  learning levels  above control  group gains +8,940 ASER  learning levels  above control  group gains Pre-post  comparison 79% of all eligible  out-of-school  girls 92% of all eligible  out-of-school  girls enrolled Enrollment of  out-of-school  girls Performance as  Percent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are  based on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of  how payments will be calculated.  2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the  original target by 34%.  3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for  Year 1), reflecting updates to the data made in Year 3 as per Appendix 14.  4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along  with 88 girls already on the list who had left the area of program coverage. This resulted in a final population of  837 out-of-school girls eligible for enrollment.\n",
            "Top  4  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  5  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  6  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  7  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  8  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  9  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  10  :   Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER  learning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which  we  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We  assessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of  Education Report (ASER) testing tool; a student’s score on ASER determined her “learning  level,” which is scored out of 16 points and forms the basis of the learning metric.   Results: On average, students in EG schools gained an additional 1.08 ASER levels compared  to  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between  treatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year  2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in  treatment villages,4 representing 116% of the final target for enrollments.   Methodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of- school girls in treatment villages. Due to the cost of conducting a census of all households, the  Working Group decided against estimating enrollments in control villages. Thus, unlike the  learning estimates, the enrollment estimates do not reflect a causal effect of EG’s program.    Results:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls  identified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2,  Educate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.\n",
            "Top  11  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  12  :   effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3  benefitted 2-3 times more than their peers who had aged out of the program prior to Year 3.  Treatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year  of the program than their peers in control schools.\n",
            "Top  13  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  14  :   4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in  program  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable  students  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact  Bond  by  60%.  The  effects  of  Educate  Girls’  program  on  learning  gains  were  large  and  statistically significant over the three-year program: Students in EG schools gained on average  an additional 1.08 learning levels, or 28%, compared to students in control schools.\n",
            "Top  15  :   section below, we present average and aggregate results for the full sample of students unless  otherwise indicated. In the appendix, to provide points of comparison with previous reports,  we also present results separately for students present at Baseline (also called “Type I-III” in  the Design Memo) and students absent at Baseline (“Type IV-V”).7  If students were absent  from school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists,  which are included in aggregate learning gains calculations and DIB payments. Since we did  not  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average  treatment effect results.\n",
            "Top  16  :   20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not  previously been reported enrolled by Educate Girls.  21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open  School (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error  rate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.\n",
            "Top  17  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  18  :   Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1  and Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of- school girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.\n",
            "Top  19  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  20  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "\n",
            "\n",
            "\n",
            "Query:  costs\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  5  :   आप हमेशा इन िछु वों िी पीठ क्यों साफ िरती हैं?” मदहिा ने बोिा, ”इन िछु वों िी पीठ साफ़ िरते हुए मैं सुख शांतत िा अनुभव िेती हू ाँ|” इन िछु वों िी पीठ पर जो िवच होता है उस पर िचरा जमा हो जाता है| जजसिी वजह से इनिी गमी पैदा िरने िी क्षमता िम हो जाती है| िम्बे समय ति अगर ऐसा ही रहे तो ये िवच िमजोर ाँ| यह सुनिर िड़िा आश्चयय से  भी हो जाते हैं| इसलिए मैं िवच िो साफ़ िरती हू बोिा, “आपिे  अिे िे िे  बदिने से तो िोई बड़ा पररवतयन नहीं आयेगा|” मदहिा ने संक्षक्षप्त में जवाब ददया, “भिे मेरे इस िमय से िोई बड़ा बदिाव नहीं आयेगा िेकिन इस एि िछु वे िी जजन्दगी में तो बदिाव आयेगा |” इसलिए हमें छोटे बदिाव से ही  शुरुआत िरनी चादहए| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 – 9 Number recognition   10 – 99 Subtraction 2 digit with borrowing Division  3 digit by 1 digit − 29 − 28 − 76 − 15 − 39 − 17 − 57 − 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) ! ! ! !\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the  Children’s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus  Foundation,  Instiglio,  and  IDinsight  (collectively,  the  “Working  Group”)  to  provide  and  improve education for girls in rural India. UBS Optimus, acting as the investor, financed EG’s  project  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by  IDinsight. Instiglio is managing the project.\n",
            "Top  8  :   School Treatment school closed and merged with another treatment school  DISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA  JHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA  KHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 शब्द अक्षर अनुच्छे द गाना खुश ब व रानी नदी किनारे रहती है| मौसी ख आिू खेत ह झ नदी में बहुत मछलियााँ हैं| रानी उनिो दाना देती है| ददन स वे सब मजे से दाना खाती हैं| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 िहानी 1 िहानी 2 राजू नाम िा एि िड़िा था| उसिी एि बड़ी बहन व एि छोटा भाई था| उसिा भाई गााँव िे  पास िे  ववद्यािय में    पढ़ने जाता था| वह खूब मेहनत िरता था| उसिी बहन    बहुत अच्छी खखिाड़ी थी| उसे िम्बी दौड़ िगाना अच्छा    िगता था| वे तीनों रोज साथ-साथ मौज-मस्ती िरते थे| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) एि िड़िा रोज सुबह एि बूढ़ी मदहिा िो तािाब िे  किनारे देखता था| वह मदहिा  रोज छोटे छोटे िछु वों िी पीठ िो साफ़ िरती थी| एि ददन उस िड़िे  ने इसिे  पीछे   िा िारण जानने िा मन बनाया| उसने मदहिा िे  पास जािर िहा, ”नमस्ते आंटी!\n",
            "Top  9  :   0.03  <0.01  <0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi  Math  English Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20  0.59  0.68 1.47 0.04  <0.01  <0.01 <0.01 0.21  0.66  0.72 1.59 0.02  <0.01  <0.01 <0.01 0.21  0.53  0.65 1.39 0.13  <0.01  <0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students  in  Grades  1Y1,  2Y1,  and  3Y1).  “Difference”  shows  the  raw  difference  in  learning  gains  between  students  in  treatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the  difference in means between treatment and control being this large (or larger) by random chance if the treatment  effect was zero.\n",
            "Top  10  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  11  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  12  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  13  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  14  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  15  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  16  :   Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled  B: Girls Eligible for Enrollment  C: Share of Girls Enrolled against  Final Target (837 Girls)   D: Share of Target (D=C/79%) 322  744  38% 48% 613  835  73% 92% 768  837  92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in  the calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01%  (Year 1) and 2.1% (Year 2) of the final target.\n",
            "Top  17  :   Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning  gains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence  intervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3,  see Appendix 8.\n",
            "Top  18  :   Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases  In cases where a treatment school merged with another treatment school or a control school  merged with another control school, IDinsight continued to assess all sampled students from  both schools.\n",
            "Top  19  :   Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to  EG program Baseline Y1  Endline Y2  Endline Y3  Endline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight’s three-year impact evaluation of Educate  Girls’ program in Bhilwara District in Rajasthan, India. The two outcomes described in this  report – learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls  – will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls  surpassed the DIB targets for both learning gains and enrollment.\n",
            "Top  20  :   •  Students with no Endline score from any round are not included in the analysis (466 students).\n",
            "\n",
            "\n",
            "\n",
            "Query:  savings\n",
            "Top  1  :   Educate Girls Development Impact Bond  Final Evaluation Report 10 June 2018    In Partnership with: Children’s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact.  Depending on client needs, we help diagnose social sector challenges, design and test potential  solutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that  client-centered, rigorous, and responsive evaluation is essential to help managers maximize  program impact.\n",
            "Top  2  :   10 33 students who should have graduated out of the program were retained. We assessed these students during  their additional retention year and use their final score to calculate learning gains.  11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the  appendix, to provide points of comparison with previous reports, we also present results separately for students  present at Baseline (also called “Type I-III” in the Design Memo) and students absent at Baseline (“Type IV-V”).  We separately report learning gains of newly enrolled girls from EG’s out-of-school girl lists, which are included  in aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control  villages, we exclude these girls from the average treatment effect results.  12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning  levels from Endline learning levels.  13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school  students even if their learning levels were very low and would bring down the school average.   14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of  observing this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability  is  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control  villages. Due to randomization we can reasonably expect that, on average, the only difference between students in  treatment villages and students in control villages is that the former have been exposed to Educate Girls’ program.  Balance checks presented in the Baseline report show that there are no statistically significant differences between  the control and treatment groups across any of the variables collected.  15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif  Jameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect,  while an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large  effect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education  intervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG’s program. In that  evaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007).  The  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of  primary school programs in rural India have found effects on math and language test scores ranging from 0.16 to  0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al.  2007).\n",
            "Top  3  :   Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline,  Assessed at Endline  Type I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline,  Not Assessed  at Endline  (Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline,  Assessed at Endline  (Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline,  Not Assessed  at Endline  (Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled  Girls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 – – – – 1.94 All -83 850  8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may  differ  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate  learning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5  since they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group  agreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls’ program  on students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.\n",
            "Top  4  :   Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average  (Treatment)  5.90 Average  (Control)  4.58 p-Value of  Difference  <0.01 3.86  3.88 3.78 3.98 3.77 3.35  4.22 3.77 1.86 1.91  1.50 1.06  0.60  -0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91  1.26 0.86 1.04 1.13 1.62  0.78 1.14 0.15 0.16  0.36 0.10  0.01  -0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01  <0.01 <0.01 <0.01 <0.01 <0.01  <0.01 <0.01 0.09 <0.01  0.06 0.08  0.73  0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score  at Baseline Math Score  at Baseline English Score  at Baseline Total General OBC  SC ST Boy Girl Bijoliya  Jahajpur Mandalgarh 2  3 4  5  6 4.78  5.14 4.64 5.02 4.90 4.98  5.00 4.91 2.01 2.07  1.86 1.17  0.61  -0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the  analysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values  represent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed  the lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those  students (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness  of the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the  respective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the  treatment effect is zero, then the difference in means between treatment and control could be this large by random  chance.\n",
            "Top  5  :   Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable   (* indicates average if answer to  preceding question is “yes”) Average  (All) Std. Dev.  (All) Average  (Treatment) Average  (Control) p-Value of  Difference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences  in means between treatment and control this large (or larger) by random chance if there were no mean differences  between treatment and control schools.\n",
            "Top  6  :   Appendix 4: Descriptive Student Statistics Variable Average  (All) Std. Dev.  (All) Average  Treatment Average  Control p-Value of  Difference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste  (fraction of total) Age Female (fraction of  total) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word  Comprehension  (fraction answering  correctly) English Sentence  Comprehension  (fraction answering  correctly) Grade (1-5) SC or ST caste  (fraction of total) Female (fraction of  total) Grade (1-5) SC or ST caste  (fraction of total) Age Children  Present at  Baseline Children  Absent At  Baseline Age Newly  Enrolled  Girls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large  (or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and  newly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8- year-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.\n",
            "Top  7  :   Learning Gains against the DIB Target  Students in EG schools gained on average an additional 1.08 ASER learning levels compared  to students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28%  or  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing  favorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect  target.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an  additional 8,940 learning levels relative to students in control villages, representing 160% of  the  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate  learning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the  difference occurring in year 3.\n",
            "Top  8  :   Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort  Treatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for  treatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were  in grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth.  Each year, EG’s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered  the program for the first time in Year 3, and Grade 5Y1 students exited the program after the  first year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.\n",
            "Top  9  :   Student Assessments  Learning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER)  assessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three  sections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1  to 5 points). IDinsight added one additional level to the Hindi section (“Story Plus”) to reduce  “ceiling effects,” in which the highest score on a section underestimates a student’s true ability.  The  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest  possible score is 3 points (1 + 1 + 1).\n",
            "Top  10  :   These differences resulted from a combination of increased learning and increased enrollment  in treatment schools, though relatively more from learning. By the end of Year 3, our study  population included 7,318 students in treatment schools and 6,786 students in control schools,  reflecting a modest increase in enrollment due to EG’s program. The majority of this difference  can  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in  treatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled  girls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in  control  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between  treatment and control schools.\n",
            "Top  11  :   Figure 4 shows the effect of EG’s program on learning gains by project year for each grade  targeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved  in  program  schools  within  that  year  compared  with  gains  among  comparable  students  in  control  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains  (+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to  cohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for  students in Grade 3 during the second year of the program (2016-17, corresponding to cohort  2Y1).\n",
            "Top  12  :   Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1  2  3  4  5 Total 1.38  1.35  1.71  0.52  0.48 1.08 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 1.44  1.55  1.70  0.69  0.48 1.07 <0.01  <0.01  <0.01  <0.01  <0.01 <0.01 Note: “Difference” shows the raw difference in learning gains between students in treatment villages and students  in  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between  treatment and control being this large (or larger) by random chance if the treatment effect was zero.\n",
            "Top  13  :   Figure 3 provides two major insights. First, program impact increases with years of program  exposure. Students in Grade 3Y1, who were exposed to EG’s programming for all three years,  had the largest learning gains of any cohort. Second, EG’s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target  regardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the  Endline surveys.   17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2  and Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.\n",
            "Top  14  :   Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade  at Baseline Year 1 Difference  from Baseline Year 2 Difference   from Baseline Year 3 Difference   from Baseline Present at Baseline, Types I-III 1  2  3  4  5  Total 1  2  3  4  5  Total 1  2  3  4  5  Total 237  400  549  1,186 –  –  –  - 93  81  101  275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162  642  949    2302 -245  64  31  96  -54 –  130  178  238    647 856  877  1905      5136 920  583  938      2583 227  254  401      1221 Note: Scores in bolded text represent the cohort’s final score. While the total aggregate gains are consistent, the sub- aggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10  text due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.\n",
            "Top  15  :   Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi  Math  English Total 0.14  0.44  0.50 1.08 0.14  0.45  0.48 1.07 <0.01  <0.01  <0.01 <0.01 0.19  0.49  0.58 1.26 0.17  0.00  0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. “Difference” shows the raw difference  in learning gains between students in treatment villages and students in control villages (treatment - control). The  p-value indicates the likelihood of the difference in means between treatment and control being this large (or larger)  by random chance if the treatment effect was zero.\n",
            "Top  16  :   Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total  Share of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning  by Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.\n",
            "Top  17  :   Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control  schools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology.  6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in  grades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the  population,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these  students’ outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school- grade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis.  If 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then  their learning gains were multiplied by 1/100% = 1 in the final analysis.\n",
            "Top  18  :   Learning gains by subject, gender, and geography  Figure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status.  Program impacts were concentrated in Math and English, where the treatment effects were  approximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with  low baseline scores, especially in Math and English, benefitted the most from EG’s program.\n",
            "Top  19  :   •  Students in grades 4 and 5 at Baseline were expected to progress to grades 6 and 7 by  Year 3. However, 32 students from Baseline grades 4 and 5 were still in grades 3-5 at  the time of the Year 3 Endline, and thus assessed this year. Likewise, two students from  Baseline grade 5 were still in grade 5 during the Year 2 Endline. We included these  assessments in the final calculation of learning gains, leading to changes in the learning  gains of students in grades 4Y1 and 5Y1 despite these cohorts generally not being part of  the Year 3 student assessments.\n",
            "Top  20  :   Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment  effects are the difference in average learning gains between treatment and control students,12  and  are  particularly  useful  for  understanding  the  magnitude  of  the  program’s  impact  and  comparing it to other interventions. Aggregate treatment effects are calculated by adding up  learning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all  students in control schools, and therefore account for differences in the number of students in  treatment and control schools due to EG’s enrollment activities and other factors.13 The final  Development Impact Bond payments are based on aggregate treatment effects.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for paper in papers:\n",
        "  key = paper['paper_id']\n",
        "  print('Begin experiment for key ', key)\n",
        "  passages = paper['filtered_paragraphs']\n",
        "  corpus_embeddings = all_corpus_embeddings[key]\n",
        "  '''\n",
        "  # Find most frequent unigrams and use them as query\n",
        "  unigrams = []\n",
        "  passages = paper['sent_tokenized_txt']\n",
        "  for passage in passages:\n",
        "    unigrams.extend(word for word in word_tokenize(passage) if not word in stopwords.words())\n",
        "  \n",
        "  frequent_unigrams = []\n",
        "  for unigram in Counter(unigrams).most_common(1000):\n",
        "    if len(unigram[0])>2:\n",
        "      frequent_unigrams.append(unigram[0])\n",
        "\n",
        "  query = set(frequent_unigrams).intersection(set(general_keywords))\n",
        "  query = ' '.join(list(query))\n",
        "  \n",
        "  query = ''\n",
        "  frequency_dict = {}\n",
        "  paper['pdf_txt'] = ' '.join(paper['filtered_paragraphs'])\n",
        "  for keyword in general_keywords:\n",
        "    frequency_dict[keyword] = paper['pdf_txt'].count(keyword)\n",
        "    #if keyword in paper['pdf_txt']:\n",
        "    #  query = query + ' ' + keyword\n",
        "  \n",
        "  print(frequency_dict)\n",
        "  for keyword in frequency_dict.keys():\n",
        "    if frequency_dict[keyword]>0:\n",
        "      query = query + ' ' + keyword'''\n",
        "\n",
        "  #print('Target population (Gold Standard): ',paper['target_population'])\n",
        "  \n",
        "  for field in IR_results.keys():\n",
        "    for query in IR_results[field].keys():\n",
        "      print('Query: ', query)\n",
        "      i = 1\n",
        "      for result in search(query,passages,corpus_embeddings,20):\n",
        "        print('Top ',i, ' : ', result.replace('\\n',' '))\n",
        "        all_IR_results[key][field][query].append(' '.join(result.split()))\n",
        "        i+=1\n",
        "      print(\"\\n\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOCKmq5XKTwu"
      },
      "source": [
        "Save the ranked candidates in Google Folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDC9mR9n6DB4"
      },
      "outputs": [],
      "source": [
        "with open('information_retrieval/IR_results.jsonl','w') as g:\n",
        "  json.dump(all_IR_results,g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqz7EQy4OnBj",
        "outputId": "51218350-be5c-4cf5-8ca0-a2a07eb39b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin experiment for key  #17340\n",
            "[('service', 65), ('intervention', 65), ('cohort', 17), ('target', 15), ('population', 12), ('identified', 10), ('intended', 6), ('participants', 6), ('attended', 3)]\n",
            "Target population (Gold Standard):  3,500 additional preschoolers (grouped into five cohorts) in two school districts—Park City and Granite. 2,620 Chicago public school children \n",
            "Query:  cohort\n",
            "Top  1  :  Participants will be served sequentially in two cohorts of 1,000 individuals each, with each cohort defined as a phase of the project.\n",
            "Top  2  :  Further analysis showed an 8.39% reduction in reoffending rates within the cohort, which is insufficient to trigger repayment for the first cohort (minimum 10% reduction required).\n",
            "Top  3  :  Peterborough Social Impact Bond: Final report on cohort 1 analysis.\n",
            "Top  4  :  Peterborough Social Impact Bond: Final report on cohort 1 analysis.\n",
            "Top  5  :  The results showed an 8.39% reduction in reoffending rates within the cohort, which was insufficient to trigger repayment for the first cohort (minimum 10% reduction required).\n",
            "Top  6  :  Cascade, Fall 2015 (89).\n",
            "Top  7  :  Key Take-Away(s):  The process of gathering and agreeing upon the sample of data to be evaluated and obtaining missing data for the first cohort took 11 months.\n",
            "Top  8  :  Rinzler et al.\n",
            "Top  9  :  & Brumme, C.R.\n",
            "Top  10  :  & Brumme, C.R.\n",
            "Top  11  :  The key purpose of this collaboration would be to identify and broadly disseminate information on leading practices and lessons learned.\n",
            "Top  12  :  Organisation for Economic Co-operation and Development.\n",
            "Top  13  :  Organisation for Economic Co-operation and Development.\n",
            "Top  14  :   While the reduced reoffending rate was not sufficient to trigger outcome payments for the first cohort, repayment could still be triggered at a future date if a 7.5% reduction across all cohorts is measured.\n",
            "Top  15  :  Provides guidance and support to 100 Resilient Cities12 member cities in the U.S. exploring Pay for Success opportunities (100RC, 2015).\n",
            "Top  16  :  As of September 2015, the Center for Employment Opportunities (CEO) had enrolled 928 individuals into programming, nearing their goal of enrolling 1,000 participants during Phase 1.\n",
            "Top  17  :   Perceived benefits spanned beyond members of the HMP Peterborough cohorts, resulting in expanded trainings for staff, better engagement of inmates regarding post-release care, improved understanding of inmate needs, and prompting of critical assessment of existing services and processes.\n",
            "Top  18  :  o Who to evaluate: Assess whether intervention recipients, control groups, or extended populations will be included.\n",
            "Top  19  :  Each year, CNCS provides opportunities for more than two million Americans of all ages and backgrounds to serve their communities and country through AmeriCorps, Senior Corps, the Social Innovation Fund, and the Volunteer Generation Fund.\n",
            "Top  20  :  This report is a product of an independent study commissioned by CNCS.\n",
            "Top  21  :  Introduction .............................................................................................................................................................................. 2 Purpose and Organization of Document ............................................................................................................................... 3 Themes from Recent Literature ............................................................................................................................................. 4 Focus on Outcomes and Targeted Feasibility Assessment ................................................................................................. 5 Assessment and Outcomes of Existing PFS Projects ................................................................................................. 5 PFS Feasibility in Specific Social Service Areas ......................................................................................................... 6 Evaluation of Interventions within Pay for Success Projects .............................................................................................. 6 Defining Appropriate Outcomes ................................................................................................................................. 7 Evaluation Design .......................................................................................................................................................... 7 Endorsements and Critiques of the PFS Model .................................................................................................................... 8 The Future of PFS ..................................................................................................................................................................... 9 Sustaining Successful PFS Projects ............................................................................................................................ 10 Increased and Alternative Funding Sources ............................................................................................................ 10 Focus on Service Improvement over Cost-Effectiveness ........................................................................................ 11 III.\n",
            "Top  22  :  The following researchers at Abt Associates prepared this report for the Corporation for National and Community Service: Suggested Citation Table of Contents I. II.\n",
            "Top  23  :  A Partnership to Combat Housing Blight [Video File].\n",
            "Top  24  :  100RC & Social Finance launch ‘Resilience Pay for Success’ initiative.\n",
            "Top  25  :  Retrieved from http://www.socialfinance.org.uk/wp-content/uploads/2015/02/Tech_Guide_2_Designing_Effective.pdf Tan, S., Fraser, A., Giacomantonio, C., Kruithof, K, Sim, M., Lagarde, M., … Mays, N. (2015).\n",
            "Top  26  :  Stanford Social Innovation Review.\n",
            "Top  27  :  Stanford Social Innovation Review.\n",
            "Top  28  :  Stanford Social Innovation Review.\n",
            "Top  29  :  Stanford Social Innovation Review.\n",
            "Top  30  :  47 MDRC.\n",
            "Top  31  :  52 Shiffman, H. (2015).\n",
            "Top  32  :  Each of these sub-topics is described in greater detail below.\n",
            "Top  33  :  Catalyst Chicago.\n",
            "Top  34  :  No additional information on this project was available at the time of publication.\n",
            "Top  35  :  Retrieved from http://catalyst-chicago.org/2015/03/emanuels-preschool-expansion-facing-enrollment-woes/ Corporation for National and Community Service Office of Research and Evaluation Schaeffer, S., Shumway, J.\n",
            "Top  36  :  Retrieved from http://kresge.org/news/strong-families-fund-finance-decade- long-pilot-pairing-affordable-housing-intensive-social The Richmond Community Foundation (RCF) (2015).\n",
            "Top  37  :  (GAO Publication No.\n",
            "Top  38  :  (GAO Publication No.\n",
            "Top  39  :  Project Welcome Home fact sheet.\n",
            "Top  40  :  Retrieved from https://www.philadelphiafed.org/communi ty- development/publications/cascade/89/01_p ay-for-success This paper summarizes a meeting of 19 participants, including investment managers and advisers, debt capital market bankers, credit analysts, legal professionals, and representatives of not-for- profit organizations.\n",
            "Top  41  :  [SOCAP] (2015, October 9).\n",
            "Top  42  :  8 From the Eighth.\n",
            "Top  43  :  Retrieved from http://www.aspeninstitute.org/sites/default/files/content/docs/pubs/ThrivingOutcomesBasedMkt.pdf .\n",
            "Top  44  :  Retrieved from http://www.uw.org/news-events/news-releases/2015-news-releases/10-07-2015-20sib-20news-20release-20final-2.pdf Corporation for National and Community Service Office of .\n",
            "Top  45  :  Abt Associates would like to thank the following individuals for their contributions, revisions, and insights on this document: Lily Zandniapour, Ph.D. – Evaluation Program Manager and Contracting Officer’s Representative, Corporation for National and Community Service; Jennifer Stoff –Senior Program Officer, Social Innovation Fund Pay for Success Program, Corporation for National and Community Service; and Mary Hyde, Ph.D. – Director of Research and Evaluation, Corporation for National and Community Service.\n",
            "Top  46  :   The California Endowment will provide $1,000,000 in subordinate loan funding at 2% interest.\n",
            "Top  47  :  Fact sheet: The NYC ABLE project for incarcerated youth.\n",
            "Top  48  :  Rinzler, D., Tegeler, P., Cunningham, M. & Pollack, C. (2015).\n",
            "Top  49  :  Corporation for National and Community Service Office of Research and Evaluation No additional information on this project was available at the time of publication.\n",
            "Top  50  :  Healthcare.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "key = '#17340'\n",
        "print('Begin experiment for key ', key)\n",
        "bm25 = BM25Okapi(tokenized_corpus[key])\n",
        "unigrams = []\n",
        "passages = paper['sent_tokenized_txt']\n",
        "\n",
        "for passage in passages:\n",
        "  unigrams.extend(word for word in word_tokenize(passage) if not word in stopwords.words())\n",
        "\n",
        "frequent_unigrams = []\n",
        "for unigram in Counter(unigrams).most_common(1000):\n",
        "  if unigram[0] in general_keywords:\n",
        "    frequent_unigrams.append(unigram)\n",
        "\n",
        "\n",
        "print(frequent_unigrams)\n",
        "\n",
        "query = 'cohort'\n",
        "corpus_embeddings = all_corpus_embeddings[key] \n",
        "\n",
        "print('Target population (Gold Standard): ',paper['target_population'])\n",
        "print('Query: ', query)\n",
        "i = 1\n",
        "for result in search(query,passages,corpus_embeddings,50):\n",
        "  print('Top ',i, ' : ', result)\n",
        "  i+=1\n",
        "print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "003d692dee9d43d08004c2d1cc94ec1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c07f3f5b7449d79832b32c1e1c1e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01f415f0e24c4e7c85b95628ddae99a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03158e548ebb4c5691539aff3b4496dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0534758537c44a289d410d70c9424938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06af69d1941240d1ba18586474a43dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0758d6cf8d36499f91ff76ba15ac6edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a3836a486984c6abbdaa1c9f9444a85",
              "IPY_MODEL_b69928128cb949938262471f229184b5",
              "IPY_MODEL_5af8afe8fe254d05a999736033d69f00"
            ],
            "layout": "IPY_MODEL_8930fac4b1fa48ed8623f0913483df41"
          }
        },
        "091d147c11cc420b9fb6bd8bfb879721": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09befa9a8d174a06ad93f8cebf0f5a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73fdee955bb5431d971c1e9fc5859077",
              "IPY_MODEL_641f7e95c3d94c9e852cbd73387e08b3",
              "IPY_MODEL_40996cdf3c594a7ebe73fc39fd6008bd"
            ],
            "layout": "IPY_MODEL_971ec3d353d840b3854abab3ab090b02"
          }
        },
        "0a3836a486984c6abbdaa1c9f9444a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0f4e1283244d208032ca5b7790f6d1",
            "placeholder": "​",
            "style": "IPY_MODEL_33caeb7513ad4126b44c7bbffb10fc4b",
            "value": "Batches: 100%"
          }
        },
        "0b14abb989524a538d233003d4ab54db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69433fe5ef4041b6b07082c3a9090c3e",
              "IPY_MODEL_f47356d978784df4b15e3805c25da741",
              "IPY_MODEL_d7bc8b01726f4632bb328c8c6b0c4d6d"
            ],
            "layout": "IPY_MODEL_c206f87cf3354981b8260d7a7566f45c"
          }
        },
        "0ce85a23775740e6a998887e02b592bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dc8d02a5705412284da8b002216bb35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0548986e2a4bd19664c469ec2ab445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efa228e28ad94688a62f459b52edbfa7",
            "placeholder": "​",
            "style": "IPY_MODEL_932b3ac3bae44c52836f8a3c3a31d275",
            "value": "100%"
          }
        },
        "12130eeb1e4b4ca9a0f513f3a0fe1fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15735cf436aa4972bc5134481b5f3977": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16606d51ede748559c3d4d714299a582": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab72ec74c744591b1f2b90ac616056c",
            "placeholder": "​",
            "style": "IPY_MODEL_75b1c687fb074ffc813379836e5884e4",
            "value": "Batches: 100%"
          }
        },
        "186340bc0aa84ed08a23bd9849b704f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc8d02a5705412284da8b002216bb35",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce85a23775740e6a998887e02b592bf",
            "value": 3
          }
        },
        "191a2f9de1fd455ab406630a7bd066b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d06c77c84dd4257a94b4550f095204b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1db76f4a2eab4e0c84b4c2d07b1187f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e7a1a547f344ae9b308578f68d1b114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2074d2142adf4bf2b9e3ef6336091812": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "207f979de05a47dfba9931a1e7f3300a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baef5fb01167411b9f439b9db400ade4",
              "IPY_MODEL_186340bc0aa84ed08a23bd9849b704f2",
              "IPY_MODEL_6047ec96e6c94701b99a67516e2719e2"
            ],
            "layout": "IPY_MODEL_01f415f0e24c4e7c85b95628ddae99a9"
          }
        },
        "2210d51ee86a4aec8f8e91d4e7fd22cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "265e606c68ad44afa323b21db21a266f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_916f771306ce4443b8d5e8ae43dc67ec",
            "placeholder": "​",
            "style": "IPY_MODEL_2ccd149dccb348d6a2084f37371ed960",
            "value": "Batches: 100%"
          }
        },
        "273b05a586f241f99bd8f9d380910066": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29624b5a8de04cf798e0b8cf13c3e1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab72ec74c744591b1f2b90ac616056c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ccd149dccb348d6a2084f37371ed960": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e22d5e099b74d74896c9413669eee9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_191a2f9de1fd455ab406630a7bd066b3",
            "placeholder": "​",
            "style": "IPY_MODEL_a023517fdb4741d3911ae3259df654f3",
            "value": "100%"
          }
        },
        "33caeb7513ad4126b44c7bbffb10fc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34d32757088c40169f8eec7ba1c91c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35395c2533c3499b849577a26d047d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06af69d1941240d1ba18586474a43dc2",
            "placeholder": "​",
            "style": "IPY_MODEL_bbd40703ef414ef0a60f31c236bb2244",
            "value": " 4/4 [00:02&lt;00:00,  2.10it/s]"
          }
        },
        "3634debf505443ed9b097c529075544f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38bcdc63ffc545f993a3cad0723ea5a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394e04a2baed473c9c3a4addc8f4489a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b6f9e5ba5214811a9d0973c51581ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b77494cefcd48a794f694842a16f453": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e070f0208384cf7934c71b07cd7e88a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f892b864c974c3491859c4a8adda0a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd07ae167ad4db983a71edf32a561e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d836bd17684e08bb62fcefaf40b0e6",
            "placeholder": "​",
            "style": "IPY_MODEL_80e770629fce44a394e127283314f2cf",
            "value": "Batches: 100%"
          }
        },
        "40996cdf3c594a7ebe73fc39fd6008bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_867acac12d3b48e1a16190d1592ff467",
            "placeholder": "​",
            "style": "IPY_MODEL_d3dad67e64af44e08705b6771324437f",
            "value": " 68/68 [00:00&lt;00:00, 2216.88it/s]"
          }
        },
        "4b7f6ec2cc8d41e5b57aa0c39bc06422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e7a1a547f344ae9b308578f68d1b114",
            "placeholder": "​",
            "style": "IPY_MODEL_ad6c88b1bca84ecebfbe3253ad8cdc1a",
            "value": " 6/6 [00:02&lt;00:00,  3.88it/s]"
          }
        },
        "51b39eb413064abc8af1f279a2be210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34d32757088c40169f8eec7ba1c91c3d",
            "placeholder": "​",
            "style": "IPY_MODEL_a945737a13624feb827f94404f1ea836",
            "value": "Batches: 100%"
          }
        },
        "52d836bd17684e08bb62fcefaf40b0e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547a9f88bc5a4ba28f95b65798c6b833": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9520c2b523544a6da8b35fd6a79c2cca",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af1868dcf14d464a851af10269bf8cbe",
            "value": 188
          }
        },
        "586b7253719840358f0cd3349cfd4096": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e22d5e099b74d74896c9413669eee9b",
              "IPY_MODEL_547a9f88bc5a4ba28f95b65798c6b833",
              "IPY_MODEL_6cd7fd691f1a4de389dd5a354cf97332"
            ],
            "layout": "IPY_MODEL_63373539e4b34f0793fce84adf4cf62e"
          }
        },
        "587b3bc1ebd34b49a32f811cf295f959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a168802bd2743458d27841652b34307": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af8afe8fe254d05a999736033d69f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fb718b9ec8440e7a22398988e2b287f",
            "placeholder": "​",
            "style": "IPY_MODEL_8a0e465bbd2446c8b7905a9af5bdb17f",
            "value": " 6/6 [00:02&lt;00:00,  3.61it/s]"
          }
        },
        "5d8fee751b3f4685b263538ad49a1100": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5da7c37b28304c818a6e65e16bdf7f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e8b5fd6f7c94258be451833ff50c71a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e93caa73c3643bca3dda0054df20082": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6047ec96e6c94701b99a67516e2719e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091d147c11cc420b9fb6bd8bfb879721",
            "placeholder": "​",
            "style": "IPY_MODEL_dce6cae3b6aa40f591d56e6fa1fedd40",
            "value": " 3/3 [00:01&lt;00:00,  1.90it/s]"
          }
        },
        "63373539e4b34f0793fce84adf4cf62e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641f7e95c3d94c9e852cbd73387e08b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273b05a586f241f99bd8f9d380910066",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_587b3bc1ebd34b49a32f811cf295f959",
            "value": 68
          }
        },
        "64dedb7900e540d8a396eed5f63275bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc27ab45ee2148ceab12b3fba8951c14",
            "placeholder": "​",
            "style": "IPY_MODEL_7dd8595b079f4fc1b885b1053fa123e4",
            "value": " 184/184 [00:00&lt;00:00, 6329.83it/s]"
          }
        },
        "69433fe5ef4041b6b07082c3a9090c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f892b864c974c3491859c4a8adda0a6",
            "placeholder": "​",
            "style": "IPY_MODEL_bf780f802f68481398cd98dbab551267",
            "value": "100%"
          }
        },
        "6a2b2b8d5512449a9a3b9d628320ea10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cd7fd691f1a4de389dd5a354cf97332": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89178dbfc8af4bd48c808af760f95f93",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bbc60984f247db9a5cba7afa1c7ce4",
            "value": " 188/188 [00:00&lt;00:00, 4956.72it/s]"
          }
        },
        "6d0f4e1283244d208032ca5b7790f6d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea399690962481da4dee0c7a96dc959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b6f9e5ba5214811a9d0973c51581ddd",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5da7c37b28304c818a6e65e16bdf7f73",
            "value": 40
          }
        },
        "711c752b5c09437f84427d1b1bdac148": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738ddc7c95c04788ad54a0dd4a612142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15735cf436aa4972bc5134481b5f3977",
            "placeholder": "​",
            "style": "IPY_MODEL_ea6424b92a6a49fda861fcaebb3d004a",
            "value": " 1265/1265 [00:00&lt;00:00, 13342.57it/s]"
          }
        },
        "73fdee955bb5431d971c1e9fc5859077": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b834706d5041589fe7777a7b2ef448",
            "placeholder": "​",
            "style": "IPY_MODEL_29624b5a8de04cf798e0b8cf13c3e1cd",
            "value": "100%"
          }
        },
        "75b1c687fb074ffc813379836e5884e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a41619167ad4546aba2d9daf6c13a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51b39eb413064abc8af1f279a2be210d",
              "IPY_MODEL_bfccabfc8003460795f883f525a197a9",
              "IPY_MODEL_b5b3c41bb94f4bbca0bb2e7fd307812f"
            ],
            "layout": "IPY_MODEL_38bcdc63ffc545f993a3cad0723ea5a4"
          }
        },
        "7b15d6a8c78341dc9d4c2d90dd6eb4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2e6c17e47b416da789e852267777ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d2fa602f01249f38a603427d5f87c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd8595b079f4fc1b885b1053fa123e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e14ee033ce5498ab127b65b0c5ffdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f0548986e2a4bd19664c469ec2ab445",
              "IPY_MODEL_86cdf5363ad64aae93b2e9ec61bd551a",
              "IPY_MODEL_738ddc7c95c04788ad54a0dd4a612142"
            ],
            "layout": "IPY_MODEL_5e93caa73c3643bca3dda0054df20082"
          }
        },
        "7ea7d1af7f574837869976ed8fb1e5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711c752b5c09437f84427d1b1bdac148",
            "max": 184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01c07f3f5b7449d79832b32c1e1c1e9e",
            "value": 184
          }
        },
        "7fb718b9ec8440e7a22398988e2b287f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e770629fce44a394e127283314f2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "867acac12d3b48e1a16190d1592ff467": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86cdf5363ad64aae93b2e9ec61bd551a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_affa91f1e2774f6ba5f5e887a4d35ec5",
            "max": 1265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df865f4d7af346318133f2c268fb8f07",
            "value": 1265
          }
        },
        "89178dbfc8af4bd48c808af760f95f93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8930fac4b1fa48ed8623f0913483df41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a0e465bbd2446c8b7905a9af5bdb17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90a8fb4a685d4fb0859854800734f580": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "916f771306ce4443b8d5e8ae43dc67ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932b3ac3bae44c52836f8a3c3a31d275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9520c2b523544a6da8b35fd6a79c2cca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971ec3d353d840b3854abab3ab090b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97936cc6bf4241958b10cb6e2dbdd5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1db76f4a2eab4e0c84b4c2d07b1187f0",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e09a1b58273c40daa6b636e8cf689304",
            "value": 6
          }
        },
        "a023517fdb4741d3911ae3259df654f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3d4803fd7394eecaa37c7ff856a0f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdb8ae7efc214171844b2b4a42f5ff5c",
            "placeholder": "​",
            "style": "IPY_MODEL_5a168802bd2743458d27841652b34307",
            "value": " 40/40 [00:12&lt;00:00, 10.91it/s]"
          }
        },
        "a88492bcd6104eac8f6e723a50fbd1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ca9487ab6648099a839fac89c37e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03158e548ebb4c5691539aff3b4496dd",
            "placeholder": "​",
            "style": "IPY_MODEL_c893715f26a7424589d4259447a1a319",
            "value": "100%"
          }
        },
        "a945737a13624feb827f94404f1ea836": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a96c20cb8caa4fe28aec75c3570d5045": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d06c77c84dd4257a94b4550f095204b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8c8919dd330447e93a8861a34ae9477",
            "value": 4
          }
        },
        "ad221b5f5cb44a94977706bea8363e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_265e606c68ad44afa323b21db21a266f",
              "IPY_MODEL_97936cc6bf4241958b10cb6e2dbdd5c8",
              "IPY_MODEL_4b7f6ec2cc8d41e5b57aa0c39bc06422"
            ],
            "layout": "IPY_MODEL_7c2e6c17e47b416da789e852267777ed"
          }
        },
        "ad6c88b1bca84ecebfbe3253ad8cdc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae8281fe85464bfebcda36358a3b3d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af1868dcf14d464a851af10269bf8cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "affa91f1e2774f6ba5f5e887a4d35ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46ccff723c94aa184d6dc0d30fb36b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e8b5fd6f7c94258be451833ff50c71a",
            "placeholder": "​",
            "style": "IPY_MODEL_d02841089d2940aca79f91360f4309a4",
            "value": "100%"
          }
        },
        "b5b3c41bb94f4bbca0bb2e7fd307812f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db27dd052ca04e6e83cc96ac7c20b14e",
            "placeholder": "​",
            "style": "IPY_MODEL_2074d2142adf4bf2b9e3ef6336091812",
            "value": " 3/3 [00:01&lt;00:00,  1.93it/s]"
          }
        },
        "b69928128cb949938262471f229184b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88492bcd6104eac8f6e723a50fbd1d7",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b77494cefcd48a794f694842a16f453",
            "value": 6
          }
        },
        "b6bbc60984f247db9a5cba7afa1c7ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baef5fb01167411b9f439b9db400ade4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_003d692dee9d43d08004c2d1cc94ec1b",
            "placeholder": "​",
            "style": "IPY_MODEL_3634debf505443ed9b097c529075544f",
            "value": "Batches: 100%"
          }
        },
        "bbd40703ef414ef0a60f31c236bb2244": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc27ab45ee2148ceab12b3fba8951c14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd95884c84940b18524bbcb74afa883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0534758537c44a289d410d70c9424938",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d8fee751b3f4685b263538ad49a1100",
            "value": 71
          }
        },
        "bf780f802f68481398cd98dbab551267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfccabfc8003460795f883f525a197a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12130eeb1e4b4ca9a0f513f3a0fe1fc6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2210d51ee86a4aec8f8e91d4e7fd22cc",
            "value": 3
          }
        },
        "c206f87cf3354981b8260d7a7566f45c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e2f98129224f93a6a12ca2ff929283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2fa602f01249f38a603427d5f87c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_e1df7922686e4b7cbf7660a080ae186b",
            "value": " 71/71 [00:00&lt;00:00, 2220.35it/s]"
          }
        },
        "c893715f26a7424589d4259447a1a319": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8c8919dd330447e93a8861a34ae9477": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdb8ae7efc214171844b2b4a42f5ff5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d02841089d2940aca79f91360f4309a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3dad67e64af44e08705b6771324437f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6c2a9ed689d4550aad7005071f3edb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7bc8b01726f4632bb328c8c6b0c4d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a8fb4a685d4fb0859854800734f580",
            "placeholder": "​",
            "style": "IPY_MODEL_ae8281fe85464bfebcda36358a3b3d4e",
            "value": " 97/97 [00:00&lt;00:00, 2730.79it/s]"
          }
        },
        "d89b7cadc9194c0a82306d4d02ca8168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da306e6dd86247a0ae5d53d57ad82381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8ca9487ab6648099a839fac89c37e91",
              "IPY_MODEL_bdd95884c84940b18524bbcb74afa883",
              "IPY_MODEL_c2e2f98129224f93a6a12ca2ff929283"
            ],
            "layout": "IPY_MODEL_d6c2a9ed689d4550aad7005071f3edb8"
          }
        },
        "db27dd052ca04e6e83cc96ac7c20b14e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd433dacd764ca89aa0830330bddfa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16606d51ede748559c3d4d714299a582",
              "IPY_MODEL_6ea399690962481da4dee0c7a96dc959",
              "IPY_MODEL_a3d4803fd7394eecaa37c7ff856a0f94"
            ],
            "layout": "IPY_MODEL_7b15d6a8c78341dc9d4c2d90dd6eb4fe"
          }
        },
        "dce6cae3b6aa40f591d56e6fa1fedd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df865f4d7af346318133f2c268fb8f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e09a1b58273c40daa6b636e8cf689304": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1df7922686e4b7cbf7660a080ae186b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5b834706d5041589fe7777a7b2ef448": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e614e681c0499e9ce92e4b4c88f325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b46ccff723c94aa184d6dc0d30fb36b8",
              "IPY_MODEL_7ea7d1af7f574837869976ed8fb1e5d5",
              "IPY_MODEL_64dedb7900e540d8a396eed5f63275bd"
            ],
            "layout": "IPY_MODEL_394e04a2baed473c9c3a4addc8f4489a"
          }
        },
        "ea6424b92a6a49fda861fcaebb3d004a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efa228e28ad94688a62f459b52edbfa7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47356d978784df4b15e3805c25da741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e070f0208384cf7934c71b07cd7e88a",
            "max": 97,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a2b2b8d5512449a9a3b9d628320ea10",
            "value": 97
          }
        },
        "f70771c693cd4169ba3b9bae4d09d35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fd07ae167ad4db983a71edf32a561e6",
              "IPY_MODEL_a96c20cb8caa4fe28aec75c3570d5045",
              "IPY_MODEL_35395c2533c3499b849577a26d047d9e"
            ],
            "layout": "IPY_MODEL_d89b7cadc9194c0a82306d4d02ca8168"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
