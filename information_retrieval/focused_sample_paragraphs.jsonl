[{"paper_id": "#2598", "title": "A mixed methods study of Maryland's monetary incentives to improve child care.", "paragraphs": [" Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 Contents  lists  available  at  ScienceDirect Early Childhood Research Quarterly A   mixed-methods   study   of   Maryland\u2019s   monetary   incentives   to\nimprove   the   quality   of   child   care   centers Erica   S.   Lee U.S.  Department  of  Education,  Washington,  DC,  United  States a r t i c l e i n f o a b s t r a c t Article  history:\nReceived  29  August  2019\nReceived  in  revised  form\n29  December  2020\nAccepted  7  January  2021\nAvailable  online  13  February  2021 Keywords:\nChild  care  quality\nEarly  childhood  education\nQuality  Rating  and  Improvement  Systems\nPublic  incentives\nAccountability  systems\nMixed-methods This   study   used   a  sequential   explanatory   equal-status   mixed-method   design   to  investigate   whether\nMaryland\u2019s   child   care   tiered   reimbursement   system   incentivized   child   care   centers   to  be   rated   at  least\n3  (and   receive   an   incentive   payment)   on   Maryland\u2019s   5-level   Quality   Rating   and   Improvement   System\n(QRIS).   The   \ufb01rst   stage   of   research   consisted   of   multilevel   logistic   regressions   to  determine   the   association\nbetween   centers\u2019   reliance   on  child   care   subsidy   payments   and   whether   the  center   had   a  rating   of  3  or\nhigher.   Reliance   on   subsidy   payments   was   de\ufb01ned   as  the  percentage   of   licensed   slots   \ufb01lled   by   children\nreceiving   a  child   care   subsidy.   State  administrative   data   on   1003   centers   that   received   a  subsidy   pay-\nment   in   January   2018   were   combined   with   demographic   data   from   the  U.S.   Census.   The   second   stage   of\nresearch   consisted   of  14   interviews   with   center   directors   to  understand   how   they   made   decisions   about\nwhich   QRIS   rating   to   attain   and   how   tiered   reimbursements   factored   into   their   decisions.   Results   from\nthe   quantitative   research   showed   that   a  greater   subsidy   density   was   associated   with   a  greater   likeli-\nhood   of   a  center   being   rated   3  and   receiving   an   incentive   payment.   However,   results   from   the   qualitative\nresearch   showed   that   few   center   directors   reported   that   tiered   payments   factored   into   their   decision   on\nwhat   QRIS   rating   to   reach   and   no  directors   were   singularly   motivated   by   the   incentives.   Rather,   direc-\ntors   reported   being   intrinsically   motivated   to   improve   QRIS   ratings   or   motivated   by   technical   assistance\nproviders.   Additionally,   directors   who   did   not   attain   a  rating   of   3  experienced   capacity   challenges.   Policy\nimplications   are   discussed.", " \u00a9  2021   Elsevier   Inc.   All   rights   reserved.", " High-quality  early  care  and  education  programs,  particularly\npreschool,  can  improve  children\u2019s  school  readiness,  especially  for\ndisadvantaged  children  (Karoly  &  Auger,  2016;  Phillips  et  al.,  2017).\nHowever,  not  all  early  care  and  education  programs  are  of  high\nquality,  nor  are  all  programs  equally  effective  (Phillips  et  al.,  2017).\nRather,  low-income  children  receiving  child  care  subsidies  funded\nby  the  federal  Child  Care  Development  Fund  (CCDF)  are  often\nserved  by  low-quality  programs  (Jones-Branch,  Torquati,  Raikes,\n&  Pope  Edwards,  2004;  Raikes,  Raikes,  &  Wilcox,  2005).  Two   ways\nstates  can  incentivize  child  care  centers  to  improve  their  quality\nare  the  supports  available  through  participation  in  states\u2019  Quality\nRating  and  Improvement  Systems  (QRIS)  and  tiered  child  care  sub-\nsidy  reimbursements.  QRIS  are  state-run  rating  systems  designed\nto  assess,  improve,  and  communicate  the  quality  of  family-  and\ncenter-based  early  care  and  education  programs.  Tiered  reimburse-\nment  systems  are  state-run  systems  that  provide  higher  child  care\nsubsidies  to  higher  quality  programs,  often  based  on  reaching\nparticular  ratings  on  their  state\u2019s  QRIS.  Across  states,  tiered  reim- E-mail  address:  eslee@fcps.edu https://doi.org/10.1016/j.ecresq.2021.01.002\n0885-2006/\u00a9  2021  Elsevier  Inc.  All  rights  reserved.", " bursement  rates  vary  by  the  age  of  the  child,  the  type  of  program,\nand  the  program\u2019s  rating.  Of  U.S.  states,  39  have  at  least  one  QRIS\n(QRIS  Compendium,  2017)  and  38  states  have  a  tiered  reimburse-\nment  system  (Schulman  &  Blank,  2016).", " In  general,  incentive  systems  are  based  on  the  underlying\nassumption  that  people  are  entirely  self-interested  (Bowles,  2016).\nWhile  the  developers  of  state  QRIS  and  tiered  reimbursement  sys-\ntems  would  likely  agree  that  the  child  care  workforce  is  not  entirely\nmotivated  by  self-interest,  many  aspects  of  the  QRIS  systems  rest\non  the  basic  economic  theory  of  self-interest.  QRIS  assume  that\nchild  care  programs  will  be  incentivized  to  improve  quality,  in  part,\nbecause  parents  will  use  the  QRIS  public  ratings  to  choose  higher\nquality  programs.  Tiered  reimbursement  systems  assume  that  child\ncare  programs  will  respond  to  \ufb01nancial  incentives  by  changing  their\nbehavior  and  improving  their  quality.", " To  understand  whether  tiered  reimbursement  systems  work\nas  economic-incentive  theory  predicts,  this  study  used  a  sequen-\ntial  explanatory  equal-status  mixed-method  design  to  answer  the\nquestion,  Does  Maryland\u2019s  tiered  reimbursement  system  incen-\ntivize  child  care  centers  to  attain  a  rating  on  Maryland\u2019s  QRIS\nthat  results  in  a  higher  reimbursement  rate?  Maryland\u2019s  QRIS E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 (called  Maryland  EXCELS)  has  \ufb01ve  rating  levels,  and  the  state\nprovides  a  higher  reimbursement  rate  to  centers  that  attain  a\nrating  of  3  or  higher  in  the  QRIS.  Maryland  EXCELS  consists  of\n\ufb01ve  domains  containing  standards  covering  licensing,  staff  qual-\ni\ufb01cations,  accreditation,  curriculum,  and  administrative  policies.\nEXCELS  is  a  block  structure  in  which  all  quality  elements  across\ndomains  in  one  rating  level  must  be  met   before  a  center  may   receive\nthe  next  level\u2019s  rating.  In  2017,  17  of  39  states  had  block  QRIS  (QRIS\nCompendium,  2017).", " The  \ufb01rst  stage  of  research  consisted  of  multilevel  logistic  regres-\nsions  to  determine  the  association  between  child  care  centers\u2019\nreliance  on  subsidy  payments  and  whether  the  center  attained\na  rating  of  3  or  higher  in  Maryland  EXCELS,  which  would  result\nin  a  tiered  incentive  payment  called  an  EXCELS  payment.  To  bet-\nter  understand  whether  and  how  the  EXCELS  payment  was  a\nfactor  in  why  a  center  reached  EXCELS  Rating  3,  in  the  second\nstage  of  the  research,  14  child  care  center  directors  were  inter-\nviewed.  Interview  topics  included  the  EXCELS  rating  they  would\nlike  to  meet,  how  they  settled  on  that  goal,  challenges  to  improving\ntheir  EXCELS  rating,  and  how  EXCELS  payments  factored  into  their\ndecisions.", " Maryland  is  a  good  case  study  due  to  its  relatively  low  sub-\nsidy  rate,  its  relatively  large  incentive  at  EXCELS  Rating  3,  and\nits  requirement  that  programs  be  enrolled  in  EXCELS  to  receive\nsubsidy  payments.  Historically,  Maryland  has  had  relatively  low\nsubsidy  reimbursement  rates:  in  2016  it  was  one  of  13  states  that\nhad  rates  that  were  at  least  33%  lower  than  the  CCDF-recommended\n75th  percentile  of  market  rate  for  4-year-old  children  served  in\ncenters  (Schulman  &  Blank,  2016).  Speci\ufb01cally,  Maryland  set  its\nvoucher  rate  at  the  9th  percentile  of  market  rate  prices  from  June\n2016  to  June  2018  (i.e.,  the  rate  covers  9%  of  centers;  Maryland  State\nDepartment  of  Education  [MSDE],  2016).  With  relatively  lower  sub-\nsidy  reimbursement  rates  than  most  states,  centers  in  Maryland\nmay   be  especially  in  need  of  the  tiered  payment.  Like  only  seven\nother  states,  Maryland  offers  its  \ufb01rst  tiered  payment  at  Rating  3  of\nits  QRIS,  rather  than  at  Ratings  1  or  2.  EXCELS  payments  are  rather\nlarge  at  EXCELS  Rating  3:  22%  on  top  of  the  rate  for  centers  serv-\ning  children  under  the  age  of  2  and  an  additional  10%  for  centers\nserving  children  over  the  age  of  2  (MSDE,  2013a).  Finally,  Maryland\nis  one  of  eight  states  that  requires  that  particular  programs  partici-\npate  in  its  QRIS  to  serve  children  receiving  child  care  subsidy.  When\nintroducing  EXCELS  to  its  programs  in  2012,  Maryland  announced\nthat  all  programs  (including  family-based  programs)  that  serve  chil-\ndren  in  its  CCDF-funded  Child  Care  Subsidy  Program  (CCSP)  would\nbe  required  to  participate  in  EXCELS  by  July  2015.  The  require-\nment  to  participate  in  EXCELS  ameliorates  selection  bias  into  the\nQRIS  among  centers  that  serve  children  in  the  CCSP.  The  paucity\nof  selection  bias  combined  with  the  sharp  incentive  at  Rating  3\nmakes  Maryland  an  ideal  case  to  study  whether  its  tiered  reim-\nbursement  incentivizes  centers  participating  in  the  CCSP  to  reach\nEXCELS  Rating  3.", " A  validation  study  of  Maryland  EXCELS  examined  whether  a\nvariety  of  observation  tools  discriminated  among  the  \ufb01ve  EXCELS\nratings.  Observation  tools  included  the  Classroom  Assessment\nScoring  System  (CLASS),  Early  Childhood  Environment  Rating\nScale-Revised  (ECERS-R),  and  Family  Child  Care  Environment  Rat-\ning  Scale-Revised  (FCCERS-R).  The  study  found  that  the  CLASS  did\nnot  differentiate  among  EXCELS  ratings  and  fewer  than  half  of  the\nERS  subscales  differentiated  among  EXCELS  ratings.  Due  to  the  lack\nof  differentiation,  the  authors  of  the  validation  study  recommended\nthat  the  MSDE  consider  collapsing  EXCELS  Ratings  4  and  5  into  one\nrating  category  (Swanson  et  al.,  2017).  Maryland\u2019s  validation  study\ndid  not  assess  the  association  between  ratings  and  child  outcomes,\nthough  validation  studies  of  other  states\u2019  QRIS  generally  found  that\nhigher  QRIS  ratings  were  not  associated  with  developmental  gains\nfor  children  (Karoly,  2014;  Tout  et  al.,  2017).", " The  author  assumed  that  the  goal  of  the  Maryland\u2019s  tiered  child\ncare  reimbursement  system  is  to  improve  the  subsidized  quality\nof  care  in  Maryland  as  measured  by  EXCELS  rating.   For  example,\nwhether  the  care  provided  by  a  center  rated  3  is  demonstrably\nbetter  than  care  provided  by  a  center  rated  2,  Maryland\u2019s  tiered\nreimbursement  system  provides  an  incentive  for  a  center  rated  2  to\nimprove  to  a  rating  of  3.  Therefore,  this  study  analyzed  whether  the\nincentive  encouraged  centers  to  become  higher  quality  and  attain\nan  EXCELS  rating  of  3  or  higher  that  resulted  in  a  tiered  reimburse-\nment.", " 1.  Background 1.1.  Quality  Rating  and  Improvement  Systems In  the  late  1990s,  states  began  to  incentivize  child  care  pro-\ngrams  funded  through  the  CCDF  to  improve  quality  through  the\nuse  of  tiered  payments  that  typically  paid  higher  rates  for  accred-\nited  programs.  As  states  realized  how  dif\ufb01cult  it  was  for  programs\nto  become  accredited  by  national  accreditation  bodies,  they  devel-\noped  QRIS  to  help  programs  improve  in  manageable  steps.  One  state\nhad  a  QRIS  in  1997,  16  states  had  a  QRIS  in  2007,  and  39  states\nhad  a  QRIS  in  2017  (National  Center  on  Early  Childhood  Quality\nAssurance,  2017;  QRIS  Compendium,  2017).", " Low-quality  child  care  programs  could  remain  open  for  a  num-\nber  of  reasons,  including  well-meaning  programs  not  knowing\nhow  to  improve  their  services  and  parents\u2019  inability  to  discern\na  low-quality  program  from  a  high-quality  program.  The  former\npossibility  is  a  challenge  to  the  supply  of  the  child  care  market\nfunctioning  properly:  some  child  care  centers  would  improve  their\nquality  if  they  had  assistance  to  do  so.  The  latter  possibility  is\na  challenge  to  the  demand  of  the  child  care  market  functioning\nproperly:  parents  do  not  have  the  information  necessary  to  seek\nand  demand  high-quality  child  care.  QRIS  are  accountability  sys-\ntems  designed  to  improve  the  quality  of  child  care  by  de\ufb01ning\nquality  standards,  providing  incentives  and  supports  for  program\nimprovement,  and  making  quality  transparent  to  programs  and\nparents.  Zellman  and  Perlman  (2008)  developed  A  Logic  Model  for\nQRIS  that  emphasized  the  market-based  aspects  of  the  QRIS  system:\nInputs  and  outputs  include  programs  being  assessed  and  develop-\ning  improvement  plans.  Initial  and  intermediate  outcomes  include\nparents  learning  about  and  using  ratings.  Longer-term  outcomes\ninclude  improving  formerly  low-quality  programs  and  closing  per-\nsistently  low-quality  programs  that  become  undersubscribed.  In\nfact,  QRIS  have  been  found  to  improve  the  quality  of  care  and\nincrease  program  quality  over  time,  though  higher  QRIS  ratings,\ngenerally,  are  not  associated  with  developmental  gains  for  children\n(Karoly,  2014;  Tout  et  al.,  2017).", " Quality  of  care  (e.g.,  curriculum  and  learning  activities)  mat-\nters  to  parents,  but  parents  often  do  not  know  how  to  identify\na  high-quality  program  (Cryer  &  Burchinal,  1997).  Although  QRIS\ncan  aid  parents  in  identifying  high-quality  care,  parents  also  value\nother  factors  when  making  their  choice  of  care  (Corcoran  &  Steinley,\n2019;  Kim  &  Fram,  2009;  Mamedova  &  Redford,  2015;  Tout  et  al.,\n2011).  Parents  often  rely  on  friends  and  family  as  their  primary\nsource  of  information  about  programs  (National  Survey  of  Early\nCare  &  Education  Project  Team,  2014),  with  only  27%  of  parents\nreporting  that  ratings  on  a  website  were  \u201cvery  important\u201d  when\nselecting  a  care  arrangement  (Corcoran  &  Steinley,  2019).  While\nparents  may   not  place  high  importance  on  website  ratings  (QRIS  or\notherwise),  a  recent  analysis  found  that  Google  and  Yelp  reviews  of\nearly  care  and  education  programs  in  Georgia  were  correlated  with\nratings  on  Georgia\u2019s  QRIS  (Early  &  Li,  2020).  Additionally,  Bassok,\nDee,  and  Latham  (2017)  found  that  parents  were  less  willing  to\nenroll  children  in  programs  that  had  a  lower  QRIS  rating.  In  sum, E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 although  recent  research  found  that  parents  might  be  using  QRIS\nto  make  care  decisions,  it  is  likely  that  the  demand  side  of  the\nQRIS  theory  is  not  functioning  as  theory  predicts,  adding  urgency\nto  the  need  to  study  the  supply  side  of  QRIS  theory,  including  tiered\nreimbursement  systems.", " 1.2.  Tiered  child  care  reimbursement  systems Despite  researchers  noting  the  importance  of  studying  the  rela-\ntionship  between  a  state\u2019s  tiered  reimbursement  system  and  the\nratings  of  the  programs  caring  for  children  receiving  subsidies  (e.g.,\nAdams,  Snyder,  &  Tout,  2003;  Blau,  2007;  Forry,  Daneri,  &  Howarth,\n2013;  Rigby,  Ryan,  &  Brooks-Gunn,  2007),  little  published  research\nanalyses  this  question.  Greenberg,  Isaacs,  Derrick-Mills,  Michie,  and\nStevens  (2018)  examined  whether  state  subsidy  payment  rates  and\nprogram-friendly  policies  were  associated  with  center\u2019s  having  a\nquality  rating.  Quality  rating  was  measured  by  a  question  on  the\n2012  National  Survey  of  Early  Care  and  Education  that  asked  cen-\nter  directors,  \u201cDoes  your  organization  have  an  overall  quality  rating\n(for  example,  accreditation,  tiered  reimbursement  or  some  other\nquality  rating  system)?\u201d  (Of\ufb01ce  of  Planning,  Research  &  Evaluation,\n2011).  The  result  was  that  a  $100  difference  between  monthly  pay-\nments  in  the  lowest  and  highest  tiers  of  a  tiered  reimbursement\nsystem  was  associated  with  63%  higher  likelihood  of  centers  earn-\ning  a  quality  rating.  Cannon,  Zellman,  Karoly,  and  Schwartz  (2017)\nrecommended  that  states  adequately  invest  in  tiered  reimburse-\nment  systems  to  ensure  that  the  incremental  reimbursement  tied  to\neach  rating  covers  the  cost  of  higher  quality  care.  Given  the  dearth\nof  research  on  tiered  reimbursement  systems,  states  would  also\nneed  to  invest  in  evaluations  to  understand  whether  their  tiered\nreimbursement  system  is  appropriately  funded.  If  the  tiered  reim-\nbursements  are  too  small,  it  may   not  be  cost-effective  for  most\nprograms  to  improve  quality:  providing  high-quality  care  is  costly.\nIn  2000,  before  it  was  common  for  states  to  have  QRIS,  18  states\n(not  Maryland)  offered  higher  child  care  subsidy  reimbursement\nrates  for  child  care  programs  that  were  nationally  accredited.  The\nrate  differential  ranged  from  an  additional  5\u201320%  of  the  base  sub-\nsidy  rate  across  states.  A  study  that  examined  data  from  child\ncare  centers  in  10  states  that  applied  for  accreditation  from  the\nNational  Association  for  the  Education  of  Young  Children  found\nmixed  results  (positive  or  null)  for  whether  the  introduction  of\nthe  differential  reimbursement  rates  resulted  in  an  increase  in  the\nnumber  of  centers  applying  for  accreditation.  Many  states  offered\nworkshops  and  onsite  support  to  help  with  the  accreditation  pro-\ncess  (Gormley  &  Lucas,  2000).  Interviews  with  a  small  sample  of\nprograms  across  states  showed  that  programs  were  skeptical  that\nthe  cost  of  advancing  in  a  tiered  reimbursement  system  (e.g.,  offer-\ning  longer  hours  or  earning  accreditation)  was  worth  the  extra\npayment  (Adams  et  al.,  2003).", " 1.3.  Incentive  systems In  general,  incentive  systems  have  been  found  to  improve  the\nmeasured  performance  (Cameron,  Banko,  &  Pierce,  2001;  Shaw  &\nGupta,  2015;  Strechter  et  al.,  2010)  and  to  not  hurt  intrinsic  motiva-\ntion  (Cameron  et  al.,  2001;  Shaw  &  Gupta,  2015).  In  addition,  when\nrewards  are  offered  for  meeting  or  surpassing  a  score  on  a  task  of\nhigh  interest,  a  positive  effect  emerged  on  task  interest  (Cameron\net  al.,  2001):  \u201cOne  possible  explanation  for  the  positive  effect  of\nthis  type  of  reward  contingency  is  that  rewards  signify  competence,\nself-ef\ufb01cacy,  or  ability  at  the  task,  and  people  enjoy  doing  activities\nthat  re\ufb02ect  their  competence\u201d  (p.  23).", " James  (2005)  developed  a  principal-agent  model  that  showed\nthat  if  incentives  from  the  principal  to  the  agent  for  a  given  behav-\nior  are  seen  as  controlling,  the  agent\u2019s  intrinsic  motivation  could\nbe  crowded  out  by  the  incentive.  James\u2019s  model  demonstrated  two conditions  under  which  an  incentive  system  could  be  seen  as  con-\ntrolling  and  potentially  crowd  out  intrinsic  behavior:  (a)  incentives\nare  large  or  (b)  the  intrinsic  motivation  is  also  the  source  of  the\nincentive  (e.g.,  a  child  care  program  is  intrinsically  motivated  to  act\nin  the  interest  of  the  state,  and  the  state  then  introduces  incentives).\nThe  model  also  showed  that  the  higher  one\u2019s  intrinsic  motiva-\ntion,  the  harder  it  is  for  an  incentive  system  to  crowd  out  one\u2019s\nintrinsic  motivation.  Applying  James\u2019s  theory  to  Maryland\u2019s  tiered\nreimbursement  system,  it  is  unlikely  that  child  care  centers  are\nmotivated  to  provide  child  care  because  of  the  state\u2019s  interest  and\nthe  EXCELS  payments  are  not  unreasonably  large.  In  fact,  a  random\nsample  of  teachers  in  child  care  centers  in  four  midwestern  states\nfound  that  they  identi\ufb01ed  with  their  work  as  a  \u201cpersonal  calling\u201d\n(Torquati,  Raikes,  &  Huddleston-Casas,  2007,  p.  266),  indicating  that\nit  is  unlikely  that  center  directors\u2019  intrinsic  motivation  to  provide\nquality  care  would  be  crowded  out  by  the  tiered  reimbursement\nsystem.  Further,  if  center  directors  want  to  be  viewed  as  providing\nhigh-quality  care,  the  tiered  reimbursement  system  provides  the\nopportunity  to  be  viewed  as  such.", " A  speci\ufb01c  type  of  performance-based  incentive  system  that  has\nsimilarities  to  QRIS  is  a  health  care  incentive  system.  A  review  of\n17  studies  published  through  2005  found  that  most  found  partial\nor  positive  effects  of  \ufb01nancial  incentives  on  measures  of  health\ncare  quality  (Petersen,  Woodard,  Urech,  Daw,  &  Sookanan,  2006).\nSimilarly,  a  Cochrane  Collaboration  systematic  review  found  that\n\ufb01nancial  incentives  generally  improved  aspects  of  care,  but  little\nevidence  emerged  for  improved  patient  outcomes  (Flodgren  et  al.,\n2011).  After  reviewing  the  evidence  on  incentives  and  describing\nnew  incentives  under  the  Affordable  Care  Act,  Doran,  Mauer,  and\nRyan  (2017)  noted  that  it  is  crucial  to  continue  improving  the  design\nof  incentive  programs.", " Additionally,  consumers  will  choose  health  plans  based,  in\npart,  on  quality  ratings  without  incentives.  For  example,  a  study\nof  ratings  disseminated  by  the  National  Committee  for  Quality\nAssurance  to  employees  choosing  a  health  plan  found  that  the\nratings  had  a  meaningful  in\ufb02uence  on  employees\u2019  choices,  par-\nticularly  for  individuals  choosing  a  plan  for  a  \ufb01rst  time  (Jin  &\nSorensen,  2005).  Similarly,  a  study  of  health-management  orga-\nnization  (HMO)  report  cards  in  1999  and  2000  sent  to  40  million\nMedicare  enrollees  found  that  consumers  used  the  report  cards  and\nmarket-based  sources  when  choosing  an  HMO   (Dafny  &  Dranove,\n2008).  Both  studies  noted  that  friends  and  family  are  often  a  source\nof  information  when  choosing  a  health  plan.", " 1.4.  Key  features  of  Maryland  EXCELS Between  2009  and  2011,  the  MSDE  Division  of  Early  Childhood\nDevelopment  worked  with  experts  to  develop  Maryland  EXCELS,  a\nQRIS  for  family-  and  center-based  child  care  programs.  Maryland\nEXCELS  contains  standards  that  cover  \ufb01ve  core  disciplines:  Licens-\ning  and  Compliance  (three  standards),  Staff  Quali\ufb01cations  and\nProfessional  Development  (one  standard),  Accreditation  and  Rating\nScales  (three  standards),  Developmentally  Appropriate  Learning\nand  Practice  (nine  standards  [only  six  standards  for  afterschool\ncenters]),  and  Administrative  Policies  and  Practices  (15  standards;\nMaryland  EXCELS,  n.d.).  Requirements  to  meet  each  standard\nincrease  for  each  of  the  \ufb01ve  EXCELS  rating  levels.  A  program  sub-\nmits  documentation  through  an  online  system  to  meet  each  of  the\nstandards.  A  program\u2019s  ratings  on  each  of  the  \ufb01ve  disciplines  are\navailable  to  the  public  on  the  Maryland  EXCELS  website,  though\na  program\u2019s  overall  rating  is  equal  to  the  lowest  of  its  individual\nratings,  as  EXCELS  is  a  block  structure.", " Despite  fewer  standards  in  the  \ufb01rst  three  disciplines  compared\nto  the  last  two,  the  \ufb01rst  three  disciplines  have  some  key  features.  A\ncenter  will  receive  a  rating  of  1  in  Licensing  and  Compliance  (and  for\ntheir  overall  rating)  if  the  center  has  had  more  than  one  inspection E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 in  the  last  12  months  with  \ufb01ndings  of  noncompliance  in  Injuri-\nous  Treatment;  Child  Protection;  Supervision;  or  Capacity,  Group\nSize,  and  Staf\ufb01ng.  Once  12  months  has  passed  from  the  noncompli-\nant  inspection,  the  center  can  republish  its  original  EXCELS  rating.\nThe  Staff  Quali\ufb01cations  and  Professional  Development  standard\nrequires  that,  beginning  at  a  rating  of  2,  60%  of  lead  staff  hold  certain\ncredentials,  with  the  type  of  credential  increasing  at  each  rating.\nFinally,  Accreditation  and  Rating  Scales  standards  require  that  a\ncenter  rated  4  complete  an  accreditation  self-study  and  request  a\nvalidation  visit  from  an  accreditor.  Only  fully  accredited  centers\nmay   attain  a  rating  of  5  in  Accreditation  and  Rating  Scales;  thus,  a\ncenter  may   not  attain  an  EXCELS  rating  of  5  without  accreditation.\nIn  2012,  MSDE  announced  that,  beginning  on  July  1,  2015,  pro-\ngrams  needed  to  be  enrolled  in  Maryland  EXCELS  to  receive  a  CCSP\nreimbursement  for  child  care  services.  EXCELS  started  as  a  volun-\ntary  system  on  July  1,  2013,  and  MSDE  and  its  partners  had  2  years\nto  ensure  all  programs  that  received  CCSP  reimbursements\u2014or\nwanted  to  receive  them  in  the  future\u2014were  enrolled  in  Maryland\nEXCELS  before  the  new  requirement  went  into  effect.  MSDE\u2019s  out-\nreach  efforts  appear  to  have  been  successful:  Of  centers  licensed\nin  January  2018,  only  18  centers  (2%)  that  received  a  CCSP  reim-\nbursement  in  January  2015  were  not  participating  in  EXCELS  in\nJuly  2015,  indicating  the  centers  could  not  have  received  a  CCSP\nreimbursement  the  month  the  participation  requirement  started.\nOnce  enrolled  in  EXCELS,  programs  have  1  year  to  meet,  at  mini-\nmum,   all  rating  1  standards  in  each  discipline  and  to  publish  their\ninitial  rating  on  the  public  EXCELS  website.  If  a  program  does  not\npublish  a  rating  within  1  year,  its  status  is  changed  to  \u201cnot  partic-\nipating.\u201d  Ratings  last  for  1  year.  Programs  must  annually  con\ufb01rm\nthey  want  to  republish  their  EXCELS  rating,  though  the  con\ufb01rma-\ntion  process  does  not  require  centers  to  upload  additional  materials\nto  the  EXCELS  system.  Of  centers  that  received  a  CCSP  reimburse-\nment  in  January  2018,  only  21  centers  (2%)  had  switched  from\nparticipating  in  EXCELS  to  not  participating  and  then  back  to  par-\nticipating.  MSDE  works  closely  with  programs  to  remind  them  to\npublish  their  annual  EXCELS  ratings,  as  programs  cannot  serve  chil-\ndren  in  the  CCSP  unless  it  annually  republishes  its  rating.  At  any\ntime,  programs  can  provide  additional  documentation  to  increase\ntheir  rating.  See  Table  1  for  a  description  of  key  EXCELS  dates  and\ncenter  participation  data,  including  that  the  number  of  centers  that\nreceived  a  CCSP  payment  and  were  higher  quality  increased  from\n73  to  205  between  July  2015  and  January  2018.  Note  that  once  a\ncenter  researched  an  EXCELS  rating  of  3  or  higher,  centers  typically\nretained  a  higher  quality  rating:  only  5%  that  were  higher  quality\nin  July  2015  were  not  higher  quality  in  January  2018.", " MSDE  supports  three  types  of  technical  assistance  to  programs\nto  improve  the  quality  of  care  and  meet  EXCELS  standards:  Program\nCoordinators,  Quality  Assurance  Specialists  (QAS),  and  Technical\nAssistance  Specialists  employed  by  regional  Child  Care  Resource\nCenters  (CCRCs).  Program  Coordinators  help  programs  navigate  the\nEXCELS  website,  review  all  materials  submitted  to  meet  a  certain\nstandard,  verify  or  ask  for  additional  information,  and  provide  a\nrating  based  on  submitted  material  (MSDE,  2017a).  QAS  provide\ntraining  on  how,  speci\ufb01cally,  programs  can  meet  EXCELS  stan-\ndards.  QAS  provide  this  training  through  monthly  workgroups  and\none-on-one  telephone  or  in-person  support.  Technical  Assistance\nSpecialists  employed  by  the  CCRCs  provide  one-on-one  technical\nassistance  to  help  programs  use  research-based  practices  that  will\nimprove  the  quality  of  child  care.  QAS  and  CCRC  staff  try  to  coor-\ndinate  technical  assistance  to  have  the  CCRC  technical  assistance\nproviders  focus  on  improving  quality  and  the  QAS  focus  on  how  to\ndocument  that  quality  to  meet  EXCELS  standards.", " Another  role  of  a  QAS  is  to  conduct  scheduled  monitoring  visits\nto  programs  participating  in  EXCELS  to  assess  whether  the  pro-\ngram  is  meeting  EXCELS  standards  for  10  observable  standards.  A\nQAS  outside  of  the  program\u2019s  region  uses  the  program\u2019s  uploaded materials  and  observes  whether  policies  are  being  implemented  as\nstated.  Monitoring  visits  do  not  lower  a  program\u2019s  rating.  Rather,\nthe  regional  QAS  assigned  to  the  program  uses  the  monitoring\nreport  to  create  an  improvement  plan  to  increase  quality.  Monitor-\ning  visits  started  in  2017  with  QAS  monitoring  25  programs  (chosen\nat  random)  every  other  month,  increasing  to  45  programs  per  cycle\nthe  following  year.", " MSDE\u2019s  marketing  and  outreach  campaign  to  make  parents\naware  of  EXCELS  has  included  staf\ufb01ng  information  booths  at  county\nfairs,  preschool  fairs,  and  health  expositions;  partnering  with  the\nBaltimore  Orioles  and  minor  league  baseball  teams;  advertising  on\nbillboards,  buses,  and  radio;  and  providing  window  stickers  and\nyard  signs  to  EXCELS  participants  (Swanson  et  al.,  2017).", " 1.5.  Key  features  of  Maryland\u2019s  Child  Care  Subsidy  Program In  each  month  of  \ufb01scal  year  2016,  Maryland  served  approxi-\nmately  18,500  families  and  24,600  children  through  its  CCSP,  which\nis  how  Maryland  administers  the  $88  million  in  federal  funds  it\nreceived  though  the  CCDF  (Of\ufb01ce  of  Child  Care,  2016,  2018).1 Sim-\nilar  to  most  states,  families  that  wish  to  use  Maryland\u2019s  \u201cChild\nCare  Scholarships\u201d  must  apply  to  MSDE  and  provide  proof  of  iden-\nti\ufb01cation,  income,  and  employment.  Unlike  many  states,  MSDE\nalso  requires  veri\ufb01cation  of  household  composition  and  immuniza-\ntion  records.  Families  must  re-certify  after  12  months  and  supply\nnew  documentation  (Tran,  Minton,  Halder,  &  Dwyer,  2018).  Once  a\nfamily  secures  a  scholarship  and  chooses  a  program,  it  is  the  respon-\nsibility  of  the  program  to  request  payment  from  MSDE   for  serving\nthe  child  in  the  CCSP.", " Although  the  CCDF  program  encourages  states  to  set  vouchers\nat  the  75th  percentile  of  market-rate  prices,  Maryland  decided  to\nserve  more  children  with  smaller  vouchers  and  its  voucher  rates\nwere  set  at  the  10th  percentile  of  market-rate  prices  in  2014  and\n2015  and  the  9th  percentile  of  market-rate  prices  in  2016  through\nJune  2018  (MSDE,  2013b,  2016).  Beginning  in  July  2018,  Maryland\nincreased  its  payment  rate  to,  at  a  minimum  per  subsidy-payment\nregion,  the  20th  percentile  of  market  rate  prices  (MSDE,  2018).  As\nan  example  of  what  these  percentiles  mean  in  practice,  the  9th  per-\ncentile  would  result  in  a  maximum  monthly  reimbursement  for\na  2-year-old  in  a  child  care  center  in  Howard  County  and  Mont-\ngomery  County  of  $708  and  the  20th  percentile  would  result  in\na  reimbursement  of  $866  (MSDE,  2017b).  Additionally,  2018  leg-\nislation  required  that  the  percentile  of  subsidy  reimbursement,  at\nminimum  per  subsidy  payment  region,  reach  the  30th  percentile  in\n2020,  the  45th  percentile  in  2021,  and  the  60th  percentile  in  2022\n(MSDE,  2018).", " Maryland  began  offering  a  tiered  reimbursement  payment  to\nprograms  in  2001,  based  on  a  rating  system  that  predated  EXCELS\n(Of\ufb01ce  of  Planning,  Research,  and  Evaluation,  2010).  Beginning  on\nJuly  1,  2013,  the  tiered  reimbursements  were  based  on  EXCELS,\nwith  child  care  centers  rated  3,  4,  or  5  receiving  a  certain  percent-\nage  of  additional  payment  from  the  state.  Speci\ufb01cally,  centers  that\nserve  children  under  age  2  receive  an  additional  22%,  37%,  and  44%\nof  their  base  rate  for  achieving  a  rating  of  3,  4,  or  5,  respectively.\nCenters  that  serve  children  aged  2  or  over  receive  an  additional\n10%,  19%,  and  26%  of  their  base  rate  for  having  a  rating  of  3,  4,\nor  5,  respectively.  A  center  has  no  restrictions  on  what  it  may  do\nwith  the  additional  money,  though  MSDE  encourages  centers  to  put\nthe  money  into  the  center,  including  by  offsetting  the  cost  of  child\ncare  for  families  receiving  a  subsidy,  providing  salary  and  bene- 1 Maryland  does  not  offer  other  Child  Care  Subsidy  Programs,  though  it  does  offer\nHead  Start  programs  and  public  preschool.  Head  Start  and  public  preschools  can\nparticipate  in  EXCELS,  but  unless  they  are  also  licensed  child  care  providers  that\nreceive  CCSP  payments,  those  programs  would  not  be  included  in  this  study.", " E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 Table   1\nKey  dates  and  participation  in  Maryland  EXCELS.", " Date Event July  2013 Number  of  licensed\ncenters Number  of  centers\nparticipating  in  EXCELS Number  of  centers  that\nreceived  a  CCSP\npayment Number  of  centers  that\nreceived  a  CCSP\npayment  and  were\nhigher  quality EXCELS  opened  for\nvoluntary  enrollment\nand  CCSP  tiered\npayments  were  tied  to\nEXCELS  ratings\nCenters  that  receive\nCCSP  payments  must\nparticipate  in  EXCELS\nMSDE  provided  data\nfor  research  study July   2015 January  2018 Note:  Data  were  provided  from  years  prior  to  2018  only  for  those  centers  that  were  licensed  in  January  2018.  Therefore,  the  counts  in  July  2013  and  July  2015  may   be\nundercounts.  A  \u201chigher  quality\u201d  center  has  an  EXCELS  rating  of  3,  4,  or  5  and  would  receive  a  tiered  reimbursement,  EXCELS  =  Maryland\u2019s  Quality  Rating  and  Improvement\nSystem,   CCSP  =  Child  Care  Subsidy  Program,  MSDE  =  Maryland  State  Department  of  Education.", " \ufb01t  enhancements  to  staff,  or  making  quality  improvements  to  the\nprogram  (MSDE,  2013a).", " that  joined  EXCELS  in  July  2015  to  complete  all  requirements  to\nreach  an  EXCELS  rating  of  3.", " Despite  tiered  reimbursement  systems  existing  for  2  decades,\nvery  little  research  exists  on  how  programs  respond  to  the  sys-\ntems  (Adams  et  al.,  2003;  Gormley  &  Lucas,  2000;  Greenberg  et  al.,\n2018).  This  study  is  the  \ufb01rst  in-depth  look  at  how  a  tiered  reim-\nbursement  system  functions  in  a  single  state.  This  study  contributes\nto  the  \ufb01eld  by  determining  whether  a  tiered  reimbursement  sys-\ntem  functions  as  theory  predicts,  by  using  a  sequential  explanatory\nequal-status  mixed-method  design  to  answer  the  question,  Does\nMaryland\u2019s  tiered  reimbursement  system  incentivize  child  care\ncenters  to  attain  a  rating  on  Maryland\u2019s  QRIS  that  results  in  a  higher\nreimbursement  rate?", " 2.  Methods In  the  \ufb01rst  stage  of  the  study,  multilevel  analyses  were  con-\nducted  to  determine  the  association  between  centers\u2019  reliance  on\nCCSP  payments  and  whether  the  center  had  an  EXCELS  rating  of  3\nor  higher.  In  the  second  stage  of  the  study,  child  care  center  direc-\ntors  in  \ufb01ve  counties  were  interviewed  to  understand  how  they\nmade  decisions  about  which  EXCELS  rating  to  attain,  how  tiered\nreimbursements  factored  into  their  decisions,  supports  they  used\nto  improve  their  EXCELS  rating,  and  challenges  to  improving  their\nEXCELS  rating.", " Leech  and  Onwuegbuzie  (2009)  de\ufb01ned  mixed-methods  stud-\nies  as  those  that  collect,  analyze,  and  interpret  quantitative  and\nqualitative  data  in  a  single  study.  This  study  is  a  sequential  explana-\ntory  equal-status  design  in  which  quantitative  data  were  analyzed\nand  informed  the  selection  of  center  directors  for  the  qualitative\ndata  collection,  qualitative  data  were  then  analyzed,  and,  \ufb01nally,\nall  data  were  interpreted  together  (as  suggested  by  Creswell,  Plano\nClark,  Gutmann,  &  Hanson,  2003).  This  study  has  an  \u201cequal-status\u201d\ndesign  because  neither  the  quantitative  or  qualitative  stages  of  the\nstudy  were  prioritized.  Rather,  all  data  were  interpreted  together\nto  answer  the  research  question.  This  study  was  approved  by  the\nUniversity  of  Maryland  College  Park  Institutional  Review  Board.", " 2.1.  Quantitative  stage 2.1.1.  Data MSDE  provided  the  author  with  administrative  data  for  all  cen-\nters  that  were  licensed  by  MSDE\u2019s  Division  of  Early  Childhood  in\nJanuary  2018,  including  data  for  the  key  independent  variable  (i.e.,\nsubsidy  density)  and  dependent  variable  (i.e.,  EXCELS  ratings  that\nindicated  higher  quality  status).  January  2018  data  are  appropriate\nto  answer  the  research  question  as  it  allows  2.5  years  for  centers For  all  centers,  the  author  received  data  on  their  address,  county,\nand  year  \ufb01rst  licensed.  For  all  centers,  for  each  of  July  2013,  January\nand  July  2014,  January  and  July  2015,  January  and  July  2016,  January\nand  July  2017,  and  January  2018,  the  author  received  licensed\ncapacity  (by  \ufb01ve  age  groups),  EXCELS  participation,  overall  EXCELS\nrating,  and  number  of  children  using  the  CCSP  (sorted  by  under  the\nage  of  2  and  age  2  and  older).  MSDE  data  contained  \ufb01ve  categories\nfor  licensed  capacity,  though  the  \ufb01rst  two  categories  (6  weeks\u201317\nmonths  and  18\u201323  months)  were  combined  in  the  analyses  and  last\nthree  categories  (2\u20135  years,  5\u201315  years,  and  16+)  were  combined  in\nthe  analyses,  to  represent  the  age  groups  with  different  EXCELS  pay-\nments.  To  create  the  covariate  licensed  slots  per  children  under  the\nage  of  5  for  each  zip  code,  the  number  of  licensed  slots  for  children\naged  6  weeks  through  aged  5,  not  yet  in  kindergarten,  were  divided\nby  the  number  of  children  under  the  age  of  5  living  in  the  zip  code\n(from  the  U.S.  Census  Bureau\u2019s  2016  American  Community  Survey;\nU.S.  Census  Bureau,  2011).  This  variable  overrepresented  the  num-\nber  of  slots  per  child,  as  the  numerator  included  slots  licensed  for\n5-year-old  children  still  in  preschool,  but  the  denominator  did  not\ninclude  children  aged  5.  A  dummy   variable  for  afterschool  programs\nwas   created  by  identifying  centers  that  were  only  licensed  to  serve\nchildren  ages  5\u201315.  A  dummy   variable  for  whether  the  center  was\nrated  3  or  higher  in  July  2015  was  created  to  control  for  the  fact  that\ncenters  that  were  already  higher  quality  when  the  requirement  to\nparticipate  in  EXCELS  started  likely  have  different  motivations  than\nother  centers  in  the  sample.  The  2016  American  Community  Sur-\nvey  provides  5-year  estimates  used  for  zip  code  level  data  on  age,\npoverty,  race/ethnicity,  women   aged  25  or  older  with  a  bachelor\u2019s\ndegree,  women  aged  20\u201364  in  the  labor  force,  and  the  unemploy-\nment  rate.  Urbanicity  data  was   from  the  2010  Census  (U.S.  Census\nBureau,  2018).", " 2.1.2.  Analysis The  independent  variable  of  interest  was  a  center\u2019s  \u201csubsidy\ndensity,\u201d  calculated  as  the  number  of  children  participating  in  the\nCCSP  divided  by  the  center\u2019s  licensed  capacity,  multiplied  by  100.\nThis  calculation  assumes  that  all  licensed  capacity  slots  are  eligi-\nble  to  be  \ufb01lled  by  children  in  the  CCSP.  Licensed  slots  not  \ufb01lled  by\nchildren  in  the  CCSP  could  be  \ufb01lled  by  fee-paying  children  or  the\nslots  could  be  vacant.  If  the  slots  are  all  \ufb01lled  by  fee-paying  children,\nthe  subsidy  density  variable  will  be  a  true  re\ufb02ection  of  the  center\u2019s\nreliance  on  CCSP  payments.  For  example,  if  a  center  enrolls  mostly\nwell-off  children  not  enrolled  in  the  CCSP,  and  only  a  few  children\nin  the  CCSP,  economic  incentive  theory  would  predict  that  the  cen-\nter  would  be  less  incentivized  than  a  center  with  higher  subsidy E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 density  to  reach  an  EXCELS  rating  of  3  or  higher  and  receive  larger\nCCSP  payments,  holding  all  else  equal.  Recall  that  research  indicates\nthat  the  \u201cdemand\u201d  side  of  the  QRIS  may   not  be  functioning  as  theory\npredicts,  so  programs  are  likely  not  motivated  to  improve  EXCELS\nratings  only  based  on  parent  demand.", " If  the  slots  that  are  unused  by  children  in  the  CCSP  are  vacant,\nthis  study\u2019s  de\ufb01nition  of  subsidy  density  is  likely  lower  than  if  the\nvariable  could  have  been  calculated  with  a  denominator  of  enroll-\nment.  For  21  child  care  centers,  the  de\ufb01nition  of  CCSP  density  is\nover  100%  and  certainly  higher  than  if  the  variable  could  have\nbeen  calculated  with  a  denominator  of  enrollment.  A  center  could\nenroll  more  children  than  its  licensed  capacity  if  all  children  are\nnot  attending  for  5  full  days  each  week.  Subsidy  density  variables\nwere  also  created  for  January  2018  for  the  two  age  categories  with\ndiffering  tiered  reimbursement  rates:  younger  than  2  years  and  2\nyears  and  older.", " To  quantitatively  test  the  hypothesis  that  tiered  reimburse-\nments  incentivize  centers  to  reach  a  rating  of  3,  two-level  logistic\nregressions  were  conducted  with  child  care  centers  (Level  1)  nested\nin  counties  (Level  2)  for  centers  with  subsidy  densities  greater  than\n0.0%  in  January  2018  (n  =  1003).  As  child  care  centers  that  do  not\nreceive  CCSP  funds  are  not  required  to  participate  in  EXCELS,  those\nthat  chose  to  participate  in  EXCELS  may   be  quite  different  from\ncenters  that  do  not  participate  in  EXCELS,  as  well  as  different  from\ncenters  that  serve  children  in  the  CCSP.  Counties  are  the  primary\npolicy  unit  in  Maryland,  with  counties  having  different  MSDE-\nfunded  technical  assistance  contacts  and  cost-of-living-adjusted\nCCSP  reimbursement  rates,  making  counties  an  important  unit  of\nanalysis.  As  there  was  only  one  center  in  more  than  a  quarter  of  zip\ncodes,  Level  2  modeling  was  carried  out  at  the  county  level,  though  a\n3-level  robustness  analysis  with  centers  nested  in  zip  codes  and  zip\ncodes  nested  in  counties  provided  similar  results  to  those  reported\nbelow.  Zip  code  information  was  included  at  level  1  and  robust\nstandard  errors  were  used.  The  dependent  variable  is  the  probabil-\nity  that  a  given  center  in  a  given  county  had  an  EXCELS  rating  of  3  or\nhigher  (i.e.,  the  center  was   \u201chigher  quality\u201d2),  the  key  independent\n(continuous)  variable  is  a  center\u2019s  subsidy  density,  and  a  host  of\ncenter  and  location  covariates,  including  measures  of  poverty  and\nmaternal  education,  controlled  for  observed  differences  between\ncenters.", " The  basic  Model  1  is  a  two-level  model  that  nests  centers  in\ncounties  and  does  not  include  any  covariates.  Model  2  includes\ncenter  covariates  and  Model  3,  the  main  model,  adds  zip  code\ncovariates  at  Level  1.  While  Level  2  of  the  model  is  unspeci\ufb01ed,\nRaudenbush  and  Bryk  (2002)  argue  that  2-level  models  that  use\ngroup-mean  centering  produce  within  group  coef\ufb01cients  indepen-\ndent  of  the  variance  between  groups  \u2013  in  this  case,  counties  and  zip\ncode  locations.  Model  3  addressed  whether,  on  average,  serving  a\nhigher  percentage  of  children  receiving  subsidies  is  associated  with\na  greater  likelihood  of  attaining  an  EXCELS  rating  of  3  or  higher.  As\ntiered  reimbursements  also  vary  by  the  age  of  the  children  served,\nModel  4  replaced  the  subsidy  density  variable  with  variables  that\nrepresent  subsidy  density  by  the  age  bands  with  different  reim-\nbursement  rates:  under  2  years  and  2  years  and  older  (see  Table  2).\nThree  robustness  analyses  with  different  dependent  or  key  inde-\npendent  variables  were  run.  In  Model  5,  subsidy  density  was  capped\nat  100%,  reducing  the  subsidy  density  of  21  centers  (maximum  of\n445.5%).  Although  it  is  possible  for  a  center  to  have  a  subsidy  den-\nsity  of  over  100%,  this  capped  independent  variable  reduced  the\npull  of  outliers  and  helped  correct  any  data  errors.  In  fact,  one  of\nthe  interviewed  child  care  center  directors  worked  at  a  center  that\nwas  part  of  a  small  franchise  of  three  centers.  In  the  interview,  they 2 Centers  that  were  participating  in  EXCELS  and  had  not  yet  published  a  rating were  grouped  with  \u201clower  quality\u201d  centers  that  had  EXCELS  ratings  of  1  or  2.", " reported  that  each  of  two   locations  had  about  50%  of  children  par-\nticipating  in  the  CCSP,  but  MSDE  administrative  data  showed  that\none  of  the  centers  had  zero  children  participating  in  the  CCSP  and\nthe  other  center  had  more  children  in  the  CCSP  than  it  was  licensed\nto  serve.  In  Model  6,  an  average  subsidy  density  variable  was  cre-\nated  that  averaged  subsidy  density  for  those  centers  that  served  at\nleast  one  child  in  the  CCSP  program  in  each  of  July  2015,  January  and\nJuly  2016,  January  and  July  2017,  and  January  2018  (n  =  865).  The\ndependent  variable  remained  whether  a  center  was  higher  quality\nin  January  2018.  Model  7  used  data  from  July  2017  for  all  time-\nvariant  independent  variables  and  used  a  center\u2019s  quality  status  in\nJuly  2017  as  the  dependent  variable  (n  =  972).  It  is  possible  that\nthe  CCSP  payment  increase  that  was   to  occur  to  July  2018  could\nhave  been  part  of  the  motivation  behind  centers  improving  EXCELS\nratings  by  January  2018.  Model  7  tested  whether  the  tiered  pay-\nment  system  served  as  an  incentive  before  a  CCSP  rate  increase\nwas  announced.", " All  models  were  run  using  the  xtlogit  command,  grouping  at\nthe  county  level,  with  random  effects  in  STATA  15.1.  All  mod-\nels  were  run  with  group-mean-centered  variables.  Models  only\ninclude  child  care  centers  that  received  a  CCSP  payment  in  January\n2018;  family  providers  were  not  included  in  this  study.", " 2.2.  Qualitative  stage 2.2.1.  Sampling  framework Center  directors  from  \ufb01ve  central  Maryland  counties  were\nincluded  in  the  qualitative  stage.  The  economic,  racial,  and  ethnic\ndemographics  of  those  counties  represent  Maryland\u2019s  diversity.  A\npurposeful  sampling  strategy  was  employed  to  select  information-\nrich  cases  (Patton,  2002).  The  quantitative  analyses  found  that  a\ncenter\u2019s  subsidy  density  was   positively  and  signi\ufb01cantly  associated\nwith  the  likelihood  that  a  child  care  center  would  be  higher  qual-\nity.  Therefore,  con\ufb01rming  and  discon\ufb01rming  centers  were  selected\nto  expand  on  and  challenge  the  quantitative  \ufb01ndings  (see  Table  3).\nCon\ufb01rming  centers  were  de\ufb01ned  as  those  either  in  the  top  half  of\nsubsidy  density  and  higher  quality  or  centers  in  the  bottom  half\nof  subsidy  density  and  lower  quality.  Discon\ufb01rming  centers  were\nde\ufb01ned  as  those  either  in  the  top  half  of  subsidy  density  and  lower\nquality  or  centers  in  the  bottom  half  of  subsidy  density  and  higher\nquality.  The  original  goal  was   to  interview  eight  pairs  of  directors\nof  centers  that  were  and  were  not  higher  quality  that  were  located\nin  the  same  zip  code,  had  similar  subsidy  densities,  and  had  par-\nticipated  in  EXCELS  for  a  similar  amount  of  time.  Centers  were\nlocated  in  zip  codes  with  varying  levels  of  poverty.  Additionally,\nbecause  afterschool  centers  were  less  likely  to  be  higher  quality\ncenters,  four  afterschool  centers  were  purposefully  included  in  the\ninterview  sample.", " 2.2.2.  Recruitment  and  \ufb01nal  sample The  author  sent  center  directors  an  introductory  e-mail  or\nletter  with  an  attached  letter  of  support  from  MSDE.  The  letter\nof  support  encouraged  program  directors  to  participate  in  inter-\nviews  to  inform  future  policy,  but  made  clear  that  MSDE   did  not\nknow  which  programs  were  selected  for  interviews  and  would  not\nlearn  if  program  directors  consented  to  participate.  A  few  days\nafter  the  initial  contact,  each  center  director  was  called.  Until  a\ncon\ufb01rmation  or  decline  to  participate  was  received,  the  author\ncalled  each  center  every  few  days,  for  a  minimum  of  10  calls.\nDuring  recruitment  of  the  four  afterschool  centers,  the  author\nlearned  that  many  decisions  regarding  EXCELS  were  not  made\nat  the  individual  center.  Therefore,  instead  of  interviewing  cen-\nter  directors  at  individual  afterschool  centers,  afterschool  program\ndirectors  who   oversaw  multiple  centers  were  interviewed.  Only\nthree  afterschool  program  directors  were  interviewed,  as  one  of  the\ndirectors  oversaw  two   of  the  afterschool  sites  originally  selected E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 Table   2\nLevel  1  independent  variables  included  in  models.", " Level  1  Independent  variables Subsidy  density \nSubsidy  density,  by  age  band  (2  variables)\nProvider  covariates\nYear  \ufb01rst  licensed \nLength  of  EXCELS  participation \nLicensed  capacity,  by  age  group \nAfterschool  program  (dummy   variable) \nHigher  quality  in  July  2015 \nZip   code  covariates\nLicensed  slots  per  child  under  5 \nUnder   5  poverty  (%) \nRace/ethnicity  (%) \nWomen  with  at  least  a  BA  (%) \nMothers  in  labor  force  (%) \nUnemployment  rate \nUrban   v.  rural  (%  rural) Model  (1) X Model  (2) Model  (3) Model  (4) X X \nX \nX \nX \nX X X \nX \nX \nX \nX X \nX \nX \nX \nX \nX \nX X X\nX\nX\nX\nX X\nX\nX\nX\nX\nX\nX Table  3\nCenter  and  location  characteristics  for  potential  center  director  interview  sample.", " Selection  purpose Typology Subsidy  density  % Child  poverty  (zip  code)  % Length  of  participation\n(6-month  time  periods) Participants\nCon\ufb01rming Con\ufb01rming Con\ufb01rming Con\ufb01rming Con\ufb01rming Con\ufb01rming Con\ufb01rming Discon\ufb01rming Discon\ufb01rming Discon\ufb01rming Nonparticipants\nCon\ufb01rming Con\ufb01rming Con\ufb01rming Discon\ufb01rming Discon\ufb01rming Discon\ufb01rming Discon\ufb01rming Top  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nBottom  half  of  subsidy\ndensity  &  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality\nBottom  half  of  subsidy\ndensity  &  Higher  quality Top  half  of  subsidy  density\n&  Higher  quality\nTop  half  of  subsidy  density\n&  Higher  quality\nBottom  half  of  subsidy\ndensity  &  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality\nTop  half  of  subsidy  density\n&  Lower  quality 103.6 66.1 52.4 25.0 20.0 12.2 7.4 45.5 18.2 0.1 54.1 30.0 0.1 45.8 42.9 37.5 23.0 13.8 9.9 18.3 18.1 18.8 6.1 18.8 5.1 18.1 0.5 5.1 3.7 0.5 24.0 9.9 13.8 18.1 Note:  The  median  subsidy  density  across  all  1003  centers  was  8.3%.  Con\ufb01rming  centers  were  de\ufb01ned  as  those  either  in  the  top  half  of  subsidy  density  and  higher  quality  or\ncenters  in  the  bottom  half  of  subsidy  density  and  lower  quality.  Discon\ufb01rming  centers  were  de\ufb01ned  as  those  either  in  the  top  half  of  subsidy  density  and  lower  quality  or\ncenters  in  the  bottom  half  of  subsidy  density  and  higher  quality.", " in  the  interview  sample.  Three  center  directors  quickly  declined\nto  participate  and  the  centers  were  replaced  with  centers  from\ntheir  same  county  that  had  similar  levels  of  subsidy  density  and\nhigher  quality  status.  Four  directors  were  ultimately  unrespon-\nsive  in  scheduling  an  interview  and  were  not  replaced  due  to  the\ndata-collection  window  ending.  After  scheduling  an  interview  with\none  center  director  at  a  franchise  location,  the  director  suggested\nthe  author  also  interview  someone  at  the  larger  organization,  as\nthe  organization  had  greater  involvement  in  EXCELS.  Therefore, the  author  interviewed  one  individual  from  a  corporate  child  care\nchain.", " The  \ufb01nal  sample  consisted  of  14  interviews  with  10  child  care\ncenter  directors,  three  afterschool  program  directors  with  multi-\nple  locations,  and  one  corporate  child  care  provider  with  multiple\nlocations.  Of  the  10  child  care  center  directors,  two  were  directors\nof  individual  corporate  child  care  centers,  four  were  directors  or\nowners  in  small  franchises  (three  or  four  locations),  and  four  were\ndirectors  or  owners  of  single-location  child  care  centers.  Seven  of E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 the  child  care  centers  sampled  were  con\ufb01rming  cases  (six  centers\nwere  in  the  top  half  of  subsidy  densities  and  higher  quality;  one\ncenter  was  in  the  bottom  half  of  subsidy  density  and  lower  quality)\nand  three  child  care  centers  were  discon\ufb01rming  cases  (two  cen-\nters  were  in  the  top  half  of  subsidy  densities  and  lower  quality;\none  center  was  in  the  bottom  half  of  subsidy  density  and  higher\nquality).", " With  the  exception  of  three  centers,  the  corporate  provider  and\nthree  afterschool  programs  oversaw  centers  that  had  subsidy  den-\nsities  in  the  bottom  half  (below  8.3%).  In  fact,  more  than  half  of  the\nafterschool  centers  overseen  by  each  of  the  three  afterschool  pro-\ngrams  did  not  serve  any  children  in  the  CCSP  in  January  2018.  The\nmajority  of  centers  overseen  by  the  corporate  provider  and  two  of\nthe  afterschool  programs  were  higher  quality;  these  centers  repre-\nsented  discon\ufb01rming  cases,  as  the  centers  had  subsidy  densities  in\nthe  bottom  half  and  were  higher  quality.  No  programs  overseen  by\nthe  third  afterschool  program  were  higher  quality;  these  centers\nrepresented  con\ufb01rming  cases,  as  the  centers  had  subsidy  densities\nin  the  bottom  half.", " 2.2.3.  Data  collection A  draft  semi-structured  interview  protocol  was  informed  by\ninterviews  with  MSDE  staff  and  technical  assistance  providers.  A\nrevised  protocol  incorporated  suggestions  from  University  of  Mary-\nland  faculty,  U.S.  Department  of  Health  and  Human  Services  staff\nknowledgeable  about  the  CCDF,3 and  MSDE  staff.  The  revised  pro-\ntocol  was  pilot-tested  with  a  center  director  and  the  \ufb01nal  protocol\naddressed  suggestions  obtained  during  the  pilot.4 In-person  inter-\nviews  were  conducted  from  September  through  November  2018.", " The  author  prepared  for  the  interviews  by  examining  programs\u2019\nEXCELS  ratings,  CCSP  longitudinal  data,  and  website.  Before  each\ninterview  started,  respondents  were  asked  for  signed  consent  to\nparticipate  in  and  audio  record  the  interview.  Four  center  direc-\ntors  did  not  consent  to  be  audio  recorded.  For  directors  who  did\nnot  consent  to  be  recorded,  the  author  took  notes  during  the  inter-\nview  and  immediately  typed  notes  into  an  electronic  version  of  the\ninterview  protocol.  For  directors  who  consented  to  be  recorded,\nminimal  notes  were  taken  during  the  interview.  On  the  same  day\nas  the  interview,  the  author  wrote  general  impressions,  emergent\nthemes,  and  key  takeaways  from  the  recorded  interviews.", " 2.2.4.  Data  analysis All  recorded  interviews  were  transcribed  into  an  Excel  docu-\nment  with  one  column  per  protocol  question  and  one  row  per\nrespondent.  The  transcribed  interviews  and  notes  from  unrecorded\ninterviews  were  coded  by  theme.  Responses  were  copied  from\nthe  Excel  document  and  pasted  into  a  thematic  outline  based\non  the  interview  protocol.  Qualitative  data  were  analyzed  for\nconsistencies  across  all  centers  and  among  centers  with  shared\ncharacteristics,  such  as  quality  status,  subsidy  density,  con\ufb01rm-\ning  or  discon\ufb01rming  status,  afterschool  programs,  and  whether  the\ncenter  was  part  of  a  large  or  small  franchise.", " 3.  Results 3.1.  Quantitative  results 3.1.1.  Descriptive  results In  January  2018,  Maryland  had  2701  licensed  child  care  centers\nand  72%  of  those  (1954  centers)  participated  in  Maryland  EXCELS.", " Table  4\nMeans  of  independent  variables,  by  Centers\u2019  January  2018  Quality  Status.", " Lower  quality  (n  =\n748) Higher  quality  (n  =\n255) Subsidy  density\nUnder  2  subsidy  density \n2+   subsidy  density \nCenter  Characteristics\nYear  center  \ufb01rst  licensed \nLength  of  EXCELS\nparticipation  (6-month\nperiods)\nUnder  2  licensed  capacity \n2+   licensed  capacity \nAfterschool  program\n(dummy)\nHigher  quality  in  July  2015\n(dummy)\nZip   Code  Characteristics\nLicensed  slots  per  child\nunder  5\nPoverty  under  5 \nAsian \nAfrican  American \nHispanic \nWomen   with  at  least  a  BA \nMothers  in  labor  force \nUnemployment  rate\nRural 18.884 \n31.117 \n18.351 2009.471 \n6.762 8.302 \n53.397 \n0.232 0.005 0.393 17.133 \n5.070 \n41.861 \n8.430 \n19.062 \n77.804 \n7.781 \n9.199 22.777\n32.045\n20.576 2007.773**\n8.322** 10.145\n65.863**\n0.141** 0.310** 0.419 15.816\n6.337**\n30.388**\n9.612\n21.170**\n77.367\n6.866**\n11.6647 Note.  Italicized  numbers  and  *  and  **  indicate  a  statistically  signi\ufb01cant  difference\nbetween  lower  quality  and  higher  quality  centers  at  the  0.05  level  and  0.01  level,\nrespectively.  Center  characteristics  data  are  from  January  2018  MSDE  data,  with  the\nexception  of  Length  of  EXCELS  participation,  as  noted  below.  Zip  Code  characteristics\ndata  are  from  2016  ACS  5-year  estimates,  except  percent  Rural,  which  is  from  the\n2010  U.S.  Census.  Subsidy  density  is  the  number  of  children  participating  in  the  CCSP\ndivided  by  the  center\u2019s  capacity,  multiplied  by  100.  Length  of  EXCELS  participation\ncounts  the  number  of  6-month  time  periods  the  center  participated  in  EXCELS  from\nJuly   2013\u2013January  2018,  with  a  maximum  of  10.  The  Afterschool  program  dummy\nis   1  if  the  center  serves  only  school-age  children.  The  Higher  quality  in  July  2015\ndummy   is  1  if  the  center  was   rated  3,  4,  or  5  before  or  during  July  2015.  Licensed\nslots  per  child  under  5  is  the  number  of  licensed  slots  ages  6  weeks\u20135  years,  not  yet\nin   kindergarten  divided  by  the  number  of  children  under  age  5  living  in  the  zip  code.\nEXCELS  =  Maryland\u2019s  Quality  Rating  and  Improvement  System.", " Of  those  1954  centers,  1007  centers  (52%)  received  a  CCSP  payment\nin  January  2018.  Four  of  the  1007  centers  that  received  a  CCSP  pay-\nment  were  located  in  zip  codes  with  missing  census  data  and  were\ndropped  from  further  analysis.  Of  the  1003  centers  included  in  the\nregression  analyses,  255  (25%)  were  rated  EXCELS  3  or  higher  in\nJanuary  2018.  Subsidy  density  ranged  from  0.3%  to  445.5%,  with  a\nmedian  of  8.3%  and  a  mean  of  19.9%.", " As  shown  in  Table  4,  a  number  of  differences  emerged  between\nthe  characteristics  of  centers  that  received  CCSP  payments  in\nJanuary  2018  that  were  and  were  not  rated  EXCELS  3  or  higher.\nHigher  quality  centers  had  been  licensed  for  more  years,  had  par-\nticipated  in  EXCELS  longer,  had  higher  capacity  for  children  aged\n2  and  older,  were  more  likely  to  be  higher  quality  in  July  2015,\nand  were  less  likely  to  be  afterschool  centers.  Higher  rated  cen-\nters  were  located  in  zip  codes  that  had  higher  percentages  of  Asian\npeople  and  women  with  a  bachelor\u2019s  degree,  lower  percentages\nof  African  American  people,  and  lower  unemployment  rates.  Addi-\ntionally,  note  that  centers  that  were  higher  quality  in  July  2015  were\ndifferent  than  centers  that  were  not  higher  quality  in  July  2015  in\nthe  following  ways:  The  former  had  been  licensed  for  more  years;\nhad  participated  in  EXCELS  longer;  had  higher  capacity  for  chil-\ndren  aged  2  and  older;  were  less  likely  to  be  afterschool  centers;\nand  were  located  in  zip  codes  with  lower  percentages  of  African\nAmerican  people,  lower  unemployment  rates,  and  higher  rurality.", " 3 U.S.  Department  of  Health  and  Human  Services  staff  reviewed  the  protocol  in\ntheir  personal  capacity,  and  not  as  representatives  of  the  U.S.  Department  of  Health\nand  Human  Services.", " 4 The  \ufb01nal  protocol  is  available  from  the  author  upon  request.", " 3.1.2.  Regression  results Model  1  is  a  two-level  model,  with  centers  nested  in  coun-\nties  that  only  include  the  independent  variable  of  interest,  subsidy E.S.  Lee Table   5\nOdds  Ratio  Estimates  for  Centers.", " Independent\nvariable Subsidy  density \nUnder  2  subsidy  density \n2+   subsidy  density \nYear   center  \ufb01rst  licensed\nLength  of  EXCELS  participation \nUnder  2  licensed  capacity \n2+   licensed  capacity \nAfterschool  program \nHigher  quality  in  July  2015 \nLicensed  slots  per  child  under  5\nUnder  5  poverty\nAsian \nAfrican  American \nHispanic \nWomen  with  at  least  a  BA \nMothers  in  labor  force \nUnemployment  rate \nRural \n2-level  model Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 Model  (1) Model  (2) Model  (3) Model  (4) OR Robust  Std.  Err.", " OR Robust  Std.  Err.", " OR Robust  Std.  Err.", " OR Robust  Std.  Err.", " 1.009** (0.002) 1.013** (0.003) 1.017** (0.004) 0.988 \n1.264** \n1.011 \n1.005 \n0.856 \n47.274** (0.015) \n(0.066) \n(0.007) \n(0.003) \n(0.266) \n(40.068) 0.991 \n1.277** \n1.009 \n1.005 \n0.808 \n53.041** \n0.784 \n1.016 \n0.979 \n0.995 \n1.016 \n1.086* \n1.014 \n0.969 \n0.995 \nX (0.017) \n(0.070) \n(0.007) \n(0.003) \n(0.244) \n(46.392) \n(0.249) \n(0.011) \n(0.036) \n(0.005) \n(0.009) \n(0.040) \n(0.013) \n(0.075) \n(0.007) 1.002 \n1.014** \n0.972 \n1.198** \n1.005 \n1.007 \n14.129 \n17.637** \n0.921 \n1.031 \n1.044 \n1.002 \n1.005 \n1.070 \n1.017 \n0.915 \n0.994 \nX (0.003)\n(0.005)\n(0.036)\n(0.082)\n(0.014)\n(0.006)\n(22.162)\n(8.418)\n(0.427)\n(0.017)\n(0.039)\n(0.007)\n(0.012)\n(0.048)\n(0.017)\n(0.093)\n(0.009) Note.  *  indicates  signi\ufb01cance  at  the  0.05  level,  and  **  indicates  signi\ufb01cance  at  the  0.01  level.  n  =  1003.  Rho  is  statistically  signi\ufb01cant  at  the  0.05  level  for  all  models.  For  model\n(1)   rho  =  0.102;  for  model  (2)  rho  =  0.165;  for  model  (3)  rho  =  0.181;  and  for  model  (4)  rho  =  0.114,  EXCELS  =  Maryland\u2019s  Quality  Rating  and  Improvement  System.", " X X density.  Without  any  center  or  zip  code  characteristics,  a  higher\nsubsidy  density  was  associated  with  an  increased  likelihood  of  a\ncenter  being  higher  quality  (OR  =  1.009;  see  Table  5).  The  odds\nratio  increased  as  more  variables  were  added  in  subsequent  models.\nOnce  program  characteristics  were  included  in  Model  2,  the  odds\nratio  increased  to  1.013  and  the  odds  ratio  increased  to  1.017  in\nModel  3  when  zip  code  characteristics  were  added.  That  is,  holding\nall  center  and  zip  code  characteristics  equal  in  counties,  the  odds\nof  being  a  higher  quality  center  increase  by  1.7%  for  each  percent-\nage  point  increase  in  subsidy  density.  As  an  example,  an  increase\nin  subsidy  density  of  25  percentage  points  leads  to  an  increase  in\nodds  of  being  higher  quality  by  52.4%  (1.017(25) =  1.524).", " Model  4  extended  Model  3  by  including  variables  for  subsidy\ndensity  by  age  band,  as  the  two  age  bands  have  different  EXCELS\npayment  rates.  Model  4  showed  that  the  subsidy  density  of  children\naged  2  and  older  was  statistically  signi\ufb01cant  (OR  =  1.013),  whereas\nthe  subsidy  density  for  children  under  the  age  of  2  was  not  statis-\ntically  signi\ufb01cant,  though  the  odds  ratio  was  still  greater  than  1.0\n(OR  =  1.002).  Results  from  robustness  analyses  produced  similar\nresults,  with  statistically  signi\ufb01cant  odds  ratios  ranging  from  1.013\nto  1.021  when  measuring  subsidy  density  in  various  ways.  Robust-\nness  Model  5,  which  capped  subsidy  density  at  100%,  produced  the\nlargest  OR  (of  1.021),  indicating  that  centers  with  subsidy  density\ngreater  than  100%  reduced  the  likelihood  that  a  center  with  high\nsubsidy  density  would  be  higher  quality.5 3.2.  Qualitative  results Qualitative  data  were  analyzed  for  consistencies  across  all\ncenters  and  among  centers  with  shared  characteristics,  such  as  con-\n\ufb01rming  or  discon\ufb01rming  status  or  afterschool  program  status.  The\nfollowing  sections  focus  on  experiences  with  EXCELS  participation,\nperception  about  parents\u2019  use  of  EXCELS  ratings  and  search  for  care,\nexperiences  with  EXCELS  payments,  and  supports  and  challenges\nto  improving  EXCELS  ratings.  Considering  many  of  the  interview\nquestions  and  themes  were  not  directly  related  to  the  CCSP,  it  is\nnot  surprising  that  \ufb01ndings  were  generally  consistent  across  var-\nious  types  of  centers,  or  the  pattern  of  agreement  did  not  align\nwith  con\ufb01rming  or  discon\ufb01rming  status.  Interestingly,  as  described 5 Full  results  available  in  Supplemental  tables.", " below,  only  three  center  directors  reported  that  EXCELS  payments\nmotivated  them  to  reach  a  higher  quality  rating,  and  two  of  those\ndirectors  were  from  discon\ufb01rming  interviewees.  While  \ufb01ndings  are\ngenerally  consistent  across  various  types  of  centers,  information  on\ndiscon\ufb01rming  and  con\ufb01rming  status,  afterschool  program  status,\nand  rating  status  are  included  to  provide  additional  context  to  the\n\ufb01ndings.", " 3.2.1.  EXCELS  participation Almost  all  directors  reported  they  participated in  EXCELS\nbecause  MSDE  required  participation  to  serve  children  in  the  CCSP.\nOne  afterschool  program  director  with  multiple  locations  said\nthey  hated  that  MSDE  tied  subsidies\u2014which  were  already  a  \u201creal\npain\u201d\u2014to  the  voluntary  EXCELS  program.  The  director  thought  all\ntheir  centers  were  already  of  high  quality  so  did  not  see  the  imme-\ndiate  bene\ufb01t  to  the  centers.  In  fact,  all  centers  entered  EXCELS\nwith  a  rating  of  3.  This  program  director\u2019s  annoyance  with  EXCELS\nwas   atypical  among  interviewees.  More  commonly,  responses  were\nsimilar  to  the  following,  from  a  con\ufb01rming  center  rated  3: If  I  can  be  candid,  we  were  required  to  [participate].  Since  we\nwere  required  to,  we   dived  right  into  it.  I  think  it  does  show-\n.\ncase  our  centers,  our  staff  quali\ufb01cations \n  it  is  time  consuming.\nIt\u2019s  a  lot  of  documentation.  I  found  it  kind  of  rewarding  at  the\nend.  It  kind  of  opened\u2014peeled  our  layers  back  to  let  the  pub-\nlic  know  what  our  policies  are  for  discipline.  It  kind  of  made  us\nthink  about  some  of  the  stuff.  Stuff  that  was  always  there,  but\nto  actually  document  and  get  it  into  policy.", " .", " .", " Though  directors  may   have  initially  participated  in  EXCELS\nbecause  it  was   required,  about  half  of  the  center  directors  reported\nthey  participated  in  EXCELS  to  improve  their  center.  Almost  all\ndirectors  reported  making  changes  due  to  EXCELS,  especially  in\ncurriculum  and  staf\ufb01ng.", " 3.2.2.  Parents  search  and  choice  of  care Center  directors  did  not  believe  EXCELS  ratings  (or  the  EXCELS\nwebsite)  factored  into  parents\u2019  choice  of  child  care.  Two   directors\nof  con\ufb01rming  centers  with  EXCELS  ratings  of  3  thought  parents\nwere  aware  of  EXCELS  ratings  even  though  they  did  not  think  the\nrating  factored  into  the  decision  to  choose  a  center.  Rather  than\nusing  the  EXCELS  website  to  learn  of  centers,  center  directors  over-\nwhelmingly  reported  that  parents  learned  of  their  centers  through E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 word-of-mouth.  Word-of-mouth  and  personal  relationships  also\nemerged  as  memorable  reasons  why  parents  chose  a  center.  In\naddition  to  recommendations,  center  directors  reported  that  par-\nents  chose  their  center  because  the  center  offered  a  safe  and  clean\nenvironment;  had  engaging,  caring  staff;  or  felt  like  a  community\nor  a  home-away-from-home.", " 3.2.3.  EXCELS  payments If  center  directors  did  not  believe  EXCELS  ratings  factored  into\nparents\u2019  search  for  or  choice  of  care,  then  tiered  reimbursement\npayments  are  the  only  incentive  for  centers  to  improve  their  QRIS\nrating.  For  EXCELS  payments  to  act  as  an  incentive  to  improve  care,\ncenters  must  be  aware  of  the  payments.  In  fact,  all  but  one  of  the\ninterviewed  directors  were  aware  of  Maryland\u2019s  tiered  reimburse-\nment  system,  though  one  did  not  learn  of  it  until  they  were  almost\nthrough  all  of  the  requirements  to  achieve  a  rating  of  3.  The  one\ncenter  director  who  was  not  aware  of  the  EXCELS  payment  was\nfrom  a  con\ufb01rming  center  with  a  rating  of  1.  During  the  interview,\nthey  asked  what  \u201cwas  the  point  of  EXCELS\u201d  and  whether  the  EXCELS\nrating  matters.  As  the  director  had  waitlists  for  all  ages,  the  EXCELS\nrating  appeared  not  to  matter  to  their  clientele,  under  10%  of  whom\nparticipated  in  the  CCSP.", " One  interviewee  from  a  con\ufb01rming  center  and  two  interviewees\nfrom  discon\ufb01rming  centers  reported  that  EXCELS  payments  fac-\ntored  into  their  decision  on  what  EXCELS  rating  to  reach  or  how\nquickly  to  reach  it,  but  no  centers  were  singularly  motivated  by\nthe  EXCELS  payments.  One  director  of  a  con\ufb01rming  center  that\nopened  in  2016  and  had  a  subsidy  density  of  about  15%  learned\nof  the  EXCELS  payments  through  a  discussion  with  a  QAS  who   used\na  monthly  CCSP  invoice  to  explain  exactly  how  much  additional\nmoney  the  center  could  receive.  The  director  remembers  thinking,\n\u201cHey,  let\u2019s  try  to  do  that\u201d  and  reported  that  the  EXCELS  payments\nwere  a  \u201cmajor  motivation\u201d  to  increase  the  center\u2019s  EXCELS  rating.\nWithin  2.5  years  of  opening,  the  center  had  a  rating  of  3.  However,\nthe  director  also  reported  that  their  original  goal  was  to  become  an\naccredited  center  (which  would  be  dif\ufb01cult  to  do  without  reaching\na  rating  of  3),  indicating  that  the  EXCELS  payment  was  one  part  of\nthe  motivation  for  reaching  a  rating  of  3.", " A  director  of  a  discon\ufb01rming  center  that  had  a  subsidy  density\nof  around  20%  had  recently  experienced  a  decrease  in  the  number\nof  children  participating  in  the  CCSP,  and  reported  a  preference\nto  achieve  a  rating  of  3  to  receive  additional  money  though  the\nEXCELS  payments.  Three  months  prior  to  their  study  interview,  a\nQAS  canceled  a  visit  to  help  conduct  an  Environment  Rating  Scale\nself-assessment  that  is  required  to  earn  a  rating  of  3.  The  director\ndid  not  contact  the  QAS  to  reschedule  and  noted  that  \u201cif  I  started\nworking  on  EXCELS  today,  or  this  week,  I  don\u2019t  want  to  work  on\nEXCELS  next  week.  I  took  this  week  to  do  it.\u201d  As  this  center  has  had\na  rating  of  2  for  over  2  years,  it  does  not  appear  that  the  incentive\nwas  enough  of  a  motivation  to  overcome  capacity  constraints.", " Finally,  one  discon\ufb01rming  interviewee  in  a  large  corporate  chain\nthat  served  a  very  small  number  of  children  in  the  CCSP  reported\nthat  the  EXCELS  payments  were  part  of  the  reason  the  chain  decided\non  a  goal  of  EXCELS  rating  3  for  all  of  its  centers.  During  the  inter-\nview,  the  director  \ufb01rst  mentioned  that  the  corporation  decided  on\na  rating  of  3  as  a  goal  because  it  was  the  highest  rating  a  center\ncould  meet  without  working  toward  accreditation,  aligned  with\nthe  requirements  in  the  EXCELS  Accreditation  and  Rating  Scales\ndomain.", " In  contrast,  three  con\ufb01rming  interviewees  speci\ufb01cally  reported\nthey  wanted  to  reach  an  EXCELS  rating  of  3  (or  higher)  to  \u201cdo  the\nright  thing  for  children\u201d  or  because  of  \u201ca  passion  that  all  kids  have\na  right  to  good  quality  child  care.  They  didn\u2019t  get  to  choose  what\ntheir  parents\u2019  income  was  going  to  be.\u201d All  directors  who  received  EXCELS  payments  were  appreciative\nof  the  EXCELS  payments,  with  the  exception  of  one  discon\ufb01rm- ing  interviewee  af\ufb01liated  with  a  local  government  that  found  the\nEXCELS  payments  hard  to  explain  to  the  budget  of\ufb01ce.  This  inter-\nviewee\u2019s  higher  quality  afterschool  sites  served  a  small  number  of\nchildren  in  the  CCSP,  perhaps  contributing  to  their  view  that  the\nEXCELS  payments  required  more  effort  than  the  payments  were\nworth.  Directors  most  commonly  reported  spending  their  EXCELS\npayments  on  supplies,  such  as  toys  and  books.", " 3.2.4.  Supports  to  improve  EXCELS  rating QAS  were  reportedly  instrumental  in  helping  directors  under-\nstand  EXCELS  standards,  decide  on  a  goal  for  an  EXCELS  rating,\nsupport  directors  in  moving  up  in  EXCELS  ratings,  and,  ultimately,\nproviding  a  positive  experience  with  EXCELS.  All  directors  appreci-\nated  the  work  of  QAS  and  their  trainings,  workshops,  and  individual\nsupport.  One  interviewee  from  a  con\ufb01rming  afterschool  program\nwith  no  higher  quality  sites  had  recently  completed  a  training  and\ncommented, The  training  they  provided  was  excellent.  You  can  read  the  web-\nsite  and  read  the  materials,  but  having  someone  break  it  down  as\nto  how  it  bene\ufb01ts  the  associates  and  how  it  bene\ufb01ts  the  centers.\nEye-opening\u2014that  aha  moment\u2014now  I  get  it!", " Another  director  of  a  con\ufb01rming  higher  quality  center  reported\nthat  the  QAS  \u201cwanted  us  to  be  successful.  \u2018I\u2019m  going  to  help  you  do\nthis\u2019\u2014almost  to  the  point\u2014\u2019if  I  have  to  come  sit  at  your  center.\u2019  They\nwere  very  motivated  to  get  you  in.\u201d  Another  director  of  a  con\ufb01rming\ncenter  recalled  that  a  QAS  motivated  them  to  reach  for  a  rating  of  3\nafter  so  easily  and  quickly  meeting  the  requirements  for  an  EXCELS\nrating  of  1.", " 3.2.5.  Challenges  to  improve  EXCELS  rating\n3.2.5.1.  Burden.  Five  of  the  eight  center  directors  not  af\ufb01liated\nwith  large  child  care  providers  reported  that  the  time  required  for\nEXCELS  was  burdensome.  All  three  of  the  interviewees  from  lower\nquality  centers  reported  that  time  burden  was  a  challenge  (one  con-\n\ufb01rming  center  and  two  discon\ufb01rming  centers),  as  did  two  higher\nquality  con\ufb01rming  center  directors.  One  lower  quality  discon\ufb01rm-\ning  center  director  who  was  part  of  a  national  chain  reported  that\nEXCELS  was  burdensome  and  they  did  not  receive  any  assistance\nfrom  the  corporation.", " In  contrast,  the  corporate  program  and  the  three  afterschool\nprograms  included  in  the  interview  sample  all  had  a  staff  member\nin  the  central  of\ufb01ce  who  was   responsible  for  uploading  documents,\nsuch  as  staff  and  parent  handbooks,  into  the  EXCELS  system.  An\nowner  of  three  con\ufb01rming  higher  quality  centers  was  aware  of  the\nassistance  the  large  corporations  provide  and  noted  the  dif\ufb01culty\nof  \ufb01nding  the  time  for  EXCELS: What  I\u2019ve  noticed  is  these  big  daycare  centers,  they  have  hun-\ndreds  of  centers  and  they  have  a  special  team  to  help  their\ncenters  get  to  a  5.  They  can  afford  that.  We   cannot  even  \ufb01nish  up\nthe  workload  that  we  have  here:  vouchers,  invoices,  payments,\nbalance  sheets,  complaints,  child  health,  teachers  calling  out.", " Instead  of  relying  on  central  staff,  center  directors  of  small\nfranchises  worked  with  their  franchise  colleagues  to  develop\nEXCELS-required  documents,  such  as  staff  and  parent  handbooks,\nso  the  documents  are  consistent  across  sites.  One  owner  of  three\ncon\ufb01rming  higher  quality  centers  hired  an  administrator  who\ndevoted  much  initial  time  to  EXCELS  documentation  for  the  three\ncenters.  Another  owner  of  three  con\ufb01rming  higher  quality  centers\nrecently  contracted  with  a  former  credentialing  specialist  to  assist\nstaff  in  gaining  their  credentials.  A  third  director  of  a  discon\ufb01rming\nlower  quality  center  was  not  technologically  adept  and  relied  on\na  relatively  young  staff  member  to  help  with  EXCELS.  The  director\nacknowledged  that  without  this  staff  member\u2019s  help,  they  would E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 have  had  to  hire  someone  to  upload  documents  to  the  EXCELS  web-\nsite.", " 3.2.5.2.  Staf\ufb01ng.  The  majority  of  center  directors  reported  that\nstaf\ufb01ng  requirements  were  a  barrier  to  moving  beyond  a  rating\nof  3.  Although  one  director  of  a  higher  quality  con\ufb01rming  center\nnoted  that  the  staff  loves  trainings,  and  credentialing  is  not  a  chal-\nlenge  in  moving  beyond  a  rating  of  3,  three  center  directors  and\nall  three  afterschool  programs  noted  a  lack  of  interest  or  passion\namong  staff  to  attend  trainings  and  increase  their  credential  level.\nAs  one  director  of  a  higher  quality  con\ufb01rming  center  explained,  the\nstaff  do  not  have  the  same  passion  the  director  had  when  entering\nthe  child  care  \ufb01eld: It\u2019s  a  different  generation  then  when  I  came  into  child  care.\nWhen  I  came  in,  I  had  passion  for  it,  I  liked  it,  I  said  this  was\nmy  livelihood  so  if  I  had  to  go  on  the  weekends,  I  didn\u2019t  grum-\nble  about  it,  I  went.  But,  this  is  a  different  generation  coming  in.\nThey  want  their  personal  time.", " Even  greater  than  for  typical  child  care  centers,  afterschool\nprograms  felt  challenges  to  securing  trained  staff,  as  afterschool\nprograms  require  a  split  shift.  No  afterschool  programs  expected\ntheir  individual  centers  to  be  able  to  advance  beyond  a  rating  of  3\nin  EXCELS.", " 3.2.5.3.  Accreditation.  Three  interviewees  from  con\ufb01rming  higher\nquality  centers  reported  that  accreditation  was  a  challenge  to\nadvancing  beyond  a  rating  of  3,  with  two  directors  expressly  men-\ntioning  the  challenge  in  paying  for  accreditation.  One  afterschool\nprogram  director  noted  that  accreditation  standards  were  too  dif-\n\ufb01cult  for  centers  to  meet,  as  the  centers  are  located  in  school\nbuildings  and  some  accreditation  requirements,  such  as  temper-\nature,  windows,  and  art  work  displays  are  outside  the  center\ndirector\u2019s  control.", " 4.  Discussion This  study  is  the  \ufb01rst  in-depth  look  at  how  a  tiered  reimburse-\nment  system  functions  in  a  single  state  and  examined  whether\na  tiered  reimbursement  system  functions  as  theory  predicts.  The\nresults  from  the  quantitative  analyses  showed  that  serving  a  higher\nproportion  of  children  in  the  CCSP  was  associated  with  an  increased\nlikelihood  of  reaching  a  rating  of  3  and  receiving  an  EXCELS  pay-\nment.  This  \ufb01nding  is  similar  to  those  of  Petersen  et  al.  (2006)  and\nFlodgren  et  al.  (2011)  concerning  \ufb01nancial  incentives  and  improved\nhealth  care  quality.  However,  even  with  the  incentive  payment,  66%\nof  children  served  by  the  CCSP  in  January  2018  were  in  lower  quality\ncenters,  indicating  that  tiered  reimbursement  systems  can  only  go\nso  far  in  alleviating  the  quality  care  gap  identi\ufb01ed  by  Jones-Branch\net  al.  (2004)  and  Raikes  et  al.  (2005).", " theory  predicts  and While  the  quantitative  results  indicate  that  Maryland\u2019s  tiered\nreimbursement  system \nis\nfunctions  as \nmarginally  improving  quality,  the  results  of  the  qualitative  research\ndid  not  provide  the  same  result:  few  child  care  center  directors\nreported  being  motivated  by  the  EXCELS  payments.  Rather,  more\ndirectors  reported  being  self-motivated  to  improve  quality  or  were\nmotivated  by  QAS.  Three  possible  reasons  frame  the  discussion  of\nthese  divergent  \ufb01ndings  that  a  tiered  reimbursement  system  may\nnot  work  as  intended:  (a)  the  incentive  amount  is  too  small,  (b)\ncenters  lack  capacity  to  meet  the  higher  quality  standards,  and  (c)\ncenters  have  intrinsic  motivation  to  meet  higher  quality  standards.\nThe  interviews  did  not  provide  support  for  the  \ufb01rst  possible  rea-\nson  a  tiered  reimbursement  system  would  not  work.  None  of  the\nfour  center  directors  who  were  rated  1  or  2  or  the  central  staff\nof  centers  with  multiple  locations  stated  the  incentives  were  too small.  Two  directors  of  programs  with  rating  of  3  mentioned  the\namounts  of  their  EXCELS  payments  during  the  course  of  the  inter-\nviews.  Both  indicated  they  did  not  view  these  amounts  as  especially\ngenerous  and  neither  director  reported  being  motivated  by  the\nEXCELS  payments  to  reach  a  rating  of  3.  Both  these  center  directors,\nas  well  as  many  others,  were  appreciative  of  the  EXCELS  payments\nand  tended  to  spend  the  payments  on  supplies.  The  limited  qualita-\ntive  data  on  the  size  of  the  incentive  indicates  that  center  directors\nmay   view  the  incentives  as  small,  but  not  too  small.", " Another  way  to  understand  the  relative  size  of  an  incentive  is\nto  think  about  the  effort  needed  to  earn  the  incentive,  with  a  lack\nof  capacity  being  the  second  possible  reason  a  tiered  incentive  sys-\ntem  may   not  function  as  theory  predicts.  Five  of  the  eight  center\ndirectors  unaf\ufb01liated  with  large  child  care  providers  reported  that\nthe  time  required  for  EXCELS  was   burdensome,  as  did  one  of  the\ncenter  directors  who  was   part  of  a  national  chain.  Directors  of  sin-\ngle  centers  had  even  less  capacity.  Of  the  three  directors  of  centers\nwith  ratings  of  1  or  2,  two  were  directors  of  single  centers  and  both\nexperienced  capacity  challenges.", " A  \ufb01nal  indication  that  lack  of  capacity  may   be  a  challenge  in\nreaching  a  rating  of  3  is  that  \ufb01ve  of  the  seven  nonparticipating  cen-\nter  directors  contacted  for  interviews  were  from  centers  rated  1  or\n2.  Some  refused  to  participate  because  they  did  not  have  time  for\nan  interview.  One  owner  noted  that  the  director  just  left  and  was\nthe  person  who  dealt  with  EXCELS.  It  seems  likely  that  centers  that\ndid  not  consent  to  an  interview  had  even  less  capacity  than  those\nwhose  directors  could  make  time  for  an  interview.", " For  the  third  possible  explanation  for  why   a  tiered  payment\nsystem  would  not  work,  the  qualitative  \ufb01ndings  indicated  that,\ninstead  of  being  motivated  to  improve  quality  to  earn  an  EXCELS\npayment,  center  directors  had  intrinsic  motivation  to  provide  the\nbest  care  they  could.  For  example,  three  center  directors  reported\nthey  wanted  to  reach  an  EXCELS  rating  of  3  or  higher  to  \u201cdo  the  right\nthing  for  children.\u201d  These  results  indicate  that  some  center  direc-\ntors  were  largely  intrinsically  motivated,  and  were  not  motivated\nby  EXCELS  payments.", " The  question  remains,  however,  if  center  directors  were  intrin-\nsically  motivated,  and  not  motivated  by  the  incentive  payment,\nwhy   would  centers  with  higher  subsidy  densities  be  more  likely  to\nreach  a  rating  of  3  in  EXCELS?  A  center\u2019s  capacity  to  meet  EXCELS\nstandards  may   be  a  proxy  for  its  capacity  to  maneuver  the  child\ncare  subsidy  system  and  recruit  children  into  its  program.  If  that\nwere  the  case,  that  quantitative  \ufb01nding  that  centers  with  a  greater\nsubsidy  density  are  more  likely  to  be  higher  quality  may  simply\nindicate  that  such  centers  have  higher  capacity  to  meet  EXCELS\nstandards  and  serve  children  in  the  CCSP.  Another  possibility  could\nbe  that  centers  with  the  most  internal  motivation  to  provide  care\nto  children  from  low-income  families  are  also  the  centers  that  are\nmost  internally  motivated  to  provide  higher  quality  care.  In  that\ncase,  subsidy  density  would  be  a  proxy  for  the  intrinsic  motivation\nto  make  a  prosocial  difference.", " A  \ufb01nal  possible  explanation  is  that  center  directors  were,  per-\nhaps  unconsciously,  providing  socially  desirable  answers  (Paulhus,\n2002).  This  possibility  was  minimized  by  \ufb01rst  asking  directors  why\nthey  settled  on  an  EXCELS  rating,  then  why   they  are  (or  are  not)\ncurrently  working  to  improve  their  EXCELS  rating,  and  \ufb01nally  ask-\ning  if  the  director  was   aware  of  the  EXCELS  payments.  The  three\ninterviewees  who  reported  that  EXCELS  payments  motivated  them\nto  improve  in  EXCELS  mentioned  this  after  they  were  asked  about\ntheir  awareness  of  EXCELS  payments,  not  when  \ufb01rst  asked  how  they\nsettled  on  a  goal  for  their  EXCELS  rating.  After  asking  the  semi-\nstructured  protocol  questions,  it  was  natural  to  ask  some  other\ndirectors  if  EXCELS  payments  were  a  motivation  to  improve  their\nEXCELS  rating.  When  asked  outright,  all  directors  said  that  EXCELS\npayments  were  not  a  motivation  for  improving  their  EXCELS  rat-\ning.", " E.S.  Lee 5.  Limitations A  key  limitation  of  using  administrative  data  for  quantitative\nanalysis  is  that  administrative  data  has  no  information  on  why  the\nincentives  were  associated  with  higher  subsidy  density.  The  quanti-\ntative  analysis  could  not  account  for  whether  programs  were  aware\nof  the  EXCELS  payments  or  how  intrinsic  motivation  and  capacity\nconstraints  may   have  contributed  to  programs\u2019  EXCELS  ratings.  A\nbene\ufb01t  of  mixed-methods  research  is  that  interviews  shed  light  on\nthese  topics.  However,  a  limitation  of  the  focused  interviews  is  that\ndirectors  of  lower  quality  centers  were  less  likely  to  agree  to  inter-\nviews,  and  they  likely  would  have  had  different  experiences  with\nEXCELS  and  the  tiered  reimbursement  system  from  those  of  higher\nquality  centers.", " 6.  Future  research These limitations  and  \ufb01ndings  make  clear  that  additional\nresearch  is  needed  on  how  tiered  reimbursement  systems  function.\nIdeally,  interviews  should  be  conducted  with  more  directors  from\nlower  quality  centers  to  better  understand  why  these  centers  are\nnot  reaching  a  rating  of  3  and  the  supports  they  need  that  may   result\nin  improvement.  Additional  interviews  could  also  be  conducted\nwith  more  directors  of  centers  with  very  high  subsidy  densities,\nas  there  could  be  a  threshold  at  which  more  directors  would  report\nthat  EXCELS  payments  incentivized  their  quality  improvement.  To\nattempt  to  analyze  the  processes  by  which  a  tiered  reimburse-\nment  system  may   improve  the  quality  of  child  care,  future  research\nshould  survey  center  directors  on  their  understanding  of  tiered  pay-\nments  (including  their  amounts),  motivation,  and  capacity.  These\ndata,  when  combined  with  administrative  data,  would  allow  an\nanalysis  of  how  motivation  and  capacity  moderate  a  center\u2019s  qual-\nity  status.  Enhanced  analyses  could  be  modeled  as  a  threshold  to\nunderstand  if  a  there  is  a  certain  subsidy  density  that  must  be\nreached  for  the  incentive  payments  to  result  in  improved  quality.\nAdditional  data  that  should  be  collected  from  centers  through\nsurveys  include  enrollment  and  auspices.  Auspices  would  allow\nan  analysis  of  whether  for-pro\ufb01t  status  is  associated  with  a  larger\nresponse  to  incentives;  the  three  interviewees  who   reported  that\nEXCELS  payments  were  motivating  were  all  af\ufb01liated  with  for-\npro\ufb01t  centers.  Enrollment  data  would  enable  an  analysis  of  the\nassociation  between  a  center\u2019s  open  capacity  and  its  response  to\nincentives.  Enrollment  data  would  also  allow  for  a  more  accurate\nsubsidy  density  independent  variable.", " Future  research  could  also  examine  whether  subsidy  density  is\nassociated  with  separately  reaching  EXCELS  ratings  of  3,  4,  and  5.\nAs  the  vast  majority  of  higher  quality  centers  in  January  2018  had  a\nrating  of  3,  too  few  centers  had  ratings  of  4  or  5  to  permit  a  quanti-\ntative  analysis.  As  Maryland  EXCELS  matures,  more  programs  will\nreach  these  higher  ratings.", " Although  Maryland\u2019s  CCSP  reimbursement  rates  were  consis-\ntently  low  for  the  years  under  analysis,  beginning  in  the  summer  of\n2018  and  extending  through  2022,  Maryland  will  greatly  increase\nits  CCSP  reimbursement  rates.  As  EXCELS  payments  are  a  percent-\nage  of  CCSP  reimbursement  rates,  it  is  possible  that  the  \ufb01ndings\npresented  in  this  study  would  be  quite  different  from  \ufb01ndings  from\na  study  carried  out  in  2022.  A  future  study  could  analyze  the  effects\nof  the  annual  CCSP  rate  increases  on  centers\u2019  likelihood  of  being\nrated  higher  quality.", " Maryland  is  a  rather  unique  state  to  study  in  that  all  programs\nthat  receive  CCSP  payments  are  required  to  participate  in  EXCELS\nand  tiered  payments  are  not  earned  until  a  program  achieves  a  rat-\ning  of  3.  Although  this  study  is  the  \ufb01rst  to  examine  a  state-level\ntiered  reimbursement  system,  future  research  should  examine\nstates  with  different  structures.  For  example,  some  states  offer Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 small  tiered  payments  at  a  rating  of  1  as  an  incentive  to  participate\nin  their  entirely  voluntary  QRIS.  The  underpinning  theory  of  such\na  system  is  quite  different  from  the  theory  underpinning  Maryland\nEXCELS  and  its  tiered  reimbursement  system.  Finally,  the  research\nreported  here  focuses  on  child  care  centers,  and  more  research  is\nneeded  to  understand  how  family  child  care  programs  respond  to\ntiered  reimbursement  systems.", " 7.  Policy  implications These  \ufb01ndings  should  give  pause  to  any  state  that  does  not\ncurrently  have  a  tiered  reimbursement  system  and  is  consider-\ning  implementing  one.  It  is  important  to  remember  that  tiered\npayments  provide  additional  funds  after  the  program  has  demon-\nstrated  a  given  level  of  quality;  states  need  to  continue  to  think\nabout  how  to  support  programs  in  reaching  higher  quality  lev-\nels.  Before  implementing  a  tiered  reimbursement  system,  a  state\nshould  conduct  focus  groups  and  surveys  to  better  understand  why\nprograms  are  not  currently  receiving  high  ratings  on  their  state\u2019s\nQRIS.  If  capacity  constraints  are  a  concern,  the  state  should  consider\nif  funds  could  be  better  spent  on  technical  assistance  or  small-\ncapacity  grants  rather  than  tiered  payments.  Recall  that  some  child\ncare  center  directors  reported  that  the  QAS  were  instrumental  in\nproviding  support  and  encouragement  to  increase  EXCELS  ratings.\nIf  staff  quali\ufb01cations  are  a  concern,  the  state  should  consider  if\nfunds  would  be  better  spent  on  increasing  career  ladders\u2019  mone-\ntary  incentives  and  technical  support  for  staff  to  climb  the  career\nladders.", " Most  children  participating  in  the  CCSP  are  still  being  served  by\nlower  quality  centers  or  marginally  higher  quality  centers.  Specif-\nically,  27%  of  child  care  centers  serving  children  in  the  CCSP  with\na  published  EXCELS  rating  were  higher  quality,  and  73%  of  higher\nquality  centers  had  a  rating  of  3  in  January  2018.  If  a  state\u2019s  goal\nis  to  provide  all  children  from  low-income  families  with  high-\nquality  care,  a  state  must  bear  in  mind  that  a  tiered  reimbursement\nsystem\u2014and  its  associated  QRIS\u2014cannot  be  the  only  avenue  by\nwhich  to  improve  the  quality  of  child  care.", " Declarations  of  interest None.", " Funding Disclaimer This  research  did  not  receive  any  speci\ufb01c  grant  from  funding agencies  in  the  public,  commercial,  or  not-for-pro\ufb01t  sectors.", " This  article  was   written  by  the  author  in  her  private  capacity.\nNo  of\ufb01cial  support  or  endorsement  by  the  U.S.  Department  of  Edu-\ncation  is  intended  or  should  be  inferred.", " CRediT  authorship  contribution  statement Erica  S.  Lee:  Conceptualization,  Methodology,  Software,  Formal analysis,  Investigation,  Writing  -  original  draft.", " Acknowledgements This  research  would  not  have  been  possible  without  the  contin-\nued  support  of  many  individuals  at  the  Maryland  State  Department\nof  Education,  particularly  Lindi  Budd.  Similarly,  I  thank  the  14  child\ncare  directors  who   took  time  to  share  their  experiences  with  me.\nI  thank  the  members  of  my   dissertation  committee,  who  offered E.S.  Lee Early  Childhood  Research  Quarterly  55  (2021)  349\u2013362 valuable  feedback  throughout  my   research:  Peter  Reuter,  Robert\nCroninger,  Christy  Tirrell-Corbin,  Joan  Lieber,  and  Chris  Foreman.\nI  also  thank  Meryl  Barofsky  and  Kimberly  Burgress  who  offered\nvaluable  feedback  on  initial  drafts  of  this  manuscript.", " Appendix  A.  Supplementary  data Supplementary  material  related  to  this  article  can  be  found,  in\nthe  online  version,  at  doi:https://doi.org/10.1016/j.ecresq.2021.01.\n002."]}, {"paper_id": "#17247", "title": "Impact Bonds In Developing Countries: Early Learnings From The Field", "paragraphs": [" IMPACT BONDS IN  \nDEVELOPING COUNTRIES: \nEarly Learnings from the Field 2 | IMPACT BONDS IN DEVELOPING COUNTRIES IMPACT BONDS IN  \nDEVELOPING COUNTRIES:   \nEarly Learnings from the Field SEPTEMBER 2017 Emily Gustafsson-Wright is a fellow at the Center for Universal Education at Brookings Izzy Boggild-Jones is a research analyst at the Center for Universal Education at Brookings Dean Segell is a manager at Convergence Justice Durland is a knowledge associate at Convergence ACKNOWLEDGEMENTS The authors would like to thank \nmany people for their contributions \nto this study. First, Alison Bukhari, \nToby Eccles, Safeena Husain, Jane \nNewman, Peter Vanderwal and Maya \nZiswiler for their helpful comments, \nfeedback and insight on earlier drafts \nof the report. In addition, we would \nlike to thank all those who supported \nwith data collection for the Deal Book, \nand provided real time updates on the \nfactsheets for each deal. We would \nalso like to acknowledge those who \nparticipated in the impact bonds \nworkshop in London in November \n2016, whose valuable insights have \nformed the core of this report. We \nare particularly grateful for the \ncontributions of stakeholders involved \nin the contracted impact bonds with \nwhom we have had more in-depth \nconversations over the last several \nyears.", " The Brookings Institution is a \nnonprofit organization devoted to \nindependent research and policy \nsolutions. Its mission is to conduct \nhigh-quality, independent research \nand, based on that research, to \nprovide innovative, practical \nrecommendations for policymakers \nand the public. The conclusions and \nrecommendations of any Brookings \npublication are solely those of its \nauthor(s), and do not reflect the views \nof the Institution, its management, or \nits other scholars.", " Brookings gratefully acknowledges \nthe program support provided to the \nCenter for Universal Education by the \nGovernment of Norway.", " Brookings recognizes that \nthe value it provides is in its \nabsolute commitment to quality, \nindependence, and impact. Activities \nsupported by its donors reflect this \ncommitment.", " Convergence is an institution that \nconnects, educates, and supports \ninvestors to execute blended finance \ntransactions that increase private \nsector investment in emerging \nmarkets. Convergence features three \nofferings: (1) Investment Network: \nAn online platform where investors \ncan connect with deals in emerging \nand frontier markets. (2) Market \nBuilding Tools: Knowledge resources \nto help investors improve their \nblended finance understanding and \ncapabilities. (3) Design Funding: Grant \nfunding for practitioners to design \ninnovative financial instruments that \nwould otherwise be too risky or \ncomplex to pursue.", " 4 | IMPACT BONDS IN DEVELOPING COUNTRIES TABLE OF CONTENTS INTRODUCTION IMPACT BOND PRIMER GLOBAL LANDSCAPE AND DEAL BOOK ANALYSIS IDENTIFYING APPROPRIATE INTERVENTIONS  \nAND SERVICE PROVIDERS MANAGING RELATIONSHIPS WITH GOVERNMENT  \nAND DONOR OUTCOME FUNDERS IDENTIFYING METRICS AND STRUCTURING PAYMENTS DEVELOPING THE OPERATING MODEL,  \nSTRUCTURING THE VEHICLE, AND RAISING CAPITAL IMPLEMENTING THE IMPACT BOND AND MEASURING IMPACT CONCLUSIONS AND NEXT STEPS REFERENCES ANNEX A:  \nCASE STUDIES OF CONTRACTED IMPACT BONDS IN DEVELOPING \nCOUNTRIES ANNEX B:  \nGLOSSARY OF ACTORS ANNEX D:  \nDEAL BOOK ANNEX C:  \nPOTENTIAL EVALUATION METHODOLOGY BY OUTCOME FUNDER GOALS 5 | EARLY LEARNINGS FROM THE FIELD INTRODUCTION While remarkable progress has \nbeen made in human development \nindicators in recent decades, \nsignificant global challenges remain. \nOver 800 million people are living on \nless than $1.25 a day (United Nations \nDevelopment Programme, 2017; \nWorld Bank, 2017), and 263 million \nchildren and young people are out \nof school (Education Commission, \n2016). The United Nations\u2019 sustainable \ndevelopment goals (SDGs) outline \nan ambitious global agenda for \nending poverty and hunger, ensuring \ngood health and quality education, \nand promoting jobs and reduced \ninequalities. However, governments \nand multilateral organizations will face \nconsiderable challenges achieving \nthese aims. In education alone, the \nEducation Commission in 2016 \nestimated a funding gap of $1.8 trillion \nper year to ensure quality education \nfor all children.", " Achieving the SDGs will require \ngovernments and multilaterals \nto develop and apply innovative \nfinancing tools to make the best \nuse of existing funds. Results-based \nfinancing represents one tool that \ngovernments and multilaterals can \nuse to ensure that funds are directed \nmost effectively toward populations \nin need. Ensuring that resources \nare spent only on interventions that achieve desired results has \nthe potential to better target social \nservices and to hold funders and \nservice providers accountable \nfor what they deliver.  Social and \ndevelopment impact bonds, one \nform of results-based financing, \nhave the potential to shift the \nfocus of participants to outcomes, \nencourage performance management \nand adaptability, promote learning \nthrough evaluation, and create a clear \ncase for investing in what works.", " In 2015, the Brookings Institution \npublished a report on the potential \nand limitations of impact bonds, \nwhich chronicled the development \nof the first 38 impact bonds in high-\nincome countries and analyzed \nthe landscape (Gustafsson-Wright \net al., 2015). This report takes the \nfield further forward, exploring the \nlessons learned in the development \nof impact bonds in low- and middle-\nincome countries, bringing together \nthe findings from interviews with \nstakeholders and research into the \nimpact bond space conducted by \nthe authors over the course of a \nyear. In addition, the report draws \non discussions from an intensive \ndaylong workshop held in London \nin November 2016, in which impact \nbond practitioners from developing \ncountries shared their experiences 6 | IMPACT BONDS IN DEVELOPING COUNTRIES and early lessons learned. The report \nincludes a Deal Book with detailed \nfact sheets for all impact bonds in \ndeveloping countries, featuring both \nthe four contracted and 24 in design \nphases, as of August 1, 2017.", " The following analysis indicates the \nwide range of deals in design phases \nin developing countries, ranging \nin terms of country, sector, size of \nreturns, and evaluation methodology. \nEmerging from the analysis, the \nrecorded discussions in a one-day \nworkshop with practitioners, and in-\ndepth interviews with stakeholders \nfrom the contracted deals, we have \nidentified five key issue areas in the design and implementation of impact \nbonds, which the following sections \nwill explore.", " 1.  Identifying appropriate interventions and service \nproviders.", " 2. Managing relationships with government and donor outcome \nfunders.", " 3. Identifying metrics and structuring payments.", " 4. Developing the operating model, structuring the vehicle, and raising \ncapital.", " 5. Implementing the impact bond and measuring impact.", " 7 | EARLY LEARNINGS FROM THE FIELD IMPACT BOND PRIMER Impact bonds blend impact investing, \nresults-based financing, and public-\nprivate partnerships (see Figure 1). \nIn an impact bond, private investors \nprovide up-front capital for social \nservices and are repaid by an \noutcome funder contingent on the \nachievement of agreed-upon results. \nIn the case of a social impact bond \n(SIB), also called pay-for-success \n(PFS) in the United States and social \nbenefit bonds (SBB) in Australia, the \noutcome funder is a government entity. In the case of a development \nimpact bond (DIB), \u201cdevelopment\u201d \nreferring to their primary application \nto low- or middle-income countries, \nthis is usually a third party such as \na donor or foundation (Center for \nGlobal Development and Social \nFinance, 2013).  Since there are \nonly three DIBs with operational \nexperience, much of the analysis of \nthis report focuses on the design and \nnegotiation phases of the impact \nbond contracting process.", " FIGURE 1 Impact Bonds: A Confluence of Trends IMPACT INVESTING PUBLIC PRIVATE \nPARTNERSHIP PAYMENT  \nBY RESULTS IMPACT \nBONDS 8 | IMPACT BONDS IN DEVELOPING COUNTRIES Source: Authors\u2019 elaboration IMPACT BOND STRUCTURE AND MECHANICS While impact bonds are structured in \nmultiple ways, the basic mechanics \ncan be described as in Figure 2. \nMost impact bonds involve three \nmain types of actors: the investors, \nwho provide up-front capital to the \nservice providers to deliver social \nservices to the population in need. \nContingent on the achievement \nof results, the outcome funder \nrepays the investors their principal \nplus an agreed-upon return on \ninvestment. Impact bonds often also \ninvolve several other key players. \nThese include an evaluator, usually \nexternal to the service provider, \nwho verifies or evaluates whether \nagreed-upon outcomes have been \nachieved. Other evaluations of the \nintervention itself may also take \nplace in parallel, and performance \nmanagement of the service provider \nis also typical, but the role of the \nevaluator is to assess whether \nimpact metrics are achieved. An \nadditional, but not necessary, actor \ncan be an intermediary who often \nhas the responsibility of raising \ncapital and arranging negotiations \namong the participants. The \nintermediary can also support the \nservice provider in performance \nmanagement. Sometimes another \nentity can provide technical assistance in, for example, selection \nof outcome metrics and repayment \nterms. Legal support from lawyers \nwho are knowledgeable in this form \nof contracting is almost always \nrequired.", " While the basic structure of impact \nbonds in developing countries \nhas tended to follow the same \npatterns observed in high-income \ncountries, a key difference is the \ngreater need for a risk management \nelement. Implementing impact \nbonds in low- and middle-income \ncountries involves the development \nof contextual understanding about \nthe needs of outcome payers and \ninvestors in a riskier environment \nthan the one faced by participants \nin high-income countries. For SIBs \nin high-income countries, one of \nthe driving forces has been the idea \nthat the payment by government is \ndrawn from the future cost savings \nprovided by successful preventive \ninterventions. In developing \ncontexts\u2014and particularly in DIBs, in \nwhich the outcome payer is not the \ngovernment\u2014quantifying the value of \ninterventions to each organization is \nmuch more complicated, and in these \ncases future savings are less likely to \nbe a driving force.", " 9 | EARLY LEARNINGS FROM THE FIELD FIGURE 2 Impact Bond Mechanics INVESTOR OUTCOME FUNDER SERVICE PROVIDER VARIATIONS ON THE IMPACT BOND STRUCTURE AND MECHANICS Although each impact bond follows \na unique path to development, four \nmajor stages have been identified \n(Gustafsson-Wright et al., 2015): \na feasibility study; structuring the \ndeal; implementation; and evaluation \nand repayment. Within the feasibility \nstudy, the social challenge is \nidentified and the feasibility of using an impact bond to resolve that \nchallenge is explored. To structure \nthe deal, an outcome funder must \nagree to enter the contract, capital \nmust be raised, the technical details \nsuch as the specific intervention and \noutcome metrics are decided, the \nservice provider is procured, and \ncontracts are finalized. After this, in 10 | IMPACT BONDS IN DEVELOPING COUNTRIES the implementation stage, services \nare provided to the population in \nneed, and the performance of the \nservice provider is monitored and \nmanaged. Finally, verification of \nagreed-upon outcomes takes place \nand payment to investors occurs \ncontingent upon their achievement.", " Table 1 summarizes the types of \norganizations that are active in each \nrole in impact bonds in developing \ncountries. A more comprehensive \noverview of the organizations working in each impact bond can be \nfound in the Deal Book, later in the \nreport. Some of the same types of \norganizations are taking on different \nroles in different impact bonds: For \nexample, foundations are acting \nas outcome funders and investors, \nwhile international organizations \nare both investors and service \nproviders. Even within the same \nimpact bond, the same actor may \nplay different roles, for example \nacting as both an investor and a \nservice provider.", " TABLE 1 Actors in impact bonds in developing countries Role Types of organization Outcome funders Foundations or philanthropists; multilaterals, bilaterals or intergovernmental \nfinancial institutions (IFI); governments; non-profits; corporate giving; \ninvestment funds Investors Foundations or philanthropists; multilaterals, bilaterals or intergovernmental \nfinancial institutions (IFI); impact investing firms; banks; investment funds; \ninstitutional investors Service providers Nonprofits, international organizations, nongovernmental organizations, \ndevelopment organizations, charities, impact investors, community organizations Intermediaries Advisory organizations Technical assistance \nproviders Social consultancy organizations, law firms, think tanks, universities Evaluators Research institutes, academics, professional services firms Source: Authors\u2019 research 11 | EARLY LEARNINGS FROM THE FIELD Impact bonds can be developed \non their own, as an individual \ntransaction outcome bond contract, \nor as part of an impact bond fund, \nin which multiple impact bonds \nare contracted for the same issue. \nSeven outcome funds have been \nlaunched in the United Kingdom \n(U.K. Centre for Social Impact \nBonds, n.d.): The first, launched in \n2012, was the Innovation Fund, a 30 \nmillion-pound pilot program that \ncontracted for outcomes for young \npeople aged 14 and over (Griffiths \net al., 2016); this was followed by \nthe Youth Engagement Fund (U.K. \nCabinet Office et al., 2014), launched \nin 2014, aimed at disadvantaged \nyoung people, and the Fair Chance \nFund, also launched in 2014, for \nyouth homelessness. The Social \nOutcomes Fund and Commissioning \nBetter Outcomes Fund ran in \nparallel to support the development \nof SIBs for complex policy areas \nand closed to applications in 2016 \n(Big Lottery Fund, n.d.). A fund for \npreventing rough sleeping (DCLG \nRough Sleeping SIB Fund) was \nannounced in 2016, providing up \nto 10 million pounds in outcome \nfunds for reducing homelessness \n(U.K. Department for Communities \nand Local Government & Prime \nMinister\u2019s Office, 2016). Finally, the \nU.K. government has committed \n80 million pounds to the Life \nChances Fund (U.K. Cabinet Office & Department for Digital, Culture, \nMedia, and Sport, 2016), for \nprospective impact bonds.", " With an impact bond outcome fund, \na rate card may be issued in which \nthe outcome funder lays out the \nprice it will pay for each outcome, \nand multiple service providers can \nbe engaged to achieve different \nresults. In the Innovation Fund rate \ncard, the Department for Work and \nPensions set maximum prices for the \noutcomes it wanted: For example, \nit was willing to pay a maximum of \n700 pounds per participant for an \nimproved attitude to school and 2,000 \npounds for sustained employment \n(U.K. Government, n.d.).1  Funding for \nSIBs was awarded after a competitive \nbidding process.", " In South Africa, two impact bonds \nin design for maternal and early \nchildhood outcomes have also been \ndesigned as impact bond outcome \nfunds (Gardiner & Gustafsson-Wright, \n2016). Another outcomes fund is \nunder discussion for India, and other \noutcomes funds for education and \nrefugees are also under discussion. \nSome have suggested that outcome \nfunds could be a means to reach \nmore beneficiaries, given that the \nindividual transactions to date have \nbeen relatively small (Bellinger et al., \n2016; Rogerson & Sch\u00e4ferhoff, 2016; \nSch\u00e4ferhoff & Burnett, 2016).", " 12 | IMPACT BONDS IN DEVELOPING COUNTRIES 1.  Equivalent to approximately \n$1,200 and $3,300, respectively, \nin June 2011.", " FIGURE 3 Potential and Limitations of Impact Bonds DEMONSTRATED EVIDENCE FOCUS\nON OUTCOMES INCENTIVIZE\nCOLLABORATION INVEST\nIN PREVENTION DRIVE PERFORMANCE\nMANAGEMENT BUILD A CULTURE \nOF MONITORING + EVALUATION REDUCE RISK\nFOR GOVERNMENT CROWD IN\nPRIVATE FUNDING ACHIEVE\nSCALE SUPPORT EXPERIMENTAL\nINTERVENTIONS SUSTAIN\nIMPACT LACKING EVIDENCE 13 | EARLY LEARNINGS FROM THE FIELD GLOBAL LANDSCAPE \nAND DEAL BOOK \nANALYSIS As of August 1, 2017, there were 90 \nSIBs contracted around the world, with \nall but one (the Colombia Workforce \nDevelopment SIB) in high-income \ncountries. Three DIBs have been contracted: Educate Girls in India \n(Gustafsson-Wright & Gardiner, 2016), \nwhich aims to boost enrollment and \nlearning, a DIB for improving cocoa \nand coffee production in Peru (Finance MAP 1 Contracted Impact Bonds Globally >30 10\u201330 5\u201310 3\u20135 <2 14 | IMPACT BONDS IN DEVELOPING COUNTRIES Alliance for Sustainable Trade, 2015) \nand the International Committee \nof the Red Cross Programme for \nHumanitarian Impact Investment (PHII). \nThe Deal Book in Annex D includes \nthe four contracted impact bonds in \ndeveloping countries, as well as 24 \nother impact bonds currently in design.", " Map 1 displays the contracted impact \nbonds around the world as of August \n1, 2017, with the darker shades of \nblue representing a higher number \nof impact bonds. The U.K., where the \nfirst impact bond was developed, \nhas the largest number, at 36, and \nthe United States follows with 16. \nThe Netherlands has eight impact bonds, Australia has six, Canada four, \nPortugal four, Israel, France, Finland \nand South Korea have two each, \nand Austria, Belgium, Colombia, \nGermany, India, Japan, New Zealand, \nPeru, Sweden, and Switzerland have \none each. The ICRC PHII is being \nimplemented in three countries: Mali, \nNigeria and the Democratic Republic \nof Congo.", " The 24 impact bonds in design \nstages in developing countries can \nbe seen in Map 2. South Africa has \nthe most, with four impact bonds in \ndesign, while Brazil, Cameroon, India, \nPalestine, and Uganda each have \ntwo.", " MAP 2 Impact Bonds in Design Process in Low- and Middle-Income Countries 15 | EARLY LEARNINGS FROM THE FIELD SECTORS IN IMPACT BONDS IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) Of the 28 impact bonds in developing \ncountries either contracted or in \ndesign phases, 11 are for interventions \nin the health sector, including the \ntreatment of cataracts, nutritional \neducation for prediabetic women, and \nimproved maternity care. After this, \nemployment (six), agriculture (five), \nand education (four) are the next \nlargest sectors. Social welfare has \ntwo impact bonds. The dominance \nof the health sector in low- and \nmiddle-income countries contrasts \nwith the sectors represented in \ncurrently contracted impact bonds \nin high-income countries, where (as of August 1, 2017) employment is the \nlargest sector (38 impact bonds), \nfollowed by social welfare (28).", " These sectoral differences between \nhigh-income and developing \ncountries likely indicate different \nneeds and priorities. Several of the \nhealth interventions in developing \ncountries focus on areas less likely to \nbe needed by populations in high-\nincome countries, such as water and \nsanitation or malnutrition. Moreover, \nthe priorities in developing countries \nwill also be driven by the priorities \nand strategic objectives of the FIGURE 4 Sectors in Impact Bonds in Low- and Middle-Income Countries \n(CONTRACTED AND IN DESIGN) AGRICULTURE EMPLOYMENT HEALTH EDUCATION SOCIAL WELFARE 16 | IMPACT BONDS IN DEVELOPING COUNTRIES outcome payers; the emphasis on \nhealth also indicates an alignment \nwith the goals of the SDGs. \nHowever, employment appears \nto be a popular sector for impact \nbonds across both high-income and \ndeveloping countries, many of which \nfocus on young people as target \nbeneficiaries. In the U.K., education \nand employability outcomes for \n14- to 24-year-olds were targeted by the Innovation Fund as a means \nof reducing long-term dependency \non the welfare state (Griffiths et al., \n2016). As a sector, employment may \ntherefore be particularly suited to \nthe impact bond model, because \nof the potential for positive public \nand private benefits\u2014particularly if \nthe targeted population is younger \nand will spend more time in the \nworkforce.", " BENEFICIARIES IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) The target populations for many \nof the impact bonds contracted or \nin design in developing countries \nare marginalized, or vulnerable \ngroups that meet specific sets of \ncriteria. Many of the impact bonds \ntarget low-income individuals, \nor those living in disadvantaged \nareas, while women and young \npeople are also frequently targeted. \nSome of the impact bonds have \nspecific criteria to target a number \nof these groups; for example, the Colombia Workforce Development \nSIB targets poor, vulnerable high \nschool graduates displaced because \nof political conflict. The number \nof beneficiaries that the impact \nbonds target range from 100 for the \nPalestine Type II Diabetes Mellitus \n(T2DM) DIB to 400,000 for the India \n(Rajasthan) Maternal and Newborn \nHealth DIB. Several target families or \nhouseholds, rather than individuals, \nsuch as the Sustainable Cocoa and \nCoffee Production DIB in Peru.", " CONTRACT LENGTH IN IMPACT BONDS IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) The length of the impact bond \ncontracts ranges from 10 months \nto five years, with an average \nlength of 42 months. As Figure 5 \nindicates, for those deals for which \nthe contract length is available, \nmore than half are for 30 to 50 months. Two impact bonds have \ncontracts of under two years: the \ncocoa and coffee DIB in Peru and \nthe Colombia Workforce SIB, both \nof which have been contracted. For \ncurrently contracted impact bonds \nin high-income countries, short 17 | EARLY LEARNINGS FROM THE FIELD FIGURE 5 Length of Contract of Impact Bonds in Low- and  \nMiddle-Income Countries (CONTRACTED AND IN DESIGN) LENGTH OF CONTRACT (IN MONTHS) Source: Impact Bonds Deal Book 18 | IMPACT BONDS IN DEVELOPING COUNTRIES contract duration is also the norm. \nThe average contract length is 54 \nmonths, or 4.5 years, although the \nmedian is 48 months, indicating that \nhalf of the contracts are shorter \nthan four years. Contract lengths are \nlikely to emerge from negotiations among stakeholders and need to be \nlong enough to ensure that results \ncan reasonably be expected in \nthat time, but not too far into the \nfuture to discourage investors or to \nconstrain the budgets of outcome \nfunders.", " CAPITAL COMMITMENT IN IMPACT BONDS IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) Many of the impact bonds in the \nDeal Book are still in their design \nphases, which means that data \navailability on up-front capital \ncommitments vary among the deals: \nFor some of the impact bonds, no \nfigure is publicly available, while for \nothers only a range can be reported. \nThe up-front capital commitments for the impact bonds in the Deal \nBook range from an estimated \n$110,000 to $7.5 million, with an \naverage commitment of $2 million. \nFive impact bonds have capital \ncommitments of below $1 million, \nwith another five between $1 million \nand $5 million. The remaining impact \nbond is above $5 million.", " TYPE OF INVESTOR IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) Foundations are the most prominent \ntype of investor organization \nwithin impact bonds in developing \ncountries: Eight of the twelve \nimpact bonds for which investor \ndata are available involve a \nfoundation or philanthropist. \nFour impact bonds will be funded \nupfront by a multilateral, bilateral or an intergovernmental financial \ninstitution (IFI). As the Deal Book \nindicates, foundations range from \nmajor international philanthropic \norganizations such as UBS Optimus \nFoundation (UBSOF) to nationally \nbased foundations such as Fundaci\u00f3n \nCorona, which works only in  \nColombia.", " 19 | EARLY LEARNINGS FROM THE FIELD FIGURE 6 Investor by Number of Impact Bonds They Are Involved in  \n(CONTRACTED AND IN DESIGN IN LOW AND MIDDLE-INCOME COUNTRIES) 1\n1\n1 FOUNDATION OR \nPHILANTHROPIST MULTILATERAL, BILATERAL \nOR INTERGOVERNMENTAL \nFINANCIAL INSTITUTION (IFI) IMPACT INVESTING FIRM BANK INVESTMENT FUND INSTITUTIONAL INVESTOR Source: Impact Bonds Deal Book TYPE OF OUTCOME FUNDER IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) The most common type of outcome \nfunder across the impact bonds in \ndeveloping countries is government. \nTwelve of the 20 impact bonds with \navailable data will be funded by \nsome kind of government entity\u2014\nthese include funding at different levels of government and from \ndifferent departments. Nonprofits \nare also frequent outcome funders \nof the impact bonds in the Deal \nBook. Multilaterals, bilaterals and IFIs \nare also outcome funders in eight \nimpact bonds.", " 20 | IMPACT BONDS IN DEVELOPING COUNTRIES FIGURE 7 Outcome Funder by Number of Impact Bonds They Are Involved in\n(CONTRACTED AND IN DESIGN IN LOW AND MIDDLE-INCOME COUNTRIES) FOUNDATION OR \nPHILANTHROPIST MULTILATERAL, BILATERAL \nOR INTERGOVERNMENTAL \nFINANCIAL INSTITUTION (IFI) GOVERNMENT NON-PROFIT CORPORATE GIVING INVESTMENT FUND Source: Impact Bonds Deal Book RANGE OF RETURNS (CONTRACTED AND IN DESIGN) Only limited data are available on \nthe maximum returns available to \ninvestors. Since many of the impact \nbonds in the Deal Book are still in \ndesign phases, only 11 provide values \nfor this indicator. The maximum \nreturns possible are defined in several \nways across the impact bonds.  \nSome provide a maximum internal \nrate of return (IRR), others have a \nmaximum percentage return, and \nsome cap the returns at a dollar \nfigure. Both the Peru coffee DIB and the Uganda Empowering Women \nand Youth in the Coffee Value \nChain DIB cap returns at $110,000. \nImpact bonds that cap returns on \ninvestment at a percentage value \nrange from 0.05 percent, for the \nMozambique Malaria DIB, to 10 \npercent for the Tajikistan Oxfam \nWASH DIB. Several impact bonds \nhave a cap on the IRR, with India \nEducate Girls DIB capped at 15 \npercent and the two South African \nSIBs capped at 16 percent.", " 21 | EARLY LEARNINGS FROM THE FIELD TABLE 2 Maximum Returns in Impact Bonds in Low- and Middle-Income \nCountries (CONTRACTED AND IN DESIGN) Impact bond name Maximum returns Cameroon Cataract DIB Colombia Workforce Development SIB India Educate Girls DIB India Education DIB Mozambique Malaria DIB 8% 8% 15% IRR 7% IRR 0.05% India (Rajasthan) Maternal and Newborn Health DIB* 8% IRR (UBSOF) \n15% (implementation partnership) Peru Sustainable Cocoa and Coffee Production DIB $110,000 (return of principal) South Africa ECD Impact Bond Innovation Fund - Health 16% IRR South Africa ECD Impact Bond Innovation Fund - Social \nDevelopment 16% IRR Tajikistan WASH SIB 10% (estimated) Uganda Empowering Women and Youth in the Coffee Value \nChain DIB $110,000 *8% annualized for UBSOF component (80% of total investment); 15% return possible for the implementation partnership investment (20%).", " 22 | IMPACT BONDS IN DEVELOPING COUNTRIES EVALUATION METHODS IN LOW- AND MIDDLE-INCOME COUNTRIES (CONTRACTED AND IN DESIGN) Validated administrative data is the \nmost commonly used evaluation \nmethod in developing countries, with \n12 impact bonds using this type of \nevaluation. This method has also been \nthe most popular among impact bonds \nin high-income countries. This may \nbe unsurprising given the high cost of \nquasi-experimental evaluations and \nexperimental designs, or randomized \ncontrolled trials (RCTs). Two impact bonds will evaluate their results using \nhistorical comparison. Two impact \nbonds will use an experimental design \nor RCT, and two more will use an \nRCT in conjunction with validated \nadministrative data (hybrid). Because \nof the relatively early development \nstage of some of the impact bonds \nin the Deal Book, information on \nevaluation methodology is available \nfor only 18 impact bonds.", " FIGURE 8 Evaluation Methodologies in Impact Bonds in Low- and Middle-Income \nCountries (CONTRACTED AND IN DESIGN) VALIDATED \nADMINISTRATIVE DATA HYBRID HISTORICAL EXPERIMENTAL DESIGN (RCT) Source: Impact Bonds Deal Book 23 | EARLY LEARNINGS FROM THE FIELD IDENTIFYING \nAPPROPRIATE \nINTERVENTIONS AND \nSERVICE PROVIDERS Impact bonds can address a \nwide range of development \nchallenges in developing countries. \nAs described in the previous \nsection, most impact bonds in \nthe design or implementation \nstage in developing countries \nhave focused on interventions \nrelated to health, employment, \nagriculture, or education. Given \nthe broad application of impact \nbonds, the process for identifying \nan appropriate intervention\u2014and \noften the service provider(s)\u2014\ntypically depends on the \u201clead\u201d \norganization. Broadly, the impact \nbond process in developing countries \nhas been initiated through two main \napproaches: 1. Outcome funder driven: An \noutcome funder\u2014such as a domestic \ngovernment, aid agency, or \nfoundation\u2014interested in an impact \nbond selects a sector and region \nof its interest. This could be driven \nby a desire to innovate or solve a \nproblem of scaling, effectiveness, or broad political will for a particular \nsector. Alternatively, sectorally \nfocused teams or individuals within \noutcome funding entities explore \nan impact bond to solve a specific \nchallenge (such as incentivizing \nresults). Often a service provider that \nthe funder is familiar with\u2014typically \na previous grantee\u2014is the focus \nof the impact bond. This approach \nto service provider selection is not \nalways possible since some outcome \nfunders, particularly aid agencies, \nadhere to strict procurement policies \nand therefore up-front selection is \nnot possible, even when preferable.", " 2. Intermediary driven: \nIntermediaries can also play a lead \nrole in initiating an impact bond by \nidentifying appropriate interventions \nand service providers.", " a. Often, the intermediary identifies \nthe development intervention \nbased on the priorities of potential \noutcome funders or investors. \nThis approach ensures alignment 24 | IMPACT BONDS IN DEVELOPING COUNTRIES between these key stakeholders \nduring the structuring phase.", " b. In other cases, an intermediary interested in developing an impact \nbond may conduct a market \nanalysis on a specific sector or \nregion to determine interventions \nand service providers that \nare appropriate for an impact \nbond before approaching key \nstakeholders.", " It is essential that practitioners take a \nconsidered approach to determining \nwhether to pursue an impact bond. \nIn general, the first step is to clearly \ndefine the challenge that could \npotentially be solved using this mechanism. Impact bonds should \nbe considered if current approaches \ndo not produce intended results (for \nexample, paying for inputs is not \nyielding desired impact). Second, \na variety of approaches or tools \nshould be considered, including \nsome that may not necessarily be \nfinance-related. Some practitioners \nhave started with the objective of \nstructuring an impact bond without \ncarefully considering whether the \ntool is appropriate. Many warn \nagainst this \u201chammer looking for \na nail\u201d approach. Several criteria \nfor identifying an appropriate \ndevelopment intervention and \nservice provider have been codified \nin the following sections.", " CRITERIA FOR AN APPROPRIATE INTERVENTION A critical first step is to understand \nwhether any type of results-based \nfinancing will improve efficiency \nand effectiveness of service \ndelivery. If paying for inputs \n(such as grant funding for service \nproviders to perform a specific set \nof interventions) is not achieving \nintended development outcomes, \nthen results-based financing, and \ntherefore potentially impact bond \nmechanisms, may be an appropriate \ntool to finance the intervention.", " The rationale for using impact bond \nfinancing can vary depending on \nthe impact bond and the actors. \nSome view impact bonds as having \nthe potential to provide flexible \nfunding for promising but unproven \ninterventions to stretch their impact \n(drive more results, for example).", " Impact bonds benefit from a \nrelatively flexible structure because \nthe key metric(s) for success is the \ndesired outcomes or impact rather \nthan inputs. In theory, impact bonds \nallow service providers to refine \nand adapt their interventions (the \ninputs) during the term of the impact \nbond to achieve a set of agreed-\nupon outcomes. Under this scenario, \nimpact bonds could be appropriate \nas a transition form of financing to \ndetermine the best-practice set of \ninterventions to achieve maximum \nimpact. Practitioners argue in this \ncase that impact bonds should not \nbe the steady state form of financing \nbut rather a transition form, as \nthey are too costly to establish and \nimplement, among other reasons. \nThe amount of flexibility enjoyed by \nservice providers will depend on the 25 | EARLY LEARNINGS FROM THE FIELD specific nature of the deal and the \ndemands of the investors. Investors \nare likely to place more emphasis \non scaling up evidence-backed \ninterventions.", " Other practitioners argue that \nimpact bonds represent an \napproach to financing the scale-\nup of proven interventions. A \npriori, outcome funders should be \nwilling to pay directly for the scale-\nup of interventions with proven \nimpact (and even potential cost \navoidance). The sweet spot for \nimpact bonds is likely somewhere \nbetween experimentation and \nscale\u2014the middle phase\u2014where \nevidence is insufficient (or there is \nsome potential political barrier) for \noutcome funders to pay but sufficient \nfor investors to engage. Thus far, \nenormous scale (large number of \nbeneficiaries) in absolute terms has \nnot been seen in the use of impact \nbonds, although some target very \nparticular populations so scale may \nbe present in relative terms.", " I do think there is a danger that \nwe talk about impact bonds \nas if they are all the same. \nRather, impact bonds can fund \nrisky, transition interventions \nat one end of the spectrum \nand more secure and proven \ninterventions at the other end.", " \u2014Impact bond practitioner An evidence-based theory of change \nis helpful in determining whether an impact bond is appropriate for \nthe intervention since payments \nare tied to outputs (such as school \nattendance) and outcomes (such as \nschool achievement). There is a need \nto have a high degree of confidence \nin the relationship between the \nintervention\u2019s proposed inputs and \noutputs, and desired outcomes \nand impact. All stakeholders\u2014\ninvestors, outcome funders, and \nservice providers\u2014must agree \nthat the proposed intervention (a \nspecific curriculum, for example) \nwill deliver the desired outcomes \n(improved learning progress, school \ncompletion) and impact (higher \nincome as a result of obtaining a \nwell-paying job).", " While the value of an evidence-\nbased theory of change is clear, \npractitioners caution that lack of \ndata can be a challenge in low-\nincome countries and that scoping \nresearch\u2014such as an experimental \nor quasi-experimental evaluation\u2014to \nestablish an evidence-based theory \nof change may be a prerequisite, \nwhile at the same time may be \nexpensive and time-consuming. As a \nresult, there is an important trade-off \nbetween establishing an evidence-\nbased theory of change and \nkeeping design costs within reason. \nIn some situations, an evidence-\nbased theory of change may exist \nfor a similar intervention or the \nsame intervention in a different \nregion, which can be leveraged as \na proxy. Regardless, it is essential \nto ensure that evidence is adequate \nto attract all stakeholders to the \nproposed intervention. In particular, \nas mentioned, an investor requires 26 | IMPACT BONDS IN DEVELOPING COUNTRIES enough evidence to be comfortable \nthat investment in an intervention \nwill result in impact and therefore \nrepayment from the outcome \nfunder. At the same time, if there is a \nvery strong evidence base, outcome \nfunders might rather fund the \nintervention directly, as they can be \nconfident that the inputs will result \nin outcomes and impact.", " You can either identify an \nintervention that already has a \nstrong evidence-based theory \nof change or invest significant \ntime and effort into building \nthe evidence base, but the \ntransaction costs may get \nprohibitively large.", " \u2014Impact bond practitioner CRITERIA FOR APPROPRIATE SERVICE PROVIDER(S) A performance-oriented service \nprovider with implementation \ncapacity is critical to the success \nof an impact bond. Practitioners \nnote that most service providers \nwould describe themselves as \nperformance-oriented, and therefore \nit is important to dig deeper into a \nservice provider\u2019s track record and \nmanagement team to ensure that this \nis the case.", " In addition to a performance culture, \nthe service provider must have the \ncapacity to carry out the impact \nbond activities: to implement \nperformance management systems, \nmonitor key metrics, and refine \nthe underlying business model and \napproach to ensure that maximum \nimpact is achieved. This may mean a \ndramatic culture shift, indicating that \nanother key element for selecting \nservice providers is choosing an organization that is open to change \nand capable of adapting to new \ndemands, which could include, for \nexample, changing recruitment \npractices and training structures.", " When we look at service \nproviders, we ask three key \nquestions: 1.  What impact is the service \nprovider having currently?", " 2.  Do they have adequate \nsystems in place to \nimplement the intervention \nand measure impact?", " 3.  Do they have learning \nsystems and a habit of \nservicing problems in an \nopen manner?", " \u2014Impact bond practitioner 27 | EARLY LEARNINGS FROM THE FIELD KEY TAKEAWAYS: IDENTIFYING APPROPRIATE INTERVENTIONS AND SERVICE PROVIDERS There are two main approaches to identifying an appropriate \ndevelopment intervention for an impact bond: An outcome funder \nselects an intervention based on its priorities, or an intermediary \nselects an intervention based on the priorities of outcome funders \n/investors or by conducting a top-down analysis of a particular \nsector and region.", " There is debate on whether impact bonds are more appropriate \nfor transition financing to determine the best-practice set of \ninterventions to achieve maximum impact, or for scale financing \nto expand reach and impact of proven interventions.", " An evidence-based theory of change for the interventions under \nconsideration is a critical component, as payments are linked \nto outcomes and therefore investors must be confident the \nintervention will drive desired impact.", " Service providers should be performance-driven and have the \ncapacity to implement the intervention.", " The costs of designing and structuring an impact bond can be \nprohibitive and should be considered and managed from early in \nthe development process.", " 28 | IMPACT BONDS IN DEVELOPING COUNTRIES MANAGING \nRELATIONSHIPS WITH \nGOVERNMENT AND \nDONOR OUTCOME \nFUNDERS A NEW ROLE FOR OUTCOME FUNDERS The research identified several issues \nrelated to impact bonds being a \nvery different way of doing business. \nImpact bonds require governments \nand donor organizations to adapt \nto a different role from that which \nthey have held in the past. At the \nmoment, outcome payers do not \nhave established contracting and \nprocurement mechanisms, meaning \nthat negotiations are often extended \nand expensive, and each contract \nprocess follows its own logic, with \nlittle opportunity to learn from \nthe experience of other deals. The \nrelationship between outcome funders \nand service providers may have to \nchange, from being more hands-\non to simply setting the outcome \nmetrics and letting service providers \nmake decisions about the specifics of \ninterventions.", " Historically grant-making institutions may have difficulty loosening control \nand taking a step back from frontline \nservices; there is a sense among \noutcome funders that impact bonds \nmight give them less control over their \nfunding. This is perhaps an unexpected \nfinding, since at the heart of the \nimpact bond structure is the idea \nthat outcome funders will pay only \nfor results, and hence gain a stronger \ncontrol over their finances.", " Although designing impact bonds \nrequires paying considerable attention \nto the price of an intervention, \ndetailed discussions about costs \nmay actually increase an outcome \nfunder\u2019s perceptions of risk. This is an \ninteresting finding from discussions \nwith impact bond practitioners, and \nsomewhat counterintuitive, since more \nclarity about costs and prices should \nallow for more reliable budgeting and \nless risk.", " 29 | EARLY LEARNINGS FROM THE FIELD FIGURE 9 Spectrum of Service Inputs to Outcomes INPUTS PROCESS OUTPUTS QUALITY OUTCOMES \u2022 Staff\n\u2022 Facilities\n\u2022 Equipment\n\u2022 Supplies\n\u2022 Material\n\u2022 Funding\n\u2022 Service recipients \u2022 Service definitions \u2022 Statements of \u2022 Measures of service volume\n\u2022 Units of service work \u2022 Results\n\u2022 Impacts\n\u2022 Accomplishments \u2022 Timeliness\n\u2022 Reliability\n\u2022 Conformity\n\u2022 Tangibles\n\u2022 Other dimensions Source: Martin, 2005 New is expensive relative to \nold, right? Any new system \ninvolves learning how to do \nthings, learning how to write \nthe contracts, learning how to \nevaluate the results, learning \nhow to put together the \ncoalitions.", " \u2014Impact bond practitioner Outcome funders may also be \nwary of the potential for impact bonds to damage their credibility; \nin particular, some critics worry \nabout paying returns to private \ninvestors (Rosenman, 2014). \nAlthough the money paid to \ninvestors above the return of the \nup-front capital is intended to \nreflect the savings gained from \nintervention, governments especially \nhave responsibilities to their \nelectorates and will be keenly aware \nof the potential for negative media \ncoverage in handing over public \nfunds to private bodies.", " 30 | IMPACT BONDS IN DEVELOPING COUNTRIES THE OUTCOME FUNDER PLAYS A CENTRAL ROLE When an outcome funder has not \ndriven the development of the impact \nbond, our research suggests that it \nis important to engage the outcome \nfunder early in the design process. \nSince the outcome funder will be \npaying for the results of the impact \nbond, it needs to be on board with \nthe timeline, costs, and metrics. Early \nengagement is particularly important \nwhen the outcome funder is going \nto be the government, since elected \ngovernments have the democratic \nlegitimacy to set policy priorities, \nmeaning government buy-in should \nlend credibility to the impact bond. \nEven if the eventual outcome funders \nwill be donors, it is still important to \nengage with the domestic government \nin developing countries to ensure \nthat the impact bond aligns with the \nadministration\u2019s policy priorities.", " However, outcome funders may also \nbe uncomfortable with the relatively \nuntested nature of the impact bond \nproduct and may prefer to stick to \nmore traditional methods of service \nprovision. Our research with impact \nbond practitioners suggests that while Engage government very early \non. Even if they will not be \npaying for outcomes. One way \nor the other, all governments \nare there to implement a social \nagenda and they have the \nsocial legitimacy to set public \npolicies.", " \u2014Impact bond practitioner funders may be wary about getting \ninvolved at the beginning of a project, \nthey may be more comfortable \ncoming on board with something \nthat is more developed. There may \ntherefore be a trade-off between \ngetting early buy-in and credibility \nfrom an outcome funder, versus a \nhigher comfort level for outcome \nfunders in later involvement.", " If the outcome funder is not present \nfrom the beginning of development, \nother stakeholders will need to \nfind out who cares about the issue \nand who is willing to pay for it. \nThe outcome funders in currently \ncontracted impact bonds (for which \nthere are available data) have almost \nall been national or local government \nentities, but as Figure 7 shows, this \nfield of outcome funders is widening \nas the impact model is applied in a \nbroader range of contexts. Social \nimpact bonds do not have to be \nfunded by one organization; a pool of \noutcome funders may each contribute \na share. This can provide more \noutcome funding, and for interventions \nat a larger scale, but may also add \nmore complexity, with each additional \norganization bringing a new set of \npriorities and interests.", " Engaging with governments and \ndonors means establishing not only \nwho the outcome funder will be, but \nalso who within an organization is the \nright person, or people, to engage with \ndirectly on an impact bond. Working \nwith the right people within an \norganization is crucial for the success \nof an impact bond. One element of 31 | EARLY LEARNINGS FROM THE FIELD this is ensuring that the impact bond \nhas a champion at a senior enough \nlevel, as well as other levels, that it can \ngain sufficient support and exposure. \nIf the outcome funder is to be the \ngovernment, then decisions need to \nbe made about whether to engage \nwith bureaucratic or political actors (or \nperhaps both): While politicians may \nhave political capital and legitimacy, \ncivil servants may have more permanence and lend the impact bond \nmore consistency.", " If we did make one mistake, it  \nwas that we had somebody who  \nwasn\u2019t senior enough \nchampioning it within the \nministry.", " \u2014Impact bond practitioner CHALLENGES TO ENGAGEMENT 1. Aligning interests: Key to \nmanaging partnerships on impact \nbonds in developing countries \nis identifying the development \nchallenges and social issues that \nstakeholders want to work on. With \nboth governments and donors, it is \nnecessary to understand their areas \nof interest and their priorities. With \ngovernments these priorities may \nbe explicit and transparent, and can \nbe gleaned from ministry plans or \nstrategies.", " Engaging with outcome funders may \nalso involve negotiating tensions \namong participants and aligning \ncompeting priorities. This may be in \nterms of impact metrics: For example, \noutcome funders may be drawn to \noutcomes targets, especially if there \nis not a clear idea of what works, \nwhile service providers and investors \nmay prefer less risky options, such \nas funding outputs. Individual donors \nmay also have multiple motivations \nfor their participation: For example, \nthey may seek not just specific \noutcomes, but also to project a certain image or secure employment \nopportunities. Understanding the \nwider priorities of the different actors \nis important to negotiate a process \nthat is agreeable to all.", " 2. Changing circumstances: One \nof the main challenges to effective \nrelationships with outcome \nfunders is navigating the changing \ncircumstances they face. This is \nparticularly the case when the \noutcome funder is a government \nor a political entity. The election \ncycle can mean that support for \nimpact bonds can disappear very \nquickly, for example after a change \nin government following an election, \nif the incoming administration does \nnot share the commitment to impact \nbonds. Conversely, time constraints \ncan also work in the other direction: \nIn some countries and organizations, \npolicy shifts and processes may \nbe slow, meaning it takes time for \nsupport for impact bonds to build.", " 3. Instability: A wider context of \npolitical instability also creates 32 | IMPACT BONDS IN DEVELOPING COUNTRIES challenges when engaging with \noutcome funders, particularly if \ngovernments are unstable and \nfuture support for impact bonds \ncannot be guaranteed. In these \nsituations, and perhaps specifically \nin developing countries, donors and \nfoundations can play a continuity \nrole, to facilitate working with \ndifferent governments over time. If \nthe civil service is permanent, there \nare also advantages to engaging with \ngovernment at the bureaucratic level, \nbecause this will ensure stability for \nthe impact bond even when political \npower changes hands.", " Even in nonpolitical contexts, \npersonnel changes within \norganizations can create challenges \nfor impact bonds. Institutional \nknowledge about impact bonds \nmay leave when people change \ndepartments, creating a sense \nof instability and slowing down \ncommunication among actors in the \nimpact bond. These issues emphasize \nthe importance of intermediaries in \ndeveloping country contexts and the \nrisk management role they can play.", " 4. Crowding-in funding: Separating \nfinancing for impact bonds from \nexisting funding is a key concern \nfor outcome funders. Governments \ndo not want to face competition for \ntheir limited funding, and charitable \norganizations interested in impact \nbonds are keen to ensure that funding \nis additional and that they will not be \ndiverting money from their donor base.", " 5. Coordinating timelines: Some \ndonors are also unwilling, or unable, \nto commit to paying for outcomes \nso far in the future, as budgets may \nbe done on an annual basis. There is \na pressure to spend existing funds, \nsince underspending may result \nin tighter budgets in future years. \nGovernments that have previously \nengaged in payment by results may \nhave an easier time. A potential \nsolution is a special purpose vehicle \n(SPV): In this way money can be \nallocated up-front to fund the \noutcome payments in future years. \nIn the case of failure to achieve \noutcomes, outcome funders would \nhave to reclaim those preallocated \nfunds.", " 6. Power imbalances: Another \npotential area of tension is specific \nto development impact bonds: \nThere may be power imbalances \nbetween international investors \nand governments in developing \ncountries, which could result in \ncontracts that are disadvantageous \nto the government.", " The main challenges came \nwhen most of the people who \nworked on the impact bond in \nthe beginning started moving \nto other departments or leaving \nthe government.", " \u2014Impact bond practitioner 33 | EARLY LEARNINGS FROM THE FIELD KEY TAKEAWAYS: MANAGING RELATIONSHIPS WITH GOVERNMENT  \nAND DONOR OUTCOME FUNDERS To identify an outcome funder, find out who cares about the issue \nand is willing to pay for it.", " Engage the outcome funder early in the process of designing an \nimpact bond.", " Key to the success of an impact bond is finding a champion in \nthe outcome funder organization who is senior enough to take \nthe process forward. This may involve sensitizing all levels of \ngovernment to the impact bond mechanism, including at the local \nimplementation level.", " Outcome funders will need time to adapt to the new processes.", " Challenges include aligning stakeholder interests; coping with \nchanging circumstances; institutional and personnel instability; \ncrowding-in funding; coordinating timelines; and stakeholder \npower imbalances.", " 34 | IMPACT BONDS IN DEVELOPING COUNTRIES IDENTIFYING METRICS \nAND STRUCTURING \nPAYMENTS METRICS WHEN TO DISCUSS METRICS\nMetrics are at the heart of an impact \nbond, since the payment is contingent \non the achievement of a specific set \nof results. This means the discussion \nof metrics will likely be lengthy and \ntime-consuming, and will need to \nbegin early. Centering the process on \nthe metric could mean establishing \nthe goal of the outcome funder and \nwhat it wants to pay for, and then \nplanning backward from that point.", " It starts with defining the \nmetrics. That\u2019s the heart. If you \ndon\u2019t have the metrics, you \ndon\u2019t have an impact bond.", " \u2014Impact bond practitioner CRITERIA FOR A STRONG METRIC\nSeveral criteria for selecting effective \nimpact metrics in developing \ncountries have been collected \nthrough practitioner experience.", " These include the need for metrics to \nbe measurable, meaningful, and set \nat the right level over an appropriate \ntimeframe. These criteria are similar \nto those found in impact bonds in \nhigh-income countries, where the key \nfeasibility criteria were that metrics \nshould be measurable and meaningful \n(Gustafsson-Wright et al., 2015).", " 1. Measurable: It is important for \nmetrics to be readily captured in the \nprocess of tracking the progress of \nan impact bond, since the release \nof outcome payments will depend \non whether the targets have been \nachieved. Certain desirable outcomes \nmay be difficult to pin down and may \ntherefore be less suitable for inclusion \nin an impact bond. A key part of this \nis ensuring that metrics are simple; \nif targets are too complicated, they \nwill be difficult to communicate to \nservice providers or wider audiences. \nService providers should understand \nwhat they are meant to be achieving. \nSimplicity may also mean cutting 35 | EARLY LEARNINGS FROM THE FIELD down on the number of metrics. For \nexample, having just one headline \nmetric can communicate the central \ngoal of the impact bond.", " 2. Meaningful: The metrics should \nalso be meaningful. They should be \naligned to outcomes or changes \nthat the outcome funder wants to \nsee and that represent a meaningful \nimprovement in a beneficiary\u2019s life. \nThere may therefore be some tension \nbetween the first and second criteria, \nand a balance needs to be struck \nbetween selecting metrics that are \neasy to measure, and measuring \nthe things that are truly valuable to \nresolving the development challenge \nat hand.", " 3. Set at an appropriate level: \nChoosing the payment threshold is \nintegral to setting the impact metrics. \nIf targets are set too high, service \nproviders might hold back from \nmaking more innovative decisions, \nbut setting the targets too low could \nmean paying up when impact isn\u2019t \nreally achieved. Setting the targets at \nthe right level is important to avoid \nperverse incentives, or temptations \nto game the system. The type of \nmetric that is selected will also have \nconsequences for risk-bearing within \nthe impact bond. There needs to be \na balance between selecting metrics \nambitious enough to transfer some \nrisk away from the outcome funder, \nbut not transferring so much risk \nthat investors are discouraged from \nengaging.", " If you\u2019re setting, for example, \ntargets way too high, it can \nactually be quite stifling for \nproviders. If you are setting \nthem way too low, you risk \ncalling it a success and then \nit\u2019s really not.", " \u2014Impact bond practitioner OUTPUTS OR OUTCOMES?\nImpact bonds are meant to shift \nfocus from inputs to outputs or \noutcomes. The choice between \noutputs and outcomes depends \non multiple factors. When outputs \nare not good proxies for outcome \nachievement, then using outcomes \nas the metrics for repayment in an \nimpact bond is preferable. Identifying \noutcomes as success criteria can \nallow for more flexibility on the part \nof service providers. This means, for \nexample, that if more effective ways \nof delivering an intervention are \ndiscovered during the project impact \nbond period, then delivery can be \nadapted. This is likely to be preferred \nby outcome funders when it is unclear \nwhat works to achieve the desired \noutcomes.", " The beauty of an outcome-\ndriven payment model is \nthat actually you\u2019re going to \nbe \u2026 incentivized to kind of \nchange that around.", " \u2014Impact bond practitioner 4. Timeframe: The timeframe for \nthe impact metrics should be short \nenough to please investors, but long \nenough to show results.", " If outputs are, however, a reliable \nmeasure of outcome achievement, it \nmay be preferable to select outputs 36 | IMPACT BONDS IN DEVELOPING COUNTRIES since they are often easier to measure, \nwhich could streamline the evaluation \nprocess. Moreover, outputs may also \nbe measurable over a shorter period, \nwhich is likely to be preferred by \ninvestors. A combination of outputs \nand outcomes may be most desirable, \nallowing for interim payments to be \nmade on outputs and longer-term \npayments to be made on outcomes. \nA selection of both can also distribute \nrisk across different types of investors \nor reduce overall investment risk.", " NEGOTIATING AMONG ACTORS\nStakeholders will need to decide \non what will be measured (inputs, \noutputs, and outcomes), as well as \nthe specific indicators the metrics \nwill capture. This will often mean \nnegotiating the priorities of different \nactors, a process that may slow the \ndevelopment of the impact bond. \nIn particular, investors may be \nuncomfortable with outcome metrics, \nperhaps because of the perception \nthat they are more difficult to achieve \nand therefore that the impact bond \nis a more risky investment, while \noutcome funders will likely seek \nmetrics that come closer to measuring \noutcomes or impact. Each stakeholder \nis situated in a specific context, which \nmay also have consequences for the \nselection of metrics. Investors and \ndonors have wider responsibilities and \nwill have to convince their respective \nconstituencies of the value of their \ninvolvement in the impact bond.", " DATA QUALITY\nIdentifying impact metrics can be a \nchallenge when working in contexts \nwith low data quality, which can be \na particular challenge in low-income \ncountries. It is important to establish whether data are available on the \nmetrics of choice or whether the \nnecessary data can be collected during \nthe intervention.", " Low data quality can exacerbate \nthe tensions among stakeholders. In \nthese environments, outcome funders \nmay prefer to fund on outcomes, if \nthere are no strong data about the \nrelationship among outcomes, inputs, \nor outputs. On the other hand, lack of \ndata may discourage investors and \nservice providers from agreeing to \noutcome metrics, because they may \nperceive the situation as more risky.", " Price setting might also be inhibited by \nlow data quality. Without quality data, \nit is difficult to cost out interventions \nand then to determine how much \nwill be paid out for the achievement \nof the metrics. Gathering quality \ndata on the costs of interventions \nallows governments and donors to \nmake cost/benefit decisions and to \nset achievable targets for service \nproviders.", " The fact that we have poor \ndata in emerging markets is \nexactly why [impact bonds \nare needed], because it drives \neveryone to start collecting \ndata about things that matter, \nlike impact.", " \u2014Impact bond practitioner Short-term solutions to low data \nquality include using proxy measures, \nor relying on indicators that the \ngovernment is collecting already. Yet 37 | EARLY LEARNINGS FROM THE FIELD the low quality of data in developing \ncountries may be exactly the reason \nthat impact bonds are appropriate for \nthose contexts: Impact bonds can drive \na focus on the importance of data, and \ndata collection. This emphasis has the \npotential to lay the groundwork for \nimprovements in data quality.", " COSTS AND PRICING\nIdentifying impact metrics cannot \nbe disconnected from pricing \noutcome payments. First, this means \nestablishing how much weight, and \ntherefore importance, different \nmetrics will have in the distribution of \npayments.", " You can also moderate the \noutcome payment triggers. \nAt the beginning they can be \nmore output based and at the \nend more outcome based.", " \u2014Impact bond practitioner An element of adaptability in the \ndecisions around metrics and prices \nmay be desirable in impact bond \nmetrics. Incorporating flexibility \ninto the design allows for learning \nto take place during the process, \nfor example if prices or metrics \nneed to be changed in light of \nexperience. This could mean \ndesigning the impact bond such that \npayment triggers shift from outputs \nto outcomes over the course of \ntransaction, or that metrics can \nbe changed during the life span of \nthe impact bond contract with the \nagreement of all parties. However, \nthis type of design carries risks, such as lack of clarity about what \nconstitutes success, and therefore \nsafeguards to protect the various \nparties would be necessary.", " INCENTIVE STRUCTURES\nDesigning the impact metrics \nincludes considering the incentive \nstructures, if any, that will be put in \nplace for service providers. Financial \nincentives may not be necessary, \nsince the reputational risk of a \nfailed impact bond may be enough \nto encourage service providers to \nperform effectively. Indeed, financial \nincentives may even be undesirable \nif they create perverse incentives \nfor service providers, diverting them \nfrom achieving the desired impacts. \nHowever, safeguards can be put in \nplace to mitigate this potential risk.", " Impact bond practitioners report \nthat investors have asked for \nservice provider incentives, to drive \nperformance and to align incentives \namong the actors. Financial returns \nto service providers could come in \nthe form of a bonus for overdelivery \nof targets or as a share of the returns, \nor providers could put their own \nmoney at risk in the contract.", " [We are] thinking about \nhow we can price metrics by \nincentivizing people to go after \nthe hardest to reach.", " \u2014Impact bond practitioner Incentives can also be incorporated \nto target the interventions at \nspecific groups of individuals. This \ncould be in the form of setting 38 | IMPACT BONDS IN DEVELOPING COUNTRIES different outcome payments for \ndifferent groups\u2014for example, \nincentivizing service providers to \nwork with the poor or marginalized.", " This should push service providers \nto serve those most in need, rather \nthan those who might be easiest to \nreach.", " STRUCTURING PAYMENTS Once the metrics have been \nestablished, the next step is to agree \non the level of payments and the \npayment schedule.", " DETERMINING THE LEVEL OF \nPAYMENT\nThere are two main elements to \nconsider when determining the level \nof payments. The first is the full cost \n(staff time, resources, and so on) \nof achieving the selected metrics. \nThese costs in results-based payment \nstructures tend to be higher than \nthe total cost of traditional grants. \nTraditional grants pay only for inputs, \nwhile results-based payments pay \nfor outcomes, which may be more \nexpensive to achieve because, \namong other factors, they require \nmonitoring and evaluation. Since \nthis is generally a new way of doing \nbusiness, outcome funders may need \nsupport to understand the reasoning \nbehind these cost differences. \nSome practitioners advocate that \nmonitoring and evaluation costs \nshould be included in the payments, \nso that the service provider is \nresponsible for its own performance \nmanagement. Others advocate that \nmonitoring and evaluation costs \nshould fall outside of the structure. \nIn this case, as a separate line item, \nthese costs may be funded by \nsomeone other than the outcome \nfunder.", " Second, an estimate of the value, or \nprice that can be attributed to the \nset of metrics, is often estimated \nby the outcome funder. This value \nor price will have a relationship to \nthe cost of inaction\u2014the cost that \nwould be incurred in the absence \nof preventive services. These can \ninclude everything from the cost of \nremedial services to broader costs\u2014\nfor example, crime and reduced labor \nproductivity. It must be noted that \nfew outcome funders can confidently \nplace a price on a set of metrics. \nOver time, outcome funders should \nbe in a better position to place a \nvalue on specific metrics as more \nof an impact bond evidence base is \nbuilt in specific sectors and regions.", " Impact bonds may be more \nexpensive than traditional grant-\nbased funding in two ways. First, the \ncost of performance management \nwill need to be incorporated into \ncost estimates. These costs will \nsupport service providers to develop \ntools and strategies for tracking \nprogress and adapting to feedback. \nThe second set of costs involves the \nevaluation at the end of the contract, \nwhich determines whether impact \nmetrics have been achieved. Given \npotentially high costs of evaluation, \nwhether to include evaluation costs \ninto payments is a key consideration. \nOne concern in determining the level 39 | EARLY LEARNINGS FROM THE FIELD of payments, as well as the wider \nrequirements of the impact bond \nstructure, is whether the demands \nwill preclude the involvement of \nsmaller actors, which have less \nbandwidth than larger service \nproviders, to engage with the \nchanges that these deals are likely to \ninvolve.", " SEQUENCING AND TIMING OF \nPAYMENTS\nPayments are made to the investor \nas metrics are achieved. The payment \nschedule outlines the sequencing \nand timing for payments from the \noutcome funder to the investor, and \nthe level of payment depends on \nthe results achieved by the service \nprovider.", " There are two approaches to \nstructuring payments. In some cases, \nthe outcome funder makes one \nbullet payment to the investor at \nthe conclusion of the impact bond. \nThis approach is relatively simple to \nstructure but represents a high risk \nfor the investor because of the length \nof time between the initial investment \nand the outcome payment.", " Alternatively, the outcome funder \nmakes a series of payments to \nthe investor. This tiered approach \nreduces risk for investors and is more \nappropriate for impact bonds with \nlong tenors. In some cases, initial or \ninterval payments are tied to output \nmetrics and then graduate toward \noutcome and impact metrics later \nin the life cycle of the bond. This \napproach ensures that investors can \nbe paid multiple times during the \nimpact bond while still maintaining \nlong-term focus on desired impact.", " In some impact bonds, outcome \nfunds are then \u201crecycled\u201d to support \nfurther service provision, a process \nthat also has the advantage of \nnecessitating smaller up-front capital \ncommitments.", " Another important consideration is to \nincorporate incentive payments for \nservice providers into the payment \nstructure. For example, some impact \nbonds set a maximum return for \ninvestors based on service provider \nperformance. If the service provider \nperforms at a high level, it receives an \nincentive payment directly from the \noutcome funder.", " When you have staggered \npayment metrics, from outputs \nto intermediate outcomes, you \nhave the opportunity to learn \nand see what\u2019s working.", " \u2014Impact bond practitioner Finally, it can be beneficial to include \nprice adjustment mechanisms in the \npayment structure of an impact bond. \nEstimating the payment size and most \nefficient payment schedule during \nthe design phase can be difficult \nand inaccurate: In particular, the \nextended design process, as well as \nlack of evidence on costs, can result \nin underpricing. Therefore, including \na price adjustment mechanism in the \npayment structure can be useful, so \nthat the payment size or schedule can \nbe adapted as necessary during the \nimpact bond. The price adjustment \ncan either occur continuously or be \nre-evaluated at predetermined times \n(such as every 18 months) over the life \nof the impact bond.", " 40 | IMPACT BONDS IN DEVELOPING COUNTRIES KEY TAKEAWAYS: IDENTIFYING METRICS AND STRUCTURING PAYMENTS Criteria for a strong impact metric are that it is measurable, \nmeaningful, and set at the appropriate level.", " Impact bonds are designed to shift the emphasis from inputs to \noutputs or outcomes. Outcomes provide more flexibility, while \noutputs may be easier to measure in the short term.", " Selecting impact metrics means negotiating priorities among \nstakeholders.", " Data quality can be a challenge for identifying impact metrics in \ndeveloping countries.", " Incentive structures for service providers can be used to align their \nincentives with investors or to target marginalized populations.", " Determining the level of payments can be either cost based, in which \nthe service provider estimates the total cost of achieving a set of \nmetrics, or market based, in which the outcome funder sets a price \nit is willing to pay for a set of metrics. There is debate on whether to \ninclude monitoring and evaluation costs in payment amounts.", " Payments from the outcome funder to the investor can be a bullet \npayment at the end of the impact bond, or a set of interim and final \npayments, which can reduce investor risk. Recycling of outcome \npayments over the course of the bond can also reduce up-front \ncapital commitments.", " It is important to create incentives for service providers through \npayment structures and to ensure that there are price adjustment \nmechanisms on a continuous basis or at set intervals.", " 41 | EARLY LEARNINGS FROM THE FIELD DEVELOPING THE \nOPERATING MODEL, \nSTRUCTURING \nTHE VEHICLE, AND \nRAISING CAPITAL OPERATING MODEL While implementation of impact \nbonds in developing countries \nremains too nascent to effectively \ndraw lessons on the operating model, \npractitioners agree that performance \nmanagement and reporting capacity \nare a critical part of the operating \nmodel. Effective performance \nmanagement ensures that service \nproviders track and adapt activities to \nachieve the greatest possible impact. \nPerformance management of the \nservice provider can be supported by \nthe investor, the intermediary, or the technical assistance adviser.", " Further, service providers should \nhave capacity to report interim \nachievements. Also, since reporting \nrequirements for impact bonds can \nbe burdensome, additional resources \nor capacity building that may be \nrequired to support back office \nfunctioning should be built into the \noperating model from the outset. In \naddition, these reporting structures \nneed to be adapted to suit the \nstructure of impact bonds.", " LEGAL STRUCTURE The legal structure is an \nimportant issue for establishing \ncontractual relationships among \nkey stakeholders; however, many \npractitioners note that the legal structure is relatively easy to \ndetermine once the other key \ncomponents (such as metrics and \npayment structure) are in place. \nThere are two possible models: 42 | IMPACT BONDS IN DEVELOPING COUNTRIES individual impact bonds and impact \nbond funds. In an individual impact \nbond, there is a single outcome \npayment contract, while in an \nimpact bond fund there are several \npayment contracts between the \noutcome funder and different service \nproviders on the same social issue.", " As Figure 10 indicates, to date \nthere are four types of impact bond \nstructure, depending on which actor \nhas the contract with the outcome \nfunder. Further details on each \nstructure can be found in Table 3.", " 1. Direct: The contract is between \nthe outcome funder and the service \nprovider, which conducts its own \nperformance management.", " 2. Intermediated: The contract is \nbetween the outcome funder and \nthe investors. The intermediary \nis still likely to play a central role, \nwhich may involve defining outcome \nmetrics and procuring service \nproviders.", " 3. Managed: The outcome funder \nhas a contract with the intermediary, \nwhich is responsible for raising \ncapital and overseeing performance \nmanagement.", " 4. High-risk managed: Another \npotential structure, perhaps most \nrelevant in developing countries, \nis the high-risk managed, where \nfurther specialist knowledge \nprovided by the implementation \nmanager and investor intermediary \ncan mitigate the challenges of \nworking in contexts with constrained \ndata capacity and political or \nfinancial instability.", " A special purpose vehicle (SPV) \nmay be created as a conduit for \nfunds, in which case the outcome \npayment contract is with the SPV. \nIn this case, contract structures \ndepend on the contract between \nthe outcome funder and the actor \nwith the leadership role within \nthe SPV. In a managed impact \nbond structure, the intermediary \nleads the SPV, which contracts \nwith the outcome funder; in an \nintermediated structure, the \ncontract is between the outcome \nfunder and an investor-led SPV; \nand in a direct structure the service \nprovider contracts directly with \nthe outcome funder. Other entities \nmay also take on a similar role to an \nSPV: For example, in the Colombia \nWorkforce Development SIB, the \nMultilateral Investment Fund (MIF) \nof the IDB will channel the outcome \nfunds of the Swiss government.", " Regardless of approach, the legal \nstructure must be tailored to the \nspecific needs of the relationship \nbetween the entities. For example, \nsome practitioners noted that it \nmay not be necessary to include \nperformance management processes \nin the contract between the outcome \nfunder and the investor, while it is \nvital to ensure that this component is \nincluded in the contract between the \ninvestor and the service provider.", " As can be expected, considerations \naround legal structure depend on \nthe frameworks available within the \nspecific country of implementation \nor chosen jurisdiction of domicile. \nThese will vary country by country \nand are driven by numerous \nconsiderations.2 43 | EARLY LEARNINGS FROM THE FIELD 2. Instiglio, with the support of \nseveral legal firms, published A \nlegal road map for social impact \nbonds in developing countries \nin November 2014, a valuable \nresource for understanding the \nlegal considerations for Colombia, \nMexico, South Africa, Mauritius, \nIndia, Chile, and Brazil.", " FIGURE 10 Impact Bond Contracting Structures DIRECT SOCIAL\nSERVICE PROVIDER INTERMEDIATED OUTCOME FUNDER INVESTORS OUTCOME FUNDER INVESTORS MANAGED OUTCOME FUNDER INTERMEDIARY HIGH-RISK MANAGED SOCIAL\nSERVICE PROVIDER INTERMEDIARY SOCIAL\nSERVICE PROVIDER INVESTORS OUTCOME FUNDER INVESTOR\nINTERMEDIARY IMPLEMENTATION\nMANAGER SOCIAL\nSERVICE PROVIDER Source: Adapted from Goodall, 2014 44 | IMPACT BONDS IN DEVELOPING COUNTRIES TABLE 3 Actor responsible by type of impact bond structure Role Managed Intermediated Direct Identify social \nchallenge Outcome funder and/\nor intermediary Outcome funder and/\nor intermediary High-risk \nmanaged Outcome \nfunder and/or \nimplementation \nmanager Outcome funder and/\nor intermediary and/\nor service provider \nand/or technical \nassistance provider Intermediary Determine \nfeasibility based \non impact bond \ncriteria Intermediary and/or \nservice provider and/or \noutcome funder Outcome funder and/\nor service provider \nand/or investor All parties Raise capital Intermediary Intermediary Intermediary and/or \nservice provider and/\nor investor Investor \nintermediary Define outcome \nMetrics Outcome funder and/\nor service provider \nand/or investor Intermediary and/or \noutcome funder Outcome funder and/\nor service provider \nand/or investor All parties Procure service \nprovider Intermediary and \noutcome funder Outcome funder and/\nor intermediary and/or \ninvestor Outcome funder or \nintermediary Implementation \nmanager Contract with \nOutcome \nFunder Intermediary \nor majority \nintermediary-\ncontrolled SPV Intermediary or \nmajority intermediary-\ncontrolled SPV Service provider \nor majority service \nprovider-controlled \nSPV Investor \nintermediary Service provider Service provider Service provider Service provider Manage \nperformance Intermediary Service provider Implementation \nmanager Intermediary \n(commissioned by \ninvestors or majority \ninvestor-controlled SPV) Evaluator or outcome \nfunder Evaluator or outcome \nfunder Outcome funder or \nexternal validator External \nvalidator Provide \nServices Measure/\nValidate \nOutcome\nAchievement Source: adapted from Goodall (2014) 45 | EARLY LEARNINGS FROM THE FIELD GOVERNANCE STRUCTURE The governance structure is another \nimportant component of an impact \nbond. Investors tend to require strong \ngovernance representation, given \nthey are invested in ensuring that \nservice providers are performing as \ncontracted. Long-term sustainability is \none consideration in the governance \nstructure. For example, one impact \nbond has a governance structure \nthat includes representatives from \ninstitutions that are likely to be follow-\non investors or outcome funders. This \nallows these institutions to develop a strong understanding and build \na sense of ownership of the impact \nbond before committing funding.", " There\u2019s an interesting question \nat the strategic-level around \ngovernance: How do you set it \nup in a way that provides the \ninvestors with a certain degree \nof control?", " \u2014Impact bond practitioner RAISING CAPITAL Raising capital from investors is a \ncritical component of any impact \nbond. While the long-term objective \nof impact bonds may be to catalyze \nlarge sums of commercial private \ncapital, to date impact bonds have \nrelied heavily on investors that are \nnot seeking market returns. These \ninvestors\u2014such as corporate or \nfamily foundations\u2014provide soft \ninvestment capital, which is often \nconcessional. These investments \nare critical for building the evidence \nbase for impact bonds and to \ndemonstrate financial and social \nreturns.", " Impact bonds can be both cost- \nand time-intensive to establish. The \ndesign can be particularly costly \nif extensive research or an RCT is \nrequested to validate the impact \nmodel or theory of change, but \nother steps\u2014such as identifying the service provider(s) and outcome \nfunder(s) and agreeing on the \npayment structure\u2014can be time-\nconsuming and costly as well. \nPractitioners emphasized that it is \nimportant to consider the design \ncosts in relation to the fundraising \ntarget.", " Up-front payment from the investor \nto the service provider can be \npartial or in full. Some impact bonds \nare structured more like debt, and \nothers like equity. In some impact \nbonds, returns to investors are \ndirectly invested back into the \nservice provider through the impact \nbond. This results in a much lower \nup-front level of commitment.", " HOW TO ATTRACT INVESTMENT \nCAPITAL \nSeveral criteria for attracting \ninvestment capital to impact 46 | IMPACT BONDS IN DEVELOPING COUNTRIES bonds have been collected through \npractitioner experience.", " 1. Evidence-based theory of change: \nPotential investors must understand \nand buy into the proposed \nintervention\u2019s theory of change. That \nis, each investor must accept that \nthe proposed inputs could produce \nthe desired metrics in a timely and \nefficient manner, and therefore allow \nthe investor to understand the risk \nprofile of the investment. This can \nrequire some education if an investor \nis not familiar with the sector, region, \nor intervention.", " 2. Attractive risk-return profile: \nUltimately, the risk-return profile of \nthe investment must be attractive \nto investors relative to other market \nopportunities. The investor must \nunderstand the risks involved in the \ninvestment, and returns must be \ncommensurate to the risk.", " 3. Patient capital: Given the nascent \nstate of impact bonds, there remains \na need for investment capital that will \ntolerate limited information, difficult \nmarket conditions, and adaptation \nduring the impact bond life.", " We haven\u2019t finished \nfundraising. It\u2019s something \nthat takes a lot of time. You \nhave to educate many of the \ndifferent players.", " \u2014Impact bond practitioner WHEN TO APPROACH INVESTORS\nThe timing on approaching investors \nis important. Investors should be \nengaged early enough in the process \nto ensure that the impact bond \nmeets their risk-return expectations, \nwhile making certain that the design \nprocess maintains focus on the \ndesired outcomes and impact.", " On one hand, it can be easier to \nraise investment capital later in the \ndesign process, once the impact \nbond is structured and the outcome \nfunder(s) is in place. Investors \nmay have more confidence in the \nviability of an impact bond once all \nkey stakeholders are engaged and \nkey terms are defined. On the other \nhand, it can be more difficult to raise \ninvestment capital once the structure \nis finalized if the metrics and the risk-\nreturn profile of the investment do \nnot suit investors.", " Alternatively, investors can be \nengaged early in the design process, \nallowing the investor to play an \nimportant role in determining the \nmetrics and structure. There may be \nconsequences in involving investors \nat this stage, such as a shift from \nimpact metrics\u2014which are often \ndifficult to measure\u2014to output \nmetrics, which are more tangible and \neasily realizable in the short term. A \ncompromise may be to reach out to \ninvestors to gain initial interest and \napproach them again once the impact \nbond is further along in development. \nThe timing of engaging investors will \nbe highly dependent on context and \nactors. However, the process can \nbe lengthy, and investors may lose \npatience if brought on too early.", " 47 | EARLY LEARNINGS FROM THE FIELD KEY TAKEAWAYS: DEVELOPING THE OPERATING MODEL, STRUCTURING \nTHE VEHICLE, AND RAISING CAPITAL Performance management and reporting capacity are a critical \npart of the impact bond operating model.", " Legal structure is relatively easy to determine once the other key \ncomponents are in place. The common approach is a contractual \narrangement between the investor and service provider and \noutcome funder and investor.", " Investors typically require robust governance representation \ngiven their strong interest in monitoring service provider \nperformance; governance can also be a useful tool in attracting \nkey stakeholders that can support the future sustainability of the \nimpact bond.", " While much impact bond discussion centers on the potential \nof impact bonds to attract commercial private capital to \ndevelopment, in reality, investors often provide soft, concessional \ninvestment and aren\u2019t considered commercial investors.", " In theory, raising investment capital after all key pieces, \nparticularly outcome funders, are in place is easy; in reality, \nhowever, investors often want to be involved in early-stage \nstructuring to ensure that the risk-return profile of their \ninvestment is acceptable.", " 48 | IMPACT BONDS IN DEVELOPING COUNTRIES IMPLEMENTING THE \nIMPACT BOND AND \nMEASURING IMPACT EVALUATION The validation of output and/or \noutcome achievement is crucial \nin impact bonds, so considerable \nenergy will go into deciding how \nsuccess will be measured. For impact \nbonds in high-income countries, \nthe methods of evaluation have \nfallen into four categories: validated \nadministrative data, historical \ncomparison, quasi-experimental, and \nrandomized controlled trials. Validated \nadministrative data is by far the most \nfrequently used method in high-\nincome countries, used in over half of \nthe currently contracted impact bonds \nfor which evaluation methodology \ndata are available.", " The choice of evaluation methodology \nin impact bonds depends on five \nconsiderations: what the outcome \nfunder is seeking to achieve; \ncontextual issues, such as the \navailability of data or the presence \nof a comparison group; the timeline \nof the contract and how much time \nthere will be for data collection; the evaluation budget; and the political \nsensitivities around the transaction or \nthe intervention.", " GOAL OF THE IMPACT BOND\nThe most desirable method of \nevaluation is likely to depend on \nthe individual impact bond, and the \nquestion that the outcome funders \n(other actors may have interests as \nwell) want to answer or what they \nare trying to achieve. An outcome \nfunder might have several potential \ngoals: to achieve a set of outcomes \nand pay only for success; to achieve \na set of outcomes at the lowest \npossible price; to determine if a \nparticular intervention or service \nprovider delivers better outcomes \nthan a counterfactual; to compare \noutcomes among interventions or \nservice providers; to determine which \nof a set of interventions or service \nproviders delivers a set of outcomes \nfor the lowest price; to determine \nwhether the impact bond structure \ndelivers better outcomes than funding 49 | EARLY LEARNINGS FROM THE FIELD inputs; or to determine whether the \nimpact bond structure delivers better \noutcomes than traditional results-\nbased financing.", " Beyond this question, other \ndeterminants of evaluation could \ninclude data availability, measurement \ntool availability, evaluation costs, \ncapacity to collect and analyze data, \nand the existence of comparison \ngroups. For example, if the goal is \nto simply achieve a set of outcomes \nand pay only for success, validated \nadministrative data can be used, \nbut if the aim is to determine the \nachievement of outcomes relative to \na counterfactual, an RCT may be the \nmost appropriate methodology. For \nmore detail, see Annex C.", " There is considerable concern around \nwhether impact bonds are suitable \nfor evaluation through an RCT. In \nsome cases, pressure may come from \nstakeholders to evaluate with an RCT, \nbecause of the rigor of this method \nand the perception that it represents \nthe \u201cgold standard\u201d of impact \nevaluation.", " For several reasons, however, an \nRCT may not be the best strategy. \nFirst, with impact bonds the focus is \non results achieved, rather than on \nthe effects of specific interventions, \nso there may be less value in an \nevaluation method that demands \nstrict adherence to particular \ntreatments. Moreover, if the goal is to \nmeet a set of targets, rather than to \ntest the causal relationship between \ntreatments and outcomes, an \nexpensive RCT may be less necessary \nfor this purpose.", " If an RCT is desired, one option for \nincorporating this method could be \nto randomize intervention. Simply \nrandomizing treatment, rather than \ninsisting that everyone be treated \nidentically, could preserve flexibility \nfor providers. In this way, those who \nreceive interventions, regardless of \nhow or when they were treated, could \nbe compared with those in a control \ngroup.", " Selecting the right evaluator for the \nimpact bond is also likely to depend on \nthe kind of evaluation the stakeholders \nhave in mind. Part of the process will \nbe understanding the methodologies \nthat different evaluators plan to use \nand whether these suit the design of \nthe impact bond or the demands of \nstakeholders.", " CONTEXT AND POPULATION\nThe complexity of the populations who \nare often served by the interventions \nis a key challenge for evaluating the \nsuccess of impact bonds. Dealing \nwith this effectively might involve \nsetting different targets by gender, \nsocioeconomic status, or location in \nrural or urban areas. If baseline levels \nof the indicators of interest differ \nsystematically by these groups, setting \ndifferent targets, or measuring growth, \nmay be preferable to a universal \ntarget.", " I\u2019m just not sure we can do \n[an RCT] effectively without \ncompromising flexibility and \ncreating rigidities that will \nscare the investors.", " \u2014Impact bond practitioner 50 | IMPACT BONDS IN DEVELOPING COUNTRIES METRICS AND DATA AVAILABILITY\nAttribution is an additional challenge \nfor evaluating impact bonds in low- \nand middle-income countries. If a \nnumber of different interventions \nare taking place on the population of \ninterest, it will be difficult to establish \nhow much of the change is due to the \nactions of one service provider. Since \nthe focus will likely be on results, this \nmay not be an issue for measuring \nthe success of the impact bond, but it \nmay be a consideration for outcome \nfunders that would like accurate \ninformation on the price of outcomes \nin the future.", " Data limitations are a key concern for \nevaluating impact bonds, especially \nin developing countries, where data quality is often low. Effective \nevaluations depend on good data \ncollection to evaluate the results \nachieved against the targets.", " COST\nEvaluation can be a costly part of an \nimpact bond, and the total cost will \ndepend on the specific requirements \nof the various parties. Collecting \nbaseline data, following large samples, \nand organizing RCTs can be expensive \nand time-consuming. Who pays for \nthe evaluation, and how it is paid for, \nwill depend on the specific impact \nbond. The cost of the evaluation may \nbe incorporated into the cost per \nbeneficiary or be paid directly to the \noutcome evaluator by the outcome \nfunder.", " PERFORMANCE MANAGEMENT As demonstrated above, performance \nmanagement is extremely important \nto investors whose capital is at risk. \nThree broad parties may take on \nthe responsibility for partner and \nperformance management, depending \non the capacity of the service provider, \nthe involvement of the investor, and \nthe desire for long-term sustainability.", " 1. Investor-led: Donors, outcome \nfunders, and other intermediaries are \noften happy to leave performance \nmanagement to the service providers \nor the investors. Investors that are \nvery involved in the impact bond \nprocess may take a management role \ngiven their expertise and resources in \nperformance management.", " 2. Service provider-led: Many \npractitioners argue that performance \nmanagement should be the \nresponsibility of the service provider. \nService providers are the closest to \nperformance data, and building out \nperformance management frameworks \ncan contribute to the long-term \nsustainability of their work. A key \nquestion for this approach is whether \nthe service providers have sufficient \ncapability to implement effective \nperformance management. Adding a \ncapacity-building component to the \nproject can assist in the transformation \nof a service provider into a results-\nfocused organization.", " 51 | EARLY LEARNINGS FROM THE FIELD 3. Third party-led: An alternative \napproach is for the impact bond \nto be structured to include a third-\nparty performance manager. The \nperformance manager would serve \nan intermediary function, managing \nthe relationship with the service \nprovider. A key advantage of this \napproach would be providing external \nconsultation and the ability to \nproblem-solve in the field. This may \nbe especially valuable in the context \nof developing countries, where these \norganizations can fulfill a confidence-\nbuilding role in a risky environment. \nHowever, this requires additional \ncapital.", " Effective performance management \nrequires strong management \ninformation systems. These could \ncomprise work planning and \nforecasting systems, data collection \nand analysis systems, and performance \nreports and dashboards. Each system \nneeds to be tailored to the unique context of the underlying impact bond. \nSystems must be well-structured \nto ensure that new data can be \ninterpreted appropriately, as well \nas flexible enough to adapt should \ndata structures change. The system \nshould also be able to handle different \ndata types (such as service provider \nreports and field reports) from \nseveral organizations. These systems \nmust effectively provide analysis \non a continuous basis to monitor \nperformance and manage it where \nrequired.", " On building in-house capacity \nfor performance management, \nI think that\u2019s key. At some \npoint, you are going to want \nthis to be sustainable, and the \nservice providers must be able \nto continue without an impact \nbond structure.", " \u2014Impact bond practitioner 52 | IMPACT BONDS IN DEVELOPING COUNTRIES KEY TAKEAWAYS: IMPLEMENTING THE IMPACT BOND AND MEASURING IMPACT The most desirable method of evaluation will depend on what the \noutcome funder is seeking to achieve by using payment by results \nfinancing.", " The choice of evaluation methodology may depend on contextual \nissues, such as the complexity of the populations served by the \nintervention, the availability of data, or the presence of a control \ngroup.", " The timeline of the contract may affect the choice of \nmethodology, depending on how much time there will be for data \ncollection.", " The evaluation budget may constrain the choice of methodology: \nRCTs can be expensive, and there may not be adequate funding.", " Political sensitivities around the contract or the intervention itself \nmay shape the choice of evaluation, for example if there is a need \nto demonstrate the causal role of a particular program.", " Effective performance management requires strong management \ninformation systems. These could comprise work planning and \nforecasting systems, data collection and analysis systems, and \nperformance reports and dashboards.", " 53 | EARLY LEARNINGS FROM THE FIELD CONCLUSIONS AND \nNEXT STEPS Tackling the outsize challenges facing \nthe developing world today will \nrequire large amounts of money, but \nit will also take creativity, flexibility, \ncollaboration, and perseverance from \na range of actors across the globe \nincluding the public, private, and \nphilanthropic sectors. This report aims \nto capture the early learnings from \none innovative tool that attempts to \nbring all of those components to bear.", " Social and development impact \nbonds are in their nascence globally \nbut in particular in the developing \nworld, with the first DIB contracted only in early 2015 and a total of three \nlaunched to date (along with one SIB \nin a developing country). While much \nremains to be learned, thus far the \npractitioners engaged in designing \nand implementing impact bonds have \ngathered a great deal of knowledge \nabout what works and what does not. \nAs this and similar tools, which tie \npayments to outputs or outcomes, \ncontinue to grow in their use, our \nhope is that their application improves \nwith the broader goal of improving \nsystems of social service financing and \ndelivery and achieving the sustainable \ndevelopment goals set forth in 2015.", " LOOKING FORWARD At present, most of the impact bond \ntransactions are small and bespoke \n(including those in high-income \ncountries). For impact bonds and \nmore traditional payment by results \nmechanisms to reach greater scale \nin terms of beneficiaries served, a \nnumber of factors will be critical.", " 1. EXPAND THE EVIDENCE BASE\nFunders want to support \ninterventions and service providers that have a record of achieving \noutcomes. In impact bonds, in \nwhich investors also hope to make \na financial return, this is even truer. \nThis means that some evidence is \nneeded to demonstrate to investors \nthat results achievement is likely. \nWhat is tricky is that at least to \ndate, impact bonds seem to fill \na niche where there is sufficient \nevidence for investors to take the \nrisk of investing but perhaps not 54 | IMPACT BONDS IN DEVELOPING COUNTRIES enough evidence for outcome \nfunders to take the risk of scaling \nthe intervention. Risk-averse \ninvestors may require evidence from \nrigorous experimental or quasi-\nexperimental evaluations to be able \nto attribute outcome achievement \nto the intervention, while for more \nrisk-friendly investors, historical data \nmay be sufficient. Regardless of \nthe rigor needed, data on program \nperformance are crucial. For many \nservice providers, however, the cost \nof data collection and evaluation \nis prohibitive, and others have \nlimited capacity to collect and \nmanage data to evaluate impact. \nPhilanthropic funding will be critical \nto help service providers build \ntheir evidence base. Investments in \ndata collection and in evaluation of \nprograms will be money well-spent, \nas this is a first step in shifting the \nfocus away from inputs to outcome \nachievement. Moreover, the more \nproviders are delivering certain \noutcomes with some evidence \nbehind them, the greater the \npotential to attract larger sums \nof (even potentially commercial) \ncapital (see Point 5 \u201csupport \nlegislation\u201d).", " 2. BUILD CAPACITY OF SERVICE \nPROVIDERS\nJust as evidence is needed to \ndemonstrate that an intervention \nor service provider is likely to \ndeliver promised outcomes, there \nneeds to be evidence that service \nproviders can use data to regularly \nmonitor performance and improve \nservice delivery in an iterative way. \nTo date, most service providers \nengaged in impact bonds globally \nhad some experience in performance management or at least had the data \navailable to be used in monitoring and \ncourse adjustment. In some cases, \nsuch training was provided in the \nlead-up to the implementation of the \nimpact bond, and in others, capacity \nbuilding in this area occurred over the \ncourse of the impact bond. Support \nto build capacity of service providers \nin data collection and performance \nmanagement is imperative not only \nto expanding the use of impact \nbonds and other payment by \nresults mechanisms, but for broader \nsystems improvement and outcome \nachievement.", " 3. EDUCATE POTENTIAL \nOUTCOME FUNDERS\nHistorically, most governments, \ndonor agencies, and foundations \nhave funded the delivery of services \nbased on inputs or process, and \none of the key arguments for using \nimpact bonds is that in the past \nthis funding has been unable to \ndemonstrate impact effectively. \nSince paying for outcomes is \nnot the norm, some education or \nsensitization will be key to opening \nthe door to this manner of funding. \nLearning about impact bonds, \nand gaining experience of the \ncomplex process of negotiations, \ntakes time and money. To develop \ninternal expertise, organizations and \ngovernments will have to invest in \nlearning about how impact bonds \nwork. Outcome funders may also \nbe concerned about the time and \ncost of setting up an impact bond. \nNegotiating among a range of \nstakeholders is a daunting process \ncompared with simply funding the \nsame intervention up-front. Tapping \ninto potentially similar experiences 55 | EARLY LEARNINGS FROM THE FIELD could be a good place to start. \nFor example, some governments \nhave used performance-based \ncontracting in military contracts \nwhile others have used traditional \nresults-based financing for social \nservices. Examples from other \ncountries (or the subnational level \nwithin a country) may provide useful \nguidance for potential outcome \nfunders. Clear guidelines and steps \nto take will also facilitate this \nshift. Both scale and sustainability \nof financing based on outputs \nand outcomes will depend on \ngovernment engagement. While \nDIBs may be a short-term solution \nproviding evidence for the use of \nthis model, SIBs, or a variation of \nthe SIB model with government as \noutcome funders, will be critical.", " 4. EDUCATE POTENTIAL IMPACT\nINVESTORS\nAs with outcome funders, impact \nbonds represent a new way of doing \nbusiness for impact investors, even \nfor those with experience investing \nin programs with social returns. \nImpact investors have a key part \nto play in the development of the \nimpact bond field and will need \neducating about the nature of the \ncontracts, the relationship with the \nother stakeholders, and their role in \nthe deal. Impact investors will likely \nhave different priorities and interests \nin the issues they wish to engage \nwith, different risk profiles, and \ndiffering levels of emphasis on social \nversus financial returns.", " 5. SUPPORT LEGISLATION\nIn some cases, legal issues may \nconstrain the contracting of an impact bond (Instiglio, 2014; Z\u00fclow \net al., 2017). There are three main \nlegal aspects to consider. The first \nis related to investments and has \nmultiple levels of complexity. For \nexample, on the investor side, it \nmay not be possible to transfer \nfunds (equity or debt) to the holder \nof the investment funds, such as \nan intermediary or to a service \nprovider before service delivery. \nContracting between investors and \n(government) outcome funders may \nalso be subject to legal constraints \nrequiring, for example, public-private \npartnership laws. The second legal \naspect is related to procurement and \noutcome funding. The procurement \nof service providers may be subject \nto open procurement regulations \nand will likely also require legal \nframeworks related to public-\nprivate partnerships, for example. \nMoreover, because payments based \non outcomes are rarely the norm, \nrules and regulations must allow for \nthis. This can become more complex \nif the monetizable savings for the \nprovision of preventive services are \ndistinctly located from the payments \nfor those services. Finally, fiscal \nimplications for the investments \n(debt and equity) and their \ndividends must be considered. Tax \nlaws may be more or less favorable \nfor investors engaging in these kinds \nof financing structures. Within each \nof those three categories, numerous \nfactors related to risk must be \nconsidered. Entities interested \nin enabling the growth of impact \nbonds should consider supporting \nefforts to enact or modify legislation \nthat facilitates and incentivizes such \ncontracting.", " 56 | IMPACT BONDS IN DEVELOPING COUNTRIES 6. ESTABLISH OUTCOME FUNDS\n In 2015, $153 billion was spent by \ndonors on official development \nassistance (ODA) (World Bank, n.d.) \nto developing countries. This was \nin addition to the money spent by \ndomestic governments on public \nservices. International organizations \nhave recognized the importance \nof domestic resource mobilization \n(DRM), as well as the challenges that \ndeveloping countries face in raising \ntaxes, including poor governance \nand high levels of informality (World \nBank, 2016). Despite the levels of \ndonor aid, and the encouragement \nof DRM, poor human development \noutcomes persist: 800 million \npeople around the world are living \non less than $1.25 a day (United \nNations Development Programme, \n2017), and 263 million children and \nyoung people are out of school \n(Education Commission, 2016). In \nthe U.S., accountability and use \nof evaluation in foreign aid have \nincreased in recent administrations, \nbut more emphasis on data is still \nneeded (Ingram, 2016). Indeed, \ndespite high levels of ODA, results \nare far from guaranteed, and gaps \nin political will or poor governance \nmay constrain the effectiveness of \ndonor spending (The Economist, \n2015). One way of ensuring that \nthis money is better spent is to \ntie funding to the achievements \nof the desired results. Outcome \nfunds, which pool donor funding, \nrepresent a mechanism for putting \nthis into action. In the international \ndevelopment context, outcome funds are under discussion for \neducation and refugees.", " 7. CREATE GLOBAL INVESTMENT \nFUNDS\nWhile spending better is critical, and \nin fact should be the No. 1 priority \nin tackling the SDGs, there is also a \nneed for simply more capital. The \nannual funding gap to achieve the \nSDGs is estimated at $2.5 trillion \n(United Nations Conference on \nTrade and Development, 2014). \nPrivate capital could help to bridge \nthis gap: In 2016, the Annual Impact \nInvestor Survey conducted by the \nGlobal Impact Investing Network \nfound that its respondents invested \n$22.1 billion in almost 8,000 \nimpact investments and planned to \nincrease their investments in 2017 \n(Global Impact Investing Network, \n2017). However, the landscape for \ninvestors in impact bonds remains \nfragmented, and potential investors \nmay be put off by the lengthy \nnegotiation period and often small \nscale of the contracts. Investment \nfunds, such as the Bridges Social \nImpact Bond Fund in the U.K., \nwhich directs private capital toward \nimpact bonds, offer a potential \nsolution to this challenge (Bridges \nFund Management, n.d.). A global \ninvestment fund that pools the \nresources of impact investors \nwith an interest in development \noutcomes could reduce the \ntransaction costs of the impact bond \nprocess and increase the scale of \nimpact investments in results-based \nfinancing.", " 57 | EARLY LEARNINGS FROM THE FIELD"]}, {"paper_id": "#17284", "title": "The RazorS Edge: Social Impact Bonds And The Financialization Of Early Childhood Services", "paragraphs": [" Journal of Urban Affairs ISSN: (Print) (Online) Journal homepage: https://www.tandfonline.com/loi/ujua20 The razor\u2019s edge: Social impact bonds and the\nfinancialization of early childhood services Allison E. Tse & Mildred E. Warner To cite this article: Allison E. Tse & Mildred E. Warner (2020) The razor\u2019s edge: Social impact\nbonds and the financialization of early childhood services, Journal of Urban Affairs, 42:6, 816-832,\nDOI: 10.1080/07352166.2018.1465347 To link to this article:  https://doi.org/10.1080/07352166.2018.1465347 Published online: 14 May 2018.", " Submit your article to this journal Article views: 5019 View related articles View Crossmark data Citing articles: 21 View citing articles Full Terms & Conditions of access and use can be found at\nhttps://www.tandfonline.com/action/journalInformation?journalCode=ujua20 JOURNAL OF URBAN AFFAIRS\n2020, VOL. 42, NO. 6, 816\u2013832\nhttps://doi.org/10.1080/07352166.2018.1465347 The razor\u2019s edge: Social impact bonds and the financialization of\nearly childhood services Allison E. Tse and Mildred E. Warner Cornell University ABSTRACT\nIn a growing number of U.S. cities, social impact bonds (SIBs) introduce an\nexperimental strategy into the politics of fiscal constraint. With limited\npolitical willpower and public funding, some have used SIBs to leverage\nnew support for social programs. We argue that cities that engage in SIBs\nwalk a razor\u2019s edge between promoting public investment and the risk of\ndeepening financialization in the social service sector. We explore efforts in\n3 cities to expand early childhood services through SIBs: Salt Lake City,\nUtah; Chicago, Illinois; and Greenville, South Carolina. We test the balance\nbetween promise and risk through four foci: systemic change, performance\nmetrics, cost structure, and social equity. We show that the context of\npolitical fiscal climate and strategic policy change matters in SIBs\u2019 justifica-\ntion and impact; whereas Salt Lake City and Greenville scaled investment up\nto the state level, Chicago merely plugged short-term local budget gaps.", " Social impact bonds (SIBs) offer an experimental strategy for U.S. cities navigating the politics of\nfiscal constraint. These emerging financing mechanisms represent an alluring possibility: increased\ninvestment in social programs through private financing (Pequeneza, 2018). We explore how U.S.\ncities have maneuvered through the allure of SIBs, and their Trojan horse\u2013like dangers, as they strive\nfor more equitable social service delivery.", " The number of SIBs has grown substantially around the world (A. Fraser, Tan, Lagarde, & Mays,\n2018) since the first SIB was developed in the United Kingdom in 2010 (Disley, Rubin, Scraggs,\nBurrowes, & Culley, 2011). At least 17 have been implemented in the United States (Finlaw, 2017),\nprimarily by cities, counties, and states, with steady Obama-era federal support for feasibility studies\nand technical assistance (Gustafsson-Wright, Gardiner, & Putcha, 2015; U.S. Government\nAccountability Office [GAO], 2015).", " But cities risk losing more than they gain due to the high costs of setting up a SIB (Edmiston &\nNicholls, 2018). In an inherent paradox, though they promise cost savings, SIB transactions are\nexpensive, because each deal is a unique, complex negotiation (Maier, Barbetta, & Godina, 2017;\nWarner, 2013). Though SIBs purport to allow flexibility and innovation, they demand well-docu-\nmented interventions, model fidelity, and strict evaluations to trigger accurate payouts (Berlin, 2016;\nMaier et al., 2017). Their reliance on performance-based management induces gamesmanship (Lowe\n& Wilson, 2017) and may overly skew their focus toward meeting a quantifiable result (Heinrich &\nChoi, 2007). But the most insidious cost of SIBs is their potential to financialize social services by\nmarketizing the \u201cpublic finance value\u201d of\ntheir vulnerable clientele (Neyland, 2017; Sinclair,\nMcHugh, & Roy, in press; Warner, 2015). Chiapello and Knoll (2017) show that SIBs follow a\nvariety of \u201cwelfare conventions,\u201d organizing objectives and stakeholders according to frameworks\nthat prioritize financial profit, competitive exchange, and entrepreneurial innovation. Though some CONTACT Mildred E. Warner\nSibley Hall, Ithaca, NY 14853-6701.\n\u00a9 2018 Urban Affairs Association mew15@cornell.edu Department of City and Regional Planning, Cornell University, 215 W.", " JOURNAL OF URBAN AFFAIRS argue that SIBs can broaden dialogue and intervention in critical social problems by demonstrating\nthe benefits of model programs (Jupp, 2015; Liebman, 2011), others warn that they may narrow the\nscope of policy conversation, reducing it to a transactional discussion of inputs and outputs which\ncan silence broader policy goals (Lake, 2015; Tan, Fraser, McHugh, & Warner, in press; Warner,\n2015).", " Negotiating the razor\u2019s edge of financialization\nCities that walk the \u201crazor\u2019s edge of financialization\u201d do so at some risk to their most vulnerable\ncitizens. By making a simple link between intervention and payout, SIBs may narrow the scope of\nsocial\nto low-cost programs with short-term returns, when more comprehensive\napproaches are needed (Lake, 2016). When SIBs are employed to push for systemic shifts in political\ncontexts facing severe fiscal constraint, however, they can reveal new possibilities for longer-term\ninvestment. Cities must negotiate this razor\u2019s edge if they are to ensure that SIBs actually enhance\nlong-term investment, rather than simply serving as a Trojan horse of financialization.", " investment The financialization of social policy through SIBs risks obscuring visions of greater social rights.\nLake (2016) characterizes the reductive nature of financialization for urban policy: \u201cThe monetization\nof policy goals . . . transforms substantive social outcomes from the status of ends in themselves to a\nmeans for reducing government spending and producing a financial return for investors\u201d (p. 57).\nSimilar lessons from the financialization of physical infrastructure show how broader public objectives\nand positive externalities are sacrificed to narrow, shorter-term, profit-seeking objectives (Sclar, 2015).\nPrivate financing has undermined the broader characteristics of physical infrastructure: universality,\naccess, and maximization of positive externalities (O\u2019Neill, 2010). Today, physical\ninfrastructure\ninvestments have become a new asset class, designed to attract private investment through public\u2013\nprivate partnerships (O\u2019Neill, 2017).", " Financialization risks marginalization of those sectors and people most in need of services by\ndelivering prescriptive social interventions while undermining social inclusion (Shortall & Warner,\n2010) and ignoring the broader structural reforms that should be at the core of social and urban\npolicy (Lake, 2016). SIBs claim that measuring performance will accurately assess an intervention\u2019s\nvalue, as a social service and public expenditure. They position public expenditures as defensible,\naccountable transactions. But by framing social services as transactions, SIBs risk becoming static,\npoint-in-time estimates of social need. If their returns were not so rigidly linked to specific out-\ncomes, they could allow for a more flexible, comprehensive approach. Achieving these outcomes,\nhowever, requires prior evidence of success, fidelity, and consistent results (Maier et al., 2017).", " City, county, and state governments are experimenting with SIBs to expand critical early child-\nhood care and education (ECE) services. We will show that in networks with a broader view and a\nbalance of power across actors in early education, government, and business, SIBs may be able to\npromote expanded ECE investment. But when financial metrics are privileged, market logic narrows\nthe focus. Though financialization risks reducing urban policy to its financial, not social, outcomes\n(Lake, 2016), we find local configurations of actors and their power relations matter. Cities can push\nback against these forces of marketization, effectively \u201criding the wave\u201d of fiscal austerity by adapting\nmarket logics to justify public investment and enhance social policy (Warner & Clifton, 2014). N.\nFraser (2011) and Ogman (2019) have used Polanyi\u2019s (1944) theory of a double movement to look at\nhow progressive coalitions of service providers seek to maintain services in times of austerity by\npartnering with more powerful economic actors in a progressive neoliberal compromise. Progressive\nlower-level service providers are using economic logics to justify enhanced investments in early\nchildhood, making the \u201ceconomic case\u201d and hoping that this will lead to an expansion in social rights\nto care (Warner & Prentice, 2013). In so doing, early childhood interventions walk a razor\u2019s edge\nbetween the potential to insert broader social objectives into a market mechanism and that\nmechanism\u2019s capacity to undermine broader social goals.", " A. E. TSE AND M. E. WARNER We use this framework to discern SIBs\u2019 possibilities for progressive impact. We look inside the\ndetails of SIB design to assess actors, political context, metrics, costs, and social equity. Whether SIBs\ncan concretize the promise of increased investment and broader public policy response depends\ncritically on the configuration of actors and mechanisms in their local and regional context.", " Testing the razor\u2019s edge: SIBs in early childhood education Early childhood care and education services are a proving ground for early SIBs in the United States.\nLessons for emerging SIBs can be drawn from the experience of using economic logics to support\nstate, regional, and local-level economic development policies that expand access and funding for\nchild care. Economic or financial logics have been used for more than 20 years in the United States\nto reimagine ECE services as investments, instead of just expenditures, and thus justify additional\npublic and private support (Bartik, 2011; Warner, 2006). These logics have nudged policy discourse\nin the United States and Canada toward thinking of early education and care as a social infra-\nstructure for economic development and thus an investable social service (Warner & Prentice, 2013).\nECE has a well-documented capacity to produce important social gains. Relatively short and\ninexpensive interventions, such as preschool or parenting education programs, can improve chil-\ndren\u2019s cognitive and social development and enhance school achievement (Heckman, Pinto, &\nSavelyev, 2013; Olds et al., 1997). Broader supports for working families, such as full-day childcare,\nreduce stress on parents and enable increased parental workforce participation and productivity\n(Gornick & Meyers, 2003; Halpern, 2004; Hipp, Morrissey, & Warner, 2017). Early childhood\ninterventions can even support market returns at the regional economy level, if broadly conceived\n(Bartik, 2011; Warner, 2006). These economic logics reconceptualize early care and education as\nsocial\ninfrastructure can expand social rights\nbecause it shifts ECE from a welfare service to an infrastructure and the universality that infra-\nstructure has historically implied (Warner & Prentice, 2013).", " infrastructure for economic development. Social A comprehensive approach that includes human development, the productivity of families, and\nthe regional economy is critical because it opens possibilities for deeper reform (Morrissey &\nWarner, 2007). The simple interventions favored by SIBs in the United States, however, such as\npreschool or parent education, fall short of this approach. Despite the possibilities for broader\nstructural change, they do not address workplace policy and comprehensive support for ECE from\nbirth onward, such as full-day childcare for working parents. Biases toward preschool alone are\nfound in business and economic arguments (Rolnick & Grunewald, 2003). Feminist economists and\neducators voice the broader concerns of parents and ECE specialists, but these have not gained the\ninvestor attention they deserve (Folbre, 2006; Halpern, 2004; Stoney, Mitchell, & Warner, 2006;\nWarner, 2009).", " Policy coalitions matter. As SIBs emerge in the ECE arena in the United States, we must assess the\nextent to which state and local policy actors are building links with private finance to increase ECE\ninvestment and promote further financialization of ECE services at the same time (see also Carter, in\npress; J. Williams, 2018). SIBs claim to demonstrate the effectiveness and cost savings of their social\ninterventions, but the economic benefits of ECE investments are already widely known. Does the SIB\nmechanism itself help to produce information that can shift the policy debate? Or does it merely\nserve as a Trojan horse opening another form of legitimacy for neoliberal policy intrusion into social\nservices?", " Methodology We show, through three ECE case studies, how U.S. cities have adapted SIBs to promote social\ninvestment while simultaneously risking further financialization of the sector. ECE services in the\nUnited States have suffered from chronic underinvestment compared to other countries. ECE clients,\nyoung children and their vulnerable families, are among the most voiceless members of society. But JOURNAL OF URBAN AFFAIRS because of ECE services\u2019 proven economic benefits, SIBs could be suitable tools to expand invest-\nment. Three SIBs currently are operating in the ECE arena in the United States: the South Carolina\nNurse\u2013Family Partnership Pay for Success Project, the Utah High Quality Preschool Program, and\nthe Chicago Child\u2013Parent Center Pay for Success Project. For each case, we conducted a document\nreview of publicly available contract and loan agreements,1 press releases, and journalistic articles.\nWe also conducted 11 semistructured interviews in January and February 2017,2 including five for\nthe South Carolina SIB, three for the Chicago SIB, and three for the Utah SIB. Among these\ninterviews, one was a public agency, one was an evaluator, two were funders, two were service\nproviders, three were technical assistance providers, and two were intermediaries. Interviewees were\nselected based on their involvement in the design, launch, and implementation of the SIB. Interview\nquestions covered program design, intervention scope, and management. During coding of these\ninterviews, themes that emerged inductively shaped the four conceptual foci that guide our analysis:\nsystemic change, performance metrics, cost structure, and social equity. We define these constructs\nbelow and follow with short descriptions of each case, summarized in Table 1: 1. Systemic change: the deeper possibilities of social transformation when sustainable finance and policy change are promoted.", " 2. Performance metrics: the scope of outcome metrics.\n3. Cost structure: the costs and returns in the transaction.\n4. Social equity: the consideration of social inclusion.", " South Carolina Nurse\u2013Family Partnership Pay for Success Project funders supporting Nurse\u2013Family Partnership (NFP) Originating from a consortium of\nin\nGreenville, South Carolina (SC SIB, 2017), this SIB launched in 2016 to expand access to NFP\nhome visitation services for 3,200 first-time mothers across the state (South Carolina Department of\nHealth and Human Services [SCDHHS], 2016). NFP pairs regular nurse visits with low-income\nmothers to improve maternal and child health. The project has four outcome metrics (South\nCarolina Department of Health and Human Services, 2016). Interviewees noted that the goal is to\nsecure sustainable public financing for NFP statewide.", " This SIB is unique in the U.S. context in that it does not offer an investor return. The\nphilanthropic consortium invested $17 million, and the state secured $13 million in Medicaid\nfunding to build a longer-term funding stream for the program.3 The maximum success payment Table 1. SIB project description summary.", " Intervention Clientele South Carolina\nNurse\u2013Family Partnership maternal and\nchild health model\n3,200 First-time low-income mothers Geographic scope State Investor type Philanthropy Investment amount Outcome metrics Outcome payor $17 million, plus $13 million from\nMedicaid\nBirth spacing\nChild injury\nPreterm births\nLow-income ZIP code coverage\nGovernment Success payment\nContract duration (including $7.5 million\n5 years success payments) Utah\nGranite School District\npreschool model\n3,500 Salt Lake City\nstudents\nCity, with statewide\nimplications\nCommercial and\nphilanthropy\n$7 million Special education\navoidance Nonprofit and\ngovernment\nMaximum unknown\n12 years Chicago\nChild\u2013Parent Centers\npreschool model\n2,618 Chicago students City Commercial and\nphilanthropy\n$17 million Special education\navoidance\nKindergarten readiness\nThird-grade literacy\nGovernment $25\u201334 million\n16 years A. E. TSE AND M. E. WARNER is $7.5 million, which will be reinvested into the program for future service provision (SCDHHS,\n2016).", " Utah High Quality Preschool Program The Utah SIB began in Salt Lake City in 2013 as a coordinated effort among the United Way of Salt\nLake, a statewide children\u2019s advocacy organization called Voices for Utah Children, and Granite\nSchool District. The first SIB in ECE, this project expands Granite School District\u2019s high-quality\npreschool model to 3,500 students in Salt Lake City (Gustafsson-Wright et al., 2015). Special\neducation avoidance is the single outcome metric. Interviewees clarified that the goal is to persuade\nthe state legislature to fund preschool (Utah State Legislature, 2017).", " Goldman Sachs and the J.B. and M.K. Pritzker Family Foundation invested $7 million in this SIB.\nInitial outcome payments have been made, but education experts criticized them based on ques-\ntionable metrics, methodology, and financial agreements, claiming that investors were overpaid\n(Popper, 2015).", " Because of the SIB, the state ultimately passed legislation to appropriate funding for preschool.\nThe legislation also caps investor return on future SIBs at 5% above the municipal market data\ngeneral obligation bond rate (State of Utah, 2014).", " Child\u2013Parent Center Pay for Success initiative\nThis SIB launched in Chicago in 2014, the year after Utah\u2019s, to increase the number of Child\u2013Parent\nCenter (CPC) preschool sites in the city and open 2,618 new student slots (City of Chicago, 2014a).\nCPC is a highly researched preschool program (Reynolds, 1997) that focuses on teacher and parent\nengagement. The project has three outcome metrics (City of Chicago, 2014a). Unlike the other cases,\nthis SIB does not appear to have a broader long-term financing goal.", " Goldman Sachs, the J.B. and M.K. Pritzker Family Foundation, and Northern Trust invested $17\nmillion (City of Chicago, 2014a, 2014b, 2014c, 2014d). The Pritzker Family Foundation used a\nprogram-related investment (City of Chicago, 2014d). The maximum potential success payment is\nabout $34 million, though the expected payment is about $25 million (City of Chicago, 2014a). An\ninitial success payment has been made (Sanchez, 2016). The project has been criticized for using\nspecial education avoidance as a metric, under concerns that this incentivizes service reduction, and\nfor using a low-risk model that increases the likelihood of success payments (Sanchez, 2016).", " Analysis\nWe evaluate each case against the four foci outlined in our methodology. Chicago\u2019s and Utah\u2019s SIBs\noperate at the city or metropolitan level. The South Carolina SIB operates at the state level. Our\nfollowing analysis shows how the cases fall along a continuum. We find that South Carolina is most\nlikely to achieve systemic change by both inserting social values in the market and using market\nplayers to shift the political dialogue. Utah is next along the continuum, using market pressure to\nshift political investment but sacrificing social objectives to investor return. We find that Chicago\nranks last because it extracts funds from the public sector in the name of expanding access to\npreschool. A summary of the three cases is provided in Table 2.", " Systemic change Interviewees explained that the goals of both the South Carolina and Utah SIBs are to secure\nsustainable public funding for early childhood services through state legislation. The designers of\neach project committed to this goal first. Then they used the SIB to demonstrate cost savings for\ngovernments and social benefits for vulnerable clientele. Both Utah and South Carolina are JOURNAL OF URBAN AFFAIRS Table 2. Evaluation of early childcare and education social impact bonds.", " Systemic change: presence of a new,\nsustainable public funding stream Performance metrics: breadth of metrics South Carolina\nYes\u2014Medicaid reimbursement Utah\nYes\u2014state income tax Broad\u2014four metrics Narrow\u2014one metric Cost structure: comparison of public\ninvestment to investor profit Maximizes public investment\u2014\nreinvests success payments Maximizes investor profit\n\u2014overestimates impact Social equity: coverage of services to Yes\u2014rural coverage vulnerable clientele Yes\u2014low-income\ncoverage Chicago\nNo\u2014funding ends after\nSIB\nIntermediate\u2014three\nmetrics\nMaximizes investor\nprofit\u2014overpays\ninvestors\nYes\u2014low-income\ncoverage politically conservative states with high child poverty rates. They are difficult contexts in which to\npush any expansion of ECE services.", " In South Carolina, the intention of the SIB was to transition funding for NFP from local philanthropy\nto permanent state and federal sources. A group of philanthropies, including The Duke Endowment,\nBlue Cross Blue Shield Foundation, Greenville First Steps, and the Children\u2019s Trust of South Carolina,\nhad been funding NFP since 2008 (SC SIB, 2017), including an expansion in 2013 through federal\nMaternal, Infant, and Early Childhood Home Visiting program funding (B. Williams, 2013). Services\nwere limited to a few hundred cases in the Greenville area, but the funders recognized that philanthropy\ncould not sustain these services in the long run. They launched the SIB to scale up NFP services and\ndemonstrate its cost savings: It\u2019s a much more expensive intervention than even the childcare vouchers that we provide. When the\ncommunity started NFP, it was 100 clients, so we were at about $450,000.\n. . . That was sustainable in\nGreenville County. We could have done that forever. There were 10 partners that we could have gotten to\nput $50,000 in . . . but what we realized was we didn\u2019t need to serve 100 clients. We needed to serve 600 clients,\nbecause we started looking at our birth rates in Greenville County alone; [there] was no way that local\nphilanthropy was going to be able to sustain a $3 million project. Our United Way\u2019s entire early childhood\ninvestment is $2 million. That\u2019s for everything that they do. . . . Early on, we realized we want to take this\nprogram to scale, and we don\u2019t want to be left with the bill because there\u2019s no way we can sustain that. (personal\ninterview, SC SIB, January 2017) Along with upfront philanthropic investment, South Carolina\u2019s Department of Health and Human\nServices secured eligibility for $13 million in state and federal Medicaid funding, which took 3 years\nto achieve and Obama administration support (SC SIB, 2017). If the SIB is successful, the partners\nplan to advocate for permanent Medicaid reimbursement or other state appropriations so that the\nprogram will have a stable, long-term funding source (SC SIB, 2017). South Carolina\u2019s experience\ndemonstrates the greater value added from an SIB with a long-term perspective and the relative lack\nof value from one used for temporary service provision: Think about where the [SIB] makes sense. It doesn\u2019t make sense for a known recurring cost you\u2019re going to face\nforever more. It can make sense for something where you either want to try to figure how to get something to\nscale or something that\u2019s a little more experimental potentially. In this case, the question was, is there a way to\nscale NFP while also bringing its cost structure down to a place where maybe it could become sustainably\nfinanced in some way to the Medicaid program? (personal interview, SC SIB, January 2017) In Utah, a patchwork of public and private providers offered preschool with zero state investment\nbefore the SIB. State income tax funding for education was restricted to kindergarten to grade 12\n(Utah SIB, 2017). The intention of the project partners was to secure state appropriations for high-\nquality preschool and alter the state\u2019s education funding formula to include preschool. The first bill\nto do so failed to pass in 2013 (Bennett, 2013). Project partners used the SIB as a \u201cproof-of-concept\u201d\nto further educate legislators about the benefits of high-quality preschool in terms of cost savings and\nchild development (Utah SIB, 2017). United Way of Salt Lake contributed $1 million for the first\ncohort\u2019s success payments, on the condition that, if successful, the state would pick up the success\npayments for the second through fifth cohorts.", " A. E. TSE AND M. E. WARNER The Utah SIB effectively shifted state investment in early childhood. In 2014, the Utah legislature\npassed H.B. 96, the Utah School Readiness Initiative, which included an ongoing appropriation of $6\nmillion from the general fund and created the Utah School Readiness Board. The board participates\nin Pay for Success transactions for the state and administers grants to public and private high-quality\npreschool programs (Utah State Legislature, 2014). In 2016, the legislature passed SB 101, the High-\nQuality School Readiness Expansion, which expands preschool slots by appropriating almost $11.7\nmillion for 3 years, mostly using federal funds (Utah State Legislature, 2016). Currently, United Way\nis participating in a ballot initiative to raise taxes and use the additional revenue to fund preschool\nand higher education. They also are working with Salt Lake County to achieve universal access to\npreschool in the county (Utah SIB, 2017).", " In Chicago, the goal of the SIB is more limited. A priority of Mayor Rahm Emanuel is to expand\nearly childhood education services, and the SIB uses private investment to increase access to CPC.\nBut after the final cohort, the city will have to find new funding to sustain the expansion or else\nreduce the number of slots (Chicago SIB, 2017). Unlike the other cases, Chicago shows no evidence\nof a broader political or fiscal goal. There are no plans to further expand preschool after the SIB ends\n(Sanchez, 2016). Our case studies suggest that SIBs that do not have systemic change as an explicit\ngoal and do not deliberately seek to change mainstream public funding arrangements offer less\ncompensation for the known risks of financialization.", " Performance metrics Performance metrics are the bedrock of SIB design. Though performance measures have helped\ntarget educational services (Boyne & Chen, 2006) and can be structured to create positive feedback to\norganizations (Schalock & Bonham, 2003), in SIBs the concern is that these measures may reduce\nflexibility and innovation (Maier et al., 2017). The selection of metrics; their threshold for success,\nmeasurement method, and evaluation rigor; and the performance of the intervention against its\noutcome targets are the sole factors that determine the payout in a SIB contract. Because of this\nnarrowly defined path to success, some researchers argue that SIBs\u2019 performance-based management\nscheme simplifies and distorts the \u201ccomplex reality\u201d of operating a social program, turning the\nintervention into a game to win successful outcomes instead of to support vulnerable clients (Lowe &\nWilson, 2017). This gamesmanship can drive management toward \u201ccreaming and parking, teaching\nto the test, reclassifying, and falsification of data\u201d (Lowe & Wilson, 2017, p. 986). We provide the\nfollowing insights into our cases\u2019 selection and breadth of metrics.", " South Carolina has the most metrics and its evaluation is the most extensive of the three. The\nproject\u2019s four outcome metrics include reduction in preterm births, healthy birth spacing, reduction\nin child injury, and coverage of services to low-income ZIP codes. All metrics were chosen based on\npast evidence of the NFP model\u2019s capacity to achieve them, to reduce the risk to NFP, and to\nmaintain the clarity of the evaluation. The SIB\u2019s designers considered that one or two metrics would\napply too much pressure to NFP to perform within acute parameters. Five or six metrics, however,\nwould reduce or distort each metric\u2019s significance (SC SIB, 2017).", " The Utah SIB uses a single metric: avoidance of special education in kindergarten through sixth\ngrade for preschoolers deemed at risk of needing those services (Utah SIB, 2017). Students who score\ntwo standard deviations below the mean on the Peabody Picture Vocabulary Test are considered \u201cat\nrisk.\u201d Education experts criticized use of the Peabody Picture Vocabulary Test because it is not\ncommonly used to screen for special education and may disadvantage non-English-speaking chil-\ndren when administered in English (Popper, 2015).", " This metric led to an overidentification of at-risk children because of the \u201cfaulty assumption that\nmany of the children in the program would have needed special education without the preschool,\u201d\nwhich in turn led to an overstatement of impact and an overpayment to Goldman Sachs (Popper,\n2015, p. B1). In the first cohort, 595 students attended preschool, 110 were deemed at-risk, but only\none actually used special education in kindergarten (United Way of Salt Lake, 2015). Goldman Sachs JOURNAL OF URBAN AFFAIRS was paid for almost the entire cohort\u2014an unprecedented level of impact for a preschool program\n(Popper, 2015). Although the SIB\u2019s partners considered these criticisms, they argued that their\nevaluation was legitimate and they did not make dramatic changes to the payment structure (Utah\nSIB, 2017). But the criticisms reveal deficiencies in the SIB\u2019s metrics. Repayment to investors is\nmaximized by reducing the number of students receiving special education, something against which\nthe U.S. Department of Education (2016) has warned.", " Chicago broadens its metrics to three: avoidance of special education, kindergarten readiness, and\nthird-grade literacy (City of Chicago, 2014a). This includes a mobility factor to account for program\nattrition (City of Chicago, 2014a), which the Utah SIB does not have (Utah SIB, 2017). The mobility\nfactor ensures that only Chicago Public Schools students who remain in the school district from\npreschool onward will be counted toward success payments. Compared to Utah, this more sophis-\nticated methodology uses additional criteria to circumvent potential criticism about the legitimacy of\na single metric. The special education metric, however, holds by far the greatest weight in the\ncontract. The other two are weak measures by contrast.", " Cost structure financialize public services.\nCost structure is a key indicator of whether or not a SIB will\nComparative analysis of the three SIBs reveals critical differences in the cost structures of each.\nUtah and Chicago overpaid their investors at the expense of public savings and future investment in\nearly childhood because nearly all savings go back to investors as returns. South Carolina will\nreinvest its returns in the NFP intervention. Without an investor premium, its SIB has become a\nvehicle for sustainable financing: It comes back . . . to one of the central irrationalities of these SIB projects, which is that ultimately nobody borrows\nmoney cheaper than governments. You go out and have other entities borrow money and then you repay them\nthat with some kind of interest or something on top of it. From a financial argument, you\u2019re basically throwing\nmoney away for whatever the difference is between the state\u2019s potential borrowing cost and then whatever you pay\nout instead at the end of the project. The solution for us was, there\u2019s no private investor premium being paid in\nour project. We basically threw the money changers out . . . to get the math to work. (personal interview, SC SIB,\nJanuary 2017) South Carolina\u2019s cost structure demonstrates that investor profit is not the focus. NFP reduced its\ncost of service by 25% for the SIB by increasing client\u2013nurse ratios in order to make its services\naffordable and sustainable for state Medicaid funding (SC SIB, 2017). Three metrics use both fixed\nand variable components to determine success, so that results can indicate a zero, intermediate, or\nhigh rate of payment. The fourth metric has a minimum threshold for any success payment (South\nCarolina Department of Health and Human Services, 2016). Overpayment is not possible because\nsuccess payments go toward further service expansion.", " In Utah, the SIB contract is not publicly available. Reports show that Goldman Sachs received a\n$260,000 success payment in 2015 (Popper, 2015) based on savings of $281,550 for the first cohort,\ncalculated using the special education cost rate of $2,607 per student (United Way of Salt Lake,\n2015). The payment to Goldman Sachs constitutes 95% of the savings from the program.", " The SIB\u2019s designers clearly considered cost savings for the public, but how substantial these can be\nwhen nearly all of the savings are given to the investors is unclear: \u201cThey were using the cost savings of\nspecial ed. avoidance to pay back the investors up to a certain point, but after sixth grade that money\nremains with the state, so in theory that helps bolster the state\u2019s education budget as well\u201d (personal\ninterview, SC SIB, January 2017). Utah\u2019s SIB is based on expected cost savings, but how close the forecasts\nare to reality is unsure, a problem also found in other SIB studies (Edmiston & Nicholls, 2018).", " Utah\u2019s SIB designers were concerned with attracting initial investors and compensating them\nfor their investment risk (Utah SIB, 2017), perhaps fairly so because theirs was the first early\neducation SIB in the nation. They used a 5% interest rate to compensate for risk, assuming that at\nleast half of the children in SIB slots would not need special education (Popper, 2015). For the A. E. TSE AND M. E. WARNER first cohort, if the program performed at a certain higher level, the investors could receive a\nhigher interest rate. This higher rate was eliminated for the second through fifth cohorts and the\npotential base rate increased (Utah SIB, 2017). The maximum return for the first and second\ncohorts is capped at 7.26% (Gustafsson-Wright et al., 2015). For future SIBs, Utah\u2019s new pre-\nschool legislation caps the interest rate at 5% above the municipal market general obligation data\nbond rate. The idea behind the cap is to relieve the state from a degree of\ninterest rate\nnegotiation while still enabling them to attract investors (Utah SIB, 2017) and be \u201cgood govern-\nment stewards\u201d (GAO, 2015, p. 47).", " The uncertainty of the Utah SIB\u2019s risk influenced its high payout rate. But Granite School\nDistrict\u2019s preschool program had a history of success and the SIB\u2019s metrics were skewed to increase\noutcome payments. The risk to investors should be relatively low. So why should investors receive a\nhigh interest rate? Though demonstrating to the state legislature that preschool is good for children,\nthe SIB handed over a massive amount of money to investors and has generated negative press about\nits usurious pricing structure (Popper, 2015).", " Despite details in available documentation about the Chicago SIB, its precise costs are uncertain.\nIn the SIB contract, success payments are expected to reach over $25 million, about $21.4 million\nfrom the Board of Education and $4.3 million from the city. But one clause caps possible board\npayments at $30 million (City of Chicago, 2014a). SIB documents cite a return of 5% for each\ninvestor (City of Chicago, 2014b, 2014c, 2014d), but a Brookings Institution report cites that the\nreturn could be up to 6% (Gustafsson-Wright et al., 2015).", " The most surprising factor in Chicago is that investors will receive success payments for 15 years\nafter the intervention. This has been criticized for doubling investor return (Spielman, 2014).\nJustification for this continued return from a single point-in-time intervention is unprecedented\nand not supported by research on ECE. Before its launch, five city council members voted against the\nSIB because of its low risk, high interest rate, and complicated structure, comparing it to Chicago\u2019s\ninfamous parking meter privatization scheme (Spielman, 2014). Chicago\u2019s public school system has\nstruggled with fiscal mismanagement for years; it dealt with a debt rating downgrade in 2015\n(Gillers, 2015) and, heading into the 2017\u20132018 school year, faced a budget deficit in the hundreds\nof millions (Perez, 2017). The reasoning behind the potential $34 million SIB payout demonstrates a\npattern of short-sightedness about the limitations of SIBs. Chicago opened 2,618 temporary slots\nwithout regard to long-term funding and the SIB returns constitute a substantial overpayment to\ninvestors.", " Social equity By expanding access to ECE services, SIBs may promote social inclusion and increase social equity.\nIn all three cases, the target clientele are low-income families. In Utah, over half of the students in\nGranite School District qualify for free or reduced-price lunch and nearly half speak English as a\nsecond language (Utah SIB, 2017). The new CPC sites in Chicago expand services from African\nAmerican families, historically those predominantly served, to Hispanic families (Sanchez, 2016). In\nSouth Carolina, the Department of Health and Human Services went a step further by pushing for\nthe inclusion of a low-income ZIP code metric, which requires that 65% of NFP coverage go to first-\ntime mothers living in a list of predetermined ZIP codes at the time of enrollment, many of which\nare in rural, high-poverty areas (South Carolina Department of Health and Human Services, 2016).\nNFP initially pushed back against this request because of the additional cost of rural outreach and\nrecruitment, but the state persevered and the metric was included (SC SIB, 2017).", " Exclusionary procedures, such as randomized control trials (RCTs), which many SIBs use, also\nraise concerns for social equity. South Carolina is the only case in our study to use an RCT. RCTs are\nconsidered the \u201cgold standard\u201d for evaluation. South Carolina\u2019s Department of Health and Human\nServices required an RCT to ensure the credibility of the SIB\u2019s results (SC SIB, 2017). But RCTs\nrequire a control group, meaning that several hundred mothers will receive neither NFP services nor JOURNAL OF URBAN AFFAIRS referrals to other home visitation programs. This raised concerns for the South Carolina SIB\ndesigners: There has been one negative thing [as] a result of Pay for Success, and that is . . . the idea that we would have to\nhave a control group that we would also track, but cannot benefit from, not only NFP services but any other\nprenatal services. . . . What we have now are a group of incredibly vulnerable, first-time, low-income mothers\nthat we found during pregnancy and know they need our help. Because we need to have some sort of objective\ncomparative data, not only can we not help them, but we can\u2019t tell them other places to go to get that help. . . .\nWe\u2019re looking at probably 100 families now a year that I can\u2019t help that I should be helping. I totally get that if it\nwasn\u2019t for those 100 families, we wouldn\u2019t be able to help all these other families. I totally get that five years ago,\nthere was no NFP and none of these families were getting help. The world is better off because we\u2019re doing what\nwe\u2019re doing, but I look at those 100 families and I think, \u201cBut their kids won\u2019t be better off.\u201d That is probably\nthe one thing I would say keeps me up at night about a Pay for Success study, is this need to have a control\ngroup that I really think is unnecessary. (personal interview, SC SIB, January 2017) To address this concern, some SIBs offer ancillary services to all. South Carolina offers its\nproject participants two additional resources beyond NFP, such as Reach Out and Read pro-\ngramming and free childcare vouchers, but not all families participate. The Chicago SIB includes\nadditional resources for parent engagement through its partnership with Metropolitan Family\nServices.", " RCTs are not the only evaluation option. Collaborative developmental evaluations, which pro-\nmote ongoing learning among partners (Benjamin & Greene, 2009; Patton, 2011), would be more\nappropriate in network situations such as SIBs. But they do not yield clear financial metrics for\nprivate investors.", " The common use of special education avoidance as a metric carries the risk of reducing services\nfor children with disabilities. The U.S. Department of Education (2016) encouraged new SIBs to \u201cset\nstrong guard rails when using special education as an outcome measure\u201d (\u00b6 8). Interviews and\ndocument review in the Utah and Chicago cases show that the projects are not trying to remove\nspecial education access (City of Chicago, 2014a; GAO, 2015; Utah SIB, 2017). But tying SIB\npayments to special education can create a financial incentive to keep services away from children\nwho need them.", " Almost universally, interviewees reported that the high-profile nature of SIBs has begun to insert\nECE into policy and funding conversations in which it would not have been before. SIB stakeholders\nare positive about the impact of their experiments with social finance. They see SIBs as helping them\nserve more children and opening the door to serving many more.", " Discussion: Experimenting on the razor\u2019s edge\nThese cases show the challenges of walking the razor\u2019s edge between using financial logic to expand\nECE investments or to narrow the conversation to financial metrics. The logic was too narrow in\nChicago and Utah and led to overpayment of investors, extracting funds from the very education\nprograms the SIBs were designed to support. By contrast, this logic was used to recast ECE\nexpenditures as investments in South Carolina and Utah so that political support for broader public\nfunding could be built. What determines the difference? We offer four considerations.", " The first factor is political context. The benefit of a SIB is the potential to broaden support for\nsustainable public funding even in conservative political environments with deep fiscal constraint, as\nin South Carolina and Utah. The inherent risk in these cases was more political than financial.\nComparatively, Chicago is a liberal political context already funding CPC. South Carolina addressed\npolitical risk by leveraging a uniquely advantageous policy framework\u2014Medicaid is the responsi-\nbility of government: From a government perspective . . . in South Carolina, early childhood investments [are] not really seen as\nworthwhile roles of government. It\u2019s not the government\u2019s role . . . to tell a mother of a six-month-old to make\nlike that\u2019s a function of\nsure she has a crib that is safely secured.", " . . . In general, politicians don\u2019t feel A. E. TSE AND M. E. WARNER government, but it is government\u2019s role to pay Medicaid costs because that\u2019s a role that\u2019s been assigned to\ngovernment. (personal interview, SC SIB, January 2017) By demonstrating cost savings within an accepted public expenditure, through Medicaid, South\nCarolina already has broadened the scope of ECE services. Although the program\u2019s eligibility for\nMedicaid is limited to 5 years, the intention to make eligibility permanent is a clear goal of the SIB.\nIn Utah, the SIB was launched in a political atmosphere that provided no funding for preschool\n(Barnett, Carolan, Squires, & Clarke Brown, 2013) and delivered the lowest per pupil expenditures in\nthe nation. Children in Utah represent a greater share of the population than in any other state, so\nstate investment is spread thin (Utah SIB, 2017). Despite mistakes in pricing and methodology,\ndemonstrating evidence for public savings induced legislative action to finance preschool.", " The second factor is a strong public actor. Complex contracting requires clear values and\nsophisticated partners who can overcome information asymmetries. The South Carolina state\ngovernment was a strong and creative partner that helped steer the project toward broader aims\nand a sustainable funding stream. Philanthropic investors were not looking to extract economic rents\nfrom the project. By contrast, Utah\u2019s partners had to use their position to attract private finance, at\ngreat cost, to pressure state policy. Private financial interests could take advantage of information\nasymmetries to extract rents. When the state did come on board, it used its political power to legally\nlimit future rent extraction. In Chicago, the same financiers as in Utah took the lead. The city\nexacted no special requirements and thus no broader aims were met. The rent extraction in Chicago\nis the most usurious\u2014claiming success payments for up to 15 years for a single-year intervention,\nwithout a sustainable funding plan. As with the Chicago Skyway and parking public\u2013private\npartnerships (Sclar, 2015), Chicago has promised public revenue to private bidders at the expense\nof further public investment.", " The third factor is coordinated networks. In both South Carolina and Utah, coalitions of local and\nstatewide advocates and philanthropies had supported their respective interventions long before the\nSIBs were implemented. These coalitions worked together with state actors to pursue long-term\npolicy solutions and funding streams, using the SIBs as leverage. But without these networks\ninserting broader objectives, comprehensive approaches could be lost to SIBs\u2019 financializing logic,\nas occurred in Chicago.", " Interviewees argued that SIBs\u2019 appeal is their capacity to translate a complex service into\nquantifiable, investable variables that can change policy. But we find that translation to be overly\nreductive, narrowing the scope of policy dialogue from the inherent social value of ECE to its\ntransactional monetary value. O\u2019Neill (2010) warns us of the danger of losing key features of\ninfrastructure\u2014universality, bundling, access, and positive externalities. Planners know that positive\nexternalities are primary reasons for investing in public services (Del Bo & Florio, 2012; Sclar, 2015).\nWhen a broader perspective is taken, all of society benefits, not just the client. But SIBs are limited to\nshort-term outcomes that can appeal to investors and clearly be attributed to cost savings. Figure 1\nshows how SIBs, by narrowing their focus to the monetizable elements of an intervention, draw\nattention away from broader social inclusion and human development.", " A common refrain of interviewees was that SIBs\u2019 cost savings argument, supported by data, is the\nonly one that is politically effective. As SIBs in ECE track metrics for cost savings, they also track\nevidence on child development. The evidence for cost savings serves as evidence for \u201cwhat works.\u201d\nBut the SIBs\u2019 interventions already were well documented. NFP operates in 42 states and is well\nresearched (Olds et al., 1997). Child\u2013parent centers have been operating in Chicago for 50 years and\nhave been studied closely (Reynolds, 1997). Even the Granite School District preschool program had\n3 years of data supporting it (Utah SIB, 2017). The benefits of preschool, moreover, are well-\ndemonstrated in other states (Barnett & Ackerman, 2006). These ideas are not gambles with public\nmoney\u2014and private finance generated through the SIB is much more expensive than public finance.\nInstead, the main benefit of data and the cost savings argument is the production of localized\ninformation to change political or institutional discourse, our fourth factor. Rather than tools to JOURNAL OF URBAN AFFAIRS Figure 1. How social impact bonds narrow the focus of social interventions.", " implement public services, SIBs are tools to produce information about both cost savings and\nintervention performance, which can shift public investment. The evidence for these interventions\u2019\nsocial and financial benefits already existed but, as other studies show, states in conservative contexts\nwith higher poverty levels and larger populations of people of color are more restrictive with social\nwelfare services and funding (Hahn, Aron, Lou, Pratt, & Okoli, 2017; on state discretion, Michener,\n2017). SIBs can encourage such cities and states to use information to justify social investments: I think data and rigor about results was the number one contributor [to a change in conversation]. Initially we\nthought that the Pay for Success mechanism, which really transfers risk from taxpayers to investors . . . would be\nattractive to policymakers in the long run. To some it is. They still see that as valuable, but I think many people\nsee it as a good way to test and start a new idea. The feeling is, if it works, maybe the state should just fund it\ndirectly without the interest cost of [Pay for Success]. (Utah SIB, 2017) One of the things I like the most about [Pay for Success] is that there\u2019s a rigorous evaluation and we all agree\nto abide by the results, and the payment is specifically associated with the outcome we\u2019re looking for. You\nlook at the overwhelming majority of what we pay for on health or education . . . and a lot of it is based on\neither inertia\u2014nothing drives the appropriations process like inertia\u2014or it\u2019s based on people telling stories in\nfront of committee. The amount of money that we appropriate that is connected to real evidence is\ndamagingly small. (SC SIB, 2017) Policymakers are interested in the local production of data. The performance-based management\ntheory underlying SIB design in fact prioritizes the production of data (Lowe & Wilson, 2017). The A. E. TSE AND M. E. WARNER persuasiveness of local evidence means that SIBs can influence policy conversations by introducing\nthe importance of ECE. As conversations change, policies and public investment can expand to this\nunderserved sector, showing that the narrowing effect of SIBs in Figure 1 can be read in the other\ndirection, at least for Utah and South Carolina. In Utah, the economic return on special education\navoidance changed state investment in early education. In South Carolina, the economic return on\nimproved public health expanded the scope of policy and secured a long-term funding stream. The\npossibility to sway policymakers with higher-level authority means that SIBs have a surprising\ncapacity to scale up public investment in social welfare from the city to the state level.", " The conception of SIBs as the imposition of the market economy onto society is not\nuniform. N. Fraser (2011) points to the importance of the structure of coalitions in determin-\ning whether progressive neoliberal coalitions will push back and redirect the market or merely\njustify new forms of financial intrusion. This explains the potential for SIBs as a vehicle for\nsocial rights and protection; South Carolina pushed back against the financializing nature of\nSIBs by arguing for the inclusion of low-income, rural areas, despite the increased costs.\nPhilanthropic funders acted in the public interest by foregoing the standard investor premium\nto sustain future investment.", " The margin, or the \u201crazor\u2019s edge,\u201d for this broader interpretation is thin because the SIB\u2019s\nlogic privileges financial gains. Figure 2 visualizes our findings on navigating this\nneoliberal\nmargin. South Carolina has no premium and no profit, instead inserting social objectives into\nthe market to move the political conversation in a conservative state. Utah ran the financial risk\nof a SIB with a long view of moving state politics, but the investor payout and methodology\nundermined its social mission by diverting public investment to private investors. Chicago\ndemonstrates the risk of financialized logic by providing short-term benefits to investors and\nclientele without the balance of long-term social objectives. This SIB falls totally on the extractive\nside of the market.", " Figure 2. The razor\u2019s edge: Do SIBs enhance investment or deepen financialization?", " JOURNAL OF URBAN AFFAIRS Conclusion: Cautioning the use of social impact bonds We have shown that SIBs in the United States have the capacity to overcome their internal financial\nlogic when they include long-term objectives to build systemic change. These objectives are not\nwritten into the contracts. They are not part of the design. But they leverage the capacity of SIBs to\nproduce localized information. This capacity is especially useful in conservative political contexts\nbecause strategic networks of social advocates can use that localized information to nudge discourse\nand alter policy. This capacity is not among the advertised claims about SIBs, but we argue that SIBs\nin the United States can be leveraged to scale up public investment to the state level by strategically\ntargeting long-term policy change.", " SIBs in the ECE sector should seek to create sustainable investment. A SIB that only pays for\ncurrent costs and does not consider how to sustain investment is not worth the transaction cost or\nthe interest rate. SIBs that overpay their investors divert funding from social services, as in Chicago\nand Utah. South Carolina shows that an investor premium is not necessary and that philanthropies\ncan significantly reduce SIB costs through their traditional grant-making roles.", " Grappling with fiscal and political constraints, U.S. cities are trying to find new sources of funding\nfor critical social interventions, but they should be wary of SIBs\u2019 marketizing framework. Although\nour cases show the potential for new investment, SIBs may undermine their own social objectives\nthrough their financializing metrics. The risks of high transaction costs, overpayment to investors,\ninflexible implementation, and loss of focus on vulnerable clients are significant. Without the benefit\nof scaling up public investment and shifting policy, cities invite a Trojan horse of public value into\ntheir neediest communities when they implement a SIB. We caution cities to pay attention to these\nrisks as they launch future SIB experiments.", " Notes 1. Contract and loan agreements are listed in the References section with dates and involved parties, typically\nincluding the public payors, intermediaries, and investors (City of Chicago, 2014a, 2014b, 2014c, 2014d;\nSCDHHS, 2016).", " 2. To preserve anonymity, citations for interviews are written as SC SIB 2017 for South Carolina interviewees, Utah SIB 2017 for Utah, and Chicago SIB 2017 for Chicago.", " 3. Under the Affordable Care Act, preventive health programs, such as the Nurse\u2013Family Partnership, can be\nfunded through federal Medicaid dollars. States have wide latitude in how they use federal Medicaid dollars, so\nfunding levels and eligibility criteria for reimbursement of services differ greatly by state. The Medicaid funding\nfor NFP in South Carolina is the first of its kind in the country.", " An earlier version of this article was presented at the Social Finance, Impact Investing, and the Financialization of the\nPublic Interest Conference organized by Eve Chiapello and Lisa Knoll at Hamburg University in Germany and\nsupported by the Humboldt Foundation.", " Acknowledgments Funding This research was supported in part by a Luigi Einaudi Chair Innovation Grant from the Cornell Institute for\nEuropean Studies and a grant from the Atkinson Center for Sustainable Futures.", " About the authors\nAllison E. Tse is a master\u2019s student in City & Regional Planning at Cornell University concentrating on economic\ndevelopment. She researches social impact bonds as well as fiscal policy and austerity in local governments in the\nUnited States. Her past work has focused on rural community and economic development in the United States.", " A. E. TSE AND M. E. WARNER Mildred E. Warner is a Professor in City and Regional Planning at Cornell University. Her research focuses on the\neconomic development, social service, and environmental policies of local governments. She is an international expert\non new models of service delivery and finance, especially in the context of privatization and decentralization.", " Mildred E. Warner http://orcid.org/0000-0002-0109-338X ORCID"]}, {"paper_id": "#17755", "title": "Independent Evaluation of the UK Department for International Developments DIBs Pilot Programme  Full Report", "paragraphs": [" Independent Evaluation of the UK Department for International Development\u2019s \nDevelopment Impact Bonds (DIBs) Pilot programme \u2013 Full Report Evaluation Report June 2019 Acknowledgements and disclaimer Acknowledgements The lead authors of this report are Korina Cox (Team Leader), James Ronicle (Lead Analyst), \nKay  Lau  (Project  Manager)  and  Sara  Rizzo  (Analyst).  The  other  team  members  who  have \ncontributed are Zachary Levey (DIBs expert), Jennifer Armitage (VfM lead) Hashim Ahmed, \nAlma Agusti Strid and Catie Erskine (Researchers). Professor Alex Nicholls has peer reviewed \nthis report.", " Thanks to the DFID DIBs team, stakeholders of the ICRC HIB, Village Enterprise DIB, Quality \nEducation India DIB and Cameroon Cataract Bond, as well as DIB stakeholders who supplied \ninformation and views.", " Disclaimer This  report  has  been  prepared  by  Ecorys  for  DFID,  for  services  specified  in  the  Terms  of \nReference and contract of engagement.", " This contract is provided under the GEFA contract. In line with the terms and conditions of the \nGEFA contract, all intellectual property rights in all material (including but not limited to reports, \ndata, designs whether or not electronically stored) produced by the Supplier or the Supplier's \nPersonnel pursuant to the performance of the Services (\"the Material\") shall be the property \nof the Supplier. Under the terms of the contract, Ecorys, as the Supplier hereby grants to DFID \na perpetual, world-wide, non-exclusive, irrevocable, royalty-free licence to use all the Material. \nDFID will be the final owner of the findings of the evaluation.", " Ecorys  will  store  all  material  related  to  the  evaluation  on  a  secure  drive  which  will  only  be \naccessible by members of the Evaluation Team. Data will be managed under the terms and \nconditions of the GEFA contract.", " Relevant links A  Summary  of  this  Evaluation  Report,  together  with  the  Evaluation  Inception  and  Design \nReport, are published at: https://devtracker.dfid.gov.uk/projects/GB-1-204722/documents i Executive Summary Overview This report is the Research Wave 1 Evaluation Report as part of the Independent Evaluation \nof DFID\u2019s Development Impact Bonds (DIBs) pilot programme. The DIBs pilot programme runs \nover a period of almost six years, from June 2017 to March 2023. DFID has allocated GBP \n6.3  million  for  the  three  projects  under  the  DFID-supported  DIBs  pilot  programme:  ICRC: \nHumanitarian  Impact  Bond  for  Physical  Rehabilitation;  Village  Enterprise:  Micro-Enterprise \nPoverty Graduation Impact Bond; and support to British Asian Trust: to design impact bonds \nfor education and other outcomes in South Asia. The programme aims to test whether DIBs \nare a tool that DFID is able to use, and start to generate understanding of how and when DIBs \ncan add value in DFID programming and support DFID\u2019s commissioning, management, and \neffectiveness in delivering programmes on a PbR basis.", " The DIBs pilot programme has the following objectives: Objective 1:  Understand the  process  of  agreeing  and managing  a project  on  a DIB  basis, \nincluding \nfinancial \nmanagement.", " funding  arrangements,  assurance  and implications for  DFID\u2019s Objective 2: Build an understanding of whether DIBs enable efficient and effective delivery of \nprogrammes in DFID priority results areas, and how they can support innovation.", " Objective  3:  Build  an  understanding  of  the  conditions  for  DIBs  to  be  an  appropriate \ncommissioning tool and the costs and benefits of using them.", " Objectives and scope of the evaluation As set out in the Terms of Reference (ToR), a DIB is a mechanism for drawing external finance \ninto payment-by-results (PbR) projects. In a DIB a donor commits to paying for development \nresults if and when they are achieved. A service provider steps up to deliver the prescribed \nresults. The key difference from standard PbR is that a DIB brings in third party \u201cinvestors\u201d \nwho  provide  the  service  provider  with  the  investment/working  capital  needed  to  deliver \nactivities designed to achieve the results. Under the DIB model, the investor also takes on a \nportion of the financial risk associated with failing to deliver the prescribed outcomes.", " The objective of the evaluation is to generate learnings and recommendations on the use of \nDIBs  as  an  instrument  for  aid  delivery,  by  using  the  experience  of  the  DFID  DIBs  pilot \nprogramme  to  generate  learning  to  inform  DFID\u2019s  future  policy  aiming  to  make  the  most \neffective use of DIBs.  The evaluation will also help DFID and pilot project partners evaluate \nwhether the tools they are developing are useful, scalable and replicable.", " The  scope  of  the  evaluation  is  the  three  projects  funded  and  supported  under  the  DFID-\nsupported DIBs pilot programme: \u2022 International Committee of the Red Cross Humanitarian Impact Bond for Physical \nRehabilitation (ICRC HIB); \u2022  Quality Education India development impact bond (QEI DIB); and ii \u2022  Village Enterprise micro-enterprise poverty graduation Impact Bond (VE DIB).", " Additionally,  since  the  evaluation  inception  phase,  a  fourth  DIB,  the  Cameroon  Cataract \nBond, has been added to the evaluation. This DIB finances the operationalisation of a hospital \nproviding cataract surgeries in Cameroon. This is not a DFID-funded pilot, but has been added \nto  the  evaluation  to  increase  the  number  of  DIBs  under  examination  and  therefore  to \nstrengthen the comparative analysis and findings.", " Evaluation  of  these  DIB  pilots  will  provide  evidence  of  how  this  DIB  mechanism  works  in \ndifferent circumstances.", " The two evaluation questions are: \u2022  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.", " \u2022  EQ2: What improvements can be made to the process of designing and agreeing DIBs to increase the model\u2019s benefits and reduce the associated transaction costs?", " This report presents the evaluation\u2019s initial findings against these questions. Given the stage \nof the interventions funded by the DIBs, findings are focused on the design stage. The effects \nof the DIB in terms of the intervention quality and outcomes remain to be seen. Furthermore, \nit is important to note that DIBs are still in a pilot phase, and the lessons learned draw on a \nsmall number of \u2018test cases\u2019. These findings will continue to be refined and developed based \non additional evidence over the remainder of the evaluation.", " The future research waves will explore how the DIB affects the delivery and performance of \nthe intervention.", " Methodology and evidence base The evaluation is based on an evaluation framework that builds on a range of hypothesised \nDIB effects and indicators. As part of the inception phase, the evaluation team drew on the \nliterature  in  order  to  understand  hypotheses  around  how  the  DIB  model  might  affect \ninterventions, and developed a list of DIB effects and indicators.", " The focus of the evaluation is the DIBs funding mechanism. The evaluation is interested in \nunderstanding the \u2018DIB effect\u2019, that is, the effect of using a DIB instead of a grant or other \nPbR mechanism. A key challenge is trying to isolate the effect of the DIB from other factors \non the different stakeholders and phases, and from the PbR effect. We use a combination of \nprocess tracing and comparative analysis to achieve this. For the next research waves, we \nwill also focus on attempting to isolate the DIB effect from the PbR Effect.", " The evidence base for this research wave is derived from the consultations and programme \ndocument review undertaken at the individual DIB level, the programme level and sector level. \nThe table below sets out the list of data sources we have drawn upon, mapped against the \nthree levels of the evaluation.", " iii Individual  Project  level  Projects  under  the  DIBs  pilot \nprogramme and identified comparison projects Interviews with key stakeholders1 (56) \u2022 \n\u2022  Programme design documents \n\u2022 \nInternal project level M&E data \n\u2022  Project reporting \n\u2022  Data from comparable projects and previous phases \n\u2022  Cost data \n\u2022  Evaluations and learning activities Programme \nDIBs \nprogramme level \npilot \u2022 with \nInterviews \nDFID  staff,  within \nthe DIBs team \u2022  Review of \nlevel programme \ndocumentation Wider DIB sector \u2022 Interviews  with \nDIB  experts  and \nstakeholders (8) \n\u2022  Review  of  key \nand \nliterature \nlearning reports Conclusions The summary interim assessment against the evaluation questions are: EQ1: Assess how the DIB model affects the design, delivery, performance and \neffectiveness of development interventions.", " The DIB mechanism has made it possible to implement Payment by Results (PbR) contracts \nin contexts where, previously, this would not have been possible because the projects were \ntoo  risky  or  too  large.  This  is  primarily  due  to  the  new  partnerships  created  between \ngovernments,  donors,  delivery  partners  and  (to  a  degree)  the  private  sector,  in  which  the \nfinancial risk is shared between these groups. The DIB has fostered new working relationships \nbetween  stakeholders  and  has  led  to greater  levels  of  collaboration  than  is  normally  seen, \nprimarily  because  the  DIB  aligns  all  stakeholders\u2019  interests  but  also  because  the  intensive \ndesign stage fosters closer working relationships between partners. A large amount of work \nhas been done in all four DIBs to build a stronger performance management infrastructure, \nincluding investing in new monitoring systems and working closely with the service providers \nto embed adaptive management systems.", " Two of the most significant landmarks in these projects is that they have demonstrated that \nprivate investors are willing to take on sizeable levels of risk in impact bonds (i.e. in the ICRC \nHIB, which includes private investment), and it is possible to launch impact bonds at a larger \nscale (i.e. the QEI DIB, which builds on the Educate Girls DIB).", " Whilst the DIB mechanism has reduced some (financial) risks for outcome funders and service \nproviders, it has increased others, such as reputational risk. There were quite strong concerns \namongst both outcome funders and service providers around using a new funding mechanism, \ndue  to the  uncertainties of  using  a  new  model,  alongside  the  heightened  attention  that  the \nmechanism  brings  to  the  projects,  increasing  unwanted  exposure  should  the  results  not \nmaterialise. This created a level of risk aversion, which we believe has diminished the level of \nrisk and innovation in the interventions \u2013 all four DIBs are funding service providers with some \ntrack record and interventions with some evidence bases.", " Some of the DIB effects seem to be closely intertwined with other effects. For example, some 1 Including designers, service providers, other outcome funders, outcome verification agents, project/performance \nmanager, project evaluators/learning partners and investors.", " iv are more \u2018novelty effects\u2019 - that is they exist because these are the first set of DIBs, and will \nlikely  diminish  over  time.  This  seems to  be  the  case for the  levels  of risk  aversion  and the \ncosts. It  is  possible  (though  not  certain) that  these will  reduce  in future DIBs.  Furthermore, \nbecause the increased rigour in the outcomes measurement is a consequence of attaching \npayments to outcomes, this effect was also seen in some of the PbR comparator sites, and is \ntherefore more of a \u2018PbR effect\u2019 than a DIB effect per se.", " The findings from these four DIBs in relation to the DIB effect broadly mirror the findings from \nthe wider literature. This is promising \u2013 the evidence of the DIB/SIB effect is currently weak, \nand so this evaluation provides further validation and a stronger understanding of how impact \nbonds affect the design and set-up of projects.", " Finally, with these benefits have come additional complexities and costs. All four DIBs were \ncomplex to design and launch, which resulted in large development costs. It is too early to \nconclude whether the benefits outweigh these costs. Stakeholders were confident that lessons \ncould be learnt from these DIBs that would reduce the complexity and cost of future DIBs, as \nwe explore in the following section.", " EQ2: What improvements can be made to the process of designing and agreeing \nDIBs to increase the model\u2019s benefits and reduce the associated transaction costs?", " Cost analysis \nAll stakeholders confirmed there had been additional costs - either actual, in kind or pro bono \n\u2013 for staff time and consultancy in designing and setting up the DIBs. These costs tend to be \nincurred by outcome funders and service providers. They relate mainly to the investor returns \nthat will be paid (either by the outcome funder or service providers).", " Stakeholders identified that some of the design and set-up costs were unique to DIBs (e.g. \ncontracts  requiring  legal  and  financial  consultancy),  but  that  others  are  commonly  seen  in \nother similar programmes, particularly with a PBR or output-based contract (such as ongoing \ncosts  of  performance  management,  project  management  and  verification).  Stakeholders \nexpected some of the DIBs costs would reduce for future DIBs.", " The cost drivers were identified by stakeholders to help understand which elements of the DIB \nare the most time-intensive or expensive. There was a large degree of agreement across the \nDIBs in terms of what these cost drivers were, including the number of organisations involved \nand  the  negotiation  process  -  particularly,  this  was  perceived  as  being  time-intensive, \nparticularly given these DIBs were being delivered for the first time. All the DIBs identified legal \nand financial advice a major cost driver taking significant staff time and expertise, and three \nout  of the four DIBs similarly highlighted the lengthy process of engaging funders and raising \nfinance from investors. One DIB identified the service provider selection process as being time \nintensive.", " Necessary conditions for the DIB model to be suitable It is too early (and there are not enough DIBs) to state whether DIBs are most appropriate in \ncertain sectors or regions, but it is evident that there are certain \u2018conditions\u2019 that increase the \nlikelihood  that  the  DIB  will  be  launched  at  all,  or  in  a  shorter  timeframe  and/or  with  lower \ntransaction costs. The conditions we have identified are as follows: \u2022  Sufficient evidence base for the proposed intervention v \u2022  Clear and measurable outcomes  \n\u2022  Feasible timeframe for achieving the outcomes \n\u2022  Acceptable level of external risk \n\u2022  Sector with strong service providers \n\u2022  Data from previous interventions \n\u2022  Consortium that has: o  strong and committed leadership; \no  sufficient capacity and skills; \no  a culture of innovation and interest in adapting and learning; \no  a consensus on the policy problem, target outcomes and appropriate approaches; \no \no  clearly defined roles for its members; \no  brought in stakeholders at the right time; and \no  a balance between bilateral and collaborative negotiations.", " the right balance between size and breadth of expertise; \u2022  Legislative framework that allows public funds to fund private sector profits \n\u2022  Taxation on the profit of the investment that is accounted for in the financial model \n\u2022  A framework enabling public sector entities to commit themselves long-term to undefined and uncertain expenses \u2022  Alignment of DIB to organisational requirements \n\u2022  Setting up arrangements in which what happens in all eventualities is clearly defined  \n\u2022  Effective processes to manage the risks of working with new actors.", " What  is  particularly  interesting  is  that  many  of  these  conditions  have  been  identified  as \nnecessary within Social Impact Bonds (SIBs) in high-income countries, suggesting that a lot \nof the learning within impact bonds is transferable to different outcome funders (donors) and \nregions (middle-income and developing countries).", " Lessons learnt and improvements that can be made to increase the model\u2019s benefits \nand reduce the associated transaction costs Below we set out the lessons with potential wider relevance for the design and set up phase \nof DIBs. These are split out against the DIB effects and different stages of design and set-up. \nAs discussed in section 3, there is not yet a predominant design for DIBs, and it is perhaps \nmore helpful to understand DIBs as a funding class within which there is great variation. The \nprecise structure and nature of a DIB depend on the stakeholders involved, their objectives \nfor using the DIB and the organisational and regulatory requirements in place.  These have \nimplications  for  the  DIB  effects  and  for  the  process  of  design  and  set  up  phase,  and \nsuchdiversity must be borne in mind when taking stock of the lessons learned to date.", " Identifying appropriate interventions 1.  Transaction costs are lower if the DIB design is able to draw on existing evidence, reducing \nsome  of  the  costs  associated  with  designing  outcome  metrics  and  the  evidence  base \nrequired to determine pricing. However, the requirement for a strong evidence base may \nlimit the expansion of the DIB into new and innovative sectors.", " 2.  The benefits of using the DIB model are the strongest when there is a value proposition to \nthe use of the DIB, whereby they resolve a specific challenge that cannot be addressed \nby other funding mechanisms. Many of the benefits of using the DIB model are similar to vi the  benefits  of  using  PbR.  However,  there  are  some  benefits  unique  to  the  DIB model, \nsuch as enabling service providers to participate in PbR without upfront capital, and the \ntendency  for  the  DIB  model  to  draw  in  a  wide  range  of  stakeholders  and  require  and \nsupport collaboration.", " Identifying metrics and structuring payments 3.  Building a database of impact bond returns, outcome metrics and rate cards and drawing \non private sector expertise on pricing risk would facilitate the growing of the DIBs market. \nHowever, context specificity may limit the usefulness of standardisation and caution is also \nadvised in terms of developing rate cards, due to the early stage of the market and limited \ndata available.", " 4.  Outcome metrics and targets work best when returns to investors and outcome funders, \nand respective incentives, are aligned. Developing outcome metrics and rate cards that \nare understood by all stakeholders and linked to other metrics within the sector/country \ncan increase the value of the learning generated, and also facilitate the broader DIB market \nand/or potential transition to a SIB. It is noted that there can be a tension between using a \nrobust model and using a less robust model that is aligned with measures used by others \nin the sector.", " Measuring impact 5.  The validation process should be designed to meet the needs of stakeholders. Different \nconsiderations may apply to different contexts. We note that there can be an automatic \npreference to use experimental approaches or quasi-experimental approaches. However, \nwhere  an  intervention  or  certain  causal  links  are  sufficiently  backed  by  evidence,  there \nmay  be  less  value  in  using  experimental  or  quasi-experimental  methods  compared  to \nvalidated administrative data.", " Identifying and selecting stakeholders and managing relationships 6.  Across three of the DIBs, it was challenging to engage outcome funders. There is a benefit \nto  identifying  outcome  funders  interested  in  using  outcome  based  contracting,  and  the \ntypes  of  interventions  they  are  interested  in  earlier  on,  and  recognising  that  outcome \nfunders  need  to  be  involved  in  the  design  of  the  DIB.  Identifying  outcome  funders  first \ncould also enable a competitive process for selecting service providers. On the other hand, \noutcome  funders  are  concerned  about  the  risks  of  getting  involved  with  a  new  funding \nmechanism,  and  it  can  be  more  efficient  for  outcome  funders  to  get  involved  at  a  later \nstage,  when  the  other  stakeholders  have  been  identified  and  the  terms  are  more \ndeveloped.", " 7.  Transaction costs for the design and set up stage can be reduced when there is strong \ncollaboration across stakeholders, drawing on each other\u2019s expertise and strengths; when \nroles are clearly defined from the start; when stakeholders are identified and brought in \nefficiently; and when there is the right balance between undertaking negotiations bilaterally \nand collaboratively.", " 8.  Different  types  of  investors  and  outcome  funders  bring  different  types  of  benefits.  For \nexample,  commercial  investors  are  able  to  bring  in  more  experience  with  testing  and \nimplementing  financing  modalities,  while  philanthropic  investors  may  be  able  to  bring vii experience  and  expertise  within  the  sector.    As  a  result,  careful  consideration  of  the \nobjectives of using the impact bond should be taken into account when identifying outcome \nfunders and investors.", " Structuring and developing the operating model 9.  The  larger  number  of  stakeholders  involved  in  the  DIBs  to  date,  and  the  often  diverse \nlegislative  frameworks,  increase  the  transaction  costs  of  this  stage  of  the  DIB \ndevelopment,  due  to  the  larger  number  of  \u2018work-arounds\u2019  and  negotiations  required. \nFurthermore, contracting with different currencies introduces foreign exchange risk. The \noptimal  solution  would  be  to  amend  the  legislative  frameworks  to  accommodate  DIBs. \nWhere  this  is  not  possible,  other  potential  solutions  include  limiting  the  number  of \nstakeholders  involved,  considering  other  pooled  financing  or  funding  structures,  using \nother ways to minimise the number of contracts involved, or standardising deals.", " Recommendations Recommendations to all DIB stakeholders \u2022  Be transparent and share lessons learned and key successes and failures (including \nDIBs that failed to launch) to facilitate dissemination of learning across the sector; \u2022  Make contracts, payment terms, feasibility studies, investor documents and learning documents publicly available; \u2022  Building a database on interest rates, outcome metrics and rate cards and drawing on \nprivate sector expertise on pricing risk would facilitate the growing of the DIBs market; \u2022  Prioritise the documentation of lessons learned and evaluation, in order to facilitate the \ndevelopment  of  a  more  finely  grained  understanding  of  what  works,    and  in  what \ncontexts.", " Recommendations to DIB designers \u2022  Clearly agree upfront the roles and responsibilities of all involved parties, including how these responsibilities may change depending on circumstances; \u2022  When  structuring  the  DIB,  ensure  that  the  contracts  and governance  arrangements have provisions for a range of potential eventualities; \u2022  Be clear about the objectives of using the DIB, and how the DIB is expected to resolve \na policy problem. Then, structure the DIB so it focuses on delivering the targeted DIB \neffects,  and  seek  to  reduce  transaction  costs  that  do  not  contribute  to  the  targeted \neffects  of  using  the  DIB.  Be  clear  what  is  needed  from  stakeholders,  including \ninvestors, outcome funders and advisors. This can affect whether hands-on or hands-\noff stakeholders are more appropriate.", " \u2022  Consider  carefully  the  number  and  types  of  stakeholders  involved,  as,  in  this  early \nstage of the market, complexities  and potential inefficiencies increase with the number \nof  stakeholders.  Consider  solutions  to  reduce  this  complexity,  such  as  limiting  the viii number of stakeholders involved or using contractual arrangements that simplify the \nprocesses required.", " \u2022  Develop outcome metrics and rate cards that are understood by all stakeholders and \nlinked  to  other  metrics  used  in  the  sector  or  country,  to  increase  the  value  of  the \nlearning generated, minimise the costs of data collection and facilitate the broader DIB \nmarket and/or potential transition to a SIB.", " \u2022  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.", " ix Contents Acknowledgements and disclaimer .......................................... i Executive Summary ................................................................... ii \nRecommendations ............................................................................................. viii \nList of Tables ......................................................................................................... 3 \nTable of Figures .................................................................................................... 5 1.0 2.0 3.0 4.0 5.0 Introduction ................................................................. 1 \nOverview of the DIBS pilot programme ............................................. 1 \nObjectives of the Evaluation .............................................................. 4 \nScope of the Research Wave 1 Report .............................................. 5 \nOverview of the Evaluation Process.................................................. 5 \nReport structure .................................................................................. 6 Evaluation Framework and Methodology .................. 8 \nEvaluation framework for the evaluation .......................................... 8 \nOverview of the methodology .......................................................... 12 \nMethodological limitations ............................................................... 18 Summary of the DIBs ................................................ 20 \nProgramme components .................................................................. 22 \nStakeholders involved in the DIBs .................................................. 26 \nDIB structures ................................................................................... 27 \nConclusion ........................................................................................ 30 Analysis and Findings \u2013 DIB Effects ........................ 31 \nThe DIB effect indicators .................................................................. 32 \nPresence of the DIB effect indicators: Summary............................ 34 \nRisk transfer effects ......................................................................... 37 \nPartnership effects ........................................................................... 40 \nFinancing and funding effects ......................................................... 43 \nDesign effects ................................................................................... 47 \nOther factors influencing the DIB effect .......................................... 51 \nAdditional effects not identified in the framework ......................... 52 \nConclusions ...................................................................................... 52 Analysis and Findings \u2013 Costs of designing and \ndelivering DIBs .......................................................... 55 \nEconomy ........................................................................................... 57 6.0 Efficiency ........................................................................................... 68 \nEffectiveness ..................................................................................... 68 \nEquity................................................................................................. 72 \nConclusion ........................................................................................ 73 Analysis and Findings \u2013 Improving the process of \ndesigning and agreeing DIBs ................................... 75 \nIdentifying appropriate interventions .............................................. 77 \nIdentifying metrics and structuring payments ................................ 81 \nMeasuring impact ............................................................................. 85 \nIdentifying and selecting stakeholders and managing relationships\n ........................................................................................................... 87 \nStructuring the vehicle and developing the operating model ........ 95 \nConclusion ........................................................................................ 97 7.0 Lessons .................................................................... 101 8.0 Recommendations .................................................. 104 \nRecommendations to all DIB stakeholders ................................... 104 \nRecommendations to DIB designers ............................................. 104 Annex A: Case study reports .................................................... 1 Annex B: Terms of Reference ................................................. 81 \nBackground and Context ................................................................. 81 \nB.1 \nWhat do we mean by other aid mechanisms? ................................ 82 \nB.2 \nHow strong is the evidence on DIBs? ............................................. 83 \nB.3 \nWhat is the DFID DIBs pilot programme? ....................................... 84 \nB.4 \nUsers of the Evaluation .................................................................... 85 \nB.5 \nEvaluation Methodology ................................................................... 90 \nB.6 \nData Sources ..................................................................................... 91 \nB.7 \nEvaluation Outputs and Timeframe ................................................. 93 \nB.8 \nLighter-Touch Interim Outputs ........................................................ 95 \nB.9 \nEvaluation Management Team ......................................................... 97 \nB.10 Annex C: Bibliography .......................................................... 106 Annex D: EQUALs criteria mapped to report sections ....... 112 Annex E: Evaluation methodology ....................................... 116 \nEvaluation Framework .................................................................... 117 \nE.1 \nDIB-level research........................................................................... 124 \nE.2 E.3 \nE.4 \nE.5 \nE.6 \nE.7 Programme-level Research ............................................................ 135 \nSector-level Research .................................................................... 135 \nApproach to data collection ........................................................... 136 \nAnalysis, Reporting and Dissemination ........................................ 137 \nInvolvement of stakeholders .......................................................... 140 Annex F: Individual DIB level plans ...................................... 144 Annex G: Data Quality Assessment ..................................... 149 \nICRC ................................................................................................. 149 \nG.1 \nQEI ................................................................................................... 150 \nG.2 \nVillage Enterprise............................................................................ 152 \nG.3  \nCameroon Cataract Bond ............................................................... 154 \nG.4 Annex H: Consultees and Sources reviewed ...................... 157 Annex I: Framework for categorising DIBs .......................... 162 Annex J: DIBs reviewed as part of programme level consultations ........................................................... 164 Annex K: Learning workshop note ....................................... 172 Annex L: VfM Analysis \u2013 Supporting Evidence ................... 174 \nICRC ................................................................................................. 174 \nL.1 \nQuality Education India DIB ........................................................... 174 \nL.2 \nVE DIB.............................................................................................. 175 \nL.3 \nCameroon Cataract Bond ............................................................... 175 \nL.4 Annex M: Literature Review .................................................. 177 \nHypothesised effects of DIBs ........................................................ 177 \na. \nInput ................................................................................................. 189 \nb. \nRecommendations .......................................................................... 196 \nc. \nWhat approaches have been used to evaluate impact bonds? What \nd. \nare the main challenges and solutions? ....................................... 198 Annex N: List of Acronyms ................................................... 201 List of Tables Table 2.1: Evaluation Framework \u2013 EQ1 ............................................... 8 Table 2.2: Evaluation Framework \u2013 EQ2 ............................................. 10 \nTable 2.3 Stakeholders consulted ....................................................... 13 \nTable 2.4 Comparator Sites ................................................................ 14 \nTable 2.5: Deliverables mapped to target audiences........................... 17 \nTable 2.6: Limitations and mitigations ................................................. 18 \nTable 3.1: Programme components .................................................... 22 \nTable 3.2: Key stakeholders ................................................................ 26 \nTable 3.3: DIBs against DIB dimensions ............................................. 27 \nTable 4.1: DIB effect indicators ........................................................... 33 \nTable 4.2: Presence of DIB effect indicators in the four DIB projects .. 35 \nTable 6.1: Project focus and measurement approach ......................... 87 \nTable 6.2: Advantages and disadvantages to different approaches to \nidentifying and engaging with stakeholders ......................................... 91 Table B.1: Alternative aid mechanism ................................................. 82 \nTable B.2 EO 1: Inception Report ....................................................... 93 \nTable B.3: EO2 \u2013 Evaluation Report on the Process of designing and \nlaunching DIBs .................................................................................... 93 \nTable B.4: EO3 \u2013  Mid-Term Evaluation Report on DIBs ..................... 94 \nTable B.5: EO4 \u2013 Final Evaluation Report on DIBs ............................. 94 \nTable B.6: Good Performance Indicators ............................................ 98 Table E.1: Evaluation Framework ..................................................... 117 \nTable E.2: DIB effects and indicators ................................................ 122 \nTable E.3: Stakeholder consultations in RW1 ................................... 125 \nTable E.4: Research Waves ............................................................. 131 \nTable E.5: VfM Framework ............................................................... 131 \nTable E.6: VfM Indicators .................................................................. 132 \nTable E.7: Costing Structure ............................................................. 133 \nTable E.8: Communication Plan ........................................................ 140 Table F.1 : Proposed consultations ................................................... 144 \nTable F.2: Value for Money data ....................................................... 145 \nTable F.3: Other data ........................................................................ 147 Table M.1: Sources consulted ........................................................... 178 \nTable M.2: Impact bond principles .................................................... 182 \nTable M.3: Categorisation of SIBs by level of innovation ................... 185 \nTable M.4: Challenges of designing impact bonds ............................ 196 Table of Figures Figure 1.1: DIBs pilot programme theory of change .............................. 3 \nFigure 4.1: DFID risk assessment of three DIBs ................................. 38 Figure M.1: Framework for synthesising evaluation evidence ........... 177 \nFigure M.2: Strengths and weaknesses of existing evidence and \nevaluation approaches and methods related to SIBs and DIBs (Drew \nand Clist 2015:27) ............................................................................. 199 1.0  Introduction Overview of the DIBS pilot programme 1.1.1 DIBs and the current stage of the market.", " i) \nii) \niii) \niv) i) ii) DIBs are understood by DFID as one type of payments by results (PbR), or a type of funding \nwhereby payments are made after the achievement of pre-agreed outcomes (DFID, 2014). In \na standard PbR contract, there are four actors: an outcome funder who funds the outcomes;  \nthe service provider delivering the intervention;  \nthe target population, benefiting from the services; and \na validating agency that validates the results on which the payments are based.", " DIBs involve two additional agents: the  investor(s),  which  provide(s)  the  working  capital  to  deliver  the  intervention  and \nmay be able to make a return on their investment, calibrated to the level of outcome \nachieved; and \nthe intermediary, which can assist with the development and commercialisation of the \nDIB, and/or with the monitoring and support of the delivery of the intervention.", " DIBs are typically implemented in developing countries, where the outcome funder is a donor \nagency or foundation often operating in a different country. Humanitarian Impact Bonds are \nessentially DIBs operating in humanitarian situations.", " The DIB market is still at an early stage of development. Boggild-Jones and Gustafsson-Wright \n(2019)2  noted  that,  as  of  January  2019,  seven  development  impact  bonds  have  been \ncontracted: Educate Girls in India; a DIB for improving cocoa and coffee production in Peru; \nthe International Committee of the Red Cross Programme for Humanitarian Impact Investment \n(PHII); the Village Enterprise DIB, Rajasthan Maternal Health DIB; the Quality Education India \nDIB and the Cameroon Cataract Bond have also been launched.", " 1.1.2 Objectives of the DIBs pilot programme DFID\u2019s 2014 PbR Strategy3 set out the ambition for PbR to become a major part of the way \nDFID works. DFID\u2019s move towards PbR is explained as part of a broader reform to ensure \ngood value for money (VfM) from the development budget is achieved. DFID recognises three \ntypes of PbR: results-based aid (RBA), results-based financing (RBF) and DIBs. DFID funded \na  study  conducted  by  Social  Finance  to  explore  the  feasibility  of  using  a  DIB  to  address \nsleeping  sickness  in  Uganda. While  this  was  not  launched,  DFID\u2019s  economic  development \nstrategy, which was released in January 2017, re-committed to \u201cassess[ing] the scope\u201d of DIBs \nas a financing tool. It is in this context that the DIBs pilot programme was launched.", " 2 https://www.brookings.edu/blog/education-plus-development/2019/01/02/a-global-snapshot-impact-bonds-in-\n2018/amp/ \n3https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/323868/Sharp\nening_incentives_to_perform_DFIDs_Strategy_on_Payment_by_Results.pdf Given the emerging evidence on impact bonds, but limited experience with DIBs specifically, \nthe main aim of the DIBs pilot is to: test whether DIBs are a tool that DFID is able to use, \u2022 \n\u2022  generate  an  understanding  of  how  and  when  DIBs  can  add  value  in  DFID programming and \u2022  generate  an  understanding  of  how  and  when  DIBs  can  be  used  to  support  DFID\u2019s \ncommissioning, management, and effectiveness in delivering programmes on a PbR \nbasis.", " DFID is piloting DIBs by supporting a small number of projects designed by other donors or \ndelivery partners where a PbR and DIB financing structure is desirable and feasible. Evidence \nis sought through the pilot that will help DFID understand when DIBs may be an appropriate \ncommissioning tool and the costs and benefits of using them.", " 1.1.3 Theory of change In the ToR, DFID supplied a Theory of Change (ToC). As part of the proposal, the evaluation \nteam updated this ToC, based on the understanding of the evidence base in relation to the \npotential,  and  challenges,  of  impact  bonds.  The  ToC  was  revised  further  following  the \ninception  phase  and  the  evaluation  team  felt  the  ToC  still  represented  everyone\u2019s \nunderstanding in relation to the impact of the programme, and was aligned with the potential \nadvantages and risks associated with impact bonds as outlined in the research. The ToC set \nout overleaf (Figure 1.1) remains unchanged from the one presented in the inception report.", " Figure 1.1: DIBs pilot programme theory of change The design process sets out the level of ambition including measurable outcomes and also establishes a robust process for verifying results so that payment can be made The DIB contract aligns all the stakeholders including donors, providers and investors to achieving the desired outcomes and the risk is transferred partly to the service provider but mostly to the investor.     The outcome payer allows more flexibility to adjust and respond to issues as they emerge and more flexibility over inputs   Investors are willing to take the financial risk by putting forward upfront payments for a return on their investment As risk shifts to investors more providers are attracted to PbR contracts- the \u2018best\u2019/ most appropriate providers are selected and investors encourage them to perform Project outputs linked to physical rehabilitation, micro enterprise, poverty, education will be generated because relevant providers are willing to become involved in PbR contracts and outcome payers transfer or share risk and new practices are instilled in projects. A PbR approach could exclude some strong service providers from involvement in projects as they are unable to secure upfront capital to deliver much needed services or are not financially secure enough to wait for payments to be made. Other strong providers cannot take the financial risk of putting up capital in case outcomes are not achieved and payments not made. Some providers could take on the financial risk but lack the capabilities to deliver a PbR contract. This means that potentially strong and innovative service providers cannot get involved in development projects. Donors to development projects carry the risk of paying for services that may not achieve strong outcomes. Donors also lack a level of control on what outcomes they wish to see achieved. A pay for service contract often lacks flexibility to adjust to changes on the ground or if underperformance starts to occur.    PbR mechanisms alone disincentives risk taking and investment- when there is underperformance there is a tendency for providers to disinvest in order to limit their losses.     INTERIM CHANGES- A shift in culture across all stakeholders to an outcome based programme which leads to more                 outcomes being achieved and more beneficiaries being supported - Limited budgets are only spent when outcomes are achieved and therefore when projects are \u2018successful\u2019- More innovative projects as providers have more flexibility to deliver what they feel will achieve outcomes-  New donors and in particular investors enter the development market encouraged by the                                   use of DIBs leading to new funding coming into the area- Real time performance information encourages a proactive approach to under performanceMEDIUM TERM IMPACTS - More service providers entering the market with better provision for beneficiaries  - More performance based PbR contracts - More investors entering the development market with fresh ideas- Development projects learn from DIB working practices and improve their performance LONGER-TERM IMPACTS More effective, efficient and relevant projects in the development context. Better use of development funding and                                                                                                         a shift or sharing of the risks and rewards across different stakeholders.  This leads to a more cost effective set of solutions to tackle issues in developing countries. OUTCOMES - More DIBs and stronger and more inclusive funding models, funding mechanisms and commissioning                                                                                                             approaches compared to PbR, grants, pay for service and alternative funding models.  PROBLEMSINPUTSOUTPUTS\f1.1.4 Selection of DIBs The pilot programme is made up of three DIBs. DFID\u2019s engagement and selection process with \nthe  DIBs  is  summarised  below.  Further  detail  on  the  DIBs  is  set  out  in  section  3,  and  in  the \nindividual case studies in Annex A.", " ICRC HIB: DFID is an outcome funder in the ICRC HIB. DFID first engaged with the ICRC HIB in \nSeptember  2016.  As  DFID  joined  at  an  advanced  stage  of  the  deal,  the  terms  were  already \nrelatively set. Key motivations for DFID to fund this HIB was the learning opportunity it presented, \nand the possibility of funding a digital centre management system and efficiency improvement \nmeasures testing on an outcome basis.", " QEI  DIB:  DFID  is  providing  funding  for  programme  management,  legal  advice,  learning  and \nevaluation. DFID joined the programme in January 2018, and fed into the project design. A key \nmotivation  for  funding  QEI  was  that  the  DIB  involved  a  rigorous  impact  evaluation  with  the \npotential to generate important learning and potentially attract new funders.", " VE DIB: DFID is an outcome funder in the VE DIB. In late 2016, DFID was approached by Instiglio, \nan organisation providing technical assistance in the creation and implementation of impact bonds \nand  results-based  financing  projects,  and  a  donor.  DFID  thought  that  VE  fitted  well  with  the \nstrategic aims of the DIBs pilot programme.", " This  evaluation  reports  also  draws  on  learning  from  the  Cameroon  Cataract  Bond.  The \nCameroon  Cataract  Bond  is  not  included  within  the  DIBs  pilot  programme.  Nonetheless, \nstakeholders agreed that adding a fourth DIB to the evaluation, using the same approach and \nresearch tools, would enrich the findings of the evaluation.", " Objectives of the Evaluation The purpose of the evaluation is to generate learnings and recommendations on the use of DIBs \nas an instrument for aid delivery, by using the experience of the DFID DIBs pilot programme to \ngenerate learning to inform DFID\u2019s future policy aiming to make the most effective use of DIBs.  \nThe evaluation will also help DFID and pilot project partners evaluate whether the tools they are \ndeveloping are useful, scalable and replicable.", " DIBs  are  a  relatively  new  tool  for  delivering  development  projects.  Hence,  the  focus  of  this \nevaluation is on learning to inform future thinking on DIBs and also wider funding mechanisms in \nthe development context. The evaluation aims to generate independent and robust evidence on \nwhether DIBs can help enable efficient and effective delivery in DFID priority result areas - taking \ninto consideration both the costs and benefits of a DIB model. The evaluation aims to draw out \nand synthesise learning about the DIBs mechanism from these projects, while also comparing \nand contrasting findings with the broader evidence base. The evaluation results will help DFID to \nmake informed choices on how and where to use DIBs in the future. This will include the potential \nto replicate and scale the DIB. The evaluation also aims to be useful for those currently involved \nor interested in getting involved in DIBs. As such, the primary users of the evaluation will be the \nDFID DIBs team, and secondary users of the learning will be organisations using or thinking about \nusing  impact  bonds.  These  include  outcome  funders,  investors  and  service  providers.  It  is expected that the evaluation will generate findings and practical recommendations for the set up \nand delivery of DIBs.", " A key focus  of this  evaluation  is  therefore  around  understanding the  benefit  of applying  a  DIB \nmodel, looking at whether any strong or weak performance in the project is attributable to the DIB \nmodel rather than, for instance, local context, the delivery team or any other mitigating factors. \nThe  evaluation  focuses  on  whether  the  DIB  leads  to  better  and  more  relevant,  efficient  and \neffective activities compared to alternative funding models. The evaluation also explores: \u2022  whether  a  DIB  model  influences  the  behaviours  of  stakeholders,  such  as  providers,  to improve programme performance; \u2022 the extent to which a DIB leads to more cost effective and better performing projects; and \u2022  whether it improves the outcomes of activities and the extent to which a DIB enables more providers to become involved in PbR projects.", " Scope of the Research Wave 1 Report As  set  out  in  the  ToR,  this  report  provides  early  feedback  on  the  process  of  selecting  and \nstructuring the DIBs included within the DIB pilot programme, as well as the Cameroon Cataract \nBond. This includes estimates of the costs involved in the feasibility and structuring stages of the \nDIB for all parties. The report focuses on the use of the DIB in funding these projects, and does \nnot set out to evaluate the intervention design or the delivery of the projects.", " On this basis, the report makes recommendations on the conditions that are needed for DIBs to \nbe suitable and optimal, and recommend possible ways to reduce costs in the design, structuring \nand implementation of DIBs. The report has been complemented by specific case study reports \nfocusing on each of the four DIBs, set out in Annex A.", " The two evaluation questions are: \u2022  EQ1:  Assess  how  the  DIB  model  affects  the  design,  delivery,  performance  and effectiveness of development interventions.", " \u2022  EQ2: What improvements can be made to the process of designing and agreeing DIBs to increase the model\u2019s benefits and reduce the associated transaction costs?", " The ToR and changes to the ToR are set out in Annex B.", " Overview of the Evaluation Process The  timing  of  the  evaluation  has  been  set  to  align  to  the  period  of  DIBs  pilot  programme, \ncommencing in May 2018 and completing in March 2023. The evaluation is divided over three \nwaves, with the majority of the research activity repeated during each wave: Wave 1: Set up (April \u2013 February 2019): Focusing on the process of designing and launching the \nDFID DIB pilot projects.", " Wave 2: Delivery (April \u2013 November 2020): Focusing on emerging lessons from the DFID DIBs \npilot projects, as well as from evidence generated by other DIBs. Most of the evaluation questions \nwill be answered during this wave.", " Wave 3: Sustainability (April 2022 \u2013 March 2023): Focusing on the legacy of the DIBs and the \nprogramme,  including  the  extent  to  which  outcomes  and  DIBs  were  sustained.  This  will  also \nupdate the interim findings from Wave 2, providing a full assessment of the DIBs pilot programme, \nincluding costs and benefits.", " Delivery of Research Wave 1 has drawn on the preparatory work undertaken during the inception \nphase, which included: \u2022  a literature review on the context and progress of the wider SIBs and DIBs sector, and an initial comparison of these mechanisms with alternative funding tools; \u2022  a  review  of  programme  documentation,  at  the  individual  DIB  level  and  DFID  pilot \u2022 programme level; \nthe  refining  of  the  conceptual  framework  and  evaluation  questions  against  the \nOECD-DAC  criteria,  development  of  DIB  effect  indicators  and  preparation  of  research \ntools; \u2022  preparatory  consultations  with  key  stakeholders  across  the  DIBs  and  scoping  of potential comparison sites; and \u2022  a preliminary stakeholder mapping.", " Following the review and validation of the methodology and research tools by DFID, Research \nWave 1 included research and analysis at the individual DIB level for the four DIBs included in \nthe  scope  of  the  evaluation,  at  the  pilot  programme  level  and  at  the  sector  level.  This  was \nconducted between June \u2013 December 2018. Further detail is set out in section 2.2.", " Initial drafts of emerging findings were completed between November and December 2018, and \nhave been revised based on internal review and comments received from DFID. Findings from \nRW1 were presented to an internal Learning Workshop on December 12th 2018. The aim was to \ncontextualise the programme evaluation findings, compare differences and similarities between \nDIBs  under  study,  share  lessons  learned  and  consider  the  implications  for  the  wider  sector. \nDiscussion at the Learning Workshop has informed preparation of this draftand implications for \nthe report, which are summarised in Annex K.", " This version of the evaluation report is complemented by individual DIB case study reports, set \nout  in  Annex  A.  The  evaluation  report  and  the  case  studies  will  be  reviewed  by  DFID,  the \nEvaluation Advisory Group that has been established for the evaluation, DFID\u2019s EQUALS quality \nassurance reviewers, and other stakeholders, including those from the DIBs under the scope of \nthe  evaluation.  On the  basis  of  this  feedback,  the  report  will  be  finalised  and  communications \nproducts will be prepared with a view to most effectively communicating the evaluation findings \nboth to DIB stakeholders and other stakeholder organisations, but also to the wider DIB sector.", " Report structure The remainder of this report is structured as follows: Section  2  sets  out the evaluation  framework that  has  been  used  to guide  the  evaluation,  and \nsummarises the main features of the methodology and the limitations of the available evidence.", " Section 3 introduces the DIBs included under the scope of the evaluation.", " Section 4 presents the analysis and findings of the evaluation in relation to EQ1, assessing how \nthe DIB model affects the design and set up phase of development interventions.", " Section 5 presents the analysis and findings of the evaluation in relation to EQ2, in terms of the \nestimated costs attributable to the use of the DIB funding mechanism.", " Section 6 identifies improvements that can be made to the process of designing and agreeing \nDIBs to increase the model\u2019s benefits and reduce the associated transaction costs.", " Section 7 discusses the lessons learned, which are of potential wider relevance for the design \nand set up phases of DIBs, against the DIB effects and different stages of designing and setting \nup development impact bonds.", " Section 8 provides recommendations based on our findings and lessons learned, split between \nthose applicable to all DIB stakeholders, and those primarily applicable to DIB designers.", " Additional information is included in annexes: \u2022  Annex A sets out the case study reports agreed with the different DIB stakeholders \u2022  Annex B contains the Terms of Reference for the evaluation \u2022  Annex C sets out the references cited within the report \u2022  Annex D maps the DFID EQUALS criteria to the relevant sections in the report \u2022  Annex E sets out the full methodology used for this evaluation \u2022  Annex F sets out the individual DIB level plans agreed with the four DIBs \u2022  Annex G sets out the Data Quality Assessments undertaken for the four DIBs \u2022  Annex H sets out the list of consultees and documents reviewed as part of this research wave level consultations \u2022  Annex I sets out a framework used for categorising the DIBs \u2022  Annex J provides some basic information on the other DIBs reviewed as part of the sector \u2022  Annex K sets out a note summarising the internal learning workshop \u2022  Annex L sets out the supporting calculations for the cost analysis \u2022  Annex M sets out the full literature review \u2022  Annex N provides a list of acronyms.", " 2.0  Evaluation Framework and Methodology This section sets out the evaluation framework that has been used to guide the evaluation (section 2.1), summarises the main features of the \nmethodology (section 2.2) and the limitations of the available evidence (section 2.3). Further details on the methodology undertaken are set \nout in Annex E.", " Evaluation framework for the evaluation The two tables below set out the evaluation framework for the evaluation, which maps the two evaluation questions (EQ1 and EQ2) to the \nOECD DAC criteria and evaluation sub-questions finalised during the inception phase. All the DAC criteria are relevant and will be applied over \nthe  course  of  the  evaluation.  The  evaluation  sub-questions  are  then  mapped  to  the  indicators  designed  during  the  inception  phase.  The \ncorresponding research waves in which these sub-questions will be covered are also marked. Annex E sets out the full evaluation framework, \nwhich links the evaluation questions and sub-questions to the corresponding data collection method.", " Table 2.1 presents the evaluation framework for evaluation question 1 (EQ1), which sets out to assess how the DIB model affects the design, \ndelivery,  performance  and  effectiveness  of  development  interventions.  Sub-questions  relating  to  the  DAC  criteria  of  effectiveness  and \nsustainability are included. Within these, there are sub-questions relating to comparisons between the DIBs within the pilot programme, and \nbetween the DIBs and projects funded through other funding mechanisms, and to spillover effects. Indicators have been developed for each \nof these sub-questions. The majority of the sub-questions related to EQ1 draw on the DIB effect indicators, which are set out in Annex E.", " Table 2.1: Evaluation Framework \u2013 EQ1 Key \nevaluation \nquestions Effectiveness and sustainability sub-questions Indicators EQ1: Assess \nhow the DIB \nmodel affects \nthe design, \ndelivery, To  what  extent  were  the  three  DIB  projects  successful  in  realising  their  aims, \noutputs, outcomes and impacts?  \nTo what extent was the level of success and failure due to the DIB model - was \nthe DIB model a small, medium or large driver of success and was it at all critical \nto the projects\u2019 overall performance?", " See DIB effect indicators set out in Annex \nE.", " Research \nWave x x x x Key \nevaluation \nquestions performance \nand \neffectiveness \nof \ndevelopment \ninterventions.", " Effectiveness and sustainability sub-questions Indicators Did the DIB model provide added value in relation to the cross-cutting issues of \ngender, poverty, human rights, HIV/AIDs, environment, anti-corruption, capacity \nbuilding and power relations? \nWhere was the DIB model most effective - was its greatest value in terms of the \ndesign, delivery, relationship development, cost effectiveness, time efficiency or \nimpact on beneficiaries? \nComparisons \nTo what extent does the effectiveness vary across the three projects and why? \nHow does the effectiveness compare to other DIBs and funding mechanisms and \nwhy? \nSpillovers To what extent did stakeholders involved in the DIB use any of the working \npractices of the model in their other work? To what extent did good practice \nwithin the DIBs spread to other interventions or organisations?", " Does the increased evidence base developed in the DIB enable the projects to \naccess additional funding?", " Sustainability What is the legacy of the use of the DIBs? How sustainable are the DIB effects?", " Extent  to  which  systems  and  practices \nimplemented  as  part  of  project  are \nembedded  across  the  wider  organisation \nand/or sustained once the DIB ends \nFunding accessed by the projects resulting \nfrom  the  evidence  base  developed  in  the \nDIB \n \nIndications \nsustained the  effects  will  be that Research \nWave x x x x x x x x x x x x x Table 2.2 sets out the evaluation framework for evaluation question 2 (EQ2), which explores improvements that can be made to the process \nof designing and agreeing DIBs to increase the model\u2019s benefits and reduce the associated transaction costs. Sub-questions relating to the \nDAC criteria of relevance, equity and efficiency are included. Within these, there are sub-questions related to drawing comparisons on the \nefficiency  between  the  DIBs  within  the  pilot  programme,  and  between  the  DIBs  and  projects  funded  through  other  funding  mechanisms. \nIndicators have been developed for each of these sub-questions.", " Table 2.2: Evaluation Framework \u2013 EQ2 Key  evaluation \nquestions Efficiency, Equity and relevance sub-questions Indicators Efficiency What (if any) are the extra costs of designing and delivering a project using a \nDIB model and how do they compare to other funding mechanisms?", " Where  are  the  extra  costs  most  prevalent  and  what  specific  items  (staff, \nmonitoring  procedures  etc.)  have  the  highest  costs?  Are  these  extra  costs \nmainly found in the design or delivery stages?", " EQ 2: What \nimprovements \ncan be made to \nthe process of \ndesigning and \nagreeing DIBs to \nincrease the \nmodel\u2019s benefits \nand reduce the \nassociated \ntransaction \ncosts?", " Do the extra costs represent value for money - to what extent do they lead to \nadditional results, impacts and benefits?", " Do  any  aspects  to  a  DIB  model  (e.g.  involving  an  investor,  undertaking \nverification of outcomes) shorten or extend the timeframes of projects? \nWho  pays  for  these  additional  costs  and  to  what  extent  do  they  see  the \nbenefits?  \nAre there any inefficiencies in a DIB model that can be reduced or are there \nany additional costs that are unnecessary? \nEquity \nHow  well  are  the  programmes  fulfilling  their  targeting  strategy?  Are  there \ncertain sub-groups which are not being reached?  \nComparisons \nTo what extent does the efficiency of the DIB set up vary between the three \nDIB projects and why?", " How does the efficiency compare to other DIBs and funding mechanisms and \nwhy?", " Research \nWave 2  3 x x x x x x x x x x x x x x x x x x x x x the where impact  bond, \nby:  \npossible \n(design,  set-up,  delivery,  and Additional  costs  of \ndisaggregated \n\u2022  stage \nlearning);  \n\u2022  actor  who \nthis  cost;  and \nincurs \n\u2022  type  of  cost  (staff  time,  consultancy  and \nexpertise costs, and the risk premium (return \ninterest)  \nto \nSavings in programme costs (including staff \ntime)  as  a  result  of  the  impact  bond. \nHow  effectively  has  risk  been  transferred  - \nwhat is the alignment of transferred risks with \nreturn?", " investors, including Any positive or negative changes to equity as \na result of the impact bond.", " Level of transaction costs of setting up a DIB \ncompare  with  the  average  costs  for  other \nfunding  mechanisms  (e.g.  fee-for-service \ncontracts) \nChanges  in  transaction  costs  over  time  (as \nfrom  previous \nprojects  start \nexperience) \nNumber of direct beneficiaries with improved \noutcomes  as  a  result  of  DFID  funded  DIB \nprojects learn to Key  evaluation \nquestions Efficiency, Equity and relevance sub-questions Indicators Relevance In what circumstances are DIBs relevant in tackling issues in the development \ncontext?", " What social issues, target groups, geographies and project scales do DIBs fit \nbest and have the greatest of impact? \nAre DIBs appropriate in development contexts - is the existence of investors \n(and  possible  profits),  payment  only  when  results  are  made  and  strong \nexpectations  around  measuring  outcomes  appropriate  for  donors  such  as \nDFID?", " To what extent are DIBs applicable to DFID\u2019s work - are they relevant across \nmost, some or a few of DFIDs priority result areas?", " service involvement Level  of  returns  and  profit  made  by  the \ninvestors and extent to which that influences \nfuture \nin  both  DIBs  and \nprojects \ndevelopment \nNumber of DFID supported DIB projects with \nimproved cost-effectiveness ratio compared \nwith \npast \nperformance \nProportion  of  new  DFID  DIB  instruments \ncommissioned \ninformed  by \nthat  are \nrecommendations  of  DFID  DIBs  evaluation \nreports. \nNumber  of  new  DFID  programmes \ninteracting  with  DIBs  guidance,  evaluation \nfindings and reports.", " providers' own Research \nWave 2  3 x x x x x x x x Overview of the methodology This  section  provides  an  overview  of  the  methodology.  We  first  set  out  the  data  collection \nmethods,  our  approach  to  analysis,  reporting  and  dissemination,  and  involvement  of \nstakeholders, before concluding with the main methodological limitations, and the mitigations \nundertaken. Further detail is set out in Annex E.", " 2.2.1 Data collection DIB level research: There were three levels of research activity in this first research wave (RW1), at the individual \nDIB level, programme level and sector level. Further detail is set out below: This level of research relates to the four DIBs under the scope of the evaluation.", " \u2022  Data  Analysis:  Expected  quantitative  figures  on  the  performance  of  the  DIBs, \nincluding performance metrics, outcome payments and returns were collected. Actual \nfigures will be collected over the next two waves. In order to ascertain the reliance we \ncan place on programme data, we have assessed the quality of the  monitoring and \nevaluation systems through our Data Quality Assessment (DQA) checklist, set out in \nAnnex G.", " \u2022  Document  Review:  Key  documents  related  to  each  DIB  were  reviewed  to  further understand the set up phase (see Annex H).", " \u2022  DIB Consultations: Consultations with key stakeholders to understand how the DIB \nmechanism is affecting the set up and development of the project, the objectives for \ngetting involved, as well as partnership working, and lessons learned in designing the \nDIB that could be applied to later stages or other DIBs.", " The  sampling  strategy  used  was  purposive.  There  was  a  limited  number  of \nstakeholders involved in the set up phase, and random sampling was not considered \nnecessary or appropriate. For the DIB-level research, for the most part, the evaluation \nteam  contacted  all  relevant  stakeholders,  namely  investors,  service  providers, \noutcome  funders,  performance  managers  and  outcome  evaluators.  All  stakeholders \ninvolved  were invited to participate in the evaluation, but some stakeholders did not \nparticipate in the evaluation. However, the team has tried to address this by drawing \non a range of programme documentation, and triangulating the findings and data from \nthe existing stakeholder interviews.", " The table below sets out the number of organisations interviewed, and the total number \nof organisations involved per impact bond stakeholder category. In parenthesis in this \ntable  under  the  \u2018interviewed\u2019  columns,  we  have  included  the  number  of  individuals \ninterviewed.  By  stakeholder  group,  we  mean  the  key  stakeholders  involved  in  the \nimpact bond model, including outcome funders, investors, service providers, outcome \nevaluators and advisors/performance managers. Details on the stakeholders involved \nin the all four DIBs are set out in section 3.", " d\ne\nw\ne\nv\nr\ne\nn t i I Table 2.3 Stakeholders consulted ICRC QEI VE Cataract \nBond d\ne\nw\ne\nv\nr\ne\nn t i I d\ne\nw\ne\nv\nr\ne\nn t i I l a t o\nT l a t o\nT l a t o\nT l a t o\nT d\ne\nw\ne\nv\nr\ne\nn t i I 1 (3) 3 (4) 3 \n2 \nn/a 5 \n74 \nn/a 3 \n9 \nn/a 5 \n1 \nn/a 3 (4) \n1 (1) \n1 (2) 2 (3) \n2 (2) \n0 3 (5) \n1 (3) \n1 (2) 3 (4) \n2 \n0 Outcome Funders \nInvestors \nPbR Comparator \nsites \nAdvisors / \nIntermediaries / \nPerformance \nManagers \n1 \nService Providers \n- \nOther funders \n1 \nOutcome Evaluator \n1 \nDIB researchers \nNotes: The \u201cinterviewed\u201d column sets out the number of organisations interviewed, and in parenthesis, the number of \nindividuals interviewed (in certain organisations, we interviewed more than one individual). The \u201ctotal\u201d column sets out \nthe total number of organisations within this stakeholder category.", " 1 (2) \n- \n0 \n1 (1) 3 (3) \n1 (2) \n1 (1) \n- 1 (2) \n0 \n0 \n- 1 (4) \n- \n0 \n- 3 \n1 \n1 \n- 1 \n- \n1 \n- 1 \n1 \n1 \n- 1 (4) 1(2) A full list of consultations is set out in Annex H.", " \u2022  Research in comparator sites: In order to develop an understanding of how the DIB \naffected  the  set  up  phase,  the  evaluation  team  also  undertook  data  collection  at \ncomparator sites. We identified two forms of comparisons: o  First,  we  identified  similar  programmes  being  delivered  by  the  same  service \nproviders funded by the DIBs, but which were funded under grants. As part of \nthe inception phase, a list of parameters which would affect the comparability \nof programmes was developed based on discussion within the evaluation team \nand DFID. These were: project purpose and objectives, service provider and \nprocesses used,  countries  of  operation,  context,  time period,  size of  project, \nlevel  of  donor  oversight/influence,  payment  structure  and  availability  of  data \nand stakeholders. The evaluation team then worked with the service providers \nand  intermediaries,  in  order  to  identify  potential  comparator  sites,  and \nassessed the similarity to our impact bonds along these parameters. We then \ninterviewed staff working on this comparator sites, to determine the extent to \nwhich  the  DIB  effect  was  also  present  in  these  sites,  to  support  our \nunderstanding of other factors which may have also contributed to these DIB \neffect indicators.", " o  Secondly,  we  identified  programmes  working  in  similar  sector  and  contexts, \nfunded under payment by results. One PbR comparator site was identified per 4 Of the seven investors, there is one cornerstone investor and one placement intermediary that identified the \nother five investors. The one investor consulted represents over 50% of the total investment.", " DIB. The criteria was PbR funded interventions working in similar sector, and, \nwhere possible, similar geographies.", " The table below summarises the comparator sites: Table 2.4 Comparator Sites DIB \nICRC HIB QEI DIB VE DIB Grant funded programme \nPhysical  Rehabilitation  Programme, \ndelivered by ICRC \nOne  programme  per  service  provider \n(three in total) \nCurrent grant-funded programme.", " PbR funded programme \nWorld Bank Global Partnership \non Output-Based Aid  \nGirls Education Challenge Helvetas livelihood programme The grant funded programme comparisons provided a useful comparator, in that they \nwere grant funded programmes delivered by the same service providers. We obtained \nuseful information on how the use of a DIB affected the design and set up phase. It \nmust  be  noted  that  there  were  some  differences  in  the  locations  and  interventions \ndelivered, between the comparator site, and the intervention funded by the DIB.", " In  terms  of  the  PbR  funded  programmes,  it  was  more  challenging  to  identify \ncomparable programmes. Nonetheless, each comparator provided useful information. \nThe World Bank GPOBA consultations provided useful information on the challenges \nof  using  output  based  aid  (OBA)  in  fragile  and  conflict  affected  contexts,  and \nrecommendations on how to better use OBA in these situations. This provided useful \ncontextual  information  to  the  challenges  of  using  results  based  approaches  in \nhumanitarian situations. The Girls Education Challenge (GEC) consultation provided \nuseful information on the costs of verification in  the PbR programmes funded under \nthe GEC, as well as the advantages and disadvantages to using PbR in an education \ncontext. This provided a reference point to compare the DIB effect with PbR effects in \nthe  education  context.  Similarly,  the  Helvetas  livelihood  programme  worked  in  a \ndifferent country context, but the intervention was similar to the one funded by the VE \nDIB, and provided useful comparison information on the costs and benefits of using \nPbR.", " Due to the late engagement of the Cameroon Cataract Bond, no comparator sites have \nyet been identified, although there are potential sites which are being discussed with \nstakeholders (see Annex E.2.5 for further detail).", " \u2022  Cost  data:  Information  on  the  additional  costs  of  setting  up  and  using  a  DIB  was \nobtained, in comparison to other funding mechanisms. The later research waves will \nexplore  the  extent  to  which  these  lead  to  additional  benefits.  Additionally,  we  also \ngathered  data  against  the  VfM  framework  set  up  during  the  inception  phase,  which \nincluded measures of economy, efficiency, effectiveness and equity of the DIBs (see \nAnnex E.2.6 for further detail).", " Programme level research: This  level  relates  to  the  DIBs  pilot  programme  and  synthesises  the  finding  across  the  four \nDIBs.", " \u2022  DFID consultations: The evaluation team held consultations with the DFID DIBs team \nand PbR staff members, in order to develop further understanding of the programme, \nand how it related to DFID priorities in this area.", " \u2022  Programme document review: The evaluation team reviewed key programme-level documents, such as internal reports written by DFID.", " \u2022 internal  workshop  brought learning  workshops:  The Internal \ntogether  key \nstakeholders from across the three DFID DIB pilots and the Cameroon Cataract Bond. \nThe workshop involved a discussion on the validity of these findings for the different \nDIBs,  and  additional  perspectives  and  nuances  across  the  range  of  DIBs  present. \nResults from the learning workshop were used to refine the evaluation team\u2019s analysis \nand findings, and have been incorporated in this evaluation report. Further detail is set \nout in Annex K.", " Sector level research: This level of research seeks to provide the wider contextualisation to our findings.", " \u2022  Literature Review: this involved a literature review on the impact bond and payment by result sector more broadly, and is set out in Annex M.", " \u2022  Document review: this involved review of reports related to other DIBs that are being \ndesigned  and  implemented,  to  ensure  the  evaluation  is  situated  within  sector \ndevelopments. A summary is set out in Annex H.", " \u2022  Other consultations: The evaluation team held consultations with DIB advisors and \nkey stakeholders of existing DIBs, and DIBs that failed to launch, to understand how \nthe DIB mechanism is affecting the set up and development of the project, as well as \npartnership working, and lessons learned in designing the DIB that could be applied to \nlater  stages  or  other  DIBs.  We  conducted  8  consultations  in  total.  A  full  list  of \nconsultations is set out in Annex H.", " A full list of consultees and documents reviewed is set out in Annex H.", " 2.2.2 Analysis The data collection generated a variety of qualitative and quantitative evidence, which enabled \nthe triangulation  of  different  data  sources set  out  above. The  data from  the  transcripts  and \nfield notes were summarised and synthesised under the headings and sub headings within \nthe  Evaluation  Framework.  Findings  from  different  data  sources  were  triangulated.  Where \nfindings between the data sets contradicted each other, each data set was further interrogated \nto examine possible explanations. Analysis took place at three levels, focusing firstly on the \nindividual DIBs; bringing this together to analyse progress at a programme level; and finally \nconsidering the implications for the wider DIB sector. We also held debriefings with all team \nmembers, including the external experts, to support in this analysis stage.", " We adopted  process  tracing  as  a  way  to analyse  the  effect  of the  DIB  on the  delivery  and \nperformance of the services. This involved the following steps: 1.  Process  induction  and  creation  of  \u2018DIB  effect\u2019  indicators:  The  evaluation  team \nproduced  a  set  of  indicators  through  which  to  measure  the  outcomes  the  DIB \nmechanism is expected to achieve, including hypotheses on how the use of the DIB could lead to these DIB effects, drawing on the theory of change. These are set out in \nSection 4.", " 2.  Examine presence of indicators in DIB areas and in non-DIB areas: We identified \nwhether the DIB effect indicators are present within the DIB and similar interventions \ndelivered through alternative funding mechanisms, using the following data sources:  \n\u2022  Consultations with stakeholders involved in DIBs and similar programmes funded through alternative funding mechanisms.", " \u2022  Qualitative and quantitative data from the DIBs and comparator sites 3.  Analyse differences between DIB and non-DIB areas: We undertook an analysis of \nthe key differences between the interventions funded by a DIB and those funded by \nanother funding mechanism. The DIB effect indicators provided a useful framework for \nthis. In a number of cases, stakeholders had been involved in both the DIB and non-\nDIB  funded  intervention,  and  we  sought  their  assessment  as  to the  key  differences \nbetween the interventions.", " 4.  Process  verification:  Differences  between  the  DIB  and  non-DIB  areas  may  not \nnecessarily be a result of the DIB mechanism. Stakeholders were asked to assess the \nextent  to  which  differences  can  be  attributed  to  the  DIB  mechanism.  Evaluator \njudgement  was  used  to  assess  perceptions  and  opinions  presented  by  different \nstakeholders,  which  were,  where  possible  triangulated  using  qualitative  and \nquantitative  data,  such  as  programme  documentation,  financial  reporting  and  M&E \ndata.", " Different stakeholders often had different opinions on the DIB effect. Where possible, we used \nthese different opinions to shed further light on the DIB effect, nuancing the effect for different \nstakeholders  and  sought  to reconcile  differing  opinions.  For  example,  different  opinions  on \nwhether the DIB enabled risk transfer shed light on risk perceptions of different stakeholders. \nDifferent opinions on whether the DIB enabled service providers to use PbR provided insights \ninto the DIB effect \u2013 for example, that in certain cases outcome funders would not have funded \nthe same intervention without a DIB, but that service providers would likely have approached \nother outcome funders. Where it was not possible to reconcile or come to an assessment of \ncontradictory opinions, we have sought to clearly set this out in our findings.", " As a result of the need to interpret and draw together different stakeholder views, evaluator \njudgement was a key component of our analysis. Hence, it was important that analysis was \nundertaken  consistently.  The  Analytical  Lead  and  Team  Leader,  both  with  significant \nexperience  of  evaluating  impact  bonds  and  outcomes  based  contracts,  quality  assured \ninterview notes and findings. To assess the robustness of findings, the following assessments \nwere also undertaken: \u2022  Assessment of the reliability of data sources, including consideration of their potential limitations and biases; and \u2022  Assessing the strength of evidence for the different DIB effects. For certain DIB effects, \nthere was more disagreement between stakeholders, and/or limited sources of other \ndata that could be used to triangulate. We have noted this in Section 4, and will revisit \nthese DIB effects in the next waves of research.", " 2.2.3 Reporting and dissemination As part of the inception phase, we undertook an analysis of stakeholders, and identified the \nthree  types  of  users:  DFID  stakeholders,  stakeholders  involved  in the  pilot  DIBs  and those \ninterested in DIBs and/or SIBs. The reporting and communication outputs have been designed \nwith these stakeholders in mind. The table below maps the deliverables to the targeted users. \nThis is followed by a brief description of each type of deliverable.", " Table 2.5: Deliverables mapped to target audiences Deliverables Primary  users:  DFID \nstakeholders Case studies \nReports \nInternal workshops \nExternal Workshops \nLearnings outputs \uf070\uf020\n\uf070 \n\uf070 \n \n\uf070 users: Secondary \nStakeholders \ninvolved  in  the  pilot \nDIBs Tertiary users: those \ninterested \nin  DIBs \nand/or SIBs \uf070 \n\uf070 \n\uf070 \n \n\uf070 \uf070 \uf070 \n\uf070 This report forms evaluation report 1, which includes early feedback on the set-up of the DIBs \n(including an estimate of set-up costs) and recommendations for expanding and improving the \nDIB  programme  and  these  DIB  mechanisms.  This  is  also  complemented  by  specific  case \nstudies focusing on each of the three DIBs (see Annex A). An internal workshop was held \nto discuss emerging findings (see Annex K).", " Following  the  publication  of  the  evaluation  report,  an  external  workshop  will  be  planned \nwhich will bring stakeholders from across the DIB sector. The purpose would be twofold: firstly, \nto bring learning into the programme and to understand the DIB effect and lessons learnt in \ndelivery in other DIBs to contextualise the programme evaluation findings; secondly to share \nlearning  out  of  the  programme;  to  share  lessons  from  the  programme  and  consider  the \nimplications  for  the  wider  sector.  Furthermore,  following  the  publication  of  the  evaluation \nreport, we will produce short stand-alone learning outputs. These will be framed as \u2018lessons \nlearnt\u2019/\u2019how tos\u2019/\u2019top tips\u2019, focusing on specific learning themes that will be useful for DFID and \nthe wider sector.", " 2.2.4 Involvement of stakeholders The evaluation has been designed and managed to meet the information and decision-making \nneeds of the intended users. Discussions were carried out with DFID and stakeholders of the \npilot DIBs in order to inform the approach and needs of stakeholders, as part of the inception \nphase.  The  scope  of  the  evaluation  and  individual  DIB  level  plans,  in  terms  of  data  to  be \nshared  and  consultations  to  be  undertaken  over  the  course  of  the  evaluation,  have  been \ndiscussed and agreed with the DIB level stakeholders. The individual DIB level plans are set \nout in Annex F.", " In line with the Paris Declaration, the evaluation is aiming to avoid duplicating data collection \nand  learning  activities,  by  leveraging  data  and  learning  outputs,  in  order  to  synthesise \nevidence, balanced with the need to ensure that the evaluation team builds on data already \ngenerated. As such, the evaluation relies on data collected by the service providers. We have \nundertaken an initial assessment of this data in the Data Quality Assessments. Furthermore, \nthe evaluation team is committed to building evaluation capacity within partner countries. The evaluation  team  includes  experts  from  the  countries  where  the  DIBs  are  in  operation.  The \nexperts provide valuable context and input into the evaluation. See Annex E.7 for further detail.", " Methodological limitations The table below sets out the key methodological limitations and the mitigations undertaken.", " Table 2.6: Limitations and mitigations Limitations Mitigations Generalisability  of  findings:  The  number  of \nDIBs both within this evaluation and in the wider \nsector is small and very varied, limiting the ability \nto  make  generalisable  conclusions  about  the \neffectiveness of DIBs. \nApproach  to  causal  inference:  The  effect  of \nusing  a  DIB  is  not  quantified.  The  use  of \nexperimental  or  quasi-experimental  methods  in \norder  to  claim  attribution  is  not  appropriate  in \nthese  contexts.  It  cannot  be  assumed  that  any \ndifferences between the DIB and non-DIB areas \ncan be attributed to the DIB mechanism. \nLimited  availability  of  cost  data:  The  cost \nanalysis is limited by the limited availability of cost \ndata,  including  in-kind  costs  such  as  staff  time, \nlimited  availability  of  comparable \nand \nbenchmark data, to assess interest rates and the \nrisk and return alignment.", " the Reliance on quantifiable outcome measures: \nOur  Cost  Effectiveness  Analysis  (CEA)  will  be \ncalculated  using  only  quantifiable  outcome \nmeasures, and may exclude other outcomes.  \nResponse bias: Different stakeholders involved \nin impact bonds have different perspectives and \ninterests \nthe  DIB  mechanism.  This  can \nintroduce  certain  biases,  and  need  to  be  taken \nis  possible \ninto  account.  For  example, \nbeneficiaries will overstate the benefits of support \nwhen being interviewed, due to a desire to please in it to  estimate The analysis and findings have been carefully presented, \nwith  reference  to  the  specific  contexts,  DIBs  and \nstakeholders that the findings relate to, where applicable. \nFurthermore, the evaluation examines the extent to which \nthe DIB effect holds true across different sites. \nThe evaluation focuses on contribution, using a process \ntracing  approach.  This  attempts \nthe \ncounterfactual through a qualitative approach. Whilst this \nprovides some estimate of the counterfactual, it still does \nnot  provide  a  thorough  or  quantitative  assessment.  For \nexample,  without  a  strong  counterfactual  it  is  difficult  to \nestimate the full extent to which risk has been transferred. \nThe team worked with stakeholders to estimate costs and \ndistinguish between one-off costs related to the fact that \nthe  stakeholder  is  using  a  DIB  for  the  first  time,  and \nrecurring  costs  which  would  be  incurred  no  matter  how \nmany  DIBs  had  been  set  up.  Where  information  was \navailable,  staff  costs  were  calculated  based  on  an \nestimate of time rate. Cost data was complemented with \nfindings from the qualitative and quantitative data to gain \nan  overall  assessment  of  the  cost  effectiveness  of  the \nDIBs. We expect that there will be less missing data for \nresearch  waves  2  and  3.  As  part  of  the  interviews,  the \nevaluation  team  has  also  explained  the  importance  of \ncapturing  all  costs,  including  staff  time,  to  be  able  to \ndetermine the full cost of using a DIB. Furthermore, RW2 \nand  RW3  focus  on  delivery,  where  there  is  a  clearer \nbudget and understanding of the costs of delivery.  \nThe CEA will be complemented by qualitative analysis of \nthe DIB effect.", " We  also  reinforced  the  anonymous  nature  of  the \ninterviews and the desire for honest accounts to reduce \nresponse bias \nAdditionally,  drawing  on  our  experience  with  SIB \nevaluations, we have used exercises and prompts to help \nstakeholders  consider \nthat \ncontributed  to  project  delivery  and  to  explain  how  their the  possible factors Limitations Mitigations the  researcher  and  project5.  It  is  also  possible \nthat  projects  and  those  who  gain  from  the  DIB \nmechanism will wish to downplay the effect of any \nperverse incentives.", " communicated, combined  with DIB  compares  to  the  other  DIBs  to  help  them  consider \nwhy there might be similarities or differences. \nUltimately,  our  evaluation  was  dependent  on  what \nstakeholders \nthe \nevaluation team\u2019s judgement and experience with impact \nbonds.  Hence,  the  risk  of bias  due  to  different  interests \nand other factors cannot be completely avoided. It must \nbe noted that the evaluation is drawing on evidence from \na pilot programme, and supporting a pathway to improved \ncapacity for more rigorous evaluation of the DIB effect. \nFor  beneficiaries,  we  will  seek  to  use  random  sampling \nmethods,  where  appropriate  and  not \nlimited  by \ngeographical constraints. \nFor  practitioners,  we  have  been  speaking  to  staff \nmembers identified as the most relevant, based on their \nexperience  with  the  design  and  set  up  phase.  During \nresearch  waves  2  and  3,  we  will  request  a  fuller  list  of \npractitioners.  We  created  a  sampling  frame  to  select  a \nrepresentative sample of stakeholders.  \nDrawing on our experience with SIB evaluations, we have \nto  help  stakeholders \nused  exercises  and  prompts \nconsider  the  possible  factors  that  contributed  to  project \ndelivery; and explain how their DIB compares to the other \nDIBs to help them consider why there might be similarities \nor differences. \nOur comparison analysis will consider the areas in which \nthe comparison projects are similar and dissimilar to the \nDIB  funded  projects.  This  will  be  used  to  guide  the \nanalysis.  \nWe will rely on our local experts, who are both sector and \ngeographical experts, to input into the process tracing.", " Sampling bias: The size of the DIBs means that \nfor  some  stakeholder  groups  (for  example, \nbeneficiaries  and  practitioners)  we  will  only  be \ninterviewing  a  sample.  To  a  degree  we  will  be \nreliant  on  the  projects  to  recruit  stakeholders  to \nbe interviewed, and they may target recruitment \nat  stakeholders  more  favourable  towards  the \nprojects Reliability  of  competing  explanations:  The \nprocess tracing approach relies on stakeholders \nassessing  the  extent  to  which  different  factors, \nincluding  the  DIB,  contributed  to  the  delivery \neffectiveness  of  the  project.  The  projects  are \nin  very  complex  scenarios,  and \noperating \nstakeholders  may  struggle \nto  accurately \narticulate  the  relative  contribution  of  different \nfactors.  Furthermore,  context  is  important,  and \nthere  remain  limitations  in  the  comparability \nbetween the DIBs and the identified comparable \nprojects and PbR comparisons.", " 5 Knox and Bukard, 2009. Qualitative Research Interviews in Psychotherapy Research Vol. 19, Number 4 \u2013 5 (July \n\u2013 September 2009).", " 3.0  Summary of the DIBs This  section  provides  further  detail  on  the  four  DIBs  included  under  the  scope  of  this \nevaluation. Further details are provided in the individual case studies set out in Annex A.", " The four DIBs are briefly summarised below: \u2022  The International Committee of the Red Cross (ICRC) Humanitarian Impact Bond \n(HIB)  for  Physical  Rehabilitation  funds  the  building  of  three  new  physical \nrehabilitation centres in Mali, Nigeria and Democratic Republic of Congo (DRC). As a \npart  of  the  HIB,  ICRC  is  also  piloting  efficiency  improvement  measures  testing  and \nbuilding a Digital Centre Management System (DCMS).", " Up to CHF 26.09 million of outcome payments will be made based on improvements \nin the Staff Efficiency Ratio (SER), from the beginning to the end of the HIB, calculated \nby the number of beneficiaries having regained mobility thanks to a mobility device, \ndivided by the number of local rehabilitation professionals. The outcome funders are \nthe  Swiss,  Belgian,  Italian  and  UK  governments  and  La  Caixa  Foundation.  The \ncornerstone investor is New Re (a subsidiary of Munich Re, a reinsurance company), \nalongside six other investors.", " \u2022  The  Quality  Education  India  (QEI)  Development  Impact  Bond  aims  to  offer  a \nsolution at scale to the learning crises in India, by funding a range of high performing \nservice providers to improve learning outcomes for more than 300,000 primary school \naged children. A further aim of the project is to drive focus towards outcomes based \ncontracts  in  the  development  sector,  with  the  long-term  aim  to  transform  the  way \neducation  interventions  are  funded  in  India.  Therefore,  engaging  the  Indian \ngovernment  is  key  in  this  project,  as  well  as  including  robust  measurements,  and \nconsidering ways to standardise processes and produce templates for future outcome-\nbased  contracts.  There  are  three  service  providers  involved,  delivering  different \ninterventions.", " Up  to  a  maximum  of  USD  9.2  million  of  outcome  payments  will  be  made  based  on \nimprovements  in  learner  outcomes,  compared  to  a  control  group.  There  are  five \noutcomes  funders,  including  Michael  &  Susan  Dell  Foundation  (MSDF)  as  the  lead \noutcome funder. The UBS Optimus Foundation raised the investment from donations.", " \u2022  The Village Enterprise Micro-enterprise Poverty Graduation Impact Bond aims to \nraise  the  income  levels  of  a  minimum  of  12,660  households  through  Village \nEnterprise\u2019s microenterprise development program, known as a Graduation program. \nIt aims to equip its beneficiaries with the resources to create sustainable businesses.", " Up to USD 4.3 million of outcome payments will be made, mainly tied to increases in \nhousehold income. The outcome funders are DfID, USAID and an anonymous donor. \nThis  capital  has  been  provided  by  nine  investors,  including  the  Delta  Fund  as  lead \ninvestor.", " \u2022  The  Cameroon  Cataract  Bond  funds  sight-restoring  cataract  surgeries,  with  the \noverall aim of enabling the Magrabi ICO Cameroon Eye Institute (MICEI), the first eye care  hospital  in  Cameroon,  to  reach  self-sufficiency  in  five  years.  The  loan  aims  to \nexpand the market reach and provide eye surgeries for up to 18,000 low- and middle-\nincome patients at a low cost, and to help the hospital become a training institute for \nthe region.", " Up to USD 2.8 million of outcomes payments will be made, including USD 2.68m in \nrepayment of principal and interest to lenders and USD 0.12m in incentive payments \nto  the  hospital,  tied  to  the  achievement  of  three  outcomes  (number  of  cataract \nsurgeries, quality of surgery and financial sustainability of the hospital). The outcome \nfunders are the Conrad N. Hilton Foundation (Hilton Foundation), The Fred Hollows \nFoundation  and  Sightsavers.  The  investors  are  the  Overseas  Private  Investment \nCorporation (OPIC) and the Netri Foundation.", " The four DIBs are operating in development/humanitarian contexts, and the service providers \nare  primarily  non-governmental  organisations.  The  DIBs  are  similar  in  duration  (all \napproximately five years in length) and timescale, operating between 2017-23.", " However, the four DIBs are quite different in other areas. The policy areas range from health \ninterventions  in  a  humanitarian  setting  (ICRC  HIB),  to  livelihood  programming  (VE  DIB), \neyecare (Cataract Bond) and education (QEI DIB). The size of the impact bonds ranges from \nUSD 2m (Cataract Bond) to CHF 26m (ICRC HIB). The repayment terms also vary significantly \nbetween the DIBs, as well as the level of capital guarantees, which ranges from 0% in the \ncase of the QEI DIB and VE DIB, to 60% in the ICRC HIB and 100% in the Cataract Bond.", " The  types  of  stakeholders  involved  also  vary.  Investors  range  from    primarily  commercial \n(ICRC HIB) to primarily charitable organisations (QEI and VE), and the nature of the outcome \nfunders range from  primarily  bilateral  donors  (ICRC  and VE),  to  primarily  foundations (QEI \nDIB  and  Cataract  Bond).  The  ICRC  HIB,  VE  DIB  and  Cataract  Bond  all  fund  one  service \nprovider each, while the QEI DIB funds three service providers.", " The following sub-sections provide further detail: \u2022  Section  3.1  compares  the  interventions  funded  by  the  DIBs  in  terms  of  the  target \ngroups, activities, anticipated outcomes and impact, timescale, total value and cross-\ncutting issues \u2022  Section 3.2 sets out the stakeholders involved in the DIBs \u2022  Section  3.3  categorises the four  DIB  structures along key  characteristics,  set  out  in more detail in Annex I \u2022  Section 3.4 draws together initial conclusions based on this section.", " Programme components Table 3.1: Programme components The table below sets out the four DIBs\u2019 anticipated impact, outcomes and outputs, target groups, timescale, geographical coverage, and the \nextent to which the intervention aims to address issues of equity, poverty and exclusion.", " Component ICRC Village Enterprise Cataract Bond Target groups  People with physical marginalised disabilities People  living  in  extreme  poverty  (on  less  than \nUSD 1.90 per day) QEI 300,000 \nchildren Activities non-government \nThree \n(NGOs) \norganisations \neducation \ndelivering \nDelivery \nprogrammes. \nmodel \ninclude \ntypes \nimproving  whole  school \nmanagement, \nlearning \nsupplementary \nand  teacher  and  school \nleader training include \nActivities \nworkshops,  trainings  and \ne-resources  as  well  as \nmeetings  with  community \ngroups.", " Identification of individuals who live on less than \nUSD 1.90 per day Creation  of  Business  Savings  Groups  (BSG), \nwhich are self-governing councils of businesses Local  mentors  deliver  a  four-month  training \nprogram \nthe \nnecessary knowledge to run a business to  equip  participants  with Seed capital  is granted to  each group of three \nparticipants,  to  enable  them  to  start  their \nbusiness Mentors  provide  continuous  guidance  to  the \nparticipants  for  one  year,  coaching  them  in \nchoosing the focus of their business, as well as \nhow  to  grow  and  manage  their  business  and \nfinances, including saving in Business Savings \nGroups.", " three  new  physical \nBuild \nrehabilitation \nin \ncounties with significant unmet \nneed \nreference \ncentres).", " (innovative centres Train local staff to deliver high \nquality  physical  rehabilitation \nservices in these centres.", " Pilot  and  rigorously  assess \npilot  efficiency \nimprovement \nmeasures across eight existing \nICRC  physical  rehabilitation \ncentres,  and  build  a  digital \nCentre  Management  System \nthat will be rolled out across all \nICRC  physical  rehabilitation \nthe  aim  of \ncentres  with \nimproving \nand \nmaintaining patient outcomes efficiency Operationalise  the  three  new \nimproved \nusing \ncentres Low-income  patients  and \nmiddle-income \npatients \nwith cataracts in urban and \nrural areas in Cameroon The  Cameroon  Cataract \nBond  will  fund  cataract-\nrelated  equipment  and \nconsumables \nand \ninvolving \nactivities, \nprovision \na \ncomprehensive \nintervention programme at \nthe  MICEI, \nincluding \noutreach/awareness, \ndiagnosis, \ntransport, \ntreatment  and  follow  up \ncare.", " of Component ICRC QEI Village Enterprise Cataract Bond Improved \nschool \nprocesses,  systems  and \ninfrastructure People  living  in  extreme  poverty  are  equipped \nwith  the  resources  to  create  a  sustainable \nbusiness Higher teacher motivation Better  content  delivery \nand  engagement  with \nstudents Increased  peer  to  peer \nlearning in teachers Improved \nstudent \nretention and attendance Improved \ninfrastructure school People  living  in  extreme  poverty  are  able  to \ncreate    businesses  and  sustainably  increase \ntheir household incomes People  living  in  extreme  poverty  are  able  to \nincrease their household incomes and therefore \nincrease  their  household  assets,  savings  and \nconsumption.", " Secondary  outcomes  resulting  from  improved \nincomes,  such  as  wellbeing,  diets,  access  to \neducation and healthcare are achieved.", " Local \nknowledge enhanced capacity and Accessibility/availability  of \ncataract  surgical  service \ndelivery improved Quality  of  cataract  care \nimproved Development  of  a  self-\nsustaining \noperating \nmodel  that  provides  more \ncataract \naffordable \nservices Reduced \ncataract \nblindness  prevalence  (by \nage group) Greater  economic  and \nsocial impact Anticipated \noutcomes operational  protocols  that  are \nbased  on  effective  efficiency \nmeasures.", " with physical \nPeople \nreceive \ndisabilities \ncomprehensive \nrehabilitation \nservices  (mobile  devices  and \nassociated \nphysiotherapy \ntreatments) the delivery Through \nof \nmobility  devices,  children  can \nattend  school  and  adults  can \nfind \nthereby  gaining \nmobility, autonomy, and dignity \nand  becoming  an  active \nmember of society.", " jobs, A significant amount of time is \nfreed  up  for  family  members \ntaking  care  of  relatives  with \ndisabilities, who can now work \nmore.  The  household  as  a \nwhole can increase its sources \nof  income  and  improve  its \nliving standards.", " A  more  socially  cohesive  and \nto  a \nstable  society \nactively \nlarger  workforce \ncontributing  to  the  country\u2019s \nprosperity.", " thanks The new centres operate more \nis \nand \nefficiently, \nsustained.", " this Component ICRC QEI Village Enterprise Cataract Bond Timescale July 2017 \u2013 June 2022 November 2017- November 2020 March 2018 \u2013 March 2023 January  2019  -  March \n2022 Geographical \nCoverage New  centres  in  Mali,  Nigeria, \nDemocratic Republic of Congo Gujarat and Delhi Regions in Uganda and Kenya Cambodia, Testing of efficiency measures \nPakistan, \nin \nMyanmar,  Zinder  and  Niamey \nin \nTogo, \nMadagascar Niger,  Mali, Total value CHF 26.1 million (USD 26.5m \nas at Jan 2019) Total committed USD 5.3 million, of which USD \n4.3 million relates to outcome payments Up to USD 11.2 million (of \nwhich  USD  9.2  million \nrelates \noutcome \nto \npayments) Outcome \nmetric(s) Staff  Efficiency  Ratio  (SER), \ncalculated  by  the  number  of \nbeneficiaries  having  regained \nmobility  thanks  to  a  mobility \ndevice, divided by the number \nof \nrehabilitation \nprofessionals.", " local in learning \nDifference \noutcomes  between \nthe \ncomparison  group  and \ngroup, \nintervention \nin  standard \nmeasured \ndeviation.", " Increase in household income, proxied through \nconsumption and assets.", " Number \nsurgeries of cataract MICEI  hospital  to  serve \npopulation  of  Cameroon \nand broader Central Africa \nregion committed total \nUSD  3.5  million \nbudget \nby \noutcome funders, of which \nUSD  2.8  million  relate  to \noutcome  payments  (USD \n2.68m to lenders and USD \n0.12m to hospital) Quality \nsurgeries of cataract Financial  sustainability  of \nthe hospital target  (linked Equity \nto \nbonus payment to service \nprovider only) The  hospital  is  working \nwith  a  model  of  cross-\nand \nsubsidisation, \nis \nworking \ntarget  of \nto  a \nproviding 40% of surgeries Addressing  of \ncross-cutting \nissues (equity, The programme targets people \nwith  physical  disabilities  who \nare  often  excluded \nfrom \nsociety,  to  provide  them  with \nrehabilitation \ncomprehensive The  aim  of  the  DIB  is  to \n300,000 \nenable \nmarginalised  children  to \nattain  or  move  towards \nattainment  of  their  age- The  programme  targets  people  living  in \nextreme poverty and aims to provide them \nwith  the  resources  to  create  and  sustain \nbusinesses, enabling them to increase their Component ICRC QEI Village Enterprise Cataract Bond learning \nappropriate \nlevels,  and \nto  address \ndisparity  between  girls \nand  boys  in  literacy  and \nnumeracy.", " household  income,  increase  their  savings \nand ultimately lift themselves out of poverty.", " to individuals in the bottom \ntwo wealth quintiles of the \npopulation  in  Cameroon \nby the end of year 5.", " poverty \nexclusion) and to services. The aim is to support \nthem \ngain  mobility, \nautonomy  and  dignity  so  that \nthey are able to become active \nmembers \nsociety. \nof \nFurthermore,  family  members \nwho  were  taking  care  of  them \nwill be able to work more, and \nthe \nthe \nhousehold  as  a  whole  can \nincrease its income.", " intention that is Stakeholders involved in the DIBs The table below sets out the key stakeholders for each impact bond: Table 3.2: Key stakeholders Stakeholder ICRC QEI VE Cataract Bond Designer ICRC and KOIS British  Asian  Trust,  Michael  & \nSusan  Dell  Foundation,  UBS \nOptimus Foundation, Dalberg.", " Instiglio and the Anonymous \nDonor The  Cataract  Bond  Design  Coalition, \nwhich  is  formed  of  The  Fred  Hollows \nFoundation, \nthe  Conrad  N.  Hilton \nFoundation,  Sightsavers,  the  African \nEye Foundation and Volta Capital \nAfrica Eye Foundation (AEF), the not-\nfor-profit arm of the Magrabi ICO \nCameroon Eye Institute (MICEI) 18,000 \nlow-income  patients  and \nmiddle-income  patients  with  cataracts \nin urban and rural areas in Cameroon Service Provider ICRC Service Users Gyan Shala, Kaivalya Education \nFoundation, SARD (Society for All \nRound Development) \n  \n300,000  primary  school  children  in \nDelhi and Gujarat.", " Governments National and district governments Users of new ICRC \ncentres, and the 8 pilot \ncentres.", " Local governments in \nMali, DRC, and Nigeria Governments \nof \nSwitzerland,  Belgium, \n \nUK  and  Italy,  and  La \nCaixa Foundation.  \nMunich  Re,  Lombard \nOdier  pension \nfund, \ncharitable  foundations \nand others Outcome \nFunders Investors Village Enterprise.", " A  minimum  of  12,660 \nhouseholds in Kenya and \nUganda \nLocal \nrepresentatives \nand Uganda \nDFID, USAID DIV and an \nanonymous donor government \nin  Kenya Michael  &  Susan  Dell  Foundation, \nBT, Comic Relief, Mittal Foundation.", " The Fred Hollows Foundation, Conrad \nN. Hilton Foundation, Sightsavers UBS  Optimus  Foundation  leads  an \ninvestment  pool  of  multiple  private \ninvestors.", " Nine impact investors, \nincluding Delta Fund Overseas \nPrivate \nCorporation (OPIC), Netri Foundation Investment Outcome Verifier  Philanthropy Advisors  Gray Matters India \nProject Manager  None None Performance \nmanager \nLearning Partner  None None Dalberg Brookings IDinsight  \nInstiglio None Instiglio AEDES \nBond manager / technical advisor: Volta \nCapital \nNone None Stakeholder Knowledge \nPartner ICRC None DIB structures QEI Tata Trusts VE None Cataract Bond None The  structure  of  the  four  DIBs  under  the  scope  of  the  evaluation  were  quite  varied.  Table  3.3  categorises  the  four  DIBs  against  a  range  of \ncharacteristics. Further detail on these characteristics are set out in Annex I.", " Table 3.3: DIBs against DIB dimensions Characteristic  Description  \nDesign phase \u2013 identifying interventions \nLead \ndesigning \nintervention Nature \npromoter/designer the on of ICRC HIB QEI DIB VE DIB Cataract Bond Service provider, with advisory \nsupport Intermediary Intermediary Outcome Funder Funding \nfor \ndesign  and  set-\nup phase Whether  a  grant  was \nprovided,  or  this  phase \nwas  self-funded  by  the \nactors involved Grant received.  \nreceived \nService  provider \nto \ngrant,  which  was  used \ndevelop the HIB and to pay the \nadvisor.  Other  actors  covered \ntheir own costs.", " is The \nthe \nfeatures  of \nintervention,  and  whether \nit \ntotally  new,  an \nexpansion  of  an  existing \nprogramme  or  involves  a \nwhose \nprogramme \nunderpinning \nprinciples \nhave already been tested \nthe \nExtent \ncontract \na \nspecific  and  well-defined \nintervention  and  service to  which \ninvolves Expansion  of \nthe  existing \nprogramme  of  a  service \nprovider.  Implementation  of  a \nprogramme proven successful \nimprovement \n(efficiency \nmeasures  testing)  and  new \nprogramme \n(Digital  Centre \nManagement System).  \nContract  involves  a  specific \nand  well-defined  intervention, \nthough  there  is  room  to  test \nand adapt Level \ninnovation of of Level \noutcome \norientation  and \nflexibility  versus the Grant received.  \nDFID  provided  a  grant  to  BAT  to  cover  a \nproportion  of  operational,  design  and \ncontracting  costs; \nremainder  was \ncovered  by  UBSOF.  Other  actors  covered \ntheir own costs. \nInstiglio \nthe \nanonymous donor for initial design work and \nstakeholder engagement.", " received  a  grant from the other As \nstakeholders \ncommitted  to  the  DIB, \noutcome \nfunders \nUSAID-DIV, \n(DFID, \nthe  anonymous \nand \ncontributed \nDonor) \nfunds \nInstiglio  to \nsupport  to  finalisation \nof the project design.", " to Outcome funders paid for the \ntechnical assistance. All other \nactors covered their own \ncosts.", " Expansion  of  the  existing  programme  of  a \nservice  provider  and  implementation  of  a \nprogramme  already  proven  successful  in \nnew schools (using new methods) of Expansion \nthe \nexisting programme of \na service provider Implementation of a \nprogramme already proven \nsuccessful but in a new \ncontext Contract focuses on achievement of specific \noutcomes \u2013 intervention defined but subject \nto  change  and  adaptation  depending  on \nneeds Contract  focuses  on \nachievement \nof \nspecific  outcomes  \u2013 \ndefined \nintervention Contract  involves  a  specific \nand well-defined intervention Characteristic  Description  \nprovider, \nspecific \nor \nspecific \noutcomes  which  enables \nintervention \nservice \nto \nproviders \ndefined  \norganise  work  as \nthey \nprefer.  \nIdentifying metrics and structuring payments Nature \npayment \noutcomes of Were  payments  made \nsquarely  for  outcomes  or \nwas some payment made \nfor inputs or activities?", " Nature of capital \nused \nfund \nto \nservices Risk  borne  by  private \ninvestors  or  distributed \namong  different  actors \nthrough  capital  protection \nmeasures and risk sharing \narrangements ICRC HIB QEI DIB Cataract Bond VE DIB \nbut  subject  to  change \nand \nadaptation \ndepending on needs payment of Majority \noutcomes. \nAround \n(EUR \npayment \nmilestone \nconstruction of centres.", " 4% on 1m) \non Presence of capital protection \nmeasures (60%) \nrisk  sharing \nPresence  of \narrangements \npotential \n\u2013 \ndownside for service provider 94% payment on outcomes  \n6%  covers  contingency  costs  on  the  DIB, \nand \nfor \ncosts \nincluding \ncommunications evaluation 100%  payment  on \noutcomes Full risk on investors.  \nPresence  of  risk  sharing  arrangements  \u2013 \npotential upside for service provider of Full risk on investors \nrisk \nPresence \nsharing  arrangements \n\u2013  potential  upside  for \nservice providers Identifying and selecting stakeholders \nSocial  intent  of \nservice \nproviders \nSocial  intent  of \ninvestors \nStructuring the vehicle and developing the operating model Are the service providers / \ninvestors  a  charity  or \ncompany  without  explicit \nsocial values?", " Commercial Strong Strong Social 100%  payment  on  outcomes \n(though  the  achievement  of \nthe \noutcomes  only  affects \ninterest payable) Presence  of  capital  protection \nmeasures (Full protection) \nPresence  of \narrangements \nupside  and  downside \nservice provider sharing \npotential \nfor risk \n\u2013 Strong Social and Commercial Type \ncontract6 of Typologies  of  structure \ndepending on which actor \nhas  the  contract  with  the \noutcome funder.", " Direct Managed  \u2013  the  key  role  is  held  by  the \ninvestor Direct 6 In a direct impact bond structure, the service provider contracts directly with the outcome funder. In a managed impact bond structure, the outcome funder holds the contract \nwith the intermediary. The intermediary plays an important leadership role throughout the process of the deal and is responsible for performance management of the service \nprovision. (Gustafsson-Wright et al, 2015) Strong Social Outcomes fund.  \nOutcome funders \ndirectly contract and \ndisburse payments to \na trustee (the \nindependent manager \nof the \u2018fund\u2019). The \ntrustee separately \nholds a direct contract \nwith the service \nprovider (stipulating \nwhen and how Characteristic  Description ICRC HIB QEI DIB Cataract Bond VE DIB \npayments held from \nfunders will be \ndisbursed to the \nservice provider for \ntheir achievement of \nresults).", " Strong \u2013 internal Strong Strong Strong on Lead \nmanaging \nperformance \nGovernance arrangements and level of involvement of stakeholders:  \nOutcome funder  Role  of Service provider Low Investor Low Service Provider Intermediary Low Moderate Investor Low High Low Low of Strength \nperformance \nmanagement \nsystem How  hands  on  are  the \nother  stakeholders? \nIs \ndedicated \na \nthere \nperformance \nmanagement function?  \nWho  takes  the  lead  in \nperformance \nmanagement?", " the  outcome \nfunder  /  investor  toward \nservice  providers  and  its \nlevel  of  control  over  the \norganisations  involved  in \nthe impact bond \nRole  of \nthe  outcome \nfunder  /  investor  toward \nservice  providers  and  its \nlevel  of  control  over  the \norganisations  involved  in \nthe impact bond Measuring impact Validation \nimpact of Payment \nbased \nexperimental/quasi-\nexperimental  or  validated \nadministrative data7 on Payment  based  on  validated \nadministrative data.  \nThis will include verification of \nrecords \nphysical \nand \nverification  of  mobility  of \nbeneficiaries.", " Payment  based  on  quasi-experimental \nmethods Payment  based  on \nexperimental methods Payment  based  on  validated \nadministrative data.", " 7 In a true experiment, eligible participants are randomly assigned to a \u2018treatment\u2019 or \u2018control\u2019 group. In quasi-experimental approaches, there is no such randomisation, but \nrather, statistical methods are used to mimic a randomised trial to estimate the impact of the intervention. Administrative data relates to data collected by programme stuff \nduring implementation.", " Conclusion The DIBs under the scope of this evaluation are very different, which makes them challenging to compare directly, as are the types of programmes \nfunded  by  the  DIBs,  the  contexts  in  which  they  are  operating,  and  the  types  of  stakeholders  involved.  The  impact  bonds  have  also  been \noperationalised in a range of legislative, taxation and accounting frameworks. The structure of impact bonds have been modified to account for \nthese contexts, actors, objectives and constraints. Perhaps what this tells us most of all is that there is no one \u2018DIB\u2019 model, and that different DIB \ncan be applied to a variety of different contexts (though it is too early to say how successfully it can be applied).  \n \nAs we show in the remainder of this report, these differences in structure, characteristics and actors can have the following implications: \u2022  The structure and characteristics of an impact bond may affect the DIB effect (Carter et al, 2018; Arena et al, 2016), explored further in section 4; \u2022 \u2022 the types of costs incurred in setting up the DIBs, explored in section 5; and the lessons on how DIBs can be structured differently to improve the benefits of using DIBs, explored in section 6.", " As such, it is necessary to consider these contextual factors in the analysis of findings, and when drawing conclusions and recommendations for \nthe wider DIB sector. Our findings in the following sections are nuanced for these differences.", " 4.0  Analysis and Findings \u2013 DIB Effects Summary The DIB mechanism has made it possible to implement Payment by Results (PbR) contracts \nin contexts where, previously, this would not have been possible because the projects were \ntoo  risky  or  too  large.  This  is  primarily  due  to  the  new  partnerships  created  between \ngovernments,  donors,  delivery  partners  and  (to  a  degree)  the  private  sector,  in  which  the \nfinancial risk is shared between these groups. The DIB has fostered new working relationships \nbetween  stakeholders  and  has  led  to  greater  levels  of  collaboration  than  is  normally  seen, \nprimarily  because  the  DIB  aligns  all  stakeholders\u2019  interests  but  also  because  the  intensive \ndesign stage forces closer partnership working. A large amount of work has been done in all \nfour  DIBs  to  build  a  stronger  performance management  infrastructure,  including  investing in \nnew  monitoring  systems  and  working  closely  with  the  service  providers  to  embed  adaptive \nmanagement systems.", " Perhaps two of the most significant landmarks in these projects is that they have demonstrated \nthat private investors are willing to take on sizeable levels of risk in impact bonds (i.e. in the \nICRC Humanitarian Impact Bond (HIB), which includes private investment), and it is possible \nto launch impact bonds at a larger scale (i.e. the Quality Education India Development Impact \nBond (QEI DIB), which builds on the Educate Girls DIB).", " Whilst the DIB mechanism has reduced some (financial) risks for outcome funders and service \nproviders, it has increased others, namely reputational risk. There were quite strong concerns \namongst both outcome funders and service providers around using a new funding mechanism, \ndue  to  the  uncertainties  of  using  a  new  model,  alongside  the  heightened  attention  that  the \nmechanism  brings  to  the  projects,  increasing  unwanted  exposure  should  the  results  not \nmaterialise. This created a level of risk aversion, which we believe has diminished the level of \nrisk and innovation in the interventions \u2013 all four DIBs are funding service providers with strong \nrecords and interventions with strong evidence bases.", " All four DIBs were complex to design and launch, which resulted in large development costs.", " Some of the DIB effects seem to be closely intertwined with other effects. For example, some \nare more \u2018novelty effects\u2019 - that is they exist because these are the first set of DIBs, and will \nlikely diminish over time. This seems to be the case for the levels of risk aversion and the costs. \nIt is possible (though not certain) that these will reduce in future DIBs. Furthermore, because \nthe increased rigour in the outcomes measurement is a consequence of attaching payments to \noutcomes, this effect was also seen in some of the PbR comparator sites, and is therefore more \nof a \u2018PbR effect\u2019 than a DIB effect per se.", " The findings from these four DIBs in relation to the DIB effect broadly mirror the findings from \nthe wider literature. This is promising \u2013 the evidence of the DIB/SIB effect is currently weak, \nand so this evaluation provides further validation and gives a stronger understanding around \nhow impact bonds affect the design and set-up of projects.", " This  section  focuses  on  Evaluation  Question  1:  How  does  the  DIB  model  affect  the  design, \ndelivery, performance and effectiveness of development interventions \u2013 otherwise known as the \n\u2018DIB effect\u2019. It focuses in particular on how the DIB model has affected the design and set-up of \nthe four projects under examination; future waves will examine how the DIB has affected project \ndelivery, performance and effectiveness. The section introduces the DIB effect indicators related \nto project set-up and design, and then describes the extent to which these DIB effect indicators \nwere  apparent  in  the  four  DIBs  included  in  the  analysis.  This  analysis  draws  primarily  on \nconsultations with stakeholders involved in the four projects. The section also considers how the \npresence of these indicators compares with other impact bonds; this draws on consultations with \nwider stakeholders and the literature review undertaken during the scoping stage.", " Sections 5 and 6 draw on this analysis, in order to identify ways to improve the design and delivery \nof DIBs. Section 5 considers the effectiveness of the DIB effect on risk transfer, within a value for \nmoney framework. Section 6 considers how the model can be improved in order to increase the \n\u2018DIB effect\u2019 \u2013 that is, the benefits of using the DIB mechanism.", " The DIB effect indicators In order to isolate the \u2018DIB effect\u2019 the evaluation is using a combination of process tracing and \ncomparative  analysis.  This  involves  the  creation  of  hypothesised  DIB  effects,  and  associated \nindicators to measure the presence of the effects. The effects and indicators were drawn from a \nliterature review and stakeholder consultations.", " Table 4.1 includes a list of all the DIB effects.8 The DIB effects are divided into effects one would \nexpect to see during the design and set-up of the DIB, and those one would expect to see during \ndelivery. As Research Wave 1 focused on the design and set-up of the DIBs, only these effects \nwere  examined  during  this  wave  (these  effects  are  highlighted  in  red  in  Table  4.1).  We  have \ncategorised the DIB effects in the design and set-up phase into four types: \u2022  Transfer of risk \u2022  Partnerships \u2022  Financing and funding \u2022  Design The evaluation will examine the other DIB effects during Research Waves 2 and 3; it should be \nnoted that it is expected that most DIB effects will materialise during project delivery.", " It should also be noted that, whilst the evaluation includes a set of indicators to measure these \neffects, evaluator judgement was necessary to judge the extent to which the effects were present, \nand  the  extent  to  which  these  can  be  attributed  to  the  DIB.  Where  evaluator  judgement  was \napplied, we have tried to make this clear in the description of the presence of the effects below. \nThese  judgements  were  tested  and  verified  with  stakeholders  during  the  internal  learning \nworkshop. We also shared a draft copy of this report with stakeholders and asked for comments.", " 8 An initial set of DIB effects and indicators were provided in the Inception Report. These were refined following RW1, \nto allow for a more nuanced description of the DIB effects.", " In some instances stakeholders did not agree on the presence of some of the effects \u2013 we have \nmade this clear in the relevant sections.", " Table 4.1: DIB effect indicators Claimed DIB effect Claimed advantages \nTransfer of financial risk from \noutcome funder to investor \nFunding  projects  which  would \nnot have been funded otherwise, \nor  not \nthe  same  guise \n(including scale) \nCrowd-in  private,  additional, \nupfront,  long-term,  stable  and \nsecured financing, which brings \nin  additional  finances  to  the \ndevelopment sector in Shift focus to outcomes \nMore  innovative  services  (or \nlarger-scale innovative services) \nbecause: \n\u2022  providers more \nflexibility and autonomy to \ndeliver  what  they  feel  will \nachieve outcomes \ntransfer \u2022  Risk have from government/outcomes \nto  service \nfunder  partly \nprovider  but  mainly \nto \ninvestor,  who  have  higher \nappetite for risk performance accountability, Drives \nmanagement \nas \nGreater \nimpact  bond  builds  leads  to \nculture  of  monitoring  and \nevaluation \nMore  careful  and \ndesign \ninterventions rigorous \nprogramme of All  of  the  above  factors  leading \nto more beneficiaries supported, \nand  more  outcomes  achieved, Indicator  to  measure  presence  of  \u2018DIB  effect\u2019  in  DIBs  and  comparator \nsites \u2022  Extent to which investment capital is at risk \u2022  Extent  to  which  outcomes  funders  would  have  either  funded  the \nproject at all, or in its current form, if it were funded through a different \nmechanism \u2022  Scale  and  source  of  funding  (including  whether  private  financing), \nand where this funding would have been directed if it had not funded \nthis project \u2022  Duration and \u2018security\u2019 of funding \n\u2022  Mobilization  ratio:  for  every  $1  of  ODA  mobilized  $x  in  private financing \u2022  Extent that supplier pre-financing was required for PbR contract  \n\u2022  Opportunity cost of using own funds \u2013 i.e. has DIB financing allowed the organization to invest in other things Set up \u2022  Perceptions on rigour of design stage \n\u2022  Level of \u2018innovation\u2019 / risk in project delivery, in terms of: \n\u2022  new type of intervention altogether (radical innovation); \n\u2022  an  established  intervention  that  has  been  adapted  (incremental innovation); or \u2022  an established intervention that has been applied to a new context, e.g. location, policy area, target population \u2022  Scale of project, in terms of delivery cost and number of beneficiaries \n\u2022  Extent and quality of external expertise \u2022  Extent to which delivery decisions are made to maximise outcomes \n\u2022  Extent  to  which  a  service  provider  feels  more  incentivised  to  offer user-specific supports (the human touch element) \u2022  Level of flexibility found within the project to alter project delivery \n\u2022  Extent to which service provider feels it can take risks and innovate   \n\u2022  Extent to which service provider feels it has autonomy over delivery  \n\u2022  Level  of  responsiveness  and  agility  of  partners  to  deal  with Delivery bottlenecks, issues and challenges \n\u2022  Extent and quality of external expertise Monitoring \u2022  Rigour  of  monitoring  and  evaluation  systems  developed,  including verification of outcomes and duration of outcomes tracking \u2022  Transparency  of outcomes  \u2013 i.e. frequency and quality  of reporting internally and externally \u2022  Strength of performance management and measurement systems \n\u2022  Use of real time performance information to inform ongoing delivery Sustained impact \u2022  Extent to which systems and practices implemented as part of project \nare embedded across the wider organisation and/or sustained once \nthe DIB ends \u2022  Number of beneficiaries supported per GBP / FTE \n\u2022  Number of outcomes achieved per GBP / FTE Claimed DIB effect Indicator  to  measure  presence  of  \u2018DIB  effect\u2019  in  DIBs  and  comparator \nsites service ultimately \nto  more \nleading \neffective and efficient services \nproviders \nMore \nentering the  PbR market due to \ntransfer of risk \nGreater  collaboration  and/or \nbetween \ncoordination \nis  an \nstakeholders  as \nalignment of interests \nClaimed disadvantages \nComplex to design there Expensive \nimplement to  set  up  and Impact  bonds  create  perverse \nincentives Performance \nmanagement \nculture lowers staff morale and \nincreases staff turnover \n\u2018Tunnel  vision\u2019:  Focus  on \nprimary outcomes comes at the \nsecondary \nof \nexpense \noutcomes; \nfor \nopportunities \nproject co-benefits are missed \nDIB  creates  additional  social \nand \nrisks, \nreputational \ndiminishing some of the claimed \nadvantages (such as innovation) \u2022  Number and type of providers participating in PbR contracts, and their historic experience with PbR contracts \u2022  Level of unrestricted funding as % of overall value of PbR contract \n\u2022  Self-reported strength of relationship of partners involved and levels of collaboration and/or coordination \u2022  Extent to which stakeholders believe the design to be complex \n\u2022  Demands  of  project  design  in  terms  of  time  and  need  for  external expertise \u2022  Length of time it took to design and launch the project \n\u2022  Set up costs \n\u2022  Cost per outcome / beneficiary \n\u2022  Proportion of total cost of project going to front line delivery against \nproportion going to project development and administration (including \nresearch and data verification, and project and funding coordination \nand management) \u2022  Profile of beneficiaries and evidence of \u2018cherry picking\u2019 \n\u2022  Level,  quality,  range  and  duration  of  support,  and  extent  to  which \ndecisions around these have been affected by the contracting model \n(e.g. leading to parking) \u2022  Levels of morale amongst staff \n\u2022  Levels of staff turnover \u2022  Range and level of secondary outcomes achieved \u2022  Extent to which stakeholders perceive the project to hold reputational and social risks Presence of the DIB effect indicators: Summary In Table 4.2 we summarise the extent to which the different DIB effect indicators were present \nacross the four DIB projects. Each effect is \u2018RAG\u2019 rated9 on the extent to which it was identified \nacross all projects, followed by individual ratings for each DIB. It should be noted that the rating \nidentifies the extent to which the effect is present, not whether it had a positive effect (i.e. both \npositive and negative effects would be marked as green if present).", " Below  the  table  we provide  more  analysis  on  the  presence  of  each of these effects,  including \nconsidering how this compares with other impact bonds.", " 9 Green = effect is present in at least three DIBs; amber = mixed evidence over presence of DIB effect; red = effect is \nnot present in at least three DIBs Table 4.2: Presence of DIB effect indicators in the four DIB projects DIB Effect ICRC HIB Village Enterprise DIB Cataract Bond Quality Education India \nDIB Transfer of risk Anticipated Emerged Anticipated Emerged Anticipated Emerged Anticipated 1.      Transfer  of \nfinancial \nrisk \noutcome \nfrom \nfunder \nto \ninvestor Yes Some financial risk \ntransferred (40% of investors\u2019 \ncapital is at risk; 60% capital \nguarantee, shared between \nthe outcome funders and \nservice provider).", " Yes 100% transfer \nof financial risk Yes 100% transfer of \nfinancial risk Yes Yes Yes Yes Yes Yes Yes No No Yes Yes Yes Yes Yes Yes No Emerged Some financial risk \ntransferred (0% of \ninvestors\u2019 capital at risk; \n4% of interest at risk; \ncapital guarantee split \nbetween outcome funder \n(76.5%) and service \nprovider (23.5%) No, could likely have been \ninvolved if no transfer of \nrisk Yes - though there were \ncomments that \ncollaboration and \ntransparency could have \nbeen improved.", " Yes Yes Yes Yes Yes Yes Yes \u2013 though there were \ncomments that collaboration \nand transparency could have \nbeen improved.", " transfer  of 2.    Reputational \nrisks \n  resulting \nfrom  the  use  of \nthe DIB  \nPartnerships \n3.     \nMore \nservice providers \nentering the PbR \nmarket  due \nto \npre-financing \nand \nrisk \n4.     \ncollaboration \nand/or \ncoordination \nbetween \nstakeholders  as \nan \nthere \nis \nalignment \nof \ninterests  \nFinancing and \nfunding Greater ICRC HIB Village Enterprise DIB Cataract Bond Quality Education India \nDIB Anticipated Emerged Anticipated Emerged Anticipated Emerged Anticipated Emerged Yes Yes Yes Yes Yes Yes No Yes Yes.", " Yes Yes Yes 7.     Longer term \nfunding \nDesign 8.     \ninnovation Enables Yes Yes Yes Yes (incremental innovation.   Yes No No No No No No Yes (incremental \ninnovation) Yes (incremental \ninnovation) More \n9.     \ncareful \nand \nrigorous  design \nof interventions Yes Mixed. Yes in terms of \nrigorous design of M&E, but \nno impact on design of \nintervention Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No.  Raised \nexternal finance \nbut most of this \nphilanthropic \nfunding that would \nhave gone into \nthe sector \nanyway No Yes, though \nmixed opinion on \nwhether this can \nbe attributed to \nthe DIB DIB Effect 5.     \nFunding \nprojects  which \nwould  not  have \nbeen \nfunded \notherwise, or not \nin \nsame \nguise the 6.      Additional \nfinancing  to  the \ndevelopment \nsector 10.    Complex  to \ndesign \nand \nexpensive  to  set \nup No, likely project could \nhave been funded without \nDIB Mixed \u2013 finance would \nhave gone into \ndevelopment sector, but \nnot eye health or \nCameroon No. Raised \nexternal \nfinance but \nmost of this \nphilanthropic \nfunding that \nwould have \ngone into the \nsector anyway  \nTo some \nextent \n  \nYes \n(incremental \ninnovation \nMixed. Yes in \nterms of \nrigorous \ndesign of M&E \n(but similar \nrigour in PbR), \nbut no impact \non design of \nintervention Risk transfer effects This section examines how the DIB mechanism affected the levels and types of risk borne by \nthe different stakeholders involved, including financial risk and reputational risk.10 4.3.1 Effect 1: Transfer of financial risk 4.3.1.1  DIB effect explanation DIB effect: Transfer of financial risk from outcome funder to investor Hypothesis:  In  a  grant  or  fee-for-service  mechanism  the  outcome  funder  is  taking  on  all \nfinancial  risk,  as  they  have  to  pay  for  the  intervention  regardless  of  whether  it  achieves  its \nintended outcomes. In  a  PbR  model  this risk  is  transferred from  the  outcome funder  to the \nservice provider, as the outcome funder only pays if outcomes are achieved; if they are not \nthe service provider loses the working capital it used to fund the intervention. In a DIB this \nfinancial risk is transferred in-part from the outcome funder and service provider onto a social \ninvestor. The investor provides the service provider with the upfront working capital, which is \nrepaid  from  the  outcomes  payments  paid  for  by  the  outcome  funder.  If  outcomes  are  not \nachieved the outcome funder does not pay, and the social investor loses their money.", " 4.3.1.2  Main finding: Extent to which DIB effect present across four projects Summary:  In  all  four  DIBs  some  financial  risk  was  transferred  from  the  outcome  funder  and \nservice provider to the investors, though the extent to which the risk was transferred varied.", " Extent to which DIB effect present across four projects QEI \ntransfer 100% \nfinancial risk VE \ntransfer of of 100% \nfinancial risk DIB Effect Transfer  of  financial  risk \nfrom  outcome  funder  to \ninvestor ICRC \nfinancial 60% risk \nSome \ntransferred \n(40%  of \ninvestors\u2019  capital  is  at \ncapital \nrisk; \nguarantee, \nshared \nbetween  the  outcome \nfunders  and  service \nprovider).", " Cataract Bond \nfinancial \nSome \nrisk \ntransferred \n(0%  of  investors\u2019 \ncapital at risk; 4% \nof  interest  at  risk; \ncapital  guarantee \nbetween \nsplit \nfunder \noutcome \n(76.5%) \nand \nservice  provider \n(23.5%) 4.3.1.3 Analysis from four projects In order to assess whether financial risk was transferred from the outcome funders and service \nproviders to the investors two questions need to be answered: \u2022  Was  there  financial  risk?  i.e.  was  there  a  risk  that  the  projects  would  not  achieve outcomes?", " 10 Risk is challenging to measure without a counterfactual. We rely on discussions with stakeholdres to \nunderstand their perceptions of risk, and the likelihood of involvement should a DIB not have been used.", " \u2022  Was this financial risk transferred to the investors? i.e. is the working capital supplied by the investors at risk if outcomes are not achieved?", " We explore each of these below.", " Was there financial risk?", " During the set up phase DFID assessed the risk of the three DIBs they were involved in (VE, \nICRC and QEI). The results from this assessment is summarised in Figure 4.1 below. This \nshows that two of the projects were deemed as being medium-high risk (ICRC and QEI) and \nthe VE DIB was classed as high risk. This perception of the level of risk was corroborated by \nstakeholders in all four of the DIBs during the consultations in RW1; stakeholders were of the \ngeneral perception that these projects were not of high risk \u2013 they included service providers \nwith strong reputations and involved interventions that had been delivered before with good \nevidence bases. However, there were some risks involved in each of the DIBs: \u2022 \u2022 \u2022 \u2022 In  ICRC,  there  were  risks  in  relation  to  delivering  the  new  efficiency  improvement \nmeasures, as these had not been used before.", " In the QEI DIB, there were risks around using a more rigorous assessment tool (known \nas  measurement  risk).  Including  a  new  standardised  assessment  of  learning  carried \nsome  additional  risk  for  the  service  providers;  while  they  are  all  familiar  with  being \nevaluated, they were not familiar with the assessment and therefore their performance \nin this context is unknown.", " In the Cataract Bond, there were risks in relation to launching the particular model of \neye care in Sub-Saharan Africa, where the model has had only limited implementation.", " In  the  VE  DIB,  there  were  \u2018cumulative\u2019  risks  around  delivering  at  a  larger  scale,  the \nneed to achieve outcome targets at a greater scale than those achieved under previous \niterations of the intervention, and the value for money uncertainty over VE deploying \nhigher  grant  sizes  and  how  their  programme  may  need  to  adapt  to  ensure \ncommensurately  high  benefits  materialise  from  that  increased  grant  size.  It  was  this \ncumulative risk that resulted in the VE DIB receiving the highest risk score from DFID.", " Figure 4.1: DFID risk assessment of three DIBs Source: Learning from Impact Bonds in use by DFID and others. DFID internal learning document. Risk scale: 0 = \nnon-applicable; 2 \u2013 minor risk; 4 = medium risk; 6 = high risk; 8 = very high risk. Average risk scores: VE: 6.15; \nICRC 4.9; QIE 4.9.", " Therefore, based on the above evidence, the projects can be classed as being of medium, \nrather than high, risk. The reason high-risk projects were not funded is partly an inherent factor related to  impact  bonds - that  they  need  a  reasonable  evidence  base in order for  potential \noutcomes  to  be  modelled  and  produce  a  credible  business  case,  and  to  provide  some \nreassurances to investors. However, we believe the projects were also medium-risk because \nof the reputational risks related to the DIB mechanism, which created a level of risk aversion \namongst outcome funders and service providers (see Effect 2: Reputational risks).", " Was the financial risk transferred to investors?", " In two of the DIBs (VE and QEI), 100% of financial risk has transferred to the investor \u2013 i.e. \ninvestors may lose all of the capital they provide to the projects, depending on the outcomes \nachieved. In the other two DIBs (Cataract Bond and ICRC) the transfer of financial risk is less \nas there are capital guarantees in place. In ICRC the investors have a 60% capital guarantee, \nshared between the outcome funders and service provider. In the Cataract Bond, some risk is \ntransferred from the outcome funder to the service provider (if targets are not met the outcome \nfunders must pay the investor 76.5% of the capital investment plus 4% interest and the service \nprovider must pay the investor 23.5%), but only a very small amount of risk is transferred from \nthe service provider to the investor, as the service provider does not have to pay any interest \n(which they  would  with a commercial loan). Therefore, the level of risk transferred in these \nDIBs is less; indeed some stakeholders in the Cataract Bond questioned the value of the DIB \nmechanism, considering the relatively small degree of risk transfer.", " Therefore, the differences in risk transfer across the four projects is quite pronounced.", " The level of risk transfer, and commensurability with returns, is further explored in section 5.3.", " 4.3.2 Effect 2: Reputational risks 4.3.2.1  DIB effect explanation DIB effect: Reputational risks resulting from the use of the DIB Hypothesis: Not all risks are mitigated through the DIB mechanism, as reputational risk still \nsits with the service provider.", " 4.3.2.2  Main finding: Extent to which DIB effect present across four projects Summary: The DIB mechanism increased the reputational risk in three of the projects.", " DIB Effect \nAdditional \nrisks \nresulting from the use of \nthe DIB ICRC QEI VE Yes Yes Yes Cataract Bond \nYes 4.3.2.3 Analysis from four projects Gustafsson-Wright  et  al  (2015)  argues  that,  whilst  the  impact  bond  mechanism  transfers \nfinancial risk from the outcome funder and service provider, it does not transfer all risks, such \nas reputational risk. Our research found that many stakeholders interviewed were of the view \nthat not only does reputational risk remain with the outcome funder and service provider in a \nDIB,  but  that  the  reputational  risk  often  increases.  Across  all  four  projects,  both  outcome \nfunders and service providers expressed concerns around the reputational risk of using such an innovative financing mechanism. Some outcome funders and service providers expressed \nconcerns regarding the increased attention and associated scrutiny that the impact bond may \nbring,  and  particularly  the  publicity  if  results  do  not  materialise.  They  were  also  concerned \nabout damages to the reputation of the organisations should they be involved in something \nwhere people \u2018profit from the poor\u2019.", " This would appear to have led to degree of risk aversion in these DIBs. This affected both the \nselection of service providers and interventions; with careful selection to ensure both service \nproviders and the interventions had established track records. The evidence would suggest \nthis risk aversion limited the extent to which other DIB effects materialised, as we describe \nfurther in this section.", " It is also interesting to note that the reputational risk was seen as having upsides as well as \ndownsides.  For  many  of  the  service  providers  across the  DIBs the  spotlight  the  DIB  would \ncreate  on  their  organisation  was  a  motivating  factor  for  joining  the  DIB.  Furthermore,  the \nbacking  of  an  external  investor  signals  confidence  in  both  the  intervention  and  the  service \nprovider, and that the outcome targets can be met.", " It is likely that certain elements of the reputation risk and potential benefits to reputation for \nthe  stakeholders  involved  will  diminish  over  time,  as  the  DIB  mechanism  becomes  more \nfamiliar, and there are a large number of DIBs in operation. However, we expect that the value \nof signalling, and the risk of failing to meet targets, will still have implications for stakeholders\u2019 \nreputation. Nonetheless, there is a combination of \u2018pilot effect\u2019 and \u2018DIB effect\u2019; i.e. it was the \npiloting of a new mechanism rather than the DIB itself that heightened the reputational risk.", " 4.3.2.4  Comparison with other impact bonds Our finding that impact bonds increase other risks is consistent with the broader literature. For \nexample, both Social Finance (2018) and Gustafsson-Wright et al (2015) note that while the \nfunder\u2019s risk has been reduced to some degree as payments are only made if it works, the \nfunder  is  subject  to  new  risks  through  increased  exposure,  risk  of  demonstrated  failure  or \npaying too much.", " Partnership effects This  section  examines  how  the  DIB  mechanism  affected  the  partnership  working  between \nstakeholders, examining how the DIB affected the types of stakeholders involved in delivery, \nand the working relationships between stakeholders.", " 4.4.1 Effect 3: More service providers entering PbR market Summary: More service providers entering the PbR market due to pre-financing and transfer \nof risk 4.4.1.1  DIB effect explanation Hypothesis:  Small  and  innovative  service  providers  are  excluded  from  PbR  contracts \nbecause they cannot risk their own working capital, or cannot raise the finance, to fund the \nproject before outcomes payments are received. This reduces the range of providers able to deliver PbR contracts, minimising the efficacy of such contracts. DIBs overcome this barrier, \nas  the  social  investor  provides  the  upfront  working  capital.  This  enables  new  providers  to \ndeliver PbR contracts who would not have been able to do so otherwise.", " 4.4.1.2  Main finding: Extent to which DIB effect present across four projects Summary: Without the presence of the external investment three of the service providers would \nnot  have  been  able  to  be  involved  in  the  projects;  the  DIB  therefore  enabled  more  service \nproviders to be involved in PbR contracts DIB Effect \nMore  service  providers \nentering \nPbR \nmarket  due \nto  pre-\nfinancing and transfer of \nrisk the ICRC QEI VE Yes Yes Yes Cataract Bond \nNo, could likely \nhave been \ninvolved if no \ntransfer of risk 4.4.1.3 Analysis from four projects In three of the DIBs the service providers reported that they would not have delivered these \ncontracts  on  a PbR  basis  at this  scale  due to  the  level  of  working  capital  required  and the \nfinancial risk of not getting this money back. For example, a World Bank note on output-based \naid in fragile and conflict situations (FCS) noted that, \u201cservice providers in FCS are sometimes \nunable to carry the full pre-financing risk.\u201d11 However, this was not the case in the Cataract Bond, where the Magrabi Foundation believe \nthey could probably have accessed a loan if necessary. Furthermore, in the case of the Village \nEnterprise,  though  it  would not  have  entered  into  a  PbR  contract,  should the  DIB  not  have \nmaterialised, it would likely have sought to access different funding to start businesses in the \nsame regions in Kenya and Uganda.", " However, this does not necessarily mean that DIBs would enable all service providers to be \ninvolved  in  PbR  contracts.  Other  barriers  existed  that  influenced  which  service  providers \nbecame  involved  in  these  projects;  in  each  DIB  the  organisation  leading  the  project \ndevelopment  undertook  a  robust  selection  process    to  identify  suitable  providers,  which \nincluded in-depth due diligence. The selection process included assessing service providers\u2019 \nprevious  levels  of  performance  and  their  ability  to  implement  adaptive  management \nprocesses. For example in the QEI DIB service providers were selected on the basis that they \nwere  able  to  commit  to  the  DIB  requirements  without  needing  to  make  substantial \norganisational  changes.  Additionally,  service  providers  need  to  be  comfortable  with  the \nreputational risks associated with DIBs, and may not be willing to take absorb such risks - in \nparticular, those without strong track records or the ability to take on adaptive management \nprocesses easily, could be excluded from DIBs.", " Furthermore,  some  service  providers  withdrew  from  the  projects,  as  they  felt  they  would \nstruggle with the capacity demands from the DIB and its related performance management. \nFor example, Educate Girls intended to be involved in the QEI DIB but the timing of this project \ncoincided  with  the  final  year  of  their  first  DIB,  limiting  their  organisational  capacity.  This 11 OBApproaches: Output-Based Aid in Fragile and Conflict Situations. Note Number 47, June 2015.", " suggests that whilst DIBs enable some service providers to be included in PbR contracts, they \nthemselves still experience barriers that may limit their participation in a project.", " 4.4.1.4  Comparison with other impact bonds The  finding  that  impact  bonds  enable  some,  but  not  all,  service  providers  to  take  on  PbR \ncontracts is a common one. In the UK there is evidence that SIBs have enabled smaller service \nproviders to enter PbR contracts who would not have been able to do so previously because \nof  the  financial  risk.  The  CBO  Evaluation  (unpublished)  found  similar  barriers  for  service \nproviders entering the market - as has been seen in these four DIBs, it was found that investors \nwork repeatedly with trusted organisations with strong and credible management teams, and \nentering into an impact bond requires a degree of capability and capacity that a large number \nof smaller service providers do not have.", " Effect 4: Greater collaboration and/or co-ordination 4.4.1.5  DIB effect explanation Summary:  Greater  collaboration  and/or  coordination  between  stakeholders  as  there  is  an \nalignment of interests Hypothesis: Linking payments to outcomes leads to an interest from all parties to improve \ndelivery and achieve better outcomes and financial returns. The close partnership can also \nbring together distinct expertise and addresses knowledge gaps across the partners.", " 4.4.1.6  Main finding: Extent to which DIB effect present across four projects Summary:  The  DIB  enabled  coordination  and  collaboration  between  new  actors,  and  also \nstrengthened this for actors that had previously worked together.", " DIB Effect \ncollaboration \nGreater \ncoordination \nand/or \nbetween  stakeholders \nas there is an alignment \nof interests ICRC \nYes  \u2013  though  there \nwere  comments  that \ncollaboration \nand \ntransparency  could \nhave been improved.", " QEI VE Yes Yes Cataract Bond \nYes - though \nthere were \ncomments that \ncollaboration \nand \ntransparency \ncould have been \nimproved during \nthe initial design \nphase.", " 4.4.1.7 Analysis from four projects In all four of the DIBs stakeholders reported that the DIB fostered new working relationships, \nand  also  strengthened  pre-existing  ones.  This  increased  collaboration  was  most  apparent \namongst outcome funders. In the Cataract Bond, for example, the desire to work together to \ncontribute  to  a  shared  goal  brought  together  three  outcome  funders  who  had  never \ncollaborated in this three-way partnership before. In the QEI DIB, all stakeholders remarked \non  the  openness  and  willingness to  work  together  in  a collaborative way,  but  most  notable \nwas the willingness of outcome funders to share information and data with each other in an \nopen way.", " \u201cThe joint  awareness and  wealth of  foundation  knowledge  that  came into  play on  this,  you \ncan\u2019t  underestimate  you  know\u2026  part  of  the  beauty  of  this  piece  in  international \ndevelopment\u2026it\u2019s about the data that\u2019s sitting within foundations, especially deep technical \nfoundations like MSDF\u2026to share and be open minded, and move a bit\u2026. And to say right \nwe\u2019re willing to put that out there to be tested.\u201d (Representative from British Asian Trust, QEI \nDIB. Comment made during case study consultations) One outcome funder felt this greater collaboration between outcome funders was because the \nfocus on outcomes brought about by the DIB mechanism ensured they were united towards \n\u201cone common outcome\u201d and although views on how to achieve this outcome varied, it was still \ndescribed  as  \u2018a  single  united  goal\u2019.  Interviewees  valued  these  new  partnerships  and \ncollaborative ways of working; they felt the partnerships brought together a merged expertise, \nwhilst the willingness to share information and data enabled organisations to gain a greater \nunderstanding of the context.", " \u201cThe  collaboration  it  has  been  remarkable from  the  UBSOF  and  Dalberg.\u201d  (Representative \nfrom service provider, QEI DIB. Comment made during case study consultations) This strengthening was mainly due to the alignment of interests between outcome funders and \nservice  providers  to  ensure  the  projects  were  designed  robustly  with  accurate  and  clear \noutcome measures. It was also a consequence of the complexities in designing the DIB, which \nrequired strong levels of communication between the different parties.", " The  extent  to  which  the  DIB  fostered  greater  collaboration  differed  between  the  four  DIBs, \nthough the collaboration appeared stronger in two (VE and QEI). There appeared to be less \ncollaboration in the ICRC HIB because discussions were held bilaterally between ICRC and \ndifferent stakeholders; this limited the opportunities for stakeholders to discuss the HIB with \neach other and some expressed frustration with this. In contrast, in QEI and VE, discussions \nwere held multilaterally, with frequent joint calls and workshops between all parties. This would \nsuggest  that  there  is  value  in  holding  multilateral  discussions  with  all  parties  during  DIB \ndevelopment.", " 4.4.1.8  Comparison with other impact bonds Collaboration and co-ordination is a strong theme present across impact bonds, both in terms \nof bringing together new partners, and strengthening pre-existing relationships. For example, \nthere are good examples in the UK where SIBs have brought very different partners together \nas funders all interested in achieving similar outcomes (such as the local authority, schools \nand philanthropists as outcome funders in the West London Zone SIB, or different government \ndepartments in the Youth Engagement Fund).", " The finding from these four DIBs therefore aligns with findings from other impact bonds.", " Financing and funding effects This sections examines how the DIB affected the funding of projects, including whether the \nDIB enabled new projects to be funded, and whether it brought additional private finance to \nthe  development  sector.  It  is  important  to  differentiate  funding  from  financing.  Funding  is \nrelated  to  the  question  of  who  ultimately  pays  over  the  long  term  \u2013  funding  is  generally provided  without  expectation  of  repayment,  unless  the  contractual  terms  are  not  met. \nFinancing is the upfront capital/cash needed, which is generally expected to be repaid, along \nwith interest.", " 4.5.1 Effect 5: Funding projects which would not have been funded otherwise 4.5.1.1  DIB effect explanation Summary: Funding projects which would not have been funded otherwise, or not in the same \nguise Hypothesis: Political accountability can make it difficult for donors to provide public funds in \nadvance  for  risky  programmes;  the  transfer  of  risk  in  a  DIB  away  from  outcome  funders \nenables them to fund risky programmes they could not do otherwise.", " 4.5.1.2  Main finding: Extent to which DIB effect present across four projects DIB Effect \nFunding  projects  which \nwould  not  have  been \nfunded  otherwise,  or  not in \nthe same guise ICRC QEI VE Yes Yes Yes Cataract Bond \nNo, likely project \ncould have been \nfunded without DIB 4.5.1.3 Analysis from four projects Summary: All four interventions had been funded previously, and so the DIB did not enable \ncompletely new interventions to be funded. However, in three of the DIBs the interventions had \nalterations and outcome funders reported that would not have funded these without the DIB; \ntherefore the DIB did alter the guise of the interventions, including enabling projects to operate \nat larger scale or with innovative elements.", " The DIB did not enable completely new interventions to be funded, as all four had been funded \npreviously in a different guise, in some instances by the same funders. For example, DFID, \nthe  Belgian  and  Swiss  governments  all  provide  core  funding  to  ICRC;  the  same  outcome \nfunders  in  the  QEI  DIB  have  funded  the  same  service  providers  to  deliver  very  similar \ninterventions; the Magrabi Foundation had already raised USD 10m of the USD 12m required \nto fund the hospital; and the Village Enterprise intervention is already operating in other parts \nof Africa. Therefore, the DIB has not enabled completely new interventions to be funded, which \nwould not have received funding otherwise.", " However, whilst the interventions themselves have been funded before, the nature of three of \nthe projects is different in the DIBs (VE, ICRC and QEI), and it is the DIB element that enabled \nthem to be delivered in a different guise. As described in Effect 1: Transfer of financial risk, \neach project had a new element that was deemed risky by the outcome funders, and in three \nof the DIBs outcome funders reported they would not have funded the projects in their current \nguise because they deemed them to be too risky.", " \u201cWe would have been unlikely to fund the Village Enterprise project if we had not had the DIB \nmodel which puts this cumulative risk onto VE and the investor and better aligns incentives \ntoward sustained higher level impacts.\u201d (DFID) \u201cThe DIB has really opened new avenues...It is a way of achieving new goals, or longer-term \ngoals, or goals at scale.\u201d (Service provider, comment made during learning workshop) The Cataract Bond is different to these three. Whilst at the time the Magrabi Foundation did \nnot have the final USD 2m to operationalise the hospital, they were reasonably confident that, \nin  the  two  years  it  took  to  raise  the  DIB  finance,  they  could  have  secured  funding  from \nelsewhere. And therefore in this project the DIB mechanism did not fund a project that would \nnot have been funded otherwise.", " 4.5.1.4  Comparison with other impact bonds The finding that  in some  of the  projects  it  is  unlikely  that  the  intervention  would have  been \nfunded  without  the  impact  bond  mechanism,  but  in  others  the  intervention  may  have  been \nfunded anyway \u2013 is consistent with the development of impact bonds in the UK. There are \nstrong examples in the UK where the project was deemed too risky, and without the transfer \nof  risk  the  project  would  not  have  been  funded  (such  as  the  Ways  to  Wellness  and \nReconnections SIBs); however there are also projects that either were already being funded \n(such  as  the  MHEP  SIB)  or  could  feasibly  have  been  funded  anyway  (such  as  the  Youth \nEngagement  Fund  SIBs).  In  these  latter  impact  bonds,  much  like  the  Cataract  Bond,  the \ndriving motivation to use the impact bond was to test its efficacy, rather than to transfer risk.", " 4.5.2 Effect 6: Additional financing to development sector 4.5.2.1  DIB effect explanation DIB effect: Additional financing to the development sector Hypothesis: The introduction of the external investor in DIBs brings additional private finance \nto the international development sector that would not have been available otherwise.", " 4.5.2.2  Main finding: Extent to which DIB effect present across four projects Summary: Whilst all four DIBs raised external finance, in the  majority of cases this was not \n\u2018new\u2019  finance,  as  it  was  from  philanthropic  sources.  In  these  DIBs  the  finance  is  better \nperceived of as existing money re-purposed rather than additional finance.", " DIB Effect \nAdditional  financing  to \nthe development sector ICRC \nYes. Private sector \nfinance that would \nnot have gone into \ninternational \ndevelopment QEI \nNo. Raised external \nfinance but most of \nthis philanthropic \nfunding that would \nhave gone into the \nsector anyway VE \nNo. Raised external \nfinance but most of \nthis philanthropic \nfunding that would \nhave gone into the \nsector anyway Cataract Bond \nMixed \u2013 finance \nwould have gone \ninto development \nsector, but not eye \nhealth or \nCameroon 4.5.2.3 Analysis from four projects In two of the DIBs (VE and QEI) the investors were mainly philanthropic investors who would \nhave provided funding to the development sector anyway. In the VE DIB, some of the investors \nwere organisations that had already donated to Village Enterprise before. One of the investors \nin  the  Cataract  Bond  (OPIC)  is  a  Development  Finance  Institution  (DFI),  and  therefore  by \ndefinition this is finance that also would have been invested.", " However, it was noted that without the DIB, it would be unlikely that finance would have been \nraised to the same level. In the QEI DIB, the investment manager (UBS Optimus Foundation) \ndescribed how the investment funding they used to invest in the DIB came from donations to \nUBSOF by clients of UBSOF.", " While  these  three  DIBs  have  not  attracted  new  finance  to  the  development  sector,  such \nfinance would not have been used in this specific context \u2013 the sharing of the risk enabled the \nfinance to be used for a highly developmental project in a context (Cameroon and eye health) \nwhere OPIC has not invested before. This DIB, therefore, did shift finance into new areas of \nthe development sector.", " The ICRC HIB, in contrast, did bring in money from investors that would not necessarily have \nbeen invested into the international development sector. As for financing in the commercial \nworld, the upfront finance is expected to be repaid, with interest, unless the programme does \nnot meet its outcome targets.", " What is apparent across these four DIBs is that there is a trade-off between risk sharing and \nthe degree to which the DIBs are attracting new finance. In those DIBs that have shifted where \nfinance is invested (ICRC and Cataract Bond) the investor is taking on less financial risk. This \nsuggests  that,  if  the  aim  is  for  DIBs  to  attract  new  finance  into  the  development  sector, \noutcome funders and service providers will have to accept that they must take on some of the \nfinancial risk.", " 4.5.2.4  Comparison with other impact bonds Gustafsson-Wright et al (2015: 37) found that additional capital from traditional private actors \nhas been limited, as this would require \u201ca different analytic mindset and acceptance of credit \napproval.\u201d  Whilst  there  have  been  some  mainstream  investors  in  the  United  States,  these \nhave typically included much higher capital guarantees (such as the Rikers Island SIB, where \nBloomberg Philanthropies guaranteed 83% of the  USD 7.2m invested by Goldman Sachs). \nThe use of private capital with more limited capital guarantees in the ICRC HIB is therefore \nquite a major step forward in impact bond development, as it demonstrates that it is possible \nto for private investors to take on sizeable levels of risk in impact bonds.", " 4.5.3 Effect 7: Longer-term funding 4.5.3.1  DIB effect explanation DIB effect: Longer-term funding Hypothesis: Projects are often funded on short-term cycles. Because impact bonds require \noutcomes  to  be  achieved,  and  typically  there  is  a  time-lag  between  the  intervention  being \ndelivered  and  the  outcome  achieved,  impact  bonds  operate  over  longer  timescales than  is \nusual. This is beneficial as it provides more secure funding for interventions.", " 4.5.3.2  Main finding: Extent to which DIB effect present across four projects Summary: The DIB provided long term funding to some extent in two of the DIBs, though the \nbreak clauses meant this did not make the funding more secure.", " DIB Effect \nLonger term funding ICRC QEI VE Yes To some extent No Cataract Bond \nNo 4.5.3.3 Analysis from four projects In ICRC, funding is generally received on an annual basis, so the ability to roll over funding \nbetween years and plan for longer term projects, such as the DCMS and EIM, was cited as a \nparticular  benefit. While the  QEI  service  providers  noted  that  they  had  received  other  long \nterm grants, the longer-term funding was nonetheless cited as a benefit of the DIB, as the DIB \ndid not require the renewal of the funding agreement on an annual basis.", " Long-term grants was not noted as a particular benefit in the VE or Cataract Bonds. In the VE \nDIB,  Village  Enterprise  who  had  already  received  such  grants  from  other  donors.  In  the \nCataract Bond, interviewees noted that it was unusual for a loan to be provided for five years, \nbut there was not anything specific about the DIB mechanism that enabled this.", " 4.5.3.4  Comparison with other impact bonds The finding that in some instances but not all the impact bond enabled longer-term funding  is \nagain  consistent  with  the  wider  literature.  There  are  examples  where  the  impact  bond \nmechanism did enable longer-term and more stable funding. For example, the Evaluation of \nthe  SIB  Trailblazers  in  Health  and  Social  Care  in  the  UK  found  that  the  SIB  Trailblazers \noperated with more stable funding and had longer-term contracts than were present typically \nunder conventional forms of financing. Furthermore, the shift away from annual funding, as \nwe saw here with ICRC, was also cited as a benefit in the Educate Girls DIB. However, there \nare also multiple impact bonds where the funding is not necessarily longer than conventional \ncontracts. Therefore, this DIB effect, both in these four DIBs and in the wider sector, is variable.", " Design effects In this section we examine how the DIB affected the design of the projects and interventions. \nThis includes the extent to which the DIB mechanism fostered innovation, how it impacted on \nthe rigour of the project design, and how it impacted on the costs involved in designing and \nsetting up the projects.", " 4.6.1 Effect 8: Enables innovation DIB effect: Enables innovation . \n4.6.1.1  DIB effect explanation Hypothesis:  Risk  transfer  from  outcome  funder  to  investor  enables  riskier,  and  more \ninnovative, interventions to be funded.", " 4.6.1.2  Main finding: Extent to which DIB effect present across four projects Summary:  The  DIB  did  not  enable  radical  innovation,  but  all  four  projects  had  elements  of \nincremental innovation within them, which was possible because of the DIB funding.", " DIB Effect \nEnables innovation ICRC QEI Yes (incremental \ninnovation) Yes (incremental \ninnovation) Yes \ninnovation) VE \n(incremental Cataract Bond \nYes \n(incremental \ninnovation) 4.6.1.3 Analysis from four projects Gustafsson-Wright categorises innovation in impact bonds into three types: \u2022  New type of intervention altogether (also known as radical innovation12); \u2022  An established intervention that has been adapted (incremental innovation); or \u2022  An established intervention that has been applied to a new context, e.g. location, policy area, target population In  all  four  DIBs  the  level  of  innovation  can  be  described  as  incremental.  As  mentioned \npreviously  all  of  the  interventions  have  been  delivered  previously  and  have  strong  track \nrecords, and therefore they cannot be deemed to be radical innovations. Indeed, one service \nprovider described how they are delivering more \u2018radical\u2019 innovations outside of the DIB. This \nlack  of  radical  innovation  is  a  consequence  of  two  factors:  first,  interventions  require  a \nreasonable level of evidence in order to build a business case and reassure investors; second, \nthe reputational risk of using a new financial mechanism has inhibited service providers\u2019 and \noutcome  funders\u2019  appetites  for  delivering  something  radical  under  the  DIB,  given  the \nsignificant focus on these first DIBs (see Effect 2: Reputational risks).", " However, whilst none of the interventions are new they have all been adapted and include \nelements of incremental innovation. The Cataract Bond includes targets related to financial \nsustainability and equity not used in previous eye hospitals, and is also adopting a particular \nmodel of eye care that has only had limited implementation in Sub-Saharan Africa; the VE DIB \nis  operating  at  a  larger  scale  and  Village  Enterprise  are  piloting  a  range  of  innovations \n(changing the levels of grant provided to beneficiaries, piloting mobile money and introducing \nadaptive  management  processes);  the  ICRC  HIB  includes  new  efficiency  improvement \nmeasures; and the QEI DIB is using new assessment tools.", " Interviewees  attributed this  incremental  innovation in part  to  the  DIB mechanism. Outcome \nfunders from the VE DIB, for example, stated that the DIB design helped to create \u2018a space\u2019 \nfor  innovation; the rationale was that  with  the  transfer  of risk from the  outcome funder,  the \nservice provider is able to deliver the intervention as they see fit and adapt it where necessary \nto achieve better results. Stakeholders within the ICRC HIB also differed in terms of the extent \nto which they attributed the measures to the DIB mechanism. Based on these comments it is \nour view that the mechanism has supported innovation by providing the service provider with \nthe funding and space to do it, though it is not yet clear that a purely outcomes focused contract \ncould not have the same effect.", " 4.6.1.4  Comparison with other impact bonds According to Gustafsson-Wright et al., 2015, so far impact bonds have not supported many \nradically  innovative  interventions,  but  some  have  supported  interventions  that  are  being \ndelivered  in  different  ways  or  to  different  populations,  as  is  the  case  with  these  four  DIBs.", " 12 Koberg et al, 2003. An empirical test of environmental, organizational, and process factors affecting incremental \nand radical innovation. Journal of High Technology Management Research 14 (2003) 21-45. Pergamon.", " Similarly, the Educate Girls DIB saw incremental innovation, but not radical innovation. These \nfour  DIBs  therefore  strengthen  the  evidence  base  that  impact  bonds  are  better  suited  for \ntesting incremental innovation than radical innovation.", " 4.6.2 Effect 9: More careful and rigorous design 4.6.2.1  DIB effect explanation DIB effect: More careful and rigorous design of interventions Hypothesis: A strong business case is necessary in order to attract external investment. This \nensures  interventions  are  well-researched  and  carefully  designed.  The  attachment  of \npayments  to  outcomes  incentivises  both  outcome  funders  and  service  providers  to  ensure \noutcomes are clearly defined and robustly measured.", " 4.6.2.2  Main finding: Extent to which DIB effect present across four projects Summary:  In  general  attaching  outcomes  to  payments  led  to  more  rigorous  design  of \nmonitoring procedures than fee-for-service contracts or grants, but appeared no different than \nother forms or PbR. The DIB had mixed impact on the design of the intervention.", " DIB Effect \nMore \nrigorous \ninterventions careful design and \nof ICRC \nMixed.  Yes  in  terms \nof rigorous design of \nM&E,  but  no  impact \non \nof \ndesign \nintervention QEI \nMixed. Yes in terms \nof rigorous design of \nM&E \n(but  similar \nrigour  in  PbR),  but \nno impact on design \nof intervention VE \nYes,  though  mixed \nopinion  on  whether \nthis \nbe \ncan \nattributed to the DIB Cataract Bond \nYes 4.6.2.3 Analysis from four projects As described in the hypothesis above, research suggests that the DIB mechanism improves \nthe rigour and design of projects in two ways: \u2022 Improving the design of the intervention itself, including referral criteria and ToC \u2022 Improving the definition and measurement of outcomes In  these  four  projects  the  DIB  mechanism  had  more  of  an  impact  on  the  latter  (rigour  of \nmonitoring  procedures)  than  the  former  (rigour  of  intervention  design).  In  terms  of  the \nmonitoring procedures, in most of the projects this was strengthened by the DIB compared to \nwhen these (or similar) interventions were delivered through grants (though not PbR, as we \ncover below). This was most apparent in the ICRC HIB, in which new efficiency improvement \nmeasures were created and new data dashboards constructed to monitor progress against \nthese measures. In the QEI DIB  the measurement tools introduced were more robust than \nthe  tools  used  when  these  interventions  were  funded  previously  through  grants,  and  the \nexternal evaluators reported that the service providers engaged more strongly with the impact \nevaluation than is usual in grant-funded programmes.", " In the Cataract DIB outcomes funders felt the reporting was more efficient and transparent \ncompared  to  other  projects  they  have  been  involved  in,  and  more  work  was  done  with the \nhospital  to  ensure they understood  the targets  set.  It  was also  strongly  apparent  in the  VE DIB; for example as a consequence of the DIB they have implemented enhanced cost tracking \nto examine spending in the DIB context versus other programs in order to improve efficiency.", " \u201cIt creates a level of rigour because we want to deliver our outcomes...The idea is that this \nwill  help  us  to  deliver  better.  We  are  developing  dashboards,  databases,  we  work  around \ninstances of collection of data and ultimately this work is going to get quality information into \nthe hands of managers, improve our outcomes by following up with staff who may be having \nperformance  issues.\u201d  (Village Enterprise representative.  Comment made  during case study \nconsultations) This  increase  in  monitoring  across  the  DIBs  was  a  consequence  of  attaching  payments  to \noutcomes;  in  all  four  DIBs  this  incentivised  stakeholders  to  ensure  outcomes  were  clearly \ndefined, understood and measured. One intermediary also remarked in the learning workshop \nthat it ensures claims around potential impact are more accurate, as they have to be delivered; \nit disincentives service providers from exaggerating potential impact to attract funding.", " \u201cIt\u2019s a more honest sales process.\u201d (Intermediary, comment made during learning workshop) Because this effect is caused by attaching payments to outcomes, it is perhaps not surprising \nto note that a similar effect was also seen in the some of the PbR comparator sites (where \npayments  were  also  attached  to  outcomes,  albeit  a  lower  proportion).  For  example, \nstakeholders  interviewed  that  were  involved  in  the  GEC  programme  also  observed  that \nattaching  payments  to  outcomes  had  the  same  effect  of  improving  the  measurement  of \noutcomes (even though only 10% of payments were linked to the achievement of outcomes). \nTherefore it appears that this DIB effect is not necessarily different to the PbR effect.", " In terms of the design of the intervention itself, there was some evidence that the DIB led to \nsome  improvements,  but  this  effect  was  not  as  strong  as  the  impact  on  the  monitoring \nprocedures. In the Cataract Bond the introduction of the equity target is leading to a greater \nfocus on outreach, targets for it, and an understanding of whether the people reached through \noutreach  are  the  poorest.  There  has  also  been  a greater  focus  on  quality  \u2013  the  hospital  is \ndoing a much deeper dive to understand quality of outcomes and introduce interventions, such \nas regular management and staff sessions, to understand and strengthen quality, something \nthat stakeholders noted seems to be less consistently done as part of grant-funded projects. \nIn the VE DIB, the service provider did make substantial improvements to the design of the \nintervention, but it is not clear whether this is due to the DIB mechanism or the involvement of \nDFID as a funder: \u201cBecause of DfID there was a lot of detail required on the design but I would not say it would \nuniformly  be  the  case  across  the  DIB  sector\u201d  (Village  Enterprise  representative.  Comment \nmade during case study consultations) In  the  ICRC  HIB  the  intervention  design  was  no  different  to  how  it  been  implemented \npreviously.", " 4.6.2.4  Comparison with other impact bonds The  measurable  effects  on  the  impact  bond  of  performance  management  procedures  are \nsupported  by  wider  evidence  of  the  SIB/DIB  effect.  In  the  Youth  Engagement  Fund  (YEF) \nevaluation  (unpublished),  for  example,  service  providers  substantially  improved  their  data collection and management systems in order to meet the additional reporting requirements to \nclaim outcomes and report to investors.", " There  is  varied  evidence  on  the  extent  to  which  impact  bonds  affect  the  design  of  the \nintervention itself, with examples where interventions have been re-designed to ensure they \nmeet the outcome metrics (again as in YEF), but also examples where this has not occurred. \nThis  is  akin to these four  DIBs,  where  some  interventions  have been  redesigned  (Cataract \nBond), but others have not (ICRC). In the wider literature, as with these four DIBs, this appears \nto be influenced by how pre-established the intervention is and the extent to which it aligns \nwith  the  outcomes  metrics;  where  it  has  a  strong  evidence  base  and  aligns  well  with  the \noutcome metrics the intervention is not usually altered.", " 4.6.3 Effect 10: Complex to design and expensive to set up 4.6.3.1  DIB effect explanation DIB effect: Complex to design and expensive to set up Hypothesis: Impact bonds include complex relationships with multiple stakeholders. Designing \nnew outcomes metrics that meet the needs of all parties can be particularly challenging. This \nlengthens the time it takes to develop new projects, increasing the set-up costs.", " 4.6.3.2  Main finding: Extent to which DIB effect present across four projects Summary: In all four projects interviewees consistently held the view that the DIB was complex \nto design and expensive to set up.", " DIB Effect \nComplex  to  design  and \nexpensive to set up Yes ICRC QEI VE Yes Yes Cataract Bond \nYes 4.6.3.3 Analysis from four projects In all four projects the DIB was complex to design and expensive to set up. The complexities \nand costs are explored in further detail in Section 5.0.", " Other factors influencing the DIB effect 4.7.1.1  DIB effect vs novelty effect It is likely that some of the DIB effects identified in the four DIBs are, in fact, \u2018novelty effects\u2019 \u2013 \nthe effect whereby individuals may perceive and respond differently in a situation that is novel \ncompared with how they would in a normal situation.13  There are a number of DIB effects that \nmay exist, or be stronger, because these are some of the first DIBs to be launched, and these \neffects may diminish over time. Whilst it is difficult to predict at this stage which effects will \ndiminish over time, based on the current research we believe the following effects are likely to \nbe partly due to a novelty effect: 13 Gravetter and Forzano, 2018. Research Methods for the behavioural Sciences. Cengage learning.", " \u2022  Transfer of risk and types of service providers and interventions funded: We note in the \nprevious sections that in these DIBs there has been a degree of risk aversion; the failure \nrisk  in  the  projects  is  more  medium  than  high,  there  was  a robust  selection  of  high-\nperforming service providers, and in some of the projects the level of risk taken on by \nthe investors has been marginal. This appears to be in part because of the \u2018novelty\u2019 of \nthe  DIB,  and  therefore  the  reputational  risks  associated  with  its  potential failure. We \nwould  expect  the  perceived  risk  of  DIBs  to  diminish  if  and  when  they  become  more \nmainstream,  and  this  may  see  more  risky  projects  being  funded  through  DIBs,  and \nhigher levels of financial risk transferred to investors.", " \u2022  Costs  and  complexity:  As  reported  in  the  previous  section,  it  is  likely  that  the  costs \nassociated with DIBs will reduce as people learn how best to design and launch DIBs, \nand replicate previous DIB designs.", " The  novelty  effect  also  appears  to  have  influenced  some  funders\u2019  decisions  to  fund  these \nprojects. In some projects the main driving factor for funding the DIB has been the interest in \nthe potential of the mechanism, rather than necessarily because of the benefits it could bring. \nOne stakeholder believed that funders\u2019 interests in DIBs was 70/30 novelty/impact, and they \nused  the  novelty  factor  of  DIBs  as  the  \u201csales  pitch\u201d  to  attract  funders.  There  was  a  broad \nrecognition that funding a DIB primarily because of its novelty factor was not sustainable, and \nthat it is important to focus on the problem first and see the DIB mechanism as one solution \nto the problem amongst others, rather than deciding upfront that the DIB mechanism should \nbe  used.  DFID,  however,  was keen  to stress  that  a key  priority for them when considering \nfunding DIBs was to ensure the impact bond had the potential to add value to the intervention \ncompared  to  alternative  funding  approaches;  the  reported  discussions  and  decisions  they \nmade during the set-up process supports this.", " 4.7.1.2  DIB effect vs PbR effect There is some evidence to suggest that  some of the effects attributed to the DIB are more \n\u2018PbR\u2019  effects,  in  that  they  are  effects  seen  when  payments  are  attached  to  outcomes, \nregardless of whether the programme is a PbR or DIB. This is most notable in the QEI DIB, \nwhere the \u2018DIB effect\u2019 of improving the design and rigour of the outcomes measurement was \nalso a benefit seen in the GEC PbR programme.", " Additional effects not identified in the framework As well as the DIB effects outlined in Table 4.1 above, stakeholders highlighted an additional, \nunanticipated  effect.  This  was  that  the  DIB  mechanism  was  shifting  the  mindset  of \nphilanthropists, and creating a new set of impact investors. One investment fund in particular \ndescribed how philanthropists who would not ordinarily use their money for impact investing \nwere attracted by the alignment of financial and social returns associated with the DIB, and \nwere for the first time using their funds for impact investing. Whilst this was not anticipated at \nthe outset of the programme, there is evidence to suggest this has also been a side-effect of \nSIBs.", " Conclusions The focus of this section has been on how the DIB mechanism has affected the design and \nset up of these four projects, examining in particular the extent to which the purported \u2018DIB \neffects\u2019 set out in DfID\u2019s Theory of Change and other literature materialised. The evidence so \nfar  would  suggest  that  the  majority  of  the  DIB  effects  have  materialised,  albeit  to  different \ndegrees and with some nuances.", " The strongest positive DIB effects have been that they have made it possible to implement \nPbR contracts in contexts where, previously, this would not have been possible because the \nprojects  were  too  risky  or  too  large.  This  is  primarily  due  to  the  new  partnerships  created \nbetween governments, donors, delivery partners and (to a degree) the private sector, in which \nthe  financial  risk  is  shared  between  the  three  groups.  The  DIB  has  fostered  new  working \nrelationships  between  stakeholders  and  has  led  to  greater  levels  of  collaboration  than  is \nnormally seen, primarily because the DIB aligns all stakeholders\u2019 interests but also because \nthe intensive design stage forces closer partnership working. A large amount of work has been \ndone  in  all four  DIBs to build a  stronger  performance management  infrastructure,  including \ninvesting in new monitoring systems and working closely with the service providers to embed \nadaptive management systems.", " Many of the DIB effects were quite consistent across the four DIBs, although some were more \nconsistently  found  (such  as  enabling  innovation  and  being  complex  to  design)  than  others \n(such as providing longer-term funding and impacting on the intervention design). Where these \nfour projects quite starkly contrast with each other is in the financial risk sharing arrangements \nbetween the outcome funders, service providers and investors, which can be divided into three \nsets: impact bonds where the financial risk is fully borne by the investors (QEI and VE); impact \nbonds where the financial risk is shared between the investors and service providers (ICRC); \nand impact bonds where the financial risk is shared between all three \u2013 the service provider, \ninvestor and outcome funder (Cataract Bond). The remainder of the evaluation will examine \nhow the different sharing of financial risk impacts on the delivery of the projects.", " The Cataract Bond stands out as the DIB as unique amongst the four impact bonds. This is \nbecause the Cataract Bond was trying to do something quite different to the other three; in the \nother  three  these  were  projects  where  it  is  unlikely  they  would  have  been  funded  in  other \ncircumstances, and the DIB (through the transfer of risk and the attachment of payments to \noutcomes) enabled the projects to be funded. The Cataract Bond, in contrast, was a \u2018proof of \nconcept\u2019 project; it is likely it could have been funded through a different mechanism, but the \nstakeholders  wanted  to  demonstrate  that  it  could  technically  be  funded  through  an  impact \nbond. It is for this reason that some of the benefits seen in the other DIBs (in relation to funding \nprojects that would not have been funded otherwise and enabling service providers to access \nworking capital) are not apparent in the Cataract Bond.", " The findings from these four DIBs \u2013 including the differences in consistency of DIB effects \u2013 \nbroadly mirrors the findings from the wider literature. This is promising \u2013 the evidence of the \nDIB/SIB effect is currently weak, and so this evaluation provides further validation and gives \na stronger understanding around how impact bonds affect the design and set-up of projects. \nWhere the DIB/SIB effects in these four compared to the wider literature is in relation to two \nareas: \u2022  Additional financing to the development sector: There are very few examples where \nimpact bonds have brought in additional private finance, and certainly with the level of risk sharing seen in the ICRC HIB. Although not achieved in the  Cataract Bond, the \npresence of a DFI in an impact bond is a marker for market-rate returns.", " \u2022  Enabling scale: These four impact bonds are substantially larger \u2013 in terms of contract \nvalue and beneficiaries supported \u2013 than impact bonds in high income countries and \ncompared to their predecessors (i.e. Educate Girls DIB).", " These two areas are important landmarks in impact bond development, as they demonstrate \nthat it is technically possible to have private investors taking on sizeable amounts of risk, and \nit is possible to launch impact bonds at a larger scale. It will be very interesting to monitor how \nthese new developments affect the delivery and performance of the impact bonds, which will \nbe explored in future waves of the evaluation.", " Finally,  with  these  benefits  have  come  additional  complexities  and  costs.  It  is  too  early  to \nconclude whether the benefits outweigh these costs. Stakeholders were confident that lessons \ncould be learnt from these DIBs that would reduce the complexity and cost of future DIBs, as \nwe explore in the following section.", " 5.0  Analysis and Findings \u2013 Costs of designing and delivering DIBs Summary Economy The  emphasis  in this  research  wave has  been on  establishing the  additional  costs  of the \nDIB.  Future research waves will explore the link between additional costs and outcomes, \nand  whether  the  DIB  led  to  efficiency  savings.  The  analysis  is  framed  around  the  4Es  of \nValue for Money, exploring economy, efficiency, effectiveness and equity.", " All stakeholders confirmed there had been additional costs - either actual, in kind or pro bono \n\u2013 for staff time and consultancy in designing and setting up the DIBs. These costs tend to \nbe incurred by outcome funders and service providers. They relate mainly to the investor \nreturns that will be paid (either by the outcome funder or service providers).", " Stakeholders identified that some of the design and set-up costs, were unique to DIBs (e.g. \ncontracts requiring legal and financial consultancy), but that others are commonly seen in \nother similar programmes, particularly with a PBR or output-based contract (such as ongoing \ncosts  of  performance  management,  project  management  and  verification).  Stakeholders \nexpected some of the DIBs costs would reduce for future DIBs.", " Efficiency Effectiveness Equity It is too early to draw conclusions on the efficiency of the DIBs.  No savings have yet been \nrealised, though opportunities for efficiency savings have been identified and these will be \nreviewed during subsequent research waves.", " The extent to which the risk and return trade-off for each DIB represents value for money \nwill continue to be explored during the evaluation.  At this stage, all DIBs have interest rates \nthat are similar to those used by other impact bonds. It would appear that across the DIBs \nthere is a positive relationship between the levels of financial and social risk, and levels of \nreturn.", " We have looked at whether the DIBs have a strategy in place to promote equity as part of \ntheir design.  This will be used in later research waves to confirm whether DIBs have been \neffective  in  promoting  equity.    However,  the  initial  finding  on  this  is  that  two  of  the  DIBs \n(Village Education and Cataract Bond) have equity built into their payment targets and there \nis also evidence of equity being considered in the design of the QEI DIB and ICRC HIB.", " This  section  addresses  the  cost  analysis  component  of  Evaluation  Question  2:  what \nimprovements can be made to the process of designing and agreeing DIBs to increase the model\u2019s  benefits  and  reduce  the  associated  transaction  costs?  This  involves  exploring  the \nfollowing sub-questions: a.  What are the costs of designing and delivering a project using a DIB model?", " o  What (if any) are the extra costs of designing and delivering a project using a DIB model and how do they compare to other funding mechanisms?", " o  Where are the most prevalent extra costs and what specific items (staff, monitoring \nprocedures etc.) have the highest costs? Are these extra costs mainly found in the \ndesign or delivery stages?", " o  Who pays for these additional costs and to what extent do they see the benefits?", " In this initial report, we look at value for money using the 4 Es framework (Economy, Efficiency, \nEffectiveness and Equity) as detailed in the Inception Report: \u2022  Under  Economy,  we  look  at  the  additional  cost  of  the  impact  bond,  on  top  of \nprogramming costs, by cost type and by stakeholder. We compare costs across the \nfour DIBs, as well as against benchmark data, and, where available, PbR costs; \u2022  For  Efficiency,  we  have  sought  evidence  of  any  positive  or  negative  changes  to efficiency as a result of the impact bond such as increased time or savings.", " \u2022 In terms of Effectiveness, the focus at this stage of the evaluation is on how effectively \nrisk has been transferred and whether the expected returns are higher than expected \nfor the risk level, as this will present limitations to the value for money delivered.", " \u2022  Finally, we have noted the approach each DIB is taking to promoting  Equity which, \nalong with the other elements of value for money, will be continued to be assessed as \nthe DIBs are implemented.", " At this early stage of the evaluation, it is too soon to draw firm conclusions for the evaluation \nquestions, so we present here thematic learnings and observations from the data in relation \nto the evaluation questions and across the 4E value for money framework.  We have started \nto build up a picture of what the additional costs of a DIB are. This is with a view to exploring \nwhether the additional costs of a DIB provide additional benefits, as well as the cost drivers to \ndeveloping DIBs. Section 6 sets out the early learning in terms of how these cost drivers can \nbe managed to reduce transaction costs. We have also started to detail the risk and return \nexpected  for  each  DIB  with  a  view  to  exploring  whether  the  amount  of  risk  transferred  is \ncommensurate  with  the  return  to  investor.  This  builds  on  the  analysis  of  risk  transfer \nundertaken in section 5.", " We have gathered data through semi-structured interviews with key stakeholders representing \noutcome funders, implementers and investors from each DIB and sourced from programme \ndocumentation.  A  limitation  of  the  data  collection  was  the  availability  of  accurate  cost \nestimates for additional cost of the impact bond.  Stakeholders described the types of costs \nand in some cases provided estimates.  Data from programme documents were also used to \nmake  estimates  of  additional  costs,  but  these  tended  to  be  financial  models  and  budget \ninformation rather than actuals.  Finally, not all stakeholders agreed to be interviewed therefore \ncosts will have been underestimated. We have used the information available to us, as far as \npossible to build up a picture of the extra costs of designing and implementing a project using \na  DIB  model,  which  types  of  costs  are  most  prevalent,  at  what  stage  (design,  set  up, implementation)  they  are  incurred  and  who  pays  for  them.  In  terms  of  external  benchmark \ndata, it was challenging to identify comparable data. We have identified benchmark data on \nreturns  to  investors,  compared  through  annualised  interest  rates.  We  have  also  drawn  on \ninformation from the PbR comparator sites.", " Findings are set out below against the 4Es framework. Additional detail on the costs of the \nimpact bond are set out in Annex L.", " Economy This section looks at the cost of the impact bond, on top of programming costs that would be \nincurred for implementing a similar programme under a different funding mechanism. Costs \nwere identified from reviewing programme financial information (budgets, financial reports and \nfinancial models) and stakeholders\u2019 interview responses. It should be noted that this exercise \nwas  done  retrospectively,  and  that  a  standardised  cost  template  was  not  used  by \nstakeholders.  Stakeholders  varied  in  terms  of  the  extent  to  which  costs  were  captured \nthroughout  the  process, with  staff time  and  pro-bono support  especially  challenging to fully \ncapture. The figures provided by the different DIBs were too partial to enable a meaningful \ncomparison.  Nonetheless,  the  figures  provide  useful  information  as  to  the  types  of  costs \nnecessary for developing and implementing a DIB.", " This section is set out as follows: \u2022  Section 5.1.1 introduces the categories of costs used \u2022  Section 5.1.2 discusses the costs incurred per DIB in more detail.", " \u2022  Section  5.1.3  then  summarises  findings  across  the  DIBs,  provides  a  preliminary \nanalysis of how these costs compare to PbR contracts, and the extent to which these \ncosts are \u2018first-time\u2019 DIB costs, or whether they can be expected for future DIBs.", " \u2022  Section 5.1.4 summarises the cost drivers across the DIBs, with further details on the cost drivers per DIB set out in Annex L.", " 5.1.1 Categories of additional costs resulting from use of the DIB Broadly, the additional costs related to an impact bond can be split into the following three \ncategories: 1.  Design and set up costs: These include additional costs required for the set-up of \nthe impact bond, including financial and legal advice and design of the impact bond.", " The  type  of  additional  costs  incurred  during  the  set-up  and  design  phase  can  be \ndescribed under three categories: \u2022  Staff  time  \u2013  this  was  provided  largely  \u2018in-kind\u2019  by  stakeholders  using  their  own \nexisting resources, unless staff time was covered by a separate grant (e.g. DFID \ntechnical assistance grant for QEI and Government of the Netherlands for ICRC).", " \u2022  External  advice  on  contract  design  and  set-up.  These  costs  were  either  funded \nthrough a grant, paid for by the lead on the impact bond or provided pro-bono by \nthe advisors, and often through a combination of the above.", " \u2022  Legal and financial advice \u2013 this was a common cost which was often provided pro- bono at least in part by professional firms 2.  Implementation costs: These include additional performance management, project \nmanagement and reporting time, on top of what would have normally been spent on a \ngrant-funded project. This also includes costs of verifying the outcome targets and, for \nsome DIBs, escrow costs. Costs can be split into the following three categories: \u2022  Contract management including performance management, project management and reporting \u2022  Verification costs manager costs \u2022  Costs related to the DIB transaction, such as escrow, legal fees and transaction As implementation is still underway, our estimate of implementation costs relating to \nthe DIB is based on expected costs, as identified through budgets and discussions \nwith stakeholders.", " 3.  Maximum payments to investor: This includes the maximum return payable to the \ninvestor,  should  the  maximum  outcome  targets  be  achieved.  This  incorporates  any \ninterest payment.", " 5.1.2 Costs for the four DIBs The tables below set out the additional costs per DIB, that is, costs that would not have been \nincurred had the intervention been funded through a grant, using the categories introduced \nabove. It is important to note that: \u2022  A significant proportion of costs were provided in-kind or pro-bono, and as such, are estimates.", " \u2022  Where we understand costs were incurred but could not be estimated, these are noted \nas \u2018Not estimated\u2019. If no costs were incurred by stakeholders for a particular activity, \nthis is noted as zero (-).", " 5.1.2.1 Additional DIB costs for ICRC HIB The total ICRC maximum committed outcome funding is CHF 26.09 million.", " Design and set up costs Outcome funder costs were estimated to be the additional time spent on setting up the DIB, \ncompared to a typical grant (based on 2 months of 2.5 FTE compared to a typical set up of \n2.5 weeks of 1 FTE) for one outcome funder. Another outcome funder was unable to estimate \nhow  much  longer,  hence  the  total  staff  time  related  to  outcome  funders  is  expected  to  be \nhigher.", " ICRC received a grant from the Government of Netherlands for a total of CHF 1.2 million, and \nestimated that they spent an additional of CHF 215k. This included CHF 699k paid to KOIS \nfor their work in developing the concept, outcome metric and due diligence work. Additionally, \nICRC extimates that the pro bono work provided by the legal firms is significantly above the \n50k initially budgeted for these tasks. ICRC considered that there was additional staff time and \npro  bono  support  which  had  not  been  quantified,  and  not  included  in  our  estimate  above, including support provided by a financial expert who joined the ICRC team between November \n2016 and August 2017, a legal firm and a legal expert.", " Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome \nFunder Investor Service \nProvider Total Design and Set up \nStaff time setting up contract, \nnegotiations, meetings feasibility \nstudy External advice on contract design \n(KOIS) \nExternal advice on legal and \nfinancial aspects of contract (pro \nbono) Implementation Costs Not \nestimated 40,500 457,739 498,239 - 698,767  \n- 698,767 - >50,000  \n- >50,000 The  expected  additional  costs related to  implementation are budgeted,  and  will  have to be \nreviewed for actual over the next research waves. Within the HIB budget, CHF 40k relates to \nverification  costs,  and  CHF  40k  to  escrow.  An  estimated  CHF  670k  relates  to  additional \nmanagement and reporting requirements, which would not have been necessary should this \nhave been a traditional grant.", " Cost by Activity (CHF) Cost by stakeholder (CHF) CHF Outcome \nFunder Investor Service \nProvider Total Implementation Performance management, \nProject management, Reporting Not \nestimated Not \nestimated 670,000  \n                     \n40,000  \n-    \n                     \n40,000  \n- 670,000  \n                      \n40,000  \n                      \n40,000 -    \n                \n- Verification Escrow Return to investors The maximum payment to investors should the maximum outcome target be met is CHF 6.4 \nmillion.", " 5.1.2.2 Additional DIB costs for QEI DIB Design and set up costs \nNo estimates were provided for the staff time in setting up contracts; however, stakeholders \nnoted that significant time was involved during the design and set up phase. Development \ntook around 2 years (June 2016 - September 2018) and involved 20-25 stakeholders in \ndifferent capacities plus external advice on specific aspects, such as legal and foreign \nexchange. Input from stakeholders included: \u2022 Internal project management time (e.g. reviewing documents, making internal \ndecisions, developing internal processes related to the project) \u2022  Attendance at cross organisation meetings to agree the model (e.g. choosing outcome metrics, pricing structures, selecting service providers) \u2022  External meetings to raise the profile of the project or engage wider stakeholders (e.g. media, learning partners, independent evaluators) \u2022  Legal contracting (involving a third party organisation and internal resource to review and complete contracts) \nInvesting in consultancy advice on specific risks (e.g. the forex risk) \u2022 \n\u2022  Setting up M&E structures and processes (e.g. specific events with service providers and selecting the performance manager) \n\u2022  Setting up outcome verification processes.", " It was not possible to estimate the time spent on these activities. The activities for which we \ndo have costs for are those where advisors were contracted, including advice on contract \ndesign and legal costs. These are set out in the table below.", " Cost by Activity (GBP) Outcome \nFunder Cost by stakeholder (GBP) \n Service \nProvider Investor GBP Total Design and Set up \nStaff time spent on setting up \ncontracts \nExternal advice on contract \ndesign (Dalberg UK) Not \nestimated Not \nestimated Not estimated - 200,000 - 200,000 Legal costs 90,707 -                          - 90,707 Implementation Costs The total budget for performance management is estimated to be GBP 646k, of which GBP \n254k is covered by DFID, and the reminder by UBSOF. Of this, GBP 55k was spent in the set \nup phase. Additional costs are expected for project management and reporting, and these will \nbe captured in the next research waves. The verification costs are expected to be USD 494k.", " Cost by Activity (GBP) Cost by stakeholder (GBP) GBP Outcome \nFunder Investor Service \nProvider Total Implementation \nPerformance management \n(Dalberg) Project management Reporting Verification (Outcomes \nEvaluation by Gray Matters \nIndia) Return to investors 254,263 392,137                        - 646,400 Not \nestimated Not \nestimated Not \nestimated Not \nestimated Not \nestimated Not \nestimated 493,570 -                        - 493,570 The maximum payment to investors should the maximum outcome target be met is GBP 596k.", " 5.1.2.3 Additional DIB costs for VE DIB The total VE DIB is USD 5.3 million (including management and evaluation costs), of which \nUSD 4.3 million represents the maximum committed outcome funding.", " Design and set up costs Stakeholders from both VE and Instiglio commented on the increased cost at the design and \nset-up stage of the DIB in the form of staff costs. VE estimated a total of 2160 hours spent on \nDIB design and structuring and Outcome Payment Agreement (OPA) negotiation and 1058 \nhours on investment fundraising and structuring. This staff time was provided in-kind.", " External advice on contract design and setting up the DIB was provided by Instiglio, funded \nby outcome funders, as well as Village Enterprise. This cost USD 86,300 and USD 169,804 \nrespectively. Legal support was provided pro-bono, and estimated to be USD 126,046 (168 \nhours) for both the OPA agreement negotiation and investments structuring/negotiation and \nspecial purpose vehicle (SPV) set up. Finally, there was a small fee for setting up the SPV. \nThe table below provides further detail.", " Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome \nFunder Investor Service \nProvider Total Design and Set up \nStaff time spent on setting up \ncontracts \nExternal advice on contract \ndesign (Design finalisation and \nstakeholder engagement ) \nConsultancy fees for setting up \nDIB (Reaching Execution \nreadiness including field trip) Legal and financial advice SPV set-up direct costs Implementation Costs Not \nestimated Not \nestimated 158,000 158,000 - - 86,300 86,300 104,804 - 65,000 169,804 Not \nestimated Not \nestimated 126,000 1,986 126,000  \n1,986 Budgeted implementation costs relating to the use of the DIB are set out in the table below. \nContract management costs cover additional grant management, financial management and \nreporting requirements relating to the use of the DIB. The verification costs excludes the USD \n70,915  costs  for  the  process  evaluation,  which  is  not  an  essential  component  of  the  DIB. \nVillage Enterprise also have their own verification process, separate from the one delivered \nby IDInsight which will incur a cost. This has not been estimated and will be revisited in the \nfollowing research waves.", " Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome \nFunder Investor Service \nProvider Total Implementation Contract management 42,311 -                  - 42,311 Cost by Activity (USD) Cost by stakeholder (USD) USD Outcome \nFunder Investor Service \nProvider Total Implementation Project management Reporting Verification (RCT and Process \nEvaluation) \nTrustee fees (including \nEscrow) Return to investors 118,585 35,958 478,162 105,300 -                  -    \n                      \n-                  - Not \n-    \nestimated  \n                      \n-                  - 118,585 35,958 478,162 105,300 The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD \n755,000.", " 5.1.2.4 Additional DIB costs for Cataract DIB The maximum committed outcome funding for the Cataract DIB is USD 3.5 million, of which \nUSD 2.8 million relate to outcome payments.", " Design and set up costs Design  and  set  up  costs  can  be  split  between  staff  time  provided  in-kind/pro-bono,  and \ncontracted time. The outcome funders, the intermediary (Volta) and legal counsel (Linklaters) \ncontributed  in-kind/pro-bono  time.  On  top  of  the  billed  hours,  Volta  Capital  provided \napproximately 25% of hours pro-bono, and Linklaters, approximately 33%.14 Outcome funders\u2019 \ncosts  were  estimated  by  assuming  1  person from  each outcome funder,  spent  1  day  each \nmonth of additional time working on the bond for 3 years.15 Additionally, outcome funders paid retainer and facility fees totalling USD 36,250 to OPIC, as \na part contribution to the cost of their due diligence.", " Cost by Activity (USD) Cost per stakeholder (USD) Design and Set-up \nStaff time spent on setting up contracts \nExternal advice on contract design \n(including Technical adviser Volta & \nLegal counsel Linklaters) Implementation Costs Outcome \nFunders Intermediary Total 100,000 66,213 166,213 255,450 - 255,450 14 This additional cost for intermediaries was calculated by taking the sum of Volta's invoiced fees during the design phase \n(USD 225,250) and multiplying by 25% to represent the additional time Volta staff spent working on the Bond that was not \nreported on or compensated. An additional USD 9,900 was estimated for the additional time from Linklaters. This was \ncalculated by multiplying their fee by 0.33 to represent their additional time above their compensated rate. \n15 The calculation assumes a rate of USD 1000 a day for a senior staffer The  table  below  sets  out  the  estimated  additional  costs  of  implementation,  compared  to  a \ntraditional grant funded project. Based on The Fred Hollow Foundation\u2019s previous experience, \nit is estimated that the additional cost of performance management, project management and \nreporting is approximately 30% of Volta\u2019s USD 175,000 fee, hence approximately USD 52,500. \nSimilarly, should a traditional grant be used, The Fred Hollow Foundation would engage an \nevaluation  consultant  to  undertake  a  mid-term  and  end  of  project  reviewer,  and  a  data \nvalidation  approach  using  spot  checks  and  internal  audit.  Hence,  the  \u2018additional\u2019  cost  of \nverification is based on an estimate of 40% of AEDES verification fee.", " Cost by Activity (USD) Cost per stakeholder (USD) Outcome \nFunder Intermediary Total Implementation \nPerformance management, \nproject management, reporting  \n(transaction manager costs \nVolta) \nVerification (AEDES) \nLoan fees  (OPIC maintenance \nfees) \nLegal fees (process agent fees - \nOPIC requirement) Return to investors 52,500  \n        64,454 30,000 1,325 -  \n- - - 52,500  \n        64,454 30,000 1,325 Finally, there is also a success fee to the hospital should it meet its targets of USD 120,000.", " The  maximum  payment  to  investors  should  the  maximum  outcome  target  be  met  is  USD \n649,333.", " 5.1.3 Summary of findings We first discuss the costs across the three phases, before comparing the costs with available \ninformation on PbR costs, and discussing findings in terms of the extent to which these costs \ncan be expected in future phases.", " The types of costs incurred, and who paid for these costs, are discussed below. It must be \nborne in mind that across several DIBs, it was acknowledged that estimates were incomplete. \nHence, it is useful to review these costs as the types and minimum level of costs required to \nlaunch and implement a DIB at this stage of the market.", " Table  5.1  presents  the  ranges  of  cost  estimates  under  these  categories  and  which \nstakeholders paid for these additional costs.", " Table 5.1: Additional DIB costs in the design, set up and implementation phases on top \nof programming costs under a grant programme Costs (including actual, budgeted, \nin-kind and pro-bono) Paid for by Cost \ncategories  \nDesign and set up \nStaff  time set \nup Where  estimated,  this  ranged  from \nUSD  150,000  to  USD  490,000.", " Generally  funded  by  organisations \n(investors, outcome funders, service Paid for by \n \nproviders)  providing  staff  time  \u2018in-\nkind\u2019,  as  well  as  advisors  and \nintermediaries  providing  pro-bono \ntime.  In  some  cases  funded  by  a \nseparate grant e.g. ICRC received a \ngrant  for  the  set  up  phase  from  the \nGovernment of Netherlands.  \nPaid  for  by  the  outcome  funder  or \nfunded  by  a  separate  grant  except \nfor QEI where Investor funded these \ncosts.  \nIn  general,  these  were  pro  bono.  \nWhere  services  were  procured \nrather  than  provided  pro  bono,  the \ncosts  were  funded  by  the  outcome \nfunder or funded by a separate grant.", " Paid  for  by  the  outcome  funder  or \nfunded by a separate grant.  In one \ncase  performance  management \ncosts  are \n(QEI)  co-funded  by \ninvestor. \nPaid  for  by  the  outcome  funder  or \nfunded by a separate grant.", " Paid  for  by  the  outcome  funder  or \nfunded by a separate grant.", " Cost \ncategories Costs (including actual, budgeted, \nin-kind and pro-bono) \nOtherwise,  stakeholders  described \nthe significant time commitment e.g. \nstaff time over two years.", " External \nadvice \ncontract \ndesign  \nLegal \nfinancial \nadvice on and Three out of the four DIBs estimated \nto be just over USD 250,000, while \none  DIB  estimated  this  to  be  USD \n687,000.  \nNot all these costs were included in \nbudgets.    Where  costs  had  been \ncaptured, these ranged from >USD \n50,000  to  USD  120,000.  However, \nin  most  cases  this  underestimated \nthe full cost as not all the pro-bono \nhours had been recorded.", " Implementation \nContract \nmanagement \ncosts These  costs  were \nin \nbudgets  and  ranged  from  between \nUSD 52,500 to USD 670,000 reflected Verification DIBs using two  with These  tended  to  be  contracts  with \nthird  parties  but  varied  in  size  with \ntwo \nvalidated \nadministration  data  having  lower \nverification costs e.g.  around USD \nlarger  costs \n50k  and \naround  USD  500-600k  (involving \nexperimental/quasi-experimental \napproaches). \nThey  types  of  costs  under  this \ncategory  varied  between  DIBs \ndepending  on  how  they have been \nset  up.  Total  costs  under \nthis \ncategory  range  from  USD  30k  to \nUSD 105k.", " Investment \nvehicle \nrelated  costs \ne.g.  Escrow \nlegal \nand \nfees \nMaximum payments to investors \nMaximum \npayments Note: Conversions done based on the exchange rate on 5 May 2019.", " 5.1.3.1  Costs Design and set up phase costs.", " These  ranged  from  USD  650k  to \nUSD 6.4m Paid for by the outcome funder.", " Based on the financial information and stakeholder interviews, the  additional costs incurred \nduring  the  Set  up  and  Design  phase  have  been  estimated.  Stakeholders  viewed  these  as \nbeing largely additional to programmes funded through grants. The stakeholders who bear the \nlargest burden of these costs tend to be the lead and driver of the bond. For example, in the case of ICRC, the majority of reported set up costs were borne by ICRC, funded in part through \na grant from the government of Netherlands. In the case of QEI, it was BAT, funded by DFID \nand UBSOF, and in the case of the Cataract DIB, the outcome funders.", " Costs  provided  are  partial,  and  it  is  difficult  to  compare  between  the  different  DIBs,  but  an \nemerging finding is that design and set up phase costs are not proportional to the size of the \nDIBs, which ranged from USD 3.5 million to CHF 26.1 million. Across all DIBs, significant staff \ntime spent on the design and set up was reported, involving thousands of hours of staff times, \nover multiple months and years. Across all DIBs, external advice was needed on design of the \nimpact bond, financial and legal advice. External advice on contract design cost around USD \n250,000 for three out of the four DIBs, and was USD 687,000 for the largest DIB. Legal and \nfinancial advice varied, but a number of DIBs reported that figures were likely to be under-\nreported, as not all pro-bono hours had been recorded.", " Implementation costs The additional costs during implementation, which include performance management, project \nmanagement  and  verification  have  also  been  estimated.  Organisations  have  estimated  the \nadditional costs involved, compared to traditional grant funded programmes.", " Contract  management  costs  attributable  to  the  DIB  ranged  between  USD  52,500  to  USD \n670,000. This will be further reviewed as part of the next research wave. Verification costs \nwere around USD 50k for the two DIBs using validated administrative data, and between USD \n500-600k for the two DIBs using experimental/quasi-experimental approaches.", " Investment  vehicle  costs  varied  depending  on  ranged  from  nothing,  to  105k.  These  costs \ndepend  on  the  contracting  mechanisms  used.  Loan,  legal  and  escrow  fees  seem  to  be \nconsistently  cost  around  USD  30,000  \u2013  40,000.  The  highest  costs  involve  fees  payable  to \ntrustees.", " Unlike the design and set up phase costs, the majority of implementation costs are included \nwithin the DIB budgets and hence covered by outcome funders, rather than provided in-kind \nor pro bono, but this will be confirmed in the next research wave.", " Maximum payments to investors The maximum payments to investors is the cost which seems to be most clearly additional \ncompared to similar programmes, and the ones which are most clearly proportional to the size \nof  the  maximum  committed  outcome  funding.  Annualised  interest  rates  provide  the  most \ncommonly used comparison of returns. As expected, the highest maximum return is for the \nICRC HIB, and the lowest, for the Cataract DIB, corresponding to the respective sizes of the \nDIBs. The cost of the payments to investors is borne mainly by outcome funders, though there \nare exceptions; in the case of underperformance in the ICRC HIB and Cataract  DIB, ICRC \nand AEF are respectively liable for some of this repayment.", " 5.1.3.1 PbR comparisons Some  of  the  additional  costs  expected  during  implementation  such  as  verification  and \nperformance management are also seen in other similar programmes, particularly with PBR \nor output-based contracts.", " Across  the  PbR  comparator  sites,  we  received  cost  information  for  two  programmes:  the \nHelvetas  livelihood  programme,  comparable  to  the  Village  Enterprise  programme,  and  the Girls  Education  Challenge  (GEC),  comparable  to  the  QEI  programme.  The  main  costs \nidentified related to verification. In the case of the Helvetas livelihoods programme, verification \nwas estimated  to  be  5%  of  total  project  costs.  In  comparison, the  VE  verification  costs  are \nexpected to be approximately 10.4% of maximum committed outcome funding, or 14.9% of \nestimated programme costs. In the GEC, the cost of evaluation for individual projects ranged \nfrom  5-20%,  averaging  15%.  In  comparison,  the  QEI  verification  costs  are  expected  to  be \napproximately 6% of maximum committed outcome funding. An important factor is likely to be \nthe  overall  size  of  the  programme  budget,  with  RCT  and  quasi-experimental  approaches \nrequiring a minimum budget and a certain amount of fixed costs that does not depend on the \nsize of the programme.", " We  were  unable  to  obtain  estimates  on  the  other  areas  of  additional  costs  related  to  PbR \nfunding.  However,  we  set  out  briefly  in  the  table  below  some  considerations  for  how  the \nadditional DIB costs are likely to compare to additional PbR costs. This will be revisited in the \nnext research waves. Furthermore, we will  work with DIB stakeholders in order to estimate \nwhich of the additional DIB costs would also have been incurred under a PbR contract.", " Table 5.2: Cost comparison between DIBs and PbR programmes Comparison between DIBs and PbR Activities  linked  to \nadditional DIB costs \nDesign and Set Up Phase \nStaff time External  advice  on \ncontract design \nLegal Costs \nImplementation Phase \nPerformance \nand \nproject management Reporting Verification Additional range and number of stakeholders involved in DIBs means that \nmore costs are expected in the set up of a DIB.  \nThe  complexity  of  DIBs  and  lack  of  standard  templates  mean  that  this  is \nmore of a feature within DIBs.", " Expected additional costs linked to both DIBs and PbR projects. However, \nexternal  performance  and  project  management  costs  are  more  common \nfeatures of DIBs, which are expected to increase costs in this area.  \nExpected that this will be a feature in both DIBs and PbR funded projects, \nthough reporting in DIBs is likely to be more extensive, given the range of \nstakeholders involved.  \nExpected costs to be similar across PbR and impact bond. However, impact \nbonds  feature  additional  stakeholders,  such  as  investors,  which  are \ninterested and tend to feed into the selection of the verification approach.", " Return to investors   Not a PbR cost.", " 5.1.3.2 First time DIB costs We  looked  at  the  extent  to  which  these  additional  costs  (during  set-up,  design  and \nimplementation) are one-off or would be incurred in future DIBs. Responses varied across the \nDIBs. For example, VE stakeholders commented that additional costs were likely to be one \noff. However, the more common opinion was, as expressed by one ICRC HIB stakeholder, \nthat \u201cSome of [these costs] obviously related to the fact that it was a first-time deal, but by far \nnot all of it.\u201d In the case of the Cataract DIB, the outcome funders considered that some of the \ncosts,  especially  staff  time  were  a  one  off  as  a  result  of  it  being  their  first  DIB,  but  that  all \ntechnical  advice  would  be  incurred  in  future  DIBs.  Further  detail  on  the  perspectives  of \ndifferent stakeholders is set out in Annex L.  In the QEI financial modelling design document \n(2016),  it  was  estimated  that  a  quarter  of  additional  costs  absorbed  by  outcome \nfunders/investors were one-off costs as opposed to ongoing.", " The  QEI  DIB  built  on  the  experiences  of  the  EducateGirls  DIB,  and  provides  interesting \ninsights into the extent to which costs can be reduced for future DIBs. A core objective of the \nQEI DIB is to provide evidence that that DIBs can be set up more efficiently, by taking learning \nforward  from  previous  projects,  like  the  Educate  Girls  DIB,  and  reusing  the  tools.  UBSOF \nstakeholders  confirmed  that  the  legal  process  was  shorter  in  the  current  DIB,  which  was \ncompleted in six months, compared to two years in the Educate Girls.  This was attributed to \nthe  increasing  efficiency  from  reusing  tools  and  applying  learning  from  this  previous \nexperience.  However,  the  number  of  stakeholders  complicated  the  contracting  process. \nAlthough  no  specific  cost  savings  were  identified,  anecdotally  stakeholders  reported  that \nactivities took less time.", " 5.1.4 Cost drivers Initial finding is that there are some fixed costs related to setting up an impact bond, related to \nlegal and contracting advice, negotiation time, etc., that are more dependent on the number \nof stakeholders, and driven by legal and financial complexity, than the pure size of the impact \nbond.  This  has  implications  for  determining  the  optimal  sizes  of  impact  bonds,  and  the \npotential for efficiencies when scaling.", " The cost drivers were identified by stakeholders to help understand which elements of the DIB \nare  the  most  time-intensive  or  expensive.  There  was  a  large  degree  of  overlap  across  the \nDIBs. All the DIBs identified legal and financial advice as a major cost driver, taking significant \nstaff time and expertise. Engaging outcome funders and raising finance from investors were \nalso identified by three out of the four DIBs. Other areas of overlap included the number of \norganisations that are involved and the negotiations, particularly being the first time, as being \ntime-intensive.  Two  DIBs  identified  the  service  provider  selection  process  as  being  time \nintensive. The table below summarises the cost drivers, which is followed by an analysis of \nthe relevance of these cost drivers for the different stakeholders.", " Table 5.3 Summary of cost drivers identified by DIB stakeholders Cost drivers Legal, \ngovernance Engaging \noutcome \nfunders Number \nof \norganisations \nto coordinate Raising \nfinance Negotiati\non \nagreeme\nnts of Service \nprovider \nselection \nprocess ICRC HIB \nQEI DIB \nVE DIB Cataract DIB X \nX \n \nX \nX X \nX X X X X X X X X Complexities around legal issues were cited across all DIBs. These affected the costs borne \nby  all  stakeholders,  in  particular  the  stakeholders  driving  the  development  of  the  DIB,  and \nstakeholders involved from the very start.", " The cost of engaging outcome funders was a cost driver, largely borne by those responsible \nfor fundraising. In the case of ICRC, this was ICRC, with support from the Belgian and Swiss \ngovernments. In the case of QEI, this was BAT, as the outcome convenor. In the case of VE, \nInstiglio and the anonymous donor led the identification of outcome funders.", " The number of organisations to coordinate and negotiate agreements was cited as a cost \ndriver. Costs in the form of staff time were largely borne by those responsible for driving the X \nX DIB. Additionally, this also drove costs for stakeholders who were engaged from the start, and \nwho were heavily engaged in negotiations.", " The costs of raising finance through the identification of investors was cited as a cost driver \nin  the  QEI  DIB,  VE  DIB  and  Cataract  DIB.  The  costs  fell  to  those  responsible  for  raising \nfinance, which were UBSOF, VE (with support from the anonymous donor and Instiglio) and \nDalberg capital, respectively.", " Finally,  the  service  provider  selection  process  was cited  as  a cost  driver  in the  VE  DIB. \nInstiglio and the anonymous donor went through an extensive selection process, in order to \nidentify a service provider with a strong track record and evidence of achievement that had \nthe capacity and motivation to learn and adapt through the process.", " Additionally, there is evidence that it may be more cost-effective for stakeholders to join later \non, provided they are willing to accept that they will be less involved in the design of the DIB \nand the contractual terms. Late joiners in both the QEI DIB and ICRC HIB noted that the time \nrequirements were not much more onerous than for a traditional grant funded project, given \nthe fact that the agreements and terms had been largely set.", " Efficiency This section looks at whether there have been any positive or negative changes to efficiency \nas a result of the impact bond.  Stakeholders were interviewed to confirm whether there \nhave been any savings in programming costs as a result of the impact bond.", " At this stage of the evaluation, no financial savings have been reported through changes in \nefficiency  in  the  intervention,  as  a  result  of  use  of  the  impact  bond  funding  mechanism. \nHowever, certain opportunities for efficiency gains during implementation were identified, and \nwill be followed up over the next two research waves. These are briefly summarised in the \ntable below: Table 5.4 Expected efficiencies DIB \nICRC HIB Expected efficiencies  \nICRC expects some efficiencies with having a 5-year grant compared with having an \nannual funding cycle.  For example, the ability to roll funds over from one year to the \nnext would save time in having to renegotiate these agreements.   \nOne of the ICRC outcome funders reported they expect savings over the course of \nthe programme in terms of management time required. \nNo opportunities for efficiency savings related to the intervention have been identified \nyet.  \nVE  has  used  the  DIB  as  an  opportunity  to  improve  its  adaptive  management \npractices, monitoring and verification processes. It is expected that this will lead to \nstronger performance management of staff, and consequently, increased efficiency. \nCataract DIB  Adaptive  management  techniques  have  been  adopted  to  support  the  reaching  of QEI DIB \n \nVE DIB targets.", " Effectiveness In  assessing  the  effectiveness  of  the  DIB  funding  mechanism,  we  are  looking  at  the \nrelationship between risk and return. This section first sets out a few relevant points from the literature  in  terms  of  understanding  risk  and  return,  a  summary  of  available  benchmarked \ninterest rates, and a discussion of the risk and return for the four DIBs.", " 5.3.1 Understanding risk and return In conventional finance, the relationship between risk and return is generally regarded to have \na positive linear relationship, at least at the portfolio level. In social finance, the relationship \nbetween financial risk and return is less well understood; this makes it more difficult to assess \nwhether the amount of risk transferred is commensurate with the risk premium paid as there \nare no standard mechanisms to calculate this.  However, there are a few useful points we can \ndraw from the literature: \u2022  The  relationship  between  financial  risk  and  return  in  social  finance  differs  from \nconventional finance in that it is not a positive linear relationship. Rather, there appears \nto be initially a positive relationship that plateaus at around 10-15% per annum. \nEven with increased financial risk, returns appear to be generally capped at this level.16 \u2022 \u2022 In social finance, the investor preferences play an important role in setting the level \nof expected return.  Social investors may expect lower rates of return than would be \ntypical in similar venture capital investments: 15-20% in developed markets and 12-\n15% in developing markets.17 It  is  important  to  separate  financial  risk  and return  from  social  risk  and  return. \nSocial  risk  is  the  \u201ccalculation  of  the  likelihood  that  an  intended  social  return  will  be \nrealized in a given investment context\u201d18 \u2022  Social risk can be understood as being largely driven by a distinctive set of variables, \nbut these can be mapped to similar categories as financial risk (probability, variance \nand uncertainty risk)19 In  terms  of  effectiveness  and  value  for  money  for  these  DIBs,  we  expect  the  level  of  risk \ninvolved, both in terms of capital protection and likelihood of achieving the outcome targets, \nto  be  in  line  with  the  agreed  return  in  terms  of  the  interest  payments.    For  example,  if  the \ninvestor expects a low return for an investment in a high-risk project or a high financial return \nfor a low risk project then this would not be regarded as an effective transfer of risk.  We are \nlooking for a positive relationship of some kind. In order to fully assess this transfer for a \nDIB, we need to look at financial and social risk and return.", " Financial risk and return: in this section, we focus on the terms of the returns to investors, \nincluding level of capital protection and annualised interest rate. This is further discussed in \nsection 4.3.1.", " Social risk and return: the outcome targets which can be understood as social return are \nsummarised in section 3. In terms of social risk, we focus on four areas: evidence base and 16 Risk and Return in Social Finance (A Nicholls, E Tomkinson 2015) \n17 Projection valuation and pricing in Social Finance paper (A Nicholls, A Paton, 2015) \n18 Risk and Return in Social Finance (A Nicholls, E Tomkinson 2015: 3) \n19 Risk and Return in Social Finance paper (A Nicholls, E Tomkinson 2015) ambitiousness  of  targets;  track  record;  project  management  systems  and  external  risk \nfactors.20 In the absence of a mature DIBs market, it is difficult to draw conclusions on how well risk has \nbeen transferred with reference to comparators. Therefore, at this stage of the evaluation, we \nhave focused on documenting the risk and return levels that are within the DIBs with a view to \nmaking comparisons between them and other impact bond models in future research waves.", " 5.3.2 Summary of available benchmarked interest rates Based on the impact bonds to date (see the table below) 30% seems to be the upper end of \nexpected financial return. At the lower end, a negative return could be expected, should targets \nnot be met.", " Table 5.5: Impact bond interest rates and capital protection Impact  Bond Deal \nSocial \nPeterborough \nThe  Benevolent  Society \nSocial Benefit Bond in Massachusetts \nJustice Pay-for-Success Juvenile Connecticut  Family  Stability \nPay-for-Success Educate Girls \nUtkrisht Impact Bond \nColombia \nImpact Bond Employability Interest Rate \n3% pa Capital Protection \nNo Equity tranche: all capital at risk \nPrincipal  protected \ntranche: \n100% capital guaranteed \nNo, but deal supported by USD \n6m \nnon-recoverable \nphilanthropic grants \nNo Protected Debt: 0-10% pa \nEquity: 0-30% pa Maximum return  \nSenior: 11% pa \nSubordinate: 18% pa \nExpected IRR base case \nSenior: 6-6.5% \nSub: 5-6% \nTargeted 10% IRR \nNo \nExpected IRR of 7%, capped 8%  No \nNo \n8% nominal return 5.3.3 Findings from the four DIBs The table below summarises the levels of risk and return across the four DIBs. All the DIBs \nhave  interest  rates  below  10%  (the  upper  end  of  return  to  investors  identified  in  the \nbenchmarking exercise), as shown in the table below.", " External  risk  factors  were  cited  in  the  ICRC  HIB  and  Cataract  DIB,  which  are  both  being \nimplemented  in  fragile  contexts.  Interestingly,  these  are  the  two  which  involve  capital \nprotection  for  investors.  Conversely,  Village  Enterprise  and  QEI  do  not  involve  any  capital \nprotection, and are also relying on experimental and quasi-experimental approaches to verify \noutcomes.", " The levels of risk and return vary significantly across the four DIBs. The Village Enterprise has \nthe  highest  (potential)  return,  in  terms  of  annualised  interest  rate.  However,  the  return  is \nexpected to be below that. This is commensurate with the levels of social and financial risk, in \nterms of achieving this level of outcomes at this scale. The Cataract DIB has the next highest \nreturn of 8% and has 100% capital protection. Targets were considered to be reasonable, but \nit is involving delivering the intervention in an untested context.  The ICRC HIB has the lowest \nrate of maximum return, though has 60% capital protection, with targets that have been set so 20 The relevance of these factors when identifying suitable interventions for funding through the DIB mechanism \nis discussed in section 6.1.", " they  are  ambitious  yet  not  infeasible.  QEI  also  has  a  low  rate  of  return  but  has  no  capital \nprotection however targets were deemed to be reasonable. Across all DIBs, there is a strong \nevidence  base.  However,  in  the  case  of  the  Cataract  DIB,  there  is  limited  evidence  in  the \nCameroon  context,  and  some  evidence  of  real  challenges  translating  this  model  to  sub-\nSaharan Arica. In the case of the VE, there is limited evidence on the delivery at this scale.", " Therefore, it would appear that there is a generally positive relationship between the levels of \nfinancial and social risk, and levels of return.", " Table 5.6: DIBs' risk and return DIB ICRC \nHIB Financial risk and return Capital \nprotection Interest rate on \ninvestment 60% Social risk  \nProject risk 21 Financial return range \ngoing from negative to \npositive (min -11.3% to 7% \np.a.) 1.  Evidence base is strong. Targets set to be ambitious yet realistic.  \nICRC have reputation on of implementing 2. \n3.  Project management in place \n4.  High external risk factors \nMed-High risk (DFID Risk score 5) QEI DIB  None 1.  Evidence base is strong. Targets set to be not too return -100% ranging \nFinancial \nfrom \n(no  capital \nguarantee)  to  8%  interest \np.a.  (capped  at  USD  0.74 \nmillion).", " Financial \nranging \nreturn \nfrom -100% to maximum of \n9.9%  IRR,  depending  on \nfinal number of households \nreached \nVE \nperformance.", " and VE DIB  None Cataract \nDIB 100% 8% p.a. if performance \ntargets met; 4% p.a. if not \nmet (OPIC); 0% p.a. if \ntargets not met (Netri).", " 2.", " ambitious.  \nImplementers have track record of achieving \noutcomes 3.  Project and performance management in place \n4.  External risk factors not especially high \nMed-High risk (DFID Risk score 5) 1.  Strong  evidence  base  for  the  intervention  with \nextensive  historical  data  from  a  previous  RCT \nconducted. Ambitious targets set.", " 2.  Village  Enterprise  have  an  organisational  track record as service provider 3.  Project governance arrangements are in place \n4.  External risk factors not especially high \nHigh risk (DFID Risk score 6.15) 1.  Good evidence base, though not specifically in Cameroon or Africa. Using tested Aravind model, \nalthough there have been previous challenges \nmaking this model financially viable in Africa.", " 2.  Provider with strong track record capable of delivering the targets 3.  Performance management arrangements in place \n4.  External risk factors high \nNo DFID risk score available Equity This  element  of the  value for  money  assessment  will  be  the focus  of  subsequent research \nwaves. For this research wave, we have explored two components of the interventions, with \nimplications for equity. Firstly, we describe the targeting strategy of each DIB so that this can \nform the basis of assessments in later research waves as to the extent to which the targeting \nstrategy  has  been  effective  in  promoting  equity.  Secondly,  we  discuss  the  design  of  the \noutcome metric and verification process, and implications for equity. This will be followed up \nover the next two research waves.", " 5.4.1 Targeting strategy The approach to equity will be guided by the individual programmes\u2019 targeting strategies, to \nunderstand  the  narrative  around  the  target  population.  We  will  seek  to  understand  the \neffectiveness of the targeting strategy of the DIB, especially in terms of the hard to reach. The \nevaluation  will  look  at  how  well  the  programmes  are  fulfilling  their  targeting  strategy  and \nwhether there are certain sub-groups which are not being reached. At this stage, the following \nstrategies are understood to be in place: \u2022 ICRC \u2013 The HIB targets disabled people in a geographic area.", " \u2022  QEI \u2013 Poor schools were selected to participate in the programme.", " \u2022  VE \u2013 The intervention identifies individuals who live in extreme poverty and are unable \nto  provide  for  their  family\u2019s  basic  needs.  VE  assesses  poverty  levels  through  a \ncommunity-based Poverty Wealth Ranking exercise coupled with the Progress-out-of-\nPoverty  Index.  The  targeting  methodology  is  set  out  in  the  Outcomes  Payment \nAgreement  and  4%  of  direct  expenses  are  budgeted  for  targeting  according  to  the \nfinancial model.", " \u2022  Cataract DIB \u2013 The financial model works through cross sub-subsidisation, and \nthere is a specific equity target, to provide 40 percent of surgeries to individuals \nbelonging to the bottom two wealth quintiles of the population in Cameroon.", " 5.4.2 Outcome metric and verification strategy The use of an outcome metric has implications for equity. There is a danger of cherry picking \ncertain  sub-populations.  This  sub-section  considers  how  the  outcome  metric,  verification \nstrategy  and  reporting  have  been  designed  with  equity  in  mind.  All  DIBs  have  considered \nissues  related  to  equity  during  the  design  process,  and  introduced  mechanisms  to  monitor \nhow well equity issues are being addressed. This will be reviewed over the new two research \nwaves.", " ICRC  HIB:  The  outcome  metric  is  based  on  people  benefitting  from  physical  rehabilitation \nservices.  However,  the  M&E  data  will  include  disaggregated  data  on  gender  and  age. \nFurthermore, a key limitation with the verification process is that it is limited to those within \nurban  areas,  accessible  by  the  verification  firm.  Over  the  next  two  research  waves,  the \nevaluation team will assess the extent to which targeting may have been affected by use of \nthe  HIB,  and  the  extent  to  which  the  verification  process  will  be  affected  by  the  limited \ngeographical coverage.", " QEI: The learning results measurement uses \u2018distance travelled\u2019 rather than level attained, so \nservice providers are rewarded based on the progress from baseline, rather than reaching a \nspecific  target  on  the  test.  This  avoids  cherry-picking  high  performing  students  who  could \nachieve  a  high  level  easily.  However,  the  lack  of  specific  targets  related  to  equity  led  one \noutcome  funder  to  push  for  the  wider  monitoring  and  reporting  architecture  to  be  showing \nenrolments, and dropouts, as well as performance by gender and grade start/end of grade, to \nenable  tracking  of  potential  concerns  of  cherry  picking  or  negative  behaviours.  Similarly, \nanother stakeholder proposed disaggregating the distributional attainment levels per grade at \nbaseline  and  endline  as  part  of  monitoring,  again  to  see  trends  of  cherry  picking  /  equity \nconcerns. The evaluation team will review this as part of the next two research waves.", " VE:  The  targeting  strategy  addresses  equity  concerns,  and  at  the  moment,  there  are  no \nparticular  risks  identified  with  the  outcome  target  or  verification  process  potentially  driving \nperverse incentives. This will be monitored over the next two research waves.", " Cataract DIB: There is a particular equity target that will be reported on over the course of the \nintervention.  The  evaluation  team  will  review  the  process  of  assessing  the  equity  target  in \nmore detail over the next two research waves, and any potential differences in terms of the \nquality of the services received between paying and non-paying patients.", " Conclusion The costs which are most clearly additional are those related to the return to investors. Other \nadditional costs were harder to estimate, given the significant proportion of in-kind and pro-\nbono costs. Stakeholders identified that some of the design and set-up costs, were unique to \nDIBs (e.g. contracts requiring legal and financial consultancy), but that others are commonly \nseen in other similar programmes, particularly with a PBR or output-based contract (such as \nongoing  costs  of  performance  management,  project  management  and  verification). \nStakeholders expected some of the DIBs costs would reduce for future DIBs.", " The  size  and  types  of  costs  incurred  seems  broadly  similar  across  the  four  DIBs,  and  not \nnecessarily  proportional  to  the  size  of  the  DIB.  Cost  drivers  involved  a  variety  of  factors, \nunlinked to the size of the DIB. This adds weight to the view held by some stakeholders that \nthere  is  a  certain  level  of  fixed  costs  involved  in  setting  up  an  impact  bond,  which  means \ntransaction costs will be proportionately lower in the case of larger impact bonds.", " There was a large degree of overlap across the DIBs, in terms of the main cost drivers.  All \nthe DIBs identified legal and financial advice as a major cost driver taking significant staff time \nand  expertise.  Engaging  outcome  funders  and  raising  finance  from  investors  were  also \nidentified by three out of the four DIBs.", " A number of stakeholders highlighted the need to put transaction costs into perspective, given \nimpact bonds are still at an early market stage. One investor noted that in the financial industry, \na new instrument is always complex to design and expensive to set up. However, the initial \ninvestment can be leveraged thereafter by launching others. This will be considered over the \nfollowing two research waves.", " The  focus  for  this  first  research  wave  has  been  on  the  costs  incurred  in  a  DIB,  and  in \ncomparison to grant based financing, and payment by results. The focus of the next research waves will be to identify actual costs incurred during implementation, and to review the benefits \narising  from  use  of  a  DIB  (in  comparison  to  grant  based  financing  and payment  by  results \ncontracts).", " Finally,  a  key  challenge  of  this  phase  has  been  retrospectively  calculating  and  comparing \ncosts,  without  the  use  of  a  standardized  cost  template,  and  across  a  wide  range  of \nstakeholders. Going forward, we will continue to work with stakeholders to collect costs related \nto using a DIB, to add to the knowledge base of the true and complete costs of using a DIB.", " 6.0   Analysis  and  Findings  \u2013  Improving  the process of designing and agreeing DIBs Summary Identifying appropriate interventions \u2022  DIBs require clear and suitable outcomes and a shared understanding of the policy problem. \nAdditionally, there are other practical constraints on the type of interventions suitable, such \nas the timeframe of the impact bond and the level of external risk factors.", " \u2022  Transaction costs are lower if the DIB design is able to draw on existing evidence, but this may limit the expansion of the DIB into new and innovative sectors.", " \u2022  The benefits of using the DIB model are strongest when there is a value proposition to the use of the DIB, whereby the DIB is resolving a specific challenge that cannot be \naddressed by other funding mechanisms.", " Identifying metrics and structuring payments \u2022  Building  a  database  on  interest  rates,  outcome  metrics  and  rate  cards  and  drawing  on \nprivate sector expertise on pricing risk would facilitate the growing of the DIBs market.", " \u2022 It  is  important  to  developing  outcome  metrics  and  rate  cards  that  are  understood  by  all \nstakeholders and linked to other metrics within the sector/country.", " Measuring impact The  validation  process  should  be  designed  to  balance  costs  with  the  evidence  requirements  of \nstakeholders. Experimental and quasi-experimental methods are significantly more expensive than \nvalidation of administrative data, and may not be necessary in all cases to measure impact.", " Identifying and selecting stakeholders and managing relationships \u2022  Transaction  costs  for  the  design  and  set  up  stage  can  be  reduced  when  there  is  strong \ncollaboration  across  stakeholders,  drawing  on  each  other\u2019s  expertise  and  strength  and \nclearly defining roles from the start.", " \u2022  The types of investors, and the way outcome funders and investors are engaged in the DIB, have implications for the types of benefits that can be expected out of the model.", " Structuring and developing the operating model \u2022  The larger number of stakeholders involved in the DIBs to date, and the often diverse legislative frameworks, increase the transaction costs of this stage of the DIB \ndevelopment. The optimal solution would be to amend the legislative frameworks to \naccommodate DIBs. Where this is not possible, other potential solutions include: limiting the number of stakeholders involved; o \no  using pooled funding structures; \no  using other ways to minimise the number of contracts involved; and \no  standardising deals.", " This section addresses the other components of Evaluation Question 2: what improvements \ncan be made to the process of designing and agreeing DIBs to increase the model\u2019s benefits \nand reduce the associated transaction costs?", " This question involves identifying lessons learned from the four DIBs and exploring how they \ncould be applied to future DIBs to improve their design and set-up. As set out in the evaluation \nframework in section 2, this involves exploring the following sub-questions: a.  Under  what  conditions  (such  as  project  and  stakeholder  attributes)  are  DIBs  an appropriate tool for key stakeholders and why?", " o In what circumstances are DIBs relevant in tackling issues in the development \ncontext?", " o  Are DIBs appropriate in development contexts  - is the existence of investors \n(and  possible  profits),  payment  only  when  results  are  made  and  strong \nexpectations  around  measuring  outcomes  appropriate  for  donors  such  as \nDFID?", " o  What social issues, target groups, geographies and project scales do DIBs fit best and have the greatest impact?", " b.  How  can  the  process  of  designing  and  agreeing  DIBs  be  improved  to  reduce  the associated transaction costs?", " o  Are there any inefficiencies in a DIB model that can be reduced or are there any additional costs that are unnecessary?", " c.  How  can  the  process  of  designing  and  agreeing  DIBs  be  improved  to  increase  the model\u2019s benefits?", " In order to frame our analysis and findings against all three questions, we draw on Gustafsson-\nWright et al\u2019s (2017) framing of the key issue areas in the design of impact bonds, which we \nhave reconfigured slightly to fit the DIBs under the scope of the evaluation. These are: 1.  Identifying appropriate interventions  \n2.  Identifying metrics and structuring payments \n3.  Measuring Impact \n4.  Identifying and selecting stakeholders and managing relationships \n5.  Structuring the vehicle and developing the operating model The remainder of this section is structured around these five development areas, and for each \ndevelopment area we explore the three relevant research questions listed above (conditions \nwhere  DIBs  are  most  appropriate;  improving  their  design;  and  reducing  their  transaction \ncosts).", " This analysis draws primarily on consultations with stakeholders involved in the four projects. \nThe section also considers how lessons learned compare with other impact bonds; this draws \non  consultations  with  wider  stakeholders  and  the  literature  review  undertaken  during  the \nscoping stage.", " The final sub-section then summarises the findings against the first sub-questions under EQ2, \nin terms of conditions required for a DIB to be a suitable tool for stakeholders. Findings against \nthe second sub-question under EQ2, in terms of lessons learned around how the DIB design \nprocess can be improved, are summarised in section 7.", " Identifying appropriate interventions This  sub-section  explores  the  necessary  considerations  when  selecting  appropriate \ninterventions to be funded through a DIB, and lessons learned on ways to reduce transaction \ncosts and increase the model\u2019s benefits at this stage.", " 6.1.1 Identifying appropriate interventions - necessary conditions Summary: DIBs require clear and suitable outcomes and a shared understanding of the policy \nproblem. Additionally, there are other practical constraints on the type of interventions suitable, \nsuch as the timeframe of the impact bond and the level of external risk factors.", " 6.1.1.1 Analysis from four projects Across  all  four  DIBs  under  the  evaluation,  the  following  factors  were  critical  in  selecting \nappropriate interventions that were deemed appropriate to be funded through a DIB: \u2022  Consensus  on the  policy  problem, target  outcomes  and  appropriate \napproaches: In all four of the DIBs there was a consensus amongst stakeholders on \nthe policy problem, target outcomes and appropriate approaches, and this consensus \nwas critical in launching the DIB. For example, In the case of the Cataract Bond, one \ninvestor commented that the eye sector was especially suitable, as there is agreement \non  approaches  and  measurement  of  prevented  blindness,  which  is  not  the  case  in \nother  sectors  of  health.  Similarly,  the  QEI  DIB  stakeholders  had  a  shared \nunderstanding  of  the  relevance  of  targeting  learning  outcomes,  as  set  out  in  the \nSustainable Development Goals (SDGs), and were able to draw on a rich evidence \nbase. Finally, the ICRC HIB and VE DIB are both funding existing programmes in areas \nwhere  there  is  relatively  strong  consensus  on  the  target  outcomes  and  appropriate \napproaches.  A  number  of  the  outcome  funders  for  these  DIBs  are  already  funding \nsimilar programmes.", " \u2022  Sufficient  evidence  base  for  the  proposed intervention:  Stakeholders  across  all \nfour DIBs noted the importance of having sufficient evidence for the effectiveness of \nthe  intervention.  In  all  four  DIBs  the  interventions  had  a  strong  evidence  base  that \noffered  reassurances  to  investors  and  also  enabled  stakeholders  to  build  strong \nbusiness cases. The evidence base was less strong in the Cataract Bond (as there is \nlimited evidence to demonstrate the effectiveness of the intervention in Sub-Saharan \nAfrica),  but  stakeholders  were  comfortable  that  there  was  sufficient  evidence  from \nprevious interventions delivered by the service providers, in other contexts.", " Investors in the ICRC HIB and QEI DIB also commented that they undertook significant \nwork reviewing the available evidence for the hypothesised causal mechanisms within \nthe interventions\u2019 theories of change. The rationale was that investors wanted to gain \nconfidence that the proposed intervention would lead to the target outcomes. It is noted \nthat  in  these  two  DIBs,  the  investors  were  commercial  investors,  and  the  UBSOF \nrespectively.  As  the  level  of  returns  to  investors  depends  on  the  achievement  of \noutcomes, evidence for the intervention having led to the target outcomes would lower \nthe risk for the investor to invest in the intervention. This was important as they were \ntaking on the risk of achieving outcomes.", " \u2022  Clear and measurable outcomes: The interventions\u2019 target outcomes also needed to \nbe clear and measurable, in order to enable the development of outcome metrics (see \nSection 6.2.2 for further discussion on identifying metrics and structuring payments) \u2022  Feasible  timeframe  for  achieving  outcomes:  The  target  outcomes  of  the \nintervention need to be feasible within the duration of the impact bond. Stakeholders \nin the QEI DIB thought that the education sector was especially appropriate, given the \nfact  that  outcomes  are  attached  to  the  academic  year  and  possible  in  a  shorter \ntimeframe, which enables outcome targets to be based on actual outcomes instead of \nproxies. Similarly, ICRC commented that certain centres within its programmes would \nnot have been suitable, given the difficulty of operationalising the centres within five \nyears. Finally, the VE DIB and Cataract Bond outcomes were tailored to the duration \nof the impact bond.", " \u2022  Acceptable level of external risk: The level of external risk is particularly relevant \nin humanitarian contexts. There is a practical incentive to identify interventions that are \nnot  subject  to  too  high  a  level  of  external  risk,  despite  the  possibility  of  using  force \nmajeure clauses. For example, in the case of the ICRC HIB, certain locations, such as \nAfghanistan, were determined to be too risky to be practical for HIB funding.", " \u2022  Sector with strong service providers: A sector with strong providers was cited by \na  QEI  stakeholder  as  a  key  requirement  for  the  use  of  a  DIB,  which  facilitated  the \ncompetitive  process  used  to  identify  service  providers.  In  contrast,  stakeholders \ninterviewed from the wider sector described how some impact bond projects have not \nprogressed because of the absence of a strong service provider market in the relevant \ncountry.", " 6.1.1.2  Comparison with other impact bonds Based  on  research  undertaken  in  the  UK  SIB  market,  the  LOUD  model22  sets  out  the  four \nfactors that determine whether a social impact bond is launched: In terms of the identification \nof appropriate interventions, two of the factors are particularly relevant: \u2022  Clear Outcomes: outcomes which are clear and attributable to the intervention, which \nthe  funder  considers  worth  paying  for,  and  the  providers  and  investors  believe  is \nachievable.", " \u2022  Shared understanding: all parties have a shared understanding about how the policy \nproblem  can  be  addressed,  and  the  proposed  intervention  is  evidence  based  or \ncredible.", " Our DIB level research finds strong overlap between the necessary conditions for identifying \nappropriate interventions and to launching SIBs in the UK. Furthermore, other impact bonds \nin  the  development  sector  have  launched  under  similar  conditions  as  those  identified  here \n(certain  sectors  with  a  stronger  evidence  base,  more  consensus  on  appropriate  solutions) \n(Gustafsson-Wright and Gardiner 2015), and other impact bonds have failed to launch where 22 https://golab.bsg.ox.ac.uk/knowledge/resources/loud-sib-model-four-factors-determine-whether-social-impact-\nbond-launched/ there was a lack of shared understanding and agreement on outcomes. This suggests these \nare necessary conditions are somewhat general to impact bonds more generally.", " 6.1.2 Identifying appropriate interventions - reducing transaction costs Summary: Transaction costs are lower if the DIB design is able to draw on existing evidence, \nbut this may limit the expansion of DIBs into new and innovative sectors. Setting up this phase \nso that all stakeholders have a stake in the successful launch of the DIB may align incentives, \nwhich can then increase the efficiency of the design and set up of the DIB.", " 6.1.2.1 Analysis from four projects Two emergent findings on the potential ways to reduce transaction costs during this stage are \nset out below: Transaction costs can be reduced for this stage of the process if there is existing data \nalready available. Across the four DIBs, there was limited additional data gathered during the \ndesign phase; rather, the availability of relevant data guided the selection of the interventions: \u2022  The Cataract Bond was able to draw on data from where The Magrabi Foundation had delivered the model elsewhere \u2022  The QEI DIB was able to draw on the strong track record of the three service providers, and on data on the education sector in India collected by GMI and MSDF \u2022  The VE DIB was able to draw on data from a previous RCT \u2022 ICRC HIB was able to draw on its extensive PRP data (though it did need to use the \ndata in a significantly different way).", " Careful consideration of the alignment of financial incentives during the set up phase \ncan  potentially  reduce  transaction  costs.  Stakeholders  across  the  DIBs  used  a \ncombination of grants for this development stage, as well as significant upfront investment of \ntheir  own  time23.  The  grants  received  were  cited  by  recipients  in  the  QEI  and  VE  DIBs  as \ncrucial to this stage. However, this diminishes the sustainability of the impact bond sector and \nthe value of using impact bonds. Furthermore, actors covering upfront costs may mean they \nare  more  invested  in  the  process  and  success  of  the  launch.  One  suggestion  raised  for \nincreasing  the  efficiency  of  this  phase  in  the  ICRC  HIB  was  to  ensure  the  DIB \ndesigners/intermediaries had a direct financial stake in the successful launch of the DIB. A \nfew stakeholders within the ICRC HIB thought that had this been the case, this might have \ncontributed to an increased focus on creating a workable DIB model.", " 6.1.2.2 Comparison with other impact bonds Other research identifies that it can be expensive to create the data needed to develop a DIB \nwhen it is not already available (Oroxom et al 2018), supporting the notion that available data \ncan reduce transaction costs.", " 23This data, where available, has been included in the estimates of the cost of the design and set up phase, see \nsection 5.", " 6.1.3 Identifying appropriate interventions - increasing the model\u2019s benefits Summary:  The  benefits  of  using  the  DIB  model  are  the  strongest  when  there  is  a  value \nproposition  to  the  use  of  the  DIB,  whereby  they  resolve  a  specific  challenge  that  cannot  be \nBox 1: Value propositions of the \nfour DIBs \naddressed by other funding mechanisms.", " 6.1.3.1 Analysis from four projects Several stakeholders across the four DIBs commented that the DIB model is best suited to \nspecific  contexts,  where  the  DIB  is  able  to  address \nspecific  challenges  that  are  not  resolved  by  other \nfunding  mechanisms,  and/or  to  meet  conditions  that \nare not met by other funding mechanisms. The box to \nthe right sets out the value propositions of the four DIBs, in \nterms of the core benefit of using the DIB, in comparison to \nother funding mechanisms.", " The  ICRC  HIB  enables  a  risk \ntransfer  to  investors  for  the  testing \nof \nimprovement \nmeasures  and  the  digital  centre \nmanagement system efficiency A potential limitation with the four DIBs under the scope of \nthe evaluation was that they were all designed \u2018DIB first\u2019; \nthere  was  first  an  interest  in  testing  the  DIB  mechanism, \nand then a suitable project was identified. This may mean \nthat  not  all  options  are  considered,  and  the  impact  bond \nmay  not  necessarily  be  the  most  appropriate  tool.  For \nexample, in the ICRC HIB, it was first decided to use a HIB \nfor  the  PRP  programme,  before  exploring  whether  there \nwere outcome funders interested in funding the PRP on an \noutcome basis. This was cited as a key lesson learned by \nICRC  stakeholders.  Similarly,  certain  stakeholders  of  the \nCataract  Bond  thought  that  it  may  have  been  simpler  to \nachieve the same outcomes with a grant. Whether the risk \nlevel  in  terms  of  the  intervention  and  ambitiousness  of \ntargets justified using a DIB was debated by a number of \nstakeholders in these two DIBs.", " 6.1.3.2  Comparison with other impact bonds The VE DIB enables the funding to \nexpand  and  scale  up  the  existing \nprogramme,  and  enable  room  to \ntest  and  adapt  different  modalities \nof giving micro-enterprise grants of improving The QEI DIB enables the scaling up \nand \nsuccessful \nprogrammes.  As  the  DIB  focuses \non  outcomes  and  not  specific \nservice providers or interventions, it \nto \nenabled  a  market  approach \nservice \nidentifying \nproviders  Thought why not PbR?", " appropriate The Cataract Bond enables \ntesting of an intervention in a risky \ncontext.", " In terms  of  identifying  interventions  that  enable  the  model\u2019s  outcomes to  be  optimised, the \nadvice is consistent that DIBs are best suited where there is market failure. Market failure can \nbe understood as situations where the value of a particular intervention exceeds the cost, yet \ndue  to  various factors,  these  interventions  are  not  being  delivered  (Gustafsson-Wright  and \nGardiner, 2015; USAID). Interviews with wider stakeholders and evidence from the literature \nsuggests  that  DIBs  are  most  valuable  when  there  is  a  need  for  more  collaboration  and \nflexibility than would be achieved under alternative funding mechanisms, or where there is a \nneed for risk transfer that is too great to be borne by service providers: \u2022  Collaboration:  For  example,  the  value  proposition  for  an  impact  bond  under \ndevelopment  in  a  middle-income  country  was  that  the  specific  social  area \n(employment) required good levels of collaboration between skills training providers, \nemployers  and the  government  that  was  not  incentivised  or  apparent  in the  current funding model. An impact bond is being developed as there is reasonable evidence to \nsuggest that impact bonds align incentives and encourage collaboration.", " \u2022  Flexibility: For example, the value proposition for a DIB in a low-income country was \nthat the intervention required high levels of flexibility to adapt to different beneficiary \ncharacteristics. There was a view amongst stakeholders that impact bonds incentivise \nflexibility and adaptation more strongly than alternative funding mechanisms.", " \u2022  Need for risk transfer too great to be borne by service providers.", " Identifying metrics and structuring payments This sub-section explores the process of identifying metrics and structuring payments, in terms \nof  the  necessary  conditions  and  lessons  learned  around  reducing  transaction  costs  and \nincreasing the model\u2019s benefits.", " 6.2.1 Identifying metrics and structuring payments - necessary conditions Summary: DIBs require substantial data to enable the development of outcome metrics and \npricing  of  the  risk.  Investor  motivations  and  risk  appetites  will  affect  the  level  of  robustness \nneeded from the data.", " 6.2.1.1 Analysis from four projects Identifying metrics In identifying metrics, data from previous interventions was cited as a key enabler by a range \nof stakeholders across the DIBs. The four DIBs were able to draw on similar sources of data \nused to identify the intervention. These sources of data, as set out in section 6.1.2.1, were \nused to identify feasible and relevant metrics.", " Structuring payments The availability of historical performance data on the targeted outcomes is a key enabler to \nstructuring payments and investor returns. For example,  a key learning from the ICRC HIB \nwas  that  metrics  need  to  be  aligned  with  historical  data  to  enable  an  assessment  of  the \nambitiousness and risk of the  outcome  targets. This  enabled  the  investor  to  use  insurance \nmodels in order to price the risk of the outcome targets.24 The commercial investors noted that \nthis was crucial to enable their participation.", " Conversely, such activities can be challenging when this data does not exist. For example, \nthe Cataract Bond did not have historical data on similar interventions within Cameroon, and \ndrew on interventions delivered in other countries. This was cited as a key barrier and limitation \nwhich deterred certain investors.", " 6.2.1.2 Comparison with other impact bonds 24 Insurance models involve the calculation of risk (based on the level of risk and volatility of outcomes based on \npast data), and the calculation of an acceptable return (which includes interest rate and percentage of capital at \nrisk). Munich Re treated its investment in the ICRC HIB as an insurance product, with a potential risk premium / \ninvestor return. The main difference was that the payment terms were reversed, with Munich Re making payments \nin advance.", " The evaluation findings on the importance of substantial data about the target population and \nintended outcomes echoes again the findings of the LOUD model25, which identifies data as \nbeing  critical  to successfully  launching  SIBs.  Some  stakeholders  interviewed  as  part  of our \nsector  level  interviews  also  noted  the  challenge  of  creating  a  business  case  for  DIBs  in \ncontexts where there was limited data, and this reflects what has been said in the wider impact \nbond  literature  to  date  (Gustafsson-Wright  et  al.,  2015;  Gustafsson-Wright  and  Gardiner, \n2016).", " 6.2.2 Identifying metrics and structuring payments - reducing transaction costs 6.2.2.1 Analysis from four projects Summary: Building a database on interest rates, outcome metrics and rate cards and drawing \non private sector expertise on pricing risk would facilitate the growing of the impact bond market. \nExtensive  modification  to  the  DIB  structure  can  be  a  barrier  to  scaling  up  DIBs  based  on \nstandardised templates.", " Identifying metrics A few stakeholders across the DIB suggested that developing templates for outcome metrics \nand  rate  cards  could  reduce  the  transaction  costs  of  setting  up  outcome  metrics.  This  is \nparticularly true in the case of DIBs in the same sector, and the QEI DIB was able to build on \nthe  work  done  in  the  Educate  Girls  DIB.  Both  the  QEI  and  VE  DIBs  have  the  ambition  to \ngenerate  lessons  and  grow  the  DIB  market  in  their  respective  sector,  and  the  metrics  are \npriced  per  outcome,  which facilitates  transferring  the  outcome  metric  to  other  interventions \nand providers.", " Costs of delivery Costing  of  the  intervention  can  be  done  more  efficiently  and  accurately  when  there  is \ntransparency between the outcome funder and provider, and when the outcome funder is able \nto  draw  on  benchmark  data.  Competitively  tendered  processes  can  build  in  competitive \npressure to the pricing. This was in the case of the QEI DIB, where a competitive selection \nprocess  was  used to  select  the  three  service  providers.  Similarly,  a  selection  process  was \nused  to  select  VE.  On  the  other  hand,  the  Cataract  Bond  and  ICRC  HIB  were  eventually \ndesigned with a specific service provider in mind, and the costing of the intervention drew on \nexisting  interventions  delivered  by  the  service  providers.  One  bilateral  donor  and  outcome \nfunder noted that increased use of a more competitive bidding process can reduce information \nasymmetry  between  outcome  funders  and  service  providers,  as  the  process  will  enable \ncomparison of pricing across a number of actors.", " Determining outcome metrics, outcome payments and return to investors In terms of development of the payment structure, transaction costs can be reduced when: \u2022  The development of outcome payments and the pricing of the risk is able to build on existing benchmarks and models used in other sectors.", " 25 https://golab.bsg.ox.ac.uk/knowledge/resources/loud-sib-model-four-factors-determine-whether-social-impact-\nbond-launched/ Limited market information available for setting the pricing and level of return to investors \nextended the time and resources necessary for this stage. A key learning from the Cataract \nBond was that basing the terms of the agreement on a better understanding of the market, \nsuch as what level of risk is investible and pricing the risk accordingly, would have been \nhelpful. Both an outcome funder and an early investor suggested that a better pricing of \nthe risk would have probably prevented the DIB from being launched with terms that ended \nup being modified several times (See Annex A case studies for further details). Similarly, \nstakeholders  within  the  ICRC  HIB  noted  the  challenge  of  finding  interest  rates  or \nguarantees  for  comparable  investments,  even  for  intermediaries  with  special  financial \nlicenses, due to the limited benchmark data available.", " Two potential solutions were raised by stakeholders involved in the four DIBs: i.  More transparency on the returns to investors and interest rates used to date \nwould be useful for the development of new DIBs, where relevant benchmark data \nexists. This was a limitation cited by the majority of stakeholders across the four \nDIBs.", " ii.  One  of  the  private  investors  commented  that  they  had  priced  the  risk  by  using \ninternal  insurance  models  on  the  historical  data.  Interest  was  expressed  by \noutcome funders and foundation investors in terms of drawing on private sector \nexpertise  in  pricing  risk,  where  limited  benchmarks  exist  in  the  development \nsector. There may be scope to capitalise on private sector expertise in this area, \nparticularly if the intention is to attract more commercial investors.", " \u2022  Use of tailored risk and return arrangements that best balance the preferences and needs of the different stakeholders.", " Across  the  four  DIBs,  innovative  methods  were  used  to  balance  the  risk  and  return \nbetween stakeholders. These included: \u2022  Arrangements  to  share  risk  with  service  providers,  by  providing  them  with  a \npotential upside or downside in the case of delivery or non-delivery can be seen as a \nway of reducing the risk of non-delivery.", " \u2022  Force majeure clauses also enable investors to terminate the contract in the event of non-delivery.", " \u2022  Financial risk is reduced for investors, through the use of capital protection, coupon payments and earlier repayments to investors.", " \u2022  The development of different risk and return profiles for different investors can \nmean that these are better tailored to investors\u2019 needs and objectives, and make the \ninvestor returns more efficient for the impact bond as a whole.", " 6.2.2.2  Comparison with other impact bonds Stakeholders  interviewed  who  are  involved  in  the  wider  impact  bond  sector  echoed  the \nargument that standardised templates could help reduce transaction costs. However, one key \nstakeholder also argued that outcome design and pricing still involves a substantial degree of \ncustomisation, depending on the specific context, partners involved, intervention and related \nrisk and requirements. Therefore the extent to which standardisation can apply is limited, and \ntemplates  will  always  need  to  be  somewhat  bespoke.  For  example,  the  debate  around \nstandard rate cards is still ongoing in the UK SIB market, a much more mature impact bond market, which suggests it may take some time before this will materialise in the more nascent \nuse of impact bonds in middle income and developing countries.", " 6.2.3 Identifying metrics and structuring payments - increasing the model\u2019s \nbenefits Summary:  Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome \nfunders,  and  correspondingly,  incentives,  are  aligned.  Developing  outcome  metrics  and  rate \ncards  that  are  understood  by  all  stakeholders  and  linked  to  other  metrics  within  the \nsector/country can increase the value of the learning generated and also facilitate the broader \nDIB market and/or potential transition to a SIB.", " .", " 6.2.3.1 Analysis from four projects Identifying metrics The model\u2019s benefits can be increased by developing an outcome metric that is understood \nby all stakeholders and linked to other metrics within the organisation/sector/country. This can \nincrease the value of the learning generated and also facilitate the broader DIB market and/or \npotential transition to a SIB.", " For example, the Cataract Bond draws on standards used by the WHO, and builds on standard \nmeasures  used  by  other  programmes  managed  by  The  Fred  Hollows  Foundation  and \nSightsavers. Conversely, for the QEI DIB, it was noted that the metric used is more rigorous \nthan other assessments as it focused on attainment of grade level learnings rather than just \nnumeracy and literacy skills. The metric is strong in avoiding in perverse incentives. However, \nit has three disadvantages: \u2022 \n\u2022 It has increased the data collection costs at the comparison schools \nthe assessment system needs to be standardized over a larger data set should the \nDIB be transitioned into a SIB \u2022  Service  providers  reported  that  they  do  not  have  a  complete  understanding  of  the \nframework. Should service providers be unable to develop a stronger understanding \nover  the  course  of  the  DIB,  this  may  limit  the  metrics\u2019  value  in  being  a  useful  and \naccessible tool for performance management.", " The  model\u2019s  benefits  can  also  be  increased  when  the  outcome  metrics  include \nconsiderations of quality. However, it can be challenging to link quality indicators to outcome \nmetrics. The ICRC outcome metric involves an element of quality (requiring the beneficiaries \nto undergo an assessment of their mobility). The Cataract Bond also has a quality indicator, \nbenefitting  from  a  WHO  standard  in  terms  of  quality  of  outcomes  which  meant  that  it  was \nrelatively easy for the Cataract Bond to set and monitor quality. Several stakeholders in the \nQEI DIB requested that end user voices are added alongside quantitative measures, to better \nunderstand the quality  of  education  provided.  However,  these are unlinked to the  outcome \nmetrics.  Over  the  next  two  research  waves,  the  evaluation  will  explore  how  these  metrics \nunlinked to payment are monitored and used to drive performance.", " Structuring payments Outcome  metrics  and  targets  work  best  when  returns  to  investors  and  outcome \nfunders, and correspondingly, incentives, are aligned. This was cited by the majority of \nrespondents as a key ambition in the design of the outcome metrics and payment structure.", " \u2022  One approach was to develop a unit cost per outcome that balances what outcome \nfunders are willing to pay for and the real cost of delivery and providing an acceptable \nreturn to investors. QEI first developed a price per outcome, and then this was checked \nagainst  previous  costs  and  expected  return  rates  to  determine  acceptability.  This \nallows for greater transferability to other interventions, and it is interesting to note that \nin  this  case  the  DIB  was  first  developed  before  specific  service  providers  were \nidentified, and a semi-competitive selection process used to identify service providers. \n\u2022  Another approach is to set targets acceptable to both outcome funders and service \nproviders, and then align these with an acceptable interest rate for both outcome \nfunders and investors to determine the outcome payment. In contrast to the VE \nand  QEI  DIB,  the  ICRC  HIB  and  the  Cataract  Bond  were  designed  with  a  service \nprovider  in mind.  Hence,  targets  were first set,  and  the  cost  of the  intervention  was \nbudgeted for, based on previous experience. The targets and budget were then linked \nto a maximum interest rate and return.", " \u2022  The VE DIB used a blended approach. A unit cost per outcome was developed, and \nultimately  the  ambition  is  to  scale  up  to  use  other  service  providers.  However,  the \npayment  function  and  targets  were  set  once  VE  as  a  provider  was  selected  and \nmetrics, targets and payments were set drawing heavily on VE\u2019s theory of change, past \nRCT data and costing.", " Measuring impact This sub-section explores the balance between reducing the transaction costs and increasing \nthe  benefits  of  the  impact  measurement  stage  in  terms  of  accuracy  and  usability.  The \nnecessary conditions for measuring impact largely relate to having clear outcomes that can \nbe attributable to the intervention, and the robust metrics to capture the targeted outcomes, \nwhich are discussed in sections 6.0 and 6.2.1 respectively.", " 6.3.1 Measuring impact - reducing transaction costs and increasing the model\u2019s \nbenefits Summary:  The  validation  process  should  be  designed  to  meet  the  needs  of  stakeholders. \nDifferent considerations may apply to different contexts; where an intervention or certain causal \nlinks are sufficiently backed by evidence, there may be less value in using quasi-experimental \nmethods compared to validated administrative data.", " 6.3.1.1 Analysis from four projects Across  the  four  DIBs,  there  were  two  methods  of  measuring  impact:  using  validated \nadministrative  data  (Cataract  DIB  and  ICRC  HIB)  and  experimental  (VE  DIB)  or  quasi-\nexperimental  methods  (QEI  DIB).  We  explore  the  alignment  between  the  objectives  and \nmethods used across the four DIBs below: Validated administrative data appears more straightforward, and works well when there is \nless of an attribution issue, and the focus is simply to pay for outputs or outcomes. For the \nCataract  Bond  and  the  ICRC  HIB,  there  is  a  direct  link  between  the  output  (surgery  and \nphysical rehabilitation, respectively) and outcomes, namely improved vision and mobility as a \nresult of the surgery/physical rehabilitation. Hence, the verification of the administrative data, \nincluding tests for quality (walking, post-operations visual acuity of 6/18 sight) was determined \nto be sufficient, simpler and cheaper, as no estimate of the counterfactual is required.", " Conversely, where attribution is more challenging, or where standardized validated outcomes \nare not available or where there is an interest in comparing approaches with a counterfactual, \nwith other interventions or between service providers, the use of an experimental or quasi-\nexperimental approach is needed. In the case of VE, stakeholders wanted to ensure that \nany increases in income were attributable to the intervention itself. There was also a motivation \nto contribute to learning about \u2018what works\u2019 in poverty reduction. It was felt that an RCT would \nbe  the  most  rigorous  means  of  achieving  this.  Furthermore,  the  use  of  a  RCT  enables \ndemonstrating the value of the DIB in driving better performance. The return to investors is \nalso assessed against previous improvement identified in the RCT. For the QEI DIB, a key \npriority  was  to  have  a  rigorous  evaluation  that  would  evidence  the  causal  effect  of  the \nintervention on a standardized scale.", " However, both QEI and VE noted that this can be expensive and time-consuming.", " \u2022  VE\u2019s  outcome  metric  is  based  on  the  ratio  of  increase  in  income  based  on  funding \ntransferred, and hence requires validation both on the increase in income (relying on \na RCT) and on funding transferred (verification). VE\u2019s outcome verification includes an \naudit to verify the transfers of seed funding from VE to beneficiary households and a \nRCT  to  estimate the  effect  of  the  programme.26  VE  notes  that  verification  would  be \ndifficult to deliver at scale. It may be that certain components, such as the verification \nof seed funding transfer, can be subject to less rigorous verification going forward.  \nIn  the  QEI  DIB,  significant  time  and  resource  were  spent  on  identifying  a  suitable \ncomparison group.", " \u2022 As set out in section 5, the proportion of costs spent on verification is significantly higher for \nQEI  and  VE  DIB,  in  comparison  to  the  Cataract  Bond  and  ICRC  HIB.  The  issue  of \nproportionality  is  closely  linked  to  the  objectives  of  the  evaluation  and  verification,  and  the \nextent to which more expensive methods are required by the stakeholders involved in the DIB.", " 6.3.1.2  Comparison with other impact bonds Similarly,  other  impact  bonds  face  important  considerations  in  terms  of  the  objectives  of \nmeasuring  performance.  We  set  out  the  three  main  objectives  to  measuring  performance \nnoted in the literature.", " 26 The evaluation firm will conduct two instances of data collection, through which end line data will be collected from a sample \nof households from each cohort. Baseline data collected by Village Enterprise may be used for creation of covariates to be \nused during the analysis. Accordingly, each group of cohorts will have its own impact estimation based on which the trustee will \npay Village Enterprise. The RCT design is an improved version of the RCT performed between 2014 and 2017 to evaluate \nVillage Enterprise\u2019s intervention in Uganda. The randomization will be made at the village level. The evaluator will randomly \nassign the villages to receive the Village Enterprise program.", " Gustafsson-Wright et al (2017) set out three main objectives to measuring performance: 10. Assessing how the DIB is driving better performance 11. Assessing  performance  against  targets,  to  protect  outcome  funders  from  paying  for under-performance 12. Generate learning/evidence, in terms of identifying interventions/service providers that \nto  other to  a  counterfactual,  or in  comparison deliver,  when  compared \ninterventions/service providers.", " Evaluation is an essential part of the impact bond structure. However, the objectives of the \nevaluation will differ among actors. Use of experimental or quasi-experimental approaches, in \nparticular, can be a costly and time-consuming part of an impact bond. Costs can be reduced \nand  benefits  increased  if  the  evaluation/impact  measurement  is  designed  with  the  specific \nobjectives of the stakeholders in mind. The specific goals of the individual DIB are going to \ndetermine  the  most  desirable  method  of  evaluation,  summarised  in  the  table  below \n(Gustafsson-Wright et al., 2017).", " Table 6.1: Project focus and measurement approach Projects focused on \nInnovation \nBuilding evidence \nReplication, drawing on an established evidence \nbase \nScaling,  using  established,  highly  evidence-\nbased interventions Measurement \nNon-experimental \nQuasi-experimental or experimental \nAgainst a counterfactual to further build evidence Simpler methodology Identifying and selecting stakeholders and managing relationships This  sub-section  explores  the  necessary  conditions  for  stakeholders  to  be  suitable  for \nparticipation in a DIB. It then sets out five lessons learned around the process of identifying \nstakeholders and managing relationships, before concluding with lessons learned around how \nthe  involvement  of  different  types  of  investors  and  outcome  funders  can  lead  to  different \nbenefits.", " 6.4.1 Identifying and selecting stakeholders and managing relationships - \nnecessary conditions Summary:  The  necessary  conditions  for  DIBs  are  similar  to  SIBs  developed  in  high-income \ncountries,  and  involve  strong  leadership,  within  and  across  organisations.  Additionally, \nstakeholders also need to have sufficient capacity and skills, as well as a willingness to adapt \nand learn. Finally, in this early stage of the market, stakeholders with strong a reputation and \ntrack records are needed to lend credibility to the DIBs.", " 6.4.1.1 Analysis from four projects The following factors in relation to stakeholders and relationships were identified in the four \nDIBs as being necessary for their successful development: \u2022  Strategic  leadership  from  each  of  the  members  of  the  leadership  team:  For \nexample, there was political commitment from the Belgian and the Swiss governments \nto support the ICRC, and clear alignment of interests and vision from the leadership \nteam across the VE stakeholders. Similarly, there was strong leadership across the \nQEI stakeholders, and strong commitment to test the DIB model in the Cataract Design \nCoalition. This strong leadership was particularly important to overcoming reservations \nfrom  other colleagues  within their  respective organisations;  in particular  overcoming \nconcerns that DIBs involved \u2018making money off the poor\u2019.", " \u2022  Sufficient  capacity  and  skills:  DIBs  require  financial,  legal  and  private  sector \nexpertise,  as  well  as  experience  with  outcome  based  contracting,  a  good \nunderstanding of metrics and evaluation methodologies and a strong understanding of \nthe sector. This was cited as a crucial element across all four DIBs, and a number of \nstakeholders  noted  that  lack  of  this  expertise  was  a  key  challenge  which  required \nadditional time and resource. Drawing on external support to supplement this expertise \nwas required across all four DIBs. As additional impact bonds are being implemented, \na clearer sense of the capacity and skills required could make this process smoother.", " \u2022  Culture of innovation and interest in adapting and learning: For example, systems \nwithin certain outcome funders and service providers are not set up to deal with multi-\nannual budgets, which require a certain flexibility and resourcefulness to address. QEI \nstakeholders commented on the importance of being able to achieve a high level of \nspeed  and  efficiency  in  the  decision-making,  implementation  and  evaluation \nprocesses. One stakeholder in the QEI DIB commented that having entities happy to \ninnovate and take on risk, such as UBSOF, is fundamental at this early stage of the \nmarket. Accommodating the DIBs required significant adaptation within organisations, \nand is not something suitable for all stakeholders, given the requirements involved and \nthe reputational risks. The selection criteria used within the VE and QEI DIB to select \nservice providers included openness to innovation and ability to adapt, in addition to \ntrack record and operational capacity.", " \u2022  As  discussed  in  section  4.3.1.3,  participation  of  stakeholders  with  a  strong \nreputation and track record lends credibility to the DIBs, especially during this early \nstage  of  the  market,  and  makes  it  easier  to  secure  other  stakeholders.  Outcome \nfunders involved in the HIB thought that they would not have been able to test the DIB \nfunding mechanism with other service providers, given the potential reputational risk \nassociated  with  using  a  new  funding  mechanism  that  involved  payments  to  private \ninvestors.", " 6.4.1.2  Comparison with other impact bonds The necessity of strong collective leadership echoes the findings of the necessary conditions \nfor a SIB to be launched in high-income countries, as set out in the LOUD model27.", " Additionally, a key finding from interviews with stakeholders from the wider impact bond sector \nwas  the  importance  of  stakeholders  being  flexible  enough  to  accommodate  high  levels  of 27 https://golab.bsg.ox.ac.uk/knowledge/resources/loud-sib-model-four-factors-determine-whether-social-impact-\nbond-launched/ internal management change, which can be a challenge in large organisations. As discussed \nin  interviews,  both  USAID  and  World  Bank  are  exploring  how  to  modify  their  contracting \nprocedures,  legal  issues  and  other  organisational  requirements,  in  order  to  better \naccommodate impact bonds.", " 6.4.2 Identifying and selecting stakeholders and managing relationships - \nreducing transaction costs Summary: Transaction costs for this stage of the design and set up process can be reduced \nwhen: there is strong collaboration across stakeholders, drawing on each other\u2019s expertise and \nstrengths;  roles  are  clearly  defined  from  the  start;  stakeholders  are  identified  and  brought  in \nefficiently;  and  there  is  the  right  balance  between  undertaking  negotiations  bilaterally  and \ncollaboratively.", " 6.4.2.1 Analysis from four projects The majority of cost drivers identified for the four DIBs, as set out in section 5.1, relate to this \nstakeholder management. Three out of the four DIBs identified that engaging outcome funders \nand raising finance were large cost drivers; two out of the four DIBs noted that the number of \norganisations involved in the DIB was another cost driver, as was the time it took to negotiate \nagreements.", " The emerging finding from the experiences of these four DIBs is that transaction costs for this \nstage of the design and set up process can be reduced where: 1.  There is strong collaboration across stakeholders, drawing on each other\u2019s expertise and strengths 2.  The DIB involves the right number and balance of stakeholders \n3.  Roles are clearly defined from the start \n4.  Stakeholders are identified and brought in efficiently \n5.  The  balance  between  undertaking  negotiations  bilaterally  and  collaboratively  is \noptimal for ensuring the negotiation process is efficient, while at the same time building \na shared understanding of the objectives of the DIB among stakeholders.", " We discuss each of these in further detail below.", " Collaboration between stakeholders with complementary experience Stakeholders  who  are  transparent  and  open  to  collaborating  and  sharing  resources  and \nexpertise  can  make  the  process  more  efficient  and  reduce  the  costs  needed  of  drawing  in \nexternal  expertise,  while  at  the  same  time  improving  the  design  of  the  impact  bond.  This \nenables skills to be shared between stakeholders, reducing the costs of having to source this \nexpertise  elsewhere.  Nonetheless,  a  balance  is  needed  between  obtaining  the  right \ncomplementary  expertise  across \ntoo  many \nstakeholders that makes communication and collaboration unwieldy, something noted by an \nICRC HIB investor.", " the  stakeholders,  and  between  having \u2022 In the case of the VE DIB, the complementary experience of the different stakeholders \nwas cited as a key enabler. For example, Village Enterprise and the anonymous donor \nhad  experience  of  poverty  alleviation,  while  Instiglio  brought  in  expertise  in  results based financing, which helped to inform a more practical design of the DIB and limited \nthe amount of external expertise needed.", " \u2022  Similarly, in the case of the Cataract Bond, stakeholders brought in expertise in the \neye care sector, as well as previous experience of operationalising the Aravind model.  \n\u2022  Finally, in the case of the QEI DIB, stakeholders commented on the complementary \nexpertise  brought  in  by  the  different  actors;  UBSOF  and  Dalberg  brought  previous \nexperience  with  DIBs,  while  BAT,  GMI  and  MSDF  had  extensive  knowledge  of \ndelivering  learning  outcomes  in  the  Indian  context,  had  previously  worked  with  the \nservice  providers  involved  in  the  DIB,  and  had  a  good  network  within  the  country. \nMSDF and GMI were also open to sharing their technical knowledge and data on the \neducation  sector  in  India,  which  was  crucial  to  help  identify  targets  and  expected \noutcomes, compare the costs of different interventions and create performance metrics \naccordingly.", " Involving the right number and balance of stakeholders Increasing the number of stakeholders tends to increase the project management time needed \nand complexity around negotiations. One investor noted that complexity was linked to the high \nnumber of stakeholders involved, and their lack of familiarity with impact bond like instruments. \nIt also creates complexities around undertaking due diligence on the respective partners, as \nexplained  in  Section  6.4.  One  investor  commented  that  in  the  private  sector,  whenever  it \ncomes to new products, one tends to limit the number of stakeholders involved to a necessary \nminimum. Limiting the stakeholders involved, especially at this stage of the market, can be a \nsolution  to  reduce  transaction  costs.  However,  other  stakeholders  noted  that  a  balance  is \nneeded, as limiting the number of stakeholders might reduce the amount of funding available, \nthe learning opportunity stakeholders can benefit from by collaborating in different DIBs, and \nalso  the  possibility  of  bringing  on  reputable  stakeholders  which  can  then  give  the  DIB \ncredibility.", " Clearly defining roles from the start A key lesson learned across the DIBs was that it is important to clearly define the roles and \nresponsibilities  of  actors.  QEI  stakeholders  noted  that  it  was  important  to  ensure  clear \nunderstanding from all partners and strong governance supporting the agreements. Similarly, \nthis was a challenge noted in the VE DIB. The function of the trustee was not clearly agreed \nupon, and Village  Enterprise  had  not  expected to lead the  identification of  investors,  which \ndelayed the process.", " Stakeholders are identified and brought in efficiently Across  the  four  DIBs,  the  DIB  leads  structured  the  stakeholder  engagement  process  in \ndifferent ways. Advantages and disadvantages were identified for these different approaches. \nThe  table  below  summarises  some  of  the  advantages  and  disadvantages  to  the  different \napproaches  of  identifying  and  engaging  with  stakeholders,  which  is  followed  by  additional \ndetail for the four DIBs in the box below.", " Table  6.2:  Advantages  and  disadvantages  to  different  approaches  to  identifying  and \nengaging with stakeholders Approaches Engaging with \nstakeholders \nearly on in the \nprocess Specific \nconsiderations \nfor engaging \nwith outcome \nfunders first Advantages \nHaving  credible  stakeholders  on  board  can  attract  others  to \nthe DIB.  \nStakeholders often want or need to be involved early enough \nto be able to feed into the terms of the DIB.  \nHaving  investors  involved  early  can  mean  they  are  able  to \nprovide  financial  and  commercial  expertise,  and  sense-\nchecking of the proposed rates. \nSome  outcome  funders  have  procurement  requirements  for \ncontracting with service providers. This can be a challenge if \nan  outcome  funder  is  approached  when  there  is  already  a \nservice provider in place.  \nFirst  identifying  whether  there  is  donor  interest  in  using \noutcome  based  mechanisms  to  fund  a  certain  sector  of \nintervention  can  make  it  easier  to  identify  outcome  funders. \nIdentifying outcome funders was cited as a challenge in all the \nDIBs  (except  for  Cataract  Bond,  which  was  designed  by \noutcome funders).", " Disadvantages \nIt can be difficult for stakeholders to \nget  involved  when  the  terms  are \nnot developed.  \nLengthy  negotiations  can  mean \nstakeholders  consider  dropping \nout Certain stakeholders in one of the \nDIBs  noted  that  engagement  with \nmultiple potential outcome funders \nat the same time was inefficient.  \nHaving outcome funders locked in \nfrom the start can sometimes limit \nthe  budget \nflexibility  around \nenvelope available, or the terms of \nthe outcome fund.", " Box  2:  Lessons  Learned  from  the  DIBs  in  approaches  to  identifying  and  engaging  with \nstakeholders Advantages of engaging with stakeholders early on in the process \u2022  Having credible stakeholders on board can attract others to the DIB and signal the credibility of the funding \nmechanism. For example, in the ICRC HIB several outcome funders pointed to the importance of ICRC\u2019s track \nrecord, and in the case of the VE DIB, there was limited interest from investors until USAID and DFID were signed \nin.", " \u2022  Stakeholders often want or need to be involved early enough to be able to feed into the terms of the DIB. \nFor  example,  an  investor  commented  that  they  needed  to  be  involved  earlier,  as  often  when  they  receive \nproposals from a service provider it is too late if they are no longer able to influence the terms and conditions and \nfeed  into  the  design  of  the  impact  bond.  Outcome  funders  who  joined  the  ICRC  HIB  at  a  later  stage  also \ncommented on the limited scope to input into the HIB\u2019s terms.", " Disadvantages of engaging with stakeholders early on in the process \u2022 \u2022 It can be easier for stakeholders to get involved when the terms are more developed. This was noted by \noutcome funders in the QEI DIB and some of the investors across the other DIBs.  \nLengthy negotiations can mean stakeholders consider dropping out. For example in one of the DIBs, the \nlength of the negotiations and process of identifying investors made several actors question whether they should \nabandon  the  idea  of  getting  involved.  Furthermore,  the  late  involvement  of  OPIC  in  the  Cataract  Bond \nnecessitated an updating of the terms, which prolonged the negotiation. Getting outcome funders and investors \nsecured in parallel might be a way to mitigate this risk in the future.", " Advantages of engaging with outcome funders first \u2022  Some  outcome  funders  have  rigorous  procurement  procedures  in  terms  of  the  selection  of  service \nproviders. This means that it can be challenging if they are approached when the DIB already has an identified \nservice provider, as was the case in the VE DIB. VE stakeholders thought that selecting outcome funders prior to \nengaging with service providers could facilitate a public request for proposal, something that was done in the case \nof the QEI DIB.  \nFirst identifying whether there is donor interest in using outcome based mechanisms to fund a certain \nsector of intervention can make it easier to identify outcome funders. Identifying outcome funders was cited \nas a challenge in all the DIBs (except for Cataract Bond, which was designed by outcome funders). Furthermore, \ngenerally, outcome funders have priority sectors and countries, and the availability of funding will be determined \nby this, whereas investors are seen to be more attracted by the terms of the impact bond.", " \u2022 Disadvantages of engaging with outcome funders first \u2022  Certain stakeholders in one of the DIBs noted that engagement with multiple potential outcome funders at the same time was inefficient.", " \u2022  Having  outcome  funders  locked  in  from  the  start  can  sometimes  limit  flexibility.  In  one  DIB,  a  specific \namount of funds was approved, which left little room for flexibility in terms of the funding value, capital guarantee \nor interest rate.", " is  a  right  balance  between  undertaking  negotiations  bilaterally  and There \ncollaboratively There are trade-offs between undertaking negotiations bilaterally or in a more collaborative \nfashion. Negotiations in a more collaborative fashion can support a shared understanding of \nthe objectives of the DIB, but be less efficient, an approach taken by QEI DIB and the Cataract \nBond. A bilateral approach with proposed terms can make the process more efficient, but it \nmeans there is not a consistent understanding of the objectives of the impact bond and less \nchance for the other actors to feed into the development of the DIB. The box below sets out \nfurther details.", " Box 3: Nature of negotiations Stakeholders within the QEI DIB and Cataract Bond highlighted the importance of taking \na  collaborative  approach  to  designing  the  DIBs  and  consistent  messaging.  QEI \nstakeholders noted they needed to organise many workshops with different stakeholders \nto keep messaging consistent about project objectives and details of the DIB model, and \nCataract stakeholders also noted the importance of having the intermediary keeping the \noutcome funders regularly updated on design issues. QEI and Cataract stakeholders also \ncommented on the importance of taking a collaborative approach to designing targets.", " Conversely,  the  ICRC  HIB  involved  more  bilateral  discussions,  which meant  that  the \nnegotiation  process  was  more  efficient.  However,  stakeholders  commented  that  more \ncollaborative  discussions  would  have  supported \nthe  development  of  a  shared \nunderstanding of the impact bond. Additionally, a frustration noted by one outcome funder \nwas that for a period, outcome funders could not see the terms offered to other funders \nand investors.", " Finally, the VE DIB was initially designed to take an approach to developing impacts bonds \nthat leverages \u2018market forces\u2019. Its design memo notes that set up costs for DIBs to date \nhave  been  high,  hypothesising  that  is  due  to  a  reliance  on  an  \u2018over-engineered \n\u201cconsensus-on-all-things-by-all-parties\u201d  approach\u2019.  Hence,  the  intention  was  for  the \noutcome funders to commit outcome funds and specify conditions, and leave the working \ncapital, negotiating terms and structuring to service providers. This was expected to make \nthe set-up of the DIB less costly and more scalable. However,  it appears that in reality, \nmulti-party  negotiations  were  needed  to  develop  the  DIB.  Interviews  identified  that \nstakeholders within VE noted that having multi-party negotiations without clear protocols \nslowed down the process.", " For  the  four  DIBs  under  the  scope  of  the  evaluation,  the  two  models  of  negotiations \n(collaborative and bilateral) appear to have unlocked different benefits. A more collaborative \napproach ensures a shared understanding of the objectives of the impact bond and enables \nmore  collaborative  co-design;  on  the  other  hand,  more  bilateral  discussions  can  be  more \nefficient, which can also potentially facilitate scaling up.", " 6.4.2.2  Comparison with other impact bonds Costs include those needed to educate the market about the new product and concept that \nthe DIB represents. One advisor explained this process was particularly costly because it often \nrequired one-to-one interactions, through workshops and regular communication.", " The literature suggests the benefits of the earlier involvement of investors. (Gustafsson-Wright \net al., 2017; Oroxom et al., 2018), and in the case of the Palestine DIB, stakeholders noted \nthe  ambition  to  involve  investors  in  the  co-design  of  the  DIB.  Similarly,  the  wider  literature \nsupports the finding that there are trade-offs between getting early buy-in and credibility from \nan outcome funder, versus a higher comfort level for outcome funders coming on board with \nsomething that is more developed (Gustafsson-Wright et al., 2017). A compromise may be to \nreach out to investors to gain initial interest and approach them again once the impact bond \nis better developed. If brought too early in the process of DIB development, which is likely to \nbe lengthy, investors may lose patience (Gustafsson-Wright et al., 2017).", " 6.4.3 Identifying and selecting stakeholders and managing relationships - \nincreasing the model\u2019s benefits Summary: The types of investors, and the way outcome funders and investors engage in the \nDIB have implications for the types of benefits that can be expected, and require consideration \nin terms of the ethical framework for engagement.", " 6.4.3.1 Analysis from four projects An emerging finding was that different types of investors and outcome funders bring different \ntypes of benefits. For example, commercial investors are able to bring in more experience with \ntesting and implementing financing modalities, while philanthropic investors may be able to \nbring  experience  and  expertise  within  the  sector.    As  a  result,  careful  consideration  of  the \nobjectives of using the impact bond should be taken into account when identifying outcome \nfunders and investors.", " Investors A hypothesised benefit of using DIBs, noted by a few outcome funders within the four DIBs, is \nthat investors not only bring capital, but can also contribute commercial sense, expertise in \npricing and quantifying risks, and market discipline in picking investments. A key advantage \ncited by the private investors and service provider involved in the ICRC HIB was the fact that \nit enabled commercial actors to be involved in providing upfront financing for the delivery of \nservices  in  humanitarian  contexts28.  One  DIB  advisor  felt  that  these  benefits  may  not \nmaterialise when traditional donors are involved as investors due to their aversion to certain \ntypes of risk (reputational, non-delivery) and other internal constraints, such as bureaucratic \nprocedures.  For  example,  one  of  the  investors  had  internal  organisation  requirements  that \nrequired  extensive  reporting  from  service  providers,  on  areas  such  as  job  creation, \ndisaggregation of jobs by gender, environmental standards and salaries.  This can increase \nthe reporting requirements of service providers rather than moving them to a focus on delivery 28 It should be noted that investors are expected to be repaid at the end of the programme, depending on the \nperformance of the programme.", " of outcomes (as might perhaps be the case where investors were focused purely on financial \nreturn).", " Conversely,  the  types  of  investors  involved,  and  terms  offered  for  their  engagement,  have \nimplications for the suitability of impact bonds for the development sector. Certain outcome \nfunders within the four DIBs highlighted the importance for them, when identifying investors, \nof finding an investor that is socially committed and aligned with the mission objectives of their \norganisation. One outcome funder noted that it was not willing to partner with investors whose \nprimary objective is to make a profit out of the DIB. This resonates with another DIB advisor\u2019s \nconcern about the risk of putting the investors\u2019 interests at the forefront of the project design, \nin  terms  of  the  returns  necessary  to  attract  them  and  their  unwillingness  to  accept  certain \npolitical and operational risks. The two main ethical considerations when involving investors \nappear  to  be  the  importance  of  avoiding  excessive  returns  to  investors  and  not  prioritising  \ndevelopment principles and priorities (for example, the humanitarian principles of impartiality \nand independence) over the commercial interest of investors.", " Outcome funders Another  key  hypothesised  benefit  of  the  DIB  model  is  that  outcome  funders  can  focus  on \npaying for outcomes and require less time to be spent on monitoring inputs or undertaking \nother project management duties. Similarly, an advisor in one of the DIBs thought that benefit \nof  using  the  DIB  is  greater  when  outcome  funders  leave  capital  raising  and  investment \nstructuring to service providers. Certain outcome funders within the ICRC HIB also expected \nsavings in terms of project management time, over the course of the HIB.", " However, our fieldwork identified that not all outcome funders are interested in taking a more \nhands-off approach in comparison to the approach taken when funding through grants. For \nexample, for all four DIBs, outcome funders or outcome convenors undertook due diligence \non the service providers, as well as some of the other actors within the impact bond. At the \nsame time, one outcome funder noted that since \u2018donor payments are tied to the real costs of \ndelivery,  it  is  essential  that  all  expenditure  is  eligible  and  verifiable.\u2019  Given  the  state  of  the \ncurrent bilateral and multilateral donor frameworks in place, there is potentially a limit to the \nextent to which bilateral and multilateral donors can purely focus on outcomes. As discussed \nin  section  5,  despite  being  able  to  transfer  financial  risk  should  outcomes  not  be  met, \nreputational  and  other  types  of  risk  remain,  which  limits  funders  ability  to  only  focus  on \noutcomes.", " Overall finding An  emerging  finding  is  that  different  stakeholders  have  different  preferences  and \nrequirements. This depends less on the role they intend to play in the impact bond, and more \non their organisational policies and their objectives for getting involved in the impact bond. For \nexample,  the  level  of  input  desired  varied  across  different  outcome  funders  and  different \ninvestors. This was referenced to organisational policies, most notably on procurement and \nestablishing of business cases, and to the objectives of getting involved in the DIB. This makes \nit difficult to generalise, even across the different DIB roles (such as outcome funder, investor, \nservice  provider,  intermediary),  and  makes  it  necessary  for  the  stakeholder  engagement \nprocess to be designed on a case-by-case basis.", " 6.4.3.2  Comparison with other impact bonds The benefits accruing to the outcome funder in outsourcing some of its management to the \nother  actors  in the  impact  bond  is  supported  by the  literature.  A key  hypothesis  (Drew  and \nClist  2015)  is  that  a  major  benefit  of  the  DIB  model  is  the  role  of  the  market  in  identifying \nsuccessful projects. If the donor is involved in specifying the nature of the intervention or the \ncontracting, the DIB model is then expected to lose an important advantage over other funding \nmechanisms.", " Structuring the vehicle and developing the operating model This  sub-section  sets  out  some  of  the  necessary  conditions  and  recommendations  around \nreducing  transaction  costs  during  the  structuring  of  the  vehicle  and  development  of  the \noperational model.", " 6.5.1 Structuring the vehicle and developing the operating model - necessary \nconditions Summary: Given the early stage of the market, organisations and legislative frameworks are \noften unable to accommodate the DIB, resulting in the need to set up SPVs or \u2018work arounds\u2019 \nin the terms of the contracts that can deviate from what a \u2018standard DIB\u2019 looks like.", " 6.5.1.1 Analysis from four projects There are a number of conditions needed to implement a DIB. These were in place for some, \nbut  not  all,  of  the  four  DIBs. Where  these  conditions  were  not  in  place,  we  summarise the \napproaches used to address these limitations.", " A legislative framework that allows public funds to fund private sector profits In  the  case  of  the  ICRC  HIB,  the  legal  frameworks  governing  development  assistance  in \nSwitzerland and Belgium presented an initial barrier. The investment in the HIB was ultimately \nable to proceed due to the granting of special waivers and exceptions.  This is not yet resolved \nfor future DIBs that may be implemented with investment from these countries. A learning is \nthat legal feasibility can absorb considerable time. More work upfront to ascertain the legal \nfeasibility of using an impact bond can provide stakeholders with a more realistic assessment \nof the potential time involved to develop an impact bond.", " Taxation on the returns of the investment which is accounted for in the financial model As  impact  bonds  draw  on  both  public  funding  and  private  finance,  it  is  subject  to  tax \nregulations. Under Swiss tax regulations, profits on investments are taxed, and stakeholders \ninvolved, including those issuing the \u2018bond\u2019 need to ensure they are compliant with regulations \non how and where returns are taxed.", " A  framework  enabling  public  sector  entities  to  commit  themselves  long-term  to \nundefined and uncertain expenses Certain donor budgeting and accounting frameworks do not allow  a commitment to a long-\nterm  expense  that  is  undefined  and  uncertain  -  a  core  component  of  an  impact  bond.  For \nexample, in the Cataract Bond, a key consideration was that the Hilton Foundation, as a grant- making organization, did not have a mechanism to make contingent grant payments at some \ntime  in  the  future,  as  per  the  pay-for-success  nature  of  a  DIB.  The  Hilton  Foundation  also \nneeded to provide the funds for the impact bond to an intermediary registered charity. As a \nresult,  the  Hilton  Foundation\u2019s  initial  outcome  funding  agreement  was  structured  like  a \nconventional grant, with a set schedule of payments and an accredited grant recipient, which \nis The Fred Hollows Foundation (FHF). As each grant payment is received, FHF forwards the \nmoney into a trust. Payments from the trust will be managed and released by FHF in line with \nthe contractual agreement of the DIB.", " Aligning DIB with organisational requirements Stakeholders involved in the DIBs have to ensure the terms of the DIB are aligned with their \norganisation requirements and different methods were used to address this.", " Firstly, in some cases it was a matter of including specific terms in contracts. For example, \nin the ICRC HIB, due to ICRC\u2019s specific legal status and specific privileges and immunities \nunder both international and domestic law, there were challenges with setting up a contract \nfor ICRC that was binding, and this required adding specific terms. Also, while ICRC and KOIS \n(who supported ICRC in designing the impact bond) originally intended to have one contract, \nultimately  different  contracts  were  needed  for  each  outcome  funder,  due  to  their  different \nrequirements and respective legal frameworks.", " Secondly,  the  set-up  of  an  intermediary  can  enable  organisations  to  bypass  internal \nrestrictions, for example: \u2022 In the QEI DIB, Tata Trusts could not pay an overseas investor, and had to find an \nIndian intermediary. Furthermore, UBSOF was unable to accept private investments. \nIn hindsight,  they thought  it might  have been  beneficial  to set  up  a  special  purpose \nvehicle  (SPV)  which  could  accept  private  investments.  However,  it  would  not  have \nbeen a straightforward process to set up a SPV, as UBSOF is a subsidiary of UBS.  \n\u2022  VE was advised to set up Village Enterprise Capital Connector Corp (VECC) to act as \na  buffer  between  VE  and  its  limited  liability  corporation  (LLC)  to  enable  greatest \nflexibility in terms of the types of investment sources that can be received into the LLC. \nAdditionally, having a separate company filing for the VECC protects the balance sheet \nand operational activities of the original VE. A barrier for the mixing of funds as well as \nthe legal buffer between the LLC and the original non-profit is advantageous in terms \nof potential liabilities.", " Finally, organisation requirements can affect the structure of the funding mechanism. For \nexample, given that OPIC is by mandate a lending organisation, the Cataract Bond had to be \nstructured  as  a  loan  or  the  coalition  would  have  had  to  get  approval  from  Congress  to  go \nahead with the investment. This affected the contractual arrangements within the DIB, which \nresulted in OPIC and Netri making loans to the Africa Eye Foundation, and a 100% capital \nguarantee for OPIC.", " Setting up arrangements in which what happens in all eventualities is clearly defined  \nStakeholders in the QEI DIB pointed to the importance of planning ahead for all eventualities, \nincluding agreements on who is able to end the contract or change the actors involved. Across \nthe QEI DIB, ICRC HIB and VE DIB contracts, force majeure and exit clauses were included.", " Effective processes to manage the risks of working with new actors The number and types of stakeholders involved in a DIB tend to be larger than and different \nto that of a grant funded or PbR contract. This presents additional risks to stakeholders.", " \u2022  One  approach  to  managing  this  risk  is  through  undertaking  and  sharing  due \ndiligence  assessments.  Several  outcome  funders  commented  on  the  additional \nresources needed to undertake due diligence on all the stakeholders involved. Across the \nfour DIBs, there were examples where stakeholders tried to manage this by sharing the \nresults of due diligence assessments, and relying on the assessments done by others.", " \u2022  Another approach is to use multiple contracts. For example, in the QEI DIB, certain \nstakeholders were not comfortable signing a contract with service providers that they had \nnot selected or conducted due diligence on. As UBSOF was also unwilling to provide an \nindemnity  clause,  eventually  separate  contracts  were  signed  between  the  different \nstakeholders, to manage this risk.29 6.5.2 Structuring the vehicle and developing the operating model - reducing \ntransaction costs Summary:  The  often  diverse  legislative  frameworks  increases  the  transaction  costs  of  this \nstage of DIB development. The optimal solution would be to amend the legislative frameworks \nto accommodate DIBs. Where this is not possible, other potential solutions include limiting the \nnumber of stakeholders involved, using pooled financing or funding structures, using other ways \nto minimise the number of contracts involved, or standardising deals.", " 6.5.2.1 Analysis from four projects As discussed in previous sections, limiting the number of stakeholders involved in the DIB and \nstandardising templates and processes  could  both  reduce the  transaction  costs  associated \nwith structuring the vehicle. However, again as noted previously, the unique nature of the DIBs \n(and in particular here the different legislative frameworks of the respective countries) limits \nthe ability for standardisation.", " 6.5.2.2  Comparison with other impact bonds A potential solution raised in our sector level findings was identifying whether a funder is more \nhands  on  or  hands  off.  Where  funders  are  willing,  an  intermediary  agency  can  be  used  to \nchannel  resources through  an  outcomes  fund;  the  rationale  here  is  that  an  outcomes  fund \nfunds multiple SIBs designed in the same way, reducing the per-DIB transaction cost as many \nelements  (such  as  the  outcome  metrics)  only  have  to  be  designed  once.  Where  outcome \nfunders are willing to be more hands off, alternatives such as pooled funding mechanisms30 \nor other financing mechanisms, as well as other contractual arrangements, may be potential \nsolutions.", " Conclusion 29 One investor noted that indemnities can work in a PbR set up, due to service providers\u2019 limited assets, but that it would be \nvery unlikely for investors to be willing to provide this indemnity.  \n30 Pooled funds are funds from many individual investors that are aggregated for the purposes of investment, as in the case of \na mutual or pension fund. Pooled funds are also used within the humanitarian and development sector to aggregate funding \nfrom multiple donors.", " This section has examined the context in which DIBs appear to be most appropriate. It is \ntoo early (and there are not enough DIBs) to state whether DIBs are most appropriate in \ncertain sectors or regions, but what is clear is that there are certain \u2018conditions\u2019 that increase \nthe likelihood that the DIB will be launched at all, or in the least launched in a shorter \ntimeframe and/or with lower transaction costs. These are as follows: \u2022  Sufficient evidence base for the proposed intervention \n\u2022  Clear and measurable outcomes  \n\u2022  Feasible timeframe for achieving the outcomes \n\u2022  Acceptable level of external risk \n\u2022  Sector with strong service providers \n\u2022  Data from previous interventions \n\u2022  Consortium that has: o  strong and committed leadership; \no  sufficient capacity and skills; \no  a culture of innovation and interest in adapting and learning; \no  a consensus on the policy problem, target outcomes and appropriate approaches; o  the right balance between size and breadth of expertise; \no  clearly defined roles for its members; \no  brought in stakeholders at the right time; and \no  a balance between bilateral and collaborative negotiations.", " \u2022  Legislative framework that allows public funds to fund private sector profits \n\u2022  Taxation on the profit of the investment which is accounted for in the financial model \n\u2022  A  framework  enabling  public  sector  entities  to  commit  themselves  long-term  to undefined and uncertain expenses \u2022  Alignment of DIB to organisational requirements \n\u2022  Setting up arrangements in which what happens in all eventualities is clearly defined  \n\u2022  Effective processes to manage the risks of working with new actors.", " What  is  particularly  interesting  is  that  many  of  these  conditions  have  been  identified  as \nnecessary within SIBs in high-income countries, suggesting that a lot of the learning within \nimpact  bonds  is  transferable  to  different  outcome  funders  (donors)  and  regions  (middle-\nincome and developing countries).In this early stage of the market, stakeholders with strong \nreputation and track records are particularly important to lend credibility to the DIBs, especially \nwhere  certain,  organisations  and  legislative  frameworks  are  often  unable  to  accommodate \nimpact bonds. For DIBs to be appropriate to these organisations, it can be necessary to set \nup special purpose vehicles (SPVs) or \u2018work arounds\u2019 in the terms of the contracts that can \ndeviate from what a \u2018standard DIB\u2019 looks like.", " As  a  result,  to  date,  certain  sectors  appear  to  be  particularly  suitable for  DIBs,  in  terms  of \nhaving clear outcomes, a shared understanding of the policy problem and sufficient data to \ndevelop targets and price risk. For example, certain markets such as eye care and education \nhave  a  strong  evaluation  and  research  history.  It  is  too  early  to  say  in  which  contexts, \nproblems, target groups, geographies and projects DIBs fit best and have the greatest impact, \nand this will continue to be explored in the following research waves.", " This  section  also  examined  how  the  process  of  designing  and  agreeing  DIBs  can  be \nimproved, in order to increase the model\u2019s benefits and reduce transaction costs. We \nnote  that  the  DIBs  under  the  scope  of  the  evaluation  vary  significantly  along  a  number  of \ncharacteristics (see section 3). The process of designing and agreeing DIBs will have to be \ntailored to the context of the intervention, and the objectives of using the DIB.. For example, \nfor the impact to be suitable to the humanitarian sector and ICRC\u2019s model of operations, it was \nnecessary to reformulate the role of the investor and governance structures, and modify the \nimpact  bond  structure  to  introduce  non-private  investors,  capital  protection  and  payments \nlinked to milestones. These findings are also limited by the fact that a very small number of \nDIBs are operating in very different contexts, and at an early market phase.", " There appears to be a tension between testing a \u2018pure\u2019 DIB, and tailoring the DIB to meet \nthe  objectives  of  stakeholders.  For  example,  certain  outcome  funders  in  the  Cataract \nPerformance Loan and the ICRC HIB were disappointed in some of the terms offered. One of \nthe outcome funders felt that the final terms dampened the most important element of the DIB \nto  them,  namely  testing  the  integrity  of  the  DIB  model,  and  especially  the  aspect  of  risk \nsharing.31 Similarly, an outcome funder in the ICRC HIB expressed disappointment that a risk \nguarantee  was  included,  as  well  as  the  fact  that  there  was  a  payment  attached  to  the \nconstruction of the centres. However, other actors felt that the changes were a promising step \nforward in terms of enabling an investor to participate. It should also be borne in mind that a \nkey concern set out by Arena (2016) and echoed by Palladium and USAID (2016) is that during \nthis  phase  of  the  market,  as  outcome  funders  and  other  actors  are  still  building  up  the \narchitecture  to  supports  DIBs,  too  much  tailoring  and  \u2018work  arounds\u2019  can  introduce \ncomplications and make it difficult to standardise processes to reduce transaction costs, thus \npotentially limiting the model\u2019s benefits.", " The  process  of  designing  and  agreeing  DIBs  will  need  to  be  structured  differently, \ndepending on the aims of the DIB. What is evident from the research is that stakeholders use \nDIBs for different reasons and in different contexts, and the process of designing and agreeing \nDIBs have to be adjusted accordingly. The evaluation found innovations in terms of how the \nfour impact bonds under study sought to reduce transaction costs and improve the benefits of \nthe model. An emerging finding is that impact bonds have to be adapted to the problem and \nconditions  at  stake.  When  selecting  suitable  interventions  for  DIBs,  stakeholders  need  to \nensure  the  DIB  structure  is  adapted  to  DIB  objectives.  Context  specificity  is  likely  to  be \nimportant,  with different design features  working best  with different  combinations  of  actors, \nand in different contexts. Furthermore, DIBs will be set up with different objectives in mind, \nand will not be aiming for all DIB effects. Therefore, the findings around improving the process \nof  designing  and agreeing  DIBs  may  not  be  relevant for  all  DIBs.  As  previously  mentioned \nthese findings are also limited by the early market stage and small number of DIBs that are \noperating in very different contexts.", " 31 This echoes Arena et al\u2019s (2016) work on identifying the configuration of the \u2018prototypical SIB structure\u2019 and their hypothesise \nthat \u2018inconsistencies\u2019 with the prototype model can inhibit the expected benefits of using the impact bond model. Similarly, Carter \net al (2018) discuss the four dimensions of a \u2018textbook\u2019 SIB that differ from PbR contracts or grants, and are hypothesised to \nunlock collaboration, prevention and innovation.", " Transaction costs need to be put into the perspective of the stage of the market. One \ninvestor noted that in the financial industry, a new instrument is always complex to design and \nexpensive to set up. However, the initial investment can be leveraged thereafter by launching \nothers. The QEI DIB was able to build on the learning from the Educate Girls, with the added \nbenefit that some actors were involved in both DIBs. The previous learning and experience \nfacilitated processes such as structuring the DIB, deciding what outcomes must be tracked \nand measured, and elaborating the legal framework, reducing the initial set up phase from a \nyear in the EducateGirls DIB to about six months in the QEI DIB.", " Nonetheless, there is a tension between reducing transaction costs and increasing the \nmodel\u2019s benefits. Transactions costs may be reduced to the extent that this limits the model\u2019s \nbenefits. According to one DIB practitioner, the DIB is an expensive tool, but striving to keep \ncost per beneficiary as low as possible by diluting the quality of services is not a wise option. \nA balance is needed between reducing transaction costs that do not directly link to the DIB \neffects, and focusing resources on those components that are expected to lead to the targeted \nDIB effects. However, this is not so clear cut. For example, extended discussions were cited \nas important to developing a shared understanding of the objectives of the impact bond. Also, \naddressing  the  legal  and  organisational  challenges  to  accommodate  an  outcome  based \ncontract  can  be  expensive,  but  taking  time  to  elaborate  an  organisational  structure  that \nenables outcome based contracting can bring benefits to the wider organisation.", " The  key  findings  are  set  out  below  mapped  against  the  five  issue  areas  of  designing  and \nagreeing  DIBs,  loosely  based  on  Gustafsson-Wright  et  al\u2019s  (2017)  categorisation.  The \nrelevance of these findings for DIBs will depend on the objectives of the specific DIB, which \nshould  be  used  to  guide  the  balance  between  reducing  costs  and  ensuring  the  DIB  is \nstructured so as to increase the model\u2019s benefits.", " 7.0  Lessons Below we set out the lessons of potential wider relevance for the design and set up phase of \nDIBs. These are split out against the DIB effects and different stages of designing and setting \nup DIBs. As discussed in section 3,   there is not yet a predominant design for DIBs, and it is \nperhaps  more  helpful  to  understand  DIBs  as  a  funding  class  within  which  there  is  great \nvariation. The precise structure and nature of the DIBs depend on the stakeholders involved, \ntheir objectives for using the DIB and the organisational and regulatory requirements in place. \nThese have implications for the DIB effects and for the process of design and set up phase. \nThis diversity must be borne in mind when taking stock of the lessons learned to date.", " DIB effects 1.  The DIB effects have varied across the four DIBs, though the majority of hypothesised DIB \neffects  were  noted  to  some  degree  in  at  least  one  out  of  the  four  DIBs.  The  strongest \npositive DIB effects have been that they have made it possible to implement PbR contracts \nin contexts where this would previously not have been possible, due to the creation of new \npartnerships and strong levels of collaboration. A large amount of work has been done in \nall four DIBs to build a strong performance management infrastructure. Additionally, across \nall four DIBs, the DIB was found to have enabled innovation and been complex to design.", " 2.  The DIB effect that varied most across the four DIBs was in terms of financial risk sharing \narrangements between the outcome funders, service providers and investors. Due to the \nnature of the stakeholders involved, the precise risk sharing varied significantly, with some \ninvestors taking on risk only related to the rate of return, ranging to investors taking on \n100% risk should targets not be met.", " 3.  There are limited examples where DIBs are bringing in private finance, and for the most \npart investors are philanthropic organisations. However, the impact bonds are substantially \nlarger, in terms of contract value and beneficiaries supported, than social impact bonds in \nhigh-income countries and compared to their predecessor DIBs.", " Identifying appropriate interventions 13. Transaction costs are lower if the DIB design is able to draw on existing evidence, reducing \nsome  of  the  costs  associated  with  designing  outcome  metrics  and  the  evidence  base \nrequired to determine pricing. However, the requirement for a strong evidence base may \nlimit the expansion of the DIB into new and innovative sectors.", " 14. The benefits of using the DIB model are the strongest when there is a value proposition to \nthe use of the DIB, whereby they resolve a specific challenge that cannot be addressed \nby other funding mechanisms. Many of the benefits of using the DIB model are similar to \nthe benefits  of  using  PbR.  However,  there  are  some  benefits  unique  to  the  DIB model, \nsuch as enabling service providers to participate in PbR without upfront capital, and the \ntendency  for  the  DIB  model  to  draw  in  a  wide  range  of  stakeholders  and  require  and \nsupport collaboration.", " Identifying metrics and structuring payments 15. Building a database of impact bond returns, outcome metrics and rate cards and drawing \non private sector expertise on pricing risk would facilitate the growing of the DIBs market. \nHowever, context specificity may limit the usefulness of standardisation and caution is also \nadvised in terms of developing rate cards, due to the early stage of the market and limited \ndata available.", " 16. Outcome metrics and targets work best when returns to investors and outcome funders, \nand respective incentives, are aligned. Developing outcome metrics and rate cards that \nare understood by all stakeholders and linked to other metrics within the sector/country \ncan increase the value of the learning generated, and also facilitate the broader DIB market \nand/or potential transition to a SIB. It is noted that there can be a tension between using a \nrobust model and using a less robust model that is aligned with measures used by others \nin the sector.", " Measuring impact 17. The validation process should be designed to meet the needs of stakeholders. Different \nconsiderations may apply to different contexts. We note that there can be an automatic \npreference to use experimental approaches or quasi-experimental approaches. However, \nwhere  an  intervention  or  certain  causal  links  are  sufficiently  backed  by  evidence,  there \nmay  be  less  value  in  using  experimental  or  quasi-experimental  methods  compared  to \nvalidated administrative data.", " Identifying and selecting stakeholders and managing relationships 18. Across three of the DIBs, it was challenging to engage outcome funders. There is a benefit \nto  identifying  outcome  funders  interested  in  using  outcome  based  contracting,  and  the \ntypes  of  interventions  they  are  interested  in  earlier  on,  and  recognising  that  outcome \nfunders  need  to  be  involved  in  the  design  of  the  DIB.  Identifying  outcome  funders  first \ncould also enable a competitive process for selecting service providers. On the other hand, \noutcome  funders  are  concerned  about  the  risks  of  getting  involved  with  a  new  funding \nmechanism, and it can be easier for outcome funders to get involved at a later stage, when \nthe other stakeholders have been identified and the terms are more developed.", " 19. Transaction costs for the design and set up stage can be reduced when there is strong \ncollaboration across stakeholders, drawing on each other\u2019s expertise and strengths; when \nroles are clearly defined from the start; when stakeholders are identified and brought in \nefficiently; and when there is the right balance between undertaking negotiations bilaterally \nand collaboratively.", " 20. Different  types  of  investors  and  outcome  funders  bring  different  types  of  benefits.  For \nexample,  commercial  investors  are  able  to  bring  in  more  experience  with  testing  and \nimplementing  financing  modalities,  while  philanthropic  investors  may  be  able  to  bring \nexperience  and  expertise  within  the  sector.    As  a  results,  careful  consideration  of  the \nobjectives of using the impact bond should be taken into account when identifying outcome \nfunders and investors.", " Structuring and developing the operating model 21. The  larger  number  of  stakeholders  involved  in  the  DIBs  to  date,  and  the  often  diverse \nlegislative  frameworks,  increase  the  transaction  costs  of  this  stage  of  the  DIB development,  due  to  the  larger  number  of  \u2018work-arounds\u2019  and  negotiations  required. \nFurthermore, contracting with different currencies introduces foreign exchange risk. The \noptimal  solution  would  be  to  amend  the  legislative  frameworks  to  accommodate  DIBs. \nWhere  this  is  not  possible,  other  potential  solutions  include  limiting  the  number  of \nstakeholders  involved,  considering  other  pooled  financing  or  funding  structures,  using \nother ways to minimise the number of contracts involved, or standardising deals.", " 8.0  Recommendations Recommendations are split into two categories: those applicable to all DIB stakeholders, and \nthose  particularly  relevant  to  DIB  designers.  This  report  covers  research  wave  1  of  the \nevaluation of a pilot programme. We also caveat that it must be borne in mind that these pilot \nprojects may also be affected by the increased attention and hence risk adverseness related \nto these projects being pilots. As a result, it is anticipated that these recommendations will be \nfurther refined over the course of the evaluation.", " Recommendations to all DIB stakeholders \u2022  Be transparent and share lessons learned and key successes and failures (including \nDIBs that failed to launch) to facilitate dissemination of learning across the sector \u2022  Make contracts, payment terms, feasibility studies, investor documents and learning documents publicly available; \u2022  Building a database on interest rates, outcome metrics and rate cards and drawing on \nprivate sector expertise on pricing risk would facilitate the growing of the DIBs market \u2022  Prioritise the documentation of lessons learned and evaluation, in order to facilitate the \ndevelopment of a more finely-grained understanding of what works, in what contexts.", " Recommendations to DIB designers \u2022  Clearly agree upfront the roles and responsibilities of all involved parties, including how these responsibilities may change depending on circumstances; \u2022  When  structuring  the  DIB,  ensure  that  the  contracts  and governance  arrangements have provisions for a range of potential eventualities; \u2022  Be clear about the objectives of using the DIB, and how the DIB is expected to resolve \na policy problem. Then, structure the DIB so it focuses on delivering the targeted DIB \neffects,  and  seek  to  reduce  transaction  costs  that  do  not  contribute  to  the  targeted \neffects  of  using  the  DIB.  Be  clear  what  is  needed  from  stakeholders,  including \ninvestors, outcome funders and advisors. This can affect whether hands-on or hands-\noff stakeholders are more appropriate.", " o  Based on the emerging evidence on the DIB effects and our review of failed \nDIBs, DIBs appear to be a worthwhile financing approach when it enables PbR \nin cases  where it  would  not  have  been  possible otherwise,  or  when the DIB \nmechanism  is  expected  to  bring  about  benefits  beyond  those  provided  by  a \nPbR approach. For example, this could be the involvement of an investor, or \nthe increased collaboration between a range of stakeholders within the sector.", " \u2022  Consider  carefully  the  number  and  types  of  stakeholders  involved,  as,  in  this  early \nstage of the market, complexity increases with the number of stakeholders. Consider solutions  to  reduce  this  complexity,  such  as  limiting  the  number  of  stakeholders \ninvolved or using contractual arrangements that simplify the processes required.", " \u2022  Develop outcome metrics and rate cards that are understood by all stakeholders and \nlinked to other metrics used in the sector / country, to increase the value of the learning \ngenerated, minimise the costs of data collection and facilitate the broader DIB market \nand/or potential transition to a SIB.", " \u2022  Collaboration is important to reducing transaction costs. Seek to draw on the expertise and experience of stakeholders within the DIB.", " Annex A: Case study reports The  case  studies  summarise  findings  from  consultations  undertaken  as  part  of  the  DFID \ncommissioned independent evaluation of the DIBs pilot programme. The case study reports \nfocus on the DIB model and early successes and lessons learned during the design and set \nup phase. Consultations were undertaken with the main stakeholders involved in the design \nand set up of the four DIBs. A full list of consultations is set out in the Annex H. Interviewees \nhave  been  given  the  opportunity  to  review  the  case  studies  and  rectify  the  findings  when \nneeded, and their feedback has been incorporated in the version of the case studies inserted \nbelow.", " A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12 A13 A14 A15 A16 A17 A18 A19 A20 A21 A22 A23 A24 A25 A26 A27 A28 A29 A30 A31 A32 A33 A34 A35 A36 A37 A38 A39 A40 A41 A42 A43 A44 A45 A46 A47 A48 A49 A50 A51 A52 A53 A54 A55 A56 A57 A58 A59 A60 A61 A62 A63 A64 A65 A66 A67 A68 A69 A70 A71 A72 A73 A74 A75 A76 A77 A78 A79 A80 Annex B: Terms of Reference Terms of Reference \nIndependent Evaluation of the Development Impact Bonds (DIBs) Pilot programme Purpose of Evaluation The primary purpose of the evaluation is to generate learning and recommendations that could \ninform decisions on the future use of DIBs as an instrument for aid delivery. The evaluation \nwill cover all three projects under the DFID-supported DIBs Pilot programme.", " In  particular,  this  evaluation  is  expected  to  generate  learning  that  will  inform  DFID\u2019s  future \npolicy  aiming  to  make  the  most  effective  use  of  DIBs  as  we  look  to  commission  new \ninstruments, or incorporate DIBs and similar structures into existing programmes.", " The evaluation will also help DFID and pilot project partners evaluate whether the tools they \nare developing are useful, scalable and replicable.", " B.1  Background and Context Programme Context. DIBs are a new mechanism for financing development programmes. \nDFID has been piloting DIBs in order to assess the costs and benefits of using DIBs compared \nto other mechanisms, and the conditions that make DIBs a suitable mechanism and enable \nDIBs to work best.", " What is a DIB? A DIB is a mechanism for drawing external finance into payment-by-results \n(PbR) projects. In a DIB a donor commits to paying for development results if and when they \nare achieved (donors are often referred to as \u201coutcome funders\u201d). A service provider steps up \nto deliver the prescribed results. The key difference from standard PbR is that a DIB brings in \nthird party \u201cinvestors\u201d (public or private organisations) who provide the service provider with \nthe investment/working capital needed to deliver results. Under the DIB model, therefore, the \ninvestor takes on a portion of the financial risk associated with failing to deliver the prescribed \noutcomes \u2013 if outcomes are not delivered, the outcome funder does not pay and the investor \ncan lose their investment. If the project delivers more results than expected, the investor can \nmake a return.", " Theory  of  Change for how the  DIB model  can drive better  outcomes?  The  DIB model \naims to improve the efficiency and cost-effectiveness of development programmes. In theory \nthe  DIB  design  process  and  structure  helps  align  and  increase  stakeholders\u2019  focus  on \nachieving the desired outcome. The involvement of investors enables: \u2713  Donors  to  use  pbr  incentives  that  work  to  increase  focus  on  the  end  result  and  on performance management, while \u2713  Enabling  a  wider  range  of  service  provider  organisations  to  take  on  pbr  contracts \n(many would otherwise struggle because they do not have access to sufficient working \ncapital); and A81 \u2713  Giving  service  providers  more  flexibility  and  building  capability  to  adapt,  course \ncorrect,  and  innovate  their  service  delivery  models  (e.g.  Through  working  with \ninvestors  to  build  performance  management  systems,  or  because  the  provider  is \nenabled to take innovation risk because the investor carries the financial risk).", " See Annex A1 for DFID Theory of Change for DIBs B.2  What do we mean by other aid mechanisms?", " Alternative aid mechanisms used by donors (e.g. outcome funders such as DFID and other \ndevelopment  partners)  include  grants  to  not  for  profit  organisations  and  pay  for  services \ncontracts where the provider is paid in alignment with the inputs/activities they are delivering \nto achieve the desired programme outcomes, as well as pay for results contracts where the \nprovider  is  paid  only  after  they  have  delivered  pre-agreed  results.    In  some  circumstances \nthese  aid  mechanisms  may  have  limitations.  There  is  extensive  literature  on  these \nconsiderations. The table highlights some of these considerations: Table B.1: Alternative aid mechanism Alternative aid mechanism Possible limitations Grants and pay for services contracts Under  these  funding  models  the  donor  will  pay  the \nprovider  for  the  inputs  and  activities  they  deliver  in \naccordance  with  the  providers  agreed  programme  of \nwork. In situations where the outcome funder is uncertain \nabout the right mix of inputs / activities needed to achieve \nthe outcome efficiently (e.g. due to a lack of evidence), the \ndonor  is  accepting  the  risk  that  the  activities  and  inputs \npaid for may not achieve the desired outcome. \nDuring  the  life  of  the  grant,  providers  may  have  fewer \nincentives  to  identify  the  most  efficient  approach  to \nachieving the outcome and to cut less efficient/ineffective \ninputs. \nThis risk can be reduced through additional investments \nby  the  donor,  e.g.  in  real  time  data  gathering,  to  help \nidentify what is/isn\u2019t working.", " research indicates Payment  by  Results  approaches  enable  donors  to \ntransfer the risk/uncertainty over whether an intervention \nwill achieve results to the provider.  \nHowever, \nthat  some  providers \n(particularly  those  with  smaller  balance  sheets,  or  less \naccess to commercial loans) would be unable pre-finance \ntheir  intervention  and  wait  for  payment  on  delivery  of \nresults, or would be unwilling to take on the financial risk \nassociated with underperforming on a PbR contract. As a \nresult providers that may be most capable of achieving the Pay for Results approaches A82 outcomes  may  not  be  able  to  take  on  these  types  of \ncontracts.3233 B.3  How strong is the evidence on DIBs?", " DIBs  are  a  new  tool  for  delivering  development  projects.  Prior  to  the  DFID  DIBs  pilot \nprogramme only two DIBs (the Educate Girls DIB in India, and Rainforest UK\u2019s DIB in coffee \nand cocoa production in Peru) have been implemented, both are very small. Existing evidence \non DIBs is therefore limited.", " However, DIBs are part of a wider impact bond family \u2013 originating from social impact bonds \n(SIBs) used domestically by governments to commission public services. To-date, over 60 \nsocial impact bonds have been commissioned. The UK is a leader in the SIB market, with \n32 SIBs. Governments in the US, Netherlands, Belgium, Germany and South Africa have \nalso made use of the instrument.", " A qualitative review of thirty-eight existing impact bonds by the Brookings Institute (2015) \nfound the following (more detail is included in DFID Business case): \u2022 \u2022 \u2022 \u2022  Existing impact bonds have focused on specific sectors: areas where government is already contracting third parties to deliver services and where service \ninputs are complex, but outcome are simple to measure \nImpact bonds can improve service delivery but deals so far have been \ncomplex \u2022  Deals have varied in terms of their structure, mechanics and stakeholder roles \n\u2022  Rigorous experimental or quasi-experimental evaluation was not always necessary for measuring impact and triggering payment \nImpact bonds lead to a shift in focus to outcomes: the study found that existing \nSIBs encouraged transparency and accountability in commissioning public services. \nInstead of paying for services, government pays for outcomes. At the same time, \nSIBs push providers to deliver on these outcomes. \nImpact bonds drive performance management: Bringing private sector mentality \ninto the provision of services can lead to more efficient and effective delivery of \nsocial services. This has been mainly seen through the push toward outcome \nachievement and fidelity to the intervention delivery model and less in terms of \nadaptation of service provision along the way.", " \u2022  The impact bond mechanism stimulates collaboration: this applies to all parties involved in impact bonds.", " 32  National  Audit  Office  (2015).  Outcome-based  payment  schemes:  government\u2019s  use  of  payment  by  results \nhttps://www.nao.org.uk/wp-content/uploads/2015/06/Outcome-based-payment-schemes-governments-use-of-\npayment-by-results.pdf  \n33 Sherene Chinfatt and Melissa Carson (2017)  Supplier Access to Prefinance in Payment by Results Contracts. \nDalberg \nhttps://www.gov.uk/dfid-research-outputs/supplier-access-to-prefinance-in-payment-by-\nresults-contracts Intelligence A83 \u2022 \u2022 Impact bonds have enabled the development of strong monitoring and \nevaluation systems: the impact bond mechanism incentivises evidence collection \nand can therefore lead to improving outcomes for service users through identifying \ninterventions that work.  \nImpact bonds can shift the focus of government toward preventive services: \nthis could have economic implications for government and society While implementing impact bonds in a development context brings specific challenges and \nwe have to be mindful that the portfolio of SIBs projects target different outcomes, emerging \nevidence  on  SIBs  shows  that  the  impact  bond  mechanism  has  the  potential  to  improve \neffectiveness and efficiency of outcome delivery, and generate valuable impact evidence.", " B.4  What is the DFID DIBs pilot programme?", " DFID  has  designed  a  programme  to  pilot  the  DIBs  mechanism  and  assess  the  costs  and \nbenefits of using DIBs, and the conditions needed for a DIB to be an appropriate programme \nfinancing tool.", " In line with the Paris Principles, the DFID pilot programme consciously works with other donors \nwho are considering DIBs and aims to deliver an evaluation that generates learning that is \nuseful  for  donors  and  service  providers  considering  DIBs  as  a  funding  mechanism,  The \nevaluation  questions  have  been \nthese \nstakeholders, and representatives of these stakeholders will be included in the steering group \nfor this evaluation (see governance section).", " through  DFIDs  engagement  with informed Under the pilot programme DFID is funding three DIB projects, each in a different way. The \nevaluation aims to draw out and synthesise learning about the DIBs mechanism from these \nprojects, while recognising the wider context of Social and Development Impact Bonds.", " The  table  below  summarises  the  three  DFID  supported  DIB  projects.  More  detail  on  each \nproject as well as a Gantt chart showing the activities and timeline for each project and the \nDFID programme overall are provided in Annex C & Annex D.", " A84 At the programme design stage DFID recognised that it would be difficult to directly compare \neffects of the DIBs mechanism with other aid mechanisms34. However, each of the DIB pilot \nprojects  will  be  delivered  by  service  providers  that  have  significant  experience  of  running \nsimilar  interventions  under  different  funding  mechanisms  such  as  core-funding  or  private \nphilanthropic grants. Where available, data on their interventions\u2019 performance could provide \nsome comparisons on programme delivery and performance/cost-effectiveness.", " B.5  Users of the Evaluation The primary user of the evaluation will be the DFID DIBs team, who will use the findings to \ninform DFID\u2019s future application of the impact bond mechanism. We want the evaluation to \ndeliver  early  findings  regarding  the  structuring  and  design  of  Pilot  DIBs  \u2013  this  will  help  us \nassess options for tailoring the mechanism to ensure value for money. For example, we will \nconsider whether DIBs should be commissioned directly at a larger scale, or incorporated into \nprogrammes  that  intend  to  use  PbR  structures.  Later  evaluation  findings  on  how  DIBs  are \nmanaged  and  how  they  affect  the  performance  of  service  providers  will  help  us  improve \ninteraction with project managers, service providers and investors throughout the project life \ncycle. These findings will also continue to inform how and when we use DIBs, and how the \ndesign, commissioning and management of DIBs can continue to be improved to deliver ever \nincreasing value for money.", " Secondary  users  of the learning generated  by  the evaluation  will  be  organisations that are \nusing or thinking about using impact bonds or similar approaches to financing development \nlocal  and  national \ninclude  outcome \nprogrammes.  Such  organisations \ngovernments in developing countries as well as public and private donors who want to achieve \nresults  for  a  given  population),  investors  (private  and  public  sector  organisations  that  are \nwilling to pre-finance social impact projects in developing countries and be repaid on a pay-\nfor-success basis), and service providers (NGOs, charities, social enterprises, private sector \norganisations that deliver services to achieve development outcomes). They will benefit from \nthe  findings  produced  by  the  evaluation,  and the  practical  recommendations  it  contains  for \nusing DIBs and DIB-like structures in the future. Please see governance section for how users \nare represented or engaged in the evaluation.", " funders  (i.e.", " B.5.1 Evaluation Purpose and Questions The table below sets out the Key Evaluation Questions, their purpose, and some proposed \nsubsidiary evaluation questions mapped to a proposed timeline for obtaining learning.", " The 2 Key Evaluation Questions are: \u2022  EQ1: Assess how the DIB model affects the design, delivery, performance and effectiveness of development interventions.", " 34 For example, input based grants and pay for service contracts or standard payment by results.", " A85 \u2022  EQ 2: What improvements can be made to the process of designing and agreeing \nDIBs  to  increase  the  model\u2019s  benefits  and  reduce  the  associated  transaction \ncosts?", " When reading the table below, please see the Evaluation Outputs Section for the proposed \ncontent of each \u2018Evaluation Output (EO)\u2019 referenced in the table.", " The  OECD-DAC  criteria  on  relevance,  efficiency  and  effectiveness  are  relevant  to  this \nevaluation.  The  evaluation  focuses  on  the  DIB  funding  mechanism,  and  the  process  of \ndesigning DIBs including the relevance and efficiency of the activities involved in designing, \nlaunching and managing a project using a DIBs model for the various stakeholders in the DIB; \nand assesses how the DIB model improves (if at all) the performance and effectiveness of \ndevelopment  programmes  in  terms  of  achieving  results  efficiently.  The  evaluation  should \nconsider  how  the  DIB  model  takes  into  account  cross-cutting  areas  that  mean  some \nbeneficiaries are more vulnerable or harder to reach (e.g. due to disability, power relations, \nenvironment, gender, poverty).", " A86 Evaluation Questions Table \u2013 mapped to the purpose of the evaluation, key Evaluation questions, proposed subsidiary questions, evaluation \noutputs, and potential data sources Purpose of Evaluation Key Evaluation Questions EQ1: Assess how the DIB \nmodel affects the design, \ndelivery, performance and \neffectiveness of \ndevelopment \ninterventions.", " To confirm whether the DIB model \nactually improves performance and \neffectiveness of development \nprogrammes, covering factors, such as: \n-  Enabling outcome funders to use PbR with more providers \n-  Changing incentives of the stakeholders -  Increasing focus on desired outcome, and managing for results -  Transferring delivery risk from outcome funder to provider/investor \n-  Role of investors, outcome funders \nand service providers in design and \ndelivery of intervention -  Incentive structure encourages provider fidelity to implementation of \nactivities that works -  Increased flexibility/ autonomy for providers enabling more innovation \nin service delivery to improve \nperformance/ results -  Service provider is incentivised to deliver for the whole cohort \u2013 despite \ncohort having differing vulnerabilities \n&/or capabilities We want to produce shared learning \nfrom across the 3 DFID funded DIB \nprojects which should serve as case \nstudies.", " Proposed Subsidiary \nEvaluation Questions Findings should be produced for \nfollowing Evaluation Outputs \n(EO): Possible data collection \nmethods and data sources 1.1 How does the DIB \nmodel affect key \nstakeholders including \nservice providers, outcome \nfunders, investors, \nbeneficiaries, and what are \nthe reasons behind the \neffects  \n \n1.2 can we say anything \nabout the sustainability of \nthe effects on \nstakeholders?", " 1.3 Which factors in a DIB \nare most important in \nimproving the performance \nof a development \nprogramme, if at all, in \nterms of achieving results \nefficiently?", " EO1 \u2013 Design Report: should \ninclude an enhanced theory of \nchange for how DIBs improve \nprogrammes. \n \nEO2 \u2013 Report on process of \ndesigning and launching DIBs \nincl. findings on effect of DIB \ndesign process on DIB \nstakeholders \n \nEO3 \u2013 Mid-Term Evaluation \nReport: on emerging findings \n \nEO4 \u2013 Final Evaluation Report EO3 \u2013 Mid-Term Evaluation \nReport: on emerging findings \u2013 \nthere will be some interim \noutcome results and payments \nfor 2 of 3 projects. \n \nEO4 \u2013 Final Evaluation: final \nfindings after project outcomes \nhave been verified.", " 1.4  How does the \nperformance and \neffectiveness35 of \ndevelopment programmes \nfinanced using a DIB \nmechanism compare with \nproviders\u2019 experience of EO4 \u2013 Final Evaluation Report:  \nproduced after project outcome \nresults have been verified. \n \nEO3 \u2013 Mid Term Evaluation \nReport if evaluator is able to \ndraw some initial conclusions Methods: Mostly qualitative. \nQuantitative methods could be \nconsidered for beneficiaries. \nSources: Access to \nstakeholders in the DFID \nfunded DIBs; quarterly/ \n6monthly project progress \nreports, internal monitoring \ndata; project level process \nreview/evaluation activities \nfocused on project \nimplementation and DIB model. \nSee Data Annex for more \ndetail.", " Methods: Qualitative  \nSources: As above + access to \nthe data used to verify if the \ndesired programme outcomes \nhave been achieved. See Data \nAnnex for which outcomes will \nhave been measured by \nexpected Mid Term and Final \nEvaluation Report dates.", " Methods: Qualitative \nSources: As above + access to \npast performance data for at \nleast 2 of the 3 DIB projects \n(ICRC & VE) \u2013 including past \ncost & effect data for same A87 providers, delivering similar \ninterventions in similar contexts.", " Methods: Qualitative  \nSources: As above + access to \nprogramme design documents; \nand project level process \nreview/ evaluation activities \nfocused on design and \nimplementation of DIB projects \n\u2013 including service provider \nselection, outcome funder \nengagement, metric selection.", " EO2 \u2013 Evaluation Report on the \nProcess of designing and \nlaunching DIBs \u2013 should include \nfindings under this evaluation \nquestion \n \nEO3&4 \u2013 continue to make \nrecommendations to improve \nprocess of commissioning and \nstructuring DIBs based on \nlessons that emerge as the DIB \nproject continue and complete \ntheir implementation phase.", " EQ 2: What improvements \ncan be made to the \nprocess of designing and \nagreeing DIBs to increase \nthe model\u2019s benefits and \nreduce the associated \ntransaction costs?", " DFID and others are interested to use \nDIBs and similar financing models in \nthe future. However, we need process \nof commissioning DIBs to be more \nefficient, accessible to more providers, \nfunders and investors, and less costly. \nStakeholders need a roadmap for an \nimproved/optimal design process \u2013 \ncovering the necessary conditions (e.g. \nprojects attributes, stakeholders \nattributes) for DIBs to be suitable; key \ntools; and the roles of stakeholders at \ndifferent design stages.", " other funding mechanisms \nin terms of efficiency and \nresults?", " 2.1 Under what conditions \nare DIBs an appropriate \ntool for the key \nstakeholders (outcome \nfunders, investors, service \nproviders, beneficiaries), \nand why? \n \n2.2 How can we improve \nthe design process to \nproduce DIBs that \nmaximise the benefits for \nstakeholders (outcome \nfunders, investors, service \nproviders, beneficiaries) \nwhile reducing transaction \ncosts? Including making \nthe design process more \nefficient and accessible to \nmore service providers, \noutcome funders and \ninvestors.", " A88 35 \u201cEffectiveness\u201d means the OECD DAC criteria of Effectiveness \u2013 A measure of the extent to which an aid activity attains (or is likely to attain) its objectives.", " DFID completed  an  evaluability  assessment  ahead  of  developing this Terms  of  Reference. \nThe evaluability assessment produced a useful framework that articulates the assumptions for \nhow  the  DIB  model  can  improve  the  performance  and  cost-effectiveness  of  development \nprogrammes, and provides some evaluative questions. This is included in Annex A2 to this \nToR, and may be useful to the evaluator in envisaging the breadth and depth of assumptions \nto be tested through the evaluation.", " There is also an opportunity for DFID and the evaluation supplier to develop a DIB evaluation \nframework that helps other stakeholders who will use impact bonds in the future and have the \nopportunity  to  commission  parallel  learning  activities,  to  encourage  the  building  or  a  larger \nbody of evidence that can be synthesised.", " The evaluation questions above supersede the evaluation questions and framework set out in \nthe DIBs Pilot programme Business Case (see \u2018Documents/References\u2019 section for link to the \nBusiness Case).", " Scope of the Evaluation The focus of the evaluation is the DIBs funding mechanism. The evaluation is intended \nto evaluate the impact bond mechanism and its effect on how the intervention was delivered, \nand the results produced by the intervention.", " The evaluation should focus on the three DIB pilot projects that DFID is supporting. Based on \nthe  scope  of  the  evaluation  questions/objectives  above,  we  expect  that  the  evaluation  will \ninclude \u2022  A retrospective review of the process of selecting interventions and structuring the dibs to inform first evaluation report in 2018, \u2022  Collection and analysis of the costs of different stages,  \n\u2022  Consideration of the appropriateness of the outcome targets and payment mechanism, \n\u2022  Analysis of the roles and engagement of different stakeholders throughout the lifecycle of the DIB.", " Country coverage: DFID does not require the evaluator to visit all project countries \u2013 it is up \nthe evaluator to specify the field activities that are necessary to deliver the requirements of \nthis evaluation efficiently. For information, the three DIB pilot projects are delivering activities \nin multiple countries: Village Enterprise is in Kenya & Uganda; the Education DIB is in Gujarat \nand Delhi; and the ICRC HIB programme is managed from ICRC HQ in Geneva, but involves \nthe building and running of new rehabilitation centres in Mali, Nigeria, and DRC). The wider \nstakeholders involved in each DIB (funders, investors) are based in Europe (mainly UK and \nGeneva) and the Americas (Canada, US, Colombia) and are easily contactable via phone and \nvideoconference. It is possible that some of the stakeholders in each project will come together \nfor project review meetings and broader DIBs market/knowledge sharing events.", " Linkages  to  other  relevant  projects:    The  evaluator  is  expected  to  review  work  that  is \nhappening in the DIBs field more generally so that we can draw on learning outside of the 3 \npilot projects DFID is supporting. A number of other impact bonds are in design, have halted A89 design,  or  are  reaching  implementation  stage (see  Brookings  Report)36.  These  include,  for \nexample, a new poverty graduation Impact Bond in Mexico, the Educate Girls DIB aiming to \nimprove  girls\u2019  learning  outcomes  in  Rajasthan,  and  the  Maternal  Health  Impact  Bond  in \nRajasthan. These projects are considering including learning activities that consider the role \nof the funding mechanism.", " DIBs by design include an evaluation or verification of the outcomes/ impact as defined in the \npayment  conditions  of  each  DIB.  Therefore  there  is  no  need  for  a  standard  impact \nevaluation to assess whether the desired outcomes of each intervention were achieved. \nThe  evaluation  should  note  that  none  of  the  DFID  pilot  DIBs  include  current  project  level \nevaluation activities that assess \u201chow\u201d the particular intervention or its components achieved \nthe measured outcomes.", " Relevant project level learning activities: A range of learning activities are planned for each \nDIB, focused on the DIB design process and the effects of using the DIB model. The supplier \nwill therefore be required to work with learning providers to take advantage of any synergies \n(see Ways of Working and Annex C).", " B.6  Evaluation Methodology It  is  the  responsibility  of  the  Supplier  to  propose  an  evaluation  methodology.  The \nsupplier  should  propose  an  evaluation  approach  and  methods  that  are  best  able  to  meet \nDFID\u2019s  evaluation  purpose,  objectives,  questions  and  timelines  DFID  does  not  have  a \npreferred approach or data collection method. DFID expects the supplier to make their causal \nreasoning explicit in their evaluation reports.  \n \nWhen assessing the evaluability of the programme, DFID felt that experimental designs for \nassessing the effectiveness of the DIB mechanism would be difficult to implement given the \nstructure of the programme, and that most of the DIB projects have started implementation. \nWe  also  recognise  that  these  are  3  different  projects,  and  the  evaluation  will  only  provide \nindicative  learning/evidence,  potentially  identifying  some  commonalities  across  the  three \nprojects, but not generating evidence that can be generalised. \n \nA key risk associated with the novel nature of these projects is that various evaluation and \nlearning activities are planned within each project and for the sector overall. Engaging with all \nthe  activities is  onerous for the  project  stakeholders,  particularly  service providers  who  are \nalso focused on implementing effective programmes.  \n \nAs far as possible, the evaluation supplier should work to avoid duplicating learning activities \nthat  are  being  completed  under  each  programme.  In  the  interests  of  transparency  and \nefficiency, the evaluator should consider where it can reasonably collaborate with project level \nlearning providers to leverage the data and learning outputs they are producing, in order to \nsynthesise  evidence  across  the  three  DFID  DIBs  pilots  and  non-DFID  impact  bonds  as \nopposed to repeating data collection activities.", " 36 https://www.brookings.edu/wp-content/uploads/2017/09/impact-bonds-in-developing-countries_web.pdf A90 To  provide  confidence  in  the  findings,  it  is  important  that  the  evaluation  supplier  uses  an \napproach  that  enables  them  to  provide  an  independent  and  unbiased  perspective  when \nanswering the evaluation questions, but we also believe this does not remove the option for \nthe  supplier  to  collaborate  and  leverage  programme  level  learning  activities,  for  example \nthrough using data already generated in DIBs (e.g. budgets, activity costings, outcomes data, \nprocess  reviews  occurring  under  some  of  the  projects  that  include  document  reviews  and \ninterviews  with  project  level  stakeholders  on  the  process  of  designing,  engaging  with  and \nimplementing a project on a DIB basis). Our focus is on generating and disseminating relevant \nand reliable learning to inform future practice. \n \nAs part of their tender, Bidders are expected to set out their proposed evaluation approach \nand methods, an evaluation framework and demonstrate how this is best able to meet DFID\u2019s \nevaluation purpose objectives, questions and timelines. Bidders should explain the limitations \nand risks of their proposed approach and methods \u2013 and how these will be managed. Bidders \nshould  also  explain  what  data  they  will  rely  on  and  collect.  There  is  scope  for  bidders/ \nevaluation supplier to propose amendments or suggestions to the evaluation questions, and \nto work with DFID to refine the evaluation questions further during the inception phase.  The \nbidder is expected to clearly define the supply chain utilised in delivering this evaluation and \nthat sufficient due diligence has taken place.", " B.7  Data Sources Annex C includes a table summarising the types of data that is expected to be made available \nby service providers and other parties to the DIB, and lists the key stakeholders in each DIB.", " Access to key-stakeholders: DFID will facilitate access to the key stakeholders and decision \nmakers  in each DIB (service provider,  other  outcome funders,  outcomes verification agent, \nproject managers and project level process evaluators \u2013 as named in Annex C). Further these \npartners are willing to share with the supplier their process data, performance management \ndata, and qualitative data, such as beneficiary feedback, subject only to privacy concerns and \nprovided that doing so does not place an undue financial burden on providers. DFID will try to \nfacilitate  access  to  investors,  but  evaluators  should  note  that  DFID  does  not  have  a  direct \nrelationship with any of the investors, and the investors have not formally committed to share \ntheir data. The location of the stakeholders is also included in Annex C.", " Outcome Funder Management information: DFID is able to provide programme documents \nincluding: business case; memos explaining decisions to fund each pilot DIB; a record of the \nproject  appraisal  process,  negotiations,  and  decisions  taken  during  the negotiation of  each \nDIB; as well as project monitoring reports received from each DIB partner. We are aware that \nother outcome funders have similar project approval memos (but cannot guarantee access to \nthese documents).", " DFID can also facilitate the Supplier to connect with other organisations that are using impact \nbonds e.g. key stakeholders in the Mexican Poverty Graduation Impact Bond, the Maternal \nHealth DIB in Rajasthan, Educate Girls DIB and others, depending on need.", " A91 The Evaluation Supplier should not expect the DIB project service providers to provide all the \ndata  that  they  may  desire  in  the  following  categories:  beneficiary  feedback,  unintended \noutcomes, long-term results.", " Evaluation Activities DFID expects bidders to propose in their bids the activities that they think are necessary to \nmeet the evaluation objectives and answer the evaluation questions. DFID expects that the \nactivities would include, but would not be limited to: Initial planning and consultation \u2022 \n\u2022  Evaluation  design.  The  overall  technical  approach  and  design  for  the  evaluation \nshould be clearly explained along with reasons for choosing the proposed design \ninstead of other possible designs.", " \u2022  Desk review of work that is happening in the field  that we can learn from (including \nexisting research and evaluation of development and social impact bonds) so as to \ndraw on learning outside of the DFID DIBs Pilot programme \u2022  Design of data collection instruments (which should be reviewed by DFID) \n\u2022  Data collection. Proposal should specify how qualitative and quantitative methods \n(if  proposed)  are  going  to  be  used  together  in  a  complimenting  fashion.  The \nmethods and scope of data collection should be supported with clear arguments for \nneed. Mechanisms for ensuring quality of data should be included in the proposal. \n\u2022  Analysis  and  reporting.  Details  should  be  provided  on  how  the  analysis  will  be conducted, especially if mostly qualitative methods are used.", " \u2022  Activities  associated  with  a  process  evaluation  of  the  DIBs  Pilots  and  the  DIB \nprogramme  over  their  lifetime,  including  documenting  relevant  processes  where \nthis is not otherwise being done \u2022  As  far  as  possible,  the  supplier  is  expected  to  collaborate  with  the  pilot  project \npartners and work to use the data being generated by each pilot and their dedicated \nlearning  activities.  This  is  to  avoid  stakeholder  fatigue  or  mounting  costs  of \nengaging with various learning activities and to minimise duplication of effort. The \nevaluator is still expected to generate independent findings. During inception, clear \nlines  of  responsibility  will  need  to  be  drawn  to  ensure  the  independence  of  the \nevaluation is maintained.", " \u2022  The evaluation design and implementation must meet standard ethical practices.", " Bidders should set out how they will deliver these activities in their proposals, and over \nwhat timeline, demonstrating the best value for money approach to deliver the evaluation \nwhile minimising costs.", " A92 B.8  Evaluation Outputs and Timeframe The Evaluator is expected to produce the following evaluation outputs (\u201cEO\u201d). Each output will \nbe reviewed by DFID\u2019s Evaluation Management Team, the Evaluation Steering Group, and \nthe DFID\u2019s independent evaluation quality assurance service. It will be accepted if it covers \nthe  required  content,  evaluation  questions  and  scope,  and  is  designed,  implemented  and \nwritten to a good or excellent quality \u2013 as assessed by DFID\u2019s evaluation quality assurance \ncriteria.  The  evaluator  will  also  be  expected  to  submit  evaluation  instruments  for  quality \nassurance before starting data collection activities.", " Table B.2 EO 1: Inception Report EO 1: Inception Report by 1 June 2018 (close of business) Expected Content \u2022  The  Supplier  is  expected  to  set  out  the  design  of  the \nevaluation in their bid. They will then have the opportunity to \nadd further detail or make adjustments during the inception \nphase.", " that  confirms the  evaluation  questions \u2022  The  inception  report  should  include  a  detailed  Evaluation \nDesign \nto  be \nanswered,  the  methodology,  analytical  plan,  final  staff \nresource allocation, work plan, timeline and milestones \n\u2022  The Report should include an updated Evaluation Framework \nfor  evaluating  Development  Impact  Bonds,  and  a  theory  of \nchange for how DIBs improve development programmes. \n\u2022  The Supplier should explain how they will leverage existing \nlearning and evidence generation activities that are planned \nat the DIBs pilot project level \u2013 and how this will result in an \nefficient and cost-effective evaluation.", " \u2022  The  design  report  should  also  include  the  instruments  that \nthe evaluator will use in upcoming evaluation activities e.g. to \nproduce first evaluation report.", " \u2022  The report should also include an updated financial plan for \nthe  evaluation  \u2013  including  highlighting  any  savings  that  are \npossible  following  detailed  design  phase  and  engagement \nwith project level learning providers.", " \u2022  The evaluation design must meet standard ethical practices \nand should have been subject to the supplier\u2019s internal quality \nassurance process before submission. \n\u2022  A brief evaluation communications plan Table B.3: EO2 \u2013 Evaluation Report on the Process of designing and launching DIBs EO2 \u2013 Evaluation Report on the Process of designing and launching DIBs  \nby 17 September 2018 (emerging findings sooner if possible) Expected Content \u2022  This report will provide early feedback on process of selecting \nand  structuring  DIBs  to  inform  potential  expansion  of  DFID\u2019s \nDIBs programme.", " \u2022  This  should  include  estimates  of  the  costs  involved  in  the \nfeasibility and structuring stages of the DIB for all parties. \nIt  should  make  recommendations  on  the  conditions  that  are \nneeded for DIBs to be suitable, and recommend possible ways \u2022 A93 Table B.4: EO3 \u2013  Mid-Term Evaluation Report on DIBs EO3 \u2013  Mid-Term Evaluation Report on DIBs by 30 September 2020 Expected Content to reduce costs in the design, structuring, and implementation \nof DIBs.", " \u2022  The  supplier  should  plan to  deliver  an initial findings presentation by 30 August 2018 \u2022  This  report  is  expected  to  answer  most  of  the  evaluation \nquestions, by drawing out emerging lessons from the DFID \nDIBs pilot projects,  as  well as from evidence  generated by \nother  DIBs.  By  this  time,  two  of  the  DFID  supported  DIBs \npilots (Village Enterprise, and BAT Education Impact Bond) \nwill  be  measuring  outcomes  that  may  trigger  interim \noutcome-tied payments.", " \u2022  The report should pay particular attention to whether there is \nany  evidence of perverse incentives being created through \nthe DIBs. \nIt may not be possible to comment on the sustainability of the \nbenefits at this time.", " \u2022 \u2022  The  report  should  include  individual  case-study  report  / \nbriefing  on  each  of  the  three  DFID  supported  DIB  pilot \nprojects  \u2013  drawing  out  findings  for  each  DIB,  noting  any \nrelevant \nsignificant  changes \nperformance management information and lessons learned.", " implementation,  and in \u2022  The  Final  Report  should  cover  the  full  scope  of  the \nevaluation as set out in this TOR, unless any adjustments \nto the scope have been agreed with DFID.", " \u2022  The  report  should  summarise  the  lessons  from  the  DIBs \npilots  and  DFID  pilot  programme,  with  disaggregated \nreports by project where applicable.", " \u2022  The  report  should  comment  on  the  sustainability  of \noutcomes  post-intervention.  For  this  reason,  we  propose \nthat this final report should be completed at least 6 months \nafter the ending of each DIB. [See Annex D Gantt Chart for \nanticipated DIB Pilot project timelines] \u2022  The  Final  Report  should  include  case-study  reports  for \neach  of  the  DFID  supported  DIB  pilot  projects  \u2013  drawing \nout findings for each DIB against the evaluation framework, \nsummarise the overall costs and benefits of each DIB, and \ncommenting on the sustainability of the results achieved, \nand the lessons learned.", " Table B.5: EO4 \u2013 Final Evaluation Report on DIBs EO4 \u2013 Final Evaluation Report on DIBs by 30 January 2023 Expected Content A94 Each of the Evaluation Reports above is expected to conform to key content standards: \u2022  An Executive Summary of 1-4 pages \n\u2022  A methodological section detailing the evaluation design and methods and how the \napproach  covered  all  aspects  of  the  terms  of  reference.  This  section  should  also \nhighlight any constraints and how these were overcome \u2022  Terms  of  reference,  and  explanation  of  any  deviation  from  the  tor  that  has  been agreed by DFID \u2022  List of people consulted / interviewed at different stages of the evaluation (check that \npeople are happy to be listed and/or any reason why names should not be listed) \u2022  List of documents reviewed \n\u2022  Key findings that clearly follow from the evidence \n\u2022  Relevant,  useful  and  implementable  recommendations  based  on  the  evaluation \u2022  Evaluation outputs should provide clear findings and practical recommendations for \nDFID  and  other  stakeholders  on  ways  we  can  develop  and  improve  the  DIB \nmechanism to drive innovation and value for money in development programmes. \n\u2022  DFID\u2019s  standard  evaluation  report template  represents good  practice for evaluation \u2022  Supplier  will  need  to  build  in  time  to  respond  to  any  comments  following  the  DFID findings report review process B.9  Lighter-Touch Interim Outputs It is important that emerging findings inform the rapidly evolving landscape of Development \nImpact  Bonds  and  similar  impact-focused  instruments,  in  particular  DFID  and  other \nStakeholder\u2019s use of them. \n \n\u2022  Annual  Briefings:  The  evaluation  Supplier  is  expected  to  provide  DFID  and  the \nEvaluation  Steering  Group  with  an  annual  briefing  (a  power-point  presentation  or  short \nreport) on the evaluation\u2019s progress, and setting out the next year\u2019s evaluation activities & \ntimelines. Where appropriate, the briefing should highlight any learning or findings from \nthe past year\u2019s evaluation activities (if there were any, and have not already been covered \nin an Evaluation Output) \u2013 helping the findings inform stakeholders earlier. This should be \na low cost activity, not requiring any additional evaluation activities by the supplier. The \nevaluator is not expected to conduct evaluation activities every year. The opportunity to \nhighlight findings will depend on the evaluation design proposed, and annual briefings may \nbe limited to updating stakeholders on evaluation activities.", " \u2022  Evidence Webinars: In their bid the evaluation provider should plan for a short 2 hour \nwebinar and presentation that would help disseminate the findings from each Evaluation \nReport / output. The supplier would be expected to present at the event and respond to \nquestions from the audience. DFID would coordinate each event and invite the relevant \naudience members. The supplier should anticipate that the webinar would be run first for \nthe Evaluation Steering Group (during review of each Evaluation Report), and potentially A95 then re-run or recorded for a wider audience of stakeholders interested in DIBs and similar \nmechanisms.", " Contract Duration, Contact Adaptability and Break Points The evaluation should get underway as soon as possible, with the ideal start date being 1 April \n2018, and will last until March 2023 to allow all outputs to be produced and quality assurance \nto be completed. \n \nDFID reserves the option to break the contract after each of the Evaluation Report outputs is \ncompleted.  Continuation  of  the  services  after  each  output  is  produced  will  be  based  on \nagreement  of  the  deliverables  and  on  satisfactory  performance  and  the  progress  of  the \nSupplier against the specified outputs.", " Skills and Qualifications of evaluation team \u2022  Experience  evaluating  international  development  projects,  including  their  cost- \u2022  Knowledge of social and development impact bonds, and the evidence and arguments \u2022  Knowledge and experience of other / traditional mechanisms used to fund international effectiveness for and against their use development projects \u2022  Experience  in  assessing  the  costs  of  developing  and  managing  international \ndevelopment  projects  and  an  understanding  of  how  these  might  be  different  under \ndifferent funding mechanisms \u2022  Experience in joint or collaborative evaluations \n\u2022  Relevant thematic expertise suited to each of the DFID pilot DIB projects, including in \neducation outcomes, and livelihoods/income generation for very poor households, as \nwell as cross cutting expertise in gender and disability.", " \u2022  DFID welcomes the use of national/local consultants where this is appropriate to the delivery of the evaluation activities.", " B.9.1 Ways of Working There is an opportunity for the supplier to collaborate with the other learning activities funded \nat project level. To make use of this data, the supplier may benefit from a close engagement \nwith  the  learning  providers,  to  support  them  to  enhance  their  analytical  approach  or  data \ncollection activities to reduce risks of bias and make the evidence they produce more reliable \nand  sharable.  The  service  providers  and  other  donors  to  the  evaluation  have  formally \ncommitted to participate in the DFID evaluation and to share data (see Annex C). We do not \nhave  a  direct  relationship  with  the  investors  but  most  are  interested  to  participate  in  the \nevaluation. DFID will have access to the material produce by the providers as expressed in \nDFID accountable grant/MoU terms. \n \nDFID will provide connections and contact details to the main stakeholders involved in each \nof the DIB projects as soon as the inception phase starts.", " A96 DFID will not provide any travel / logistical support to the provider, nor any support for any in-\ncountry appointments.", " Evaluation Governance Arrangements and Stakeholder Involvement The  evaluation  supplier\u2019s  key  point  of  contact  will  be  the  DFID  DIBs  Team  Programme \nManager.", " B.10  Evaluation Management Team \u2022  Role:  Commissions,  approves  and  manages  the  evaluation.  Supplier  reports  to \u2022  Formed  of:  DFID  DIBs  Advisor  and  DIBs  Programme  Manager  and  PSD  Evaluation \u2022  The DFID DIBs Programme Manager will be the evaluation supplier\u2019s day to day point Management Team.", " Advisor.", " of contact.", " B.10.1 Evaluation Steering Group: \u2022  Role: To review and agree the content and methodology at design stage. To review the \nproducts and the findings, and consider relevance of the recommendations. To confirm \nthat the evaluation was implemented as planned, with robust methods robust, and that \nthe  findings  follow  from  the  evidence.  To  consider  if  recommendations  are  suitable/ \nfeasible and how recommendations will be acted on in the future. To take on board and \ndisseminate the evidence.", " \u2022  Formed  of:  Representatives  of  the  stakeholders  involved  in  each  of  the  3  DIBs  \u2013 \nincluding the service providers: ICRC and Village Enterprise; other donors e.g. USAID, \nBelgium,  Switzerland,  British  Asian  Trust,  MSDF;  investors  e.g.  UBS  Optimus \nFoundation;  and  involved  project  managers  such  as  Instiglio,  the  DFID  DIBs  team, \nDFID PbR Advisor, and DFID Evaluation Advisor.", " \u2022  Coordination: DFID Programme Manager will ensure the draft evaluation products are \nshared with members of the Steering Group, inviting the Steering Group\u2019s comments \nand feedback \u2013 either in writing or via a coordination session. DFID will consolidate the \nfeedback into concise actionable comments that will be shared with the evaluator. \n\u2022  Decisions:  The  Steering  Group  advises  DFID.  While  DFID  will  seek  to  achieve \nconsensus where differences of opinion emerge, DFID ultimately has discretion over \nthe action to take.", " B.10.2 EQUALS \u2013 DFID\u2019s Independent Evaluation Quality Assurance Service \u2022  Formed of: Independent expert evaluation quality assurance service.  \n\u2022  Role: To review evaluation design and each evaluation report for content and quality, providing a quality score for each product based of specific quality criteria.", " A97 B.10.3 Contract Key Performance Indicators The  following  indicators  set  out  what  DFID  considers  to  be  Good  Performance  by  the \nEvaluator  these  indicators  will  be  reviewed  annually  by  DFID  and  the  Supplier  based  on \nevidence of supplier performance during the contract lifetime. These may be adjusted during \nthe life of the contract in consultation with the supplier: Table B.6: Good Performance Indicators Area Description Target Indicator Delivery and VfM Outputs are delivered on time, and do not \nquestions \nevaluation \nleave \nany \nanalytical \nthe \nand \nunanswered, \nreasoning is clearly set out.", " and activities Supplier  demonstrates  how  evaluation \nchosen \napproach \nrepresent value for money across life of \ncontract. \n \nIncluding  proactive \nidentification  of \nefficiencies  and  savings  \u2013  e.g.  where \nopportunities arise that enable evaluator \nlearning  synergies  and \nto \nremove duplicative activities.", " leverage Evaluator  manages  risks  proactively, \nletting  DFID  know  if  risks  are  emerging \nthat could push the evaluation off track.  \nIf some questions are difficult to answer, \ninforming DFID well in advance.  \nMaintains  a \nrelationship with DFID.", " transparent  and  open answer 100% of outputs are delivered on \ntime, \nagreed \nevaluation  questions  and  are \nrated \nby \nEQUALS.", " excellent good/ all by reporting Qualitative \nEvaluator  \n \n \nValue of savings generated.", " 100%  of  outputs  answer  all \nevaluation  questions,  or  have \nsought  agreement  from  DFID  to \namend or remove a question well \nin advance.", " Risk Management Financial \nManagement Robust cost control in line with contract. \n \nAccurate  and \nforecasting and invoices.", " timely  submission  of Costs remain within budget  \n \nForecasts are submitted on time, \nwith  \u22645%  variance  with  actual \nexpenditure.", " Performance \navailability \npersonnel and \nof High  quality  team  of  personnel  with \nrelevant skills is maintained across life of \nevaluation.  Knowledge \nis  maintained \nacross staff changes.", " Performance of team.  \nPersonnel  with  appropriate  level \nof expertise are available across \nlife of requirement.", " Stakeholder \nEngagement Transparent,  honest  and  collaborative \nrelationship  with  the  Service  Providers \nand  learning  providers  in  DFID  DIBs  \u2013 \nto \nwith  advance  warning  provided \nstakeholders  of  need  to  engage  with \nevaluator providers/ \nover \nduplication Fewer  than  4  complaints  from \nDIB \nservice \n(a) \nstakeholders \nof \nunexplained \nactivities  already  complete  by \nlearning providers,  \n(b) \nonerous \nexcessively \nengagement  of  stakeholders  by \nevaluator.", " A98 Consideration  of  the \nwider Outcomes tied \n/ Impact Bond Field Consideration  given  to  the  evidence \nbeing  generated  in  the  wider  impact \nbond \nto \nfacilitate  the  wider  field  to  generate \nevidence field,  and  proactive  effort Evaluation  outputs  show  how \nlearning from the wider field has \nbeen considered.", " B.10.4 Budget and Payments tied to Outputs The Evaluator is expected to tie payments to delivery of the four main Evaluation Outputs \u2013 \nthe Evaluation Reports \u2013 with each payment commensurate to the work involved in that stage. \nThe  payments  will  be  made  when  each  output  is  accepted  by  DFID  as  being  of  good  or \nexcellent quality, where the requirements have been met with no shortcomings.", " We  expect  to  see  an  efficiently  designed  evaluation  that  meets  these  requirements.  We \nwelcome efforts by the evaluator to find savings during the life of the evaluation.", " The maximum budget available for this evaluation is GBP 300,000 (exclusive of VAT) Documents / References \u2022  DIBs Pilot Business Case \n\u2022  DIBs Pilot Business Case Addendum \n\u2022  DIBs Pilot programme Logframe \n\u2022  Village Enterprise DIB \u2013 Instiglio\u2019s Learning/Process Review document (giving more info on their approach) B.10.5 Duty of Care The  Supplier  is  responsible  for  the  safety  and  well-being  of  their  Personnel  (as  defined  in \nSection  2  of  the  Contract)  and Third  Parties affected  by  their  activities  under  this  contract, \nincluding appropriate security arrangements. They will also be responsible for the provision of \nsuitable security arrangements for their domestic and business property.", " The Supplier is responsible for ensuring appropriate safety and security briefings for all of their \nPersonnel working under this contract and ensuring that their Personnel register and receive \nbriefing as outlined above. Travel advice is also available on the FCO website and the Supplier \nmust ensure they (and their Personnel) are up to date with the latest position.", " This contract will require the Supplier to operate in conflict-affected areas and parts of it are \nhighly insecure. The security situation is volatile and subject to change at short notice. The \nSupplier  should  be  comfortable working  in such  an  environment  and  should  be  capable  of \ndeploying to any areas required within the region in order to deliver the Contract.", " The  Supplier  is  responsible  for  ensuring  that  appropriate  arrangements,  processes  and \nprocedures are in place for their Personnel, taking into account the environment they will be \nworking  in  and  the  level  of  risk  involved  in  delivery  of  the  Contract  (such  as  working  in \ndangerous, fragile and hostile environments etc.). The Supplier must develop their response \non the basis of being fully responsible for Duty of Care in line with the details provided above \nand the risk assessment matrix developed by DFID (see Annex 1) of this ToR). The Supplier \nmust confirm in their response that: \u2022  They fully accept responsibility for Security and Duty of Care.", " A99 \u2022  They  understand  the  potential  risks  and  have  the  knowledge  and  experience  to \u2022  They have the capability to manage their Duty of Care responsibilities throughout the develop an effective risk plan.", " life of the contract.", " Acceptance of responsibility must be supported with evidence of capability and DFID reserves \nthe  right  to  clarify  any  aspect  of  this  evidence.  In  providing  evidence  Tenderers  should \nconsider and respond to the following questions: a) b) c) d) e) f) Have you completed a risk assessment for this project that does not rely solely on \ninformation  provided  by  DFID and are you  satisfied  that  you  understand the  risk \nmanagement implications? \nHave you prepared a plan that you consider appropriate to manage these risks (or \nwill you do so if you are awarded the contract) and are you confident/comfortable \nthat you can implement this effectively? \nHave  you  ensured  or  will  you  ensure  that  your  staff  are  appropriately  trained \n(including specialist training where required) before they are deployed and will you \nensure that on-going training is provided where necessary. \nHave you an appropriate mechanism in place to monitor risk on a live / on-going \nbasis (or will you put one in place if you are awarded the contract). \nHave  you  ensured  or  will  you  ensure  that  your  staff  are  provided  with  and  have \naccess to suitable equipment and will you ensure that this is reviewed and provided \non an on-going basis? \nHave you appropriate systems in place to manage an emergency / incident if one \narises?", " The  positive evaluation of the  Supplier\u2019s proposal for the  provision  of  the  Services  and  the \naward of this Contract is not an endorsement by DFID of any arrangements which the Supplier \nhas  made for  the  health,  safety,  security  of  life and  property  and  wellbeing  of  the  Supplier \nPersonnel in relation to the provision of the Services. \n \nWe recommend that you make it easy for the review team to assess your responses by \nincluding a table in your tender pack that shows your responses to each of the Duty of \nCare  acceptance  and  capability  questions,  and  guides  the  review  team  to  any \nsupplementary evidence of capability that you provide.", " A100 Annex 1 \u2013 Initial Country Risk Assessment by DFID The programme under evaluation involves activities in multiple countries. DFID has provided \nan overall initial risk assessment for the programme locations as shown below: A101 DFID Overall Initial Project/Intervention Summary Risk Assessment Matrix Dec-17Read in conjunction with the FCO Travel Advisory on each countryCountryHIGH RISK LOCATIONSMEDIUM RISK LOCATIONSDate ConductedThemeDFID Risk ScoreDFID Risk ScoreOverall Rating5 - VERY HIGH RISK3 - MEDIUM RISKFCO Travel Advice52Host Nation Travel AdviceN/AN/ATransportation55Security[*]53Civil Unrest53Violence/crime53Terrorism*54War41Hurricane13Earthquake****13Flood*****23Medical Services**53Nature of Project Intervention32Mean (ignoring nature of project)43Mode (ignoring nature of project)5312345Very Low RiskLow RiskMedium RiskHigh RiskVery High RiskMedium*The FCO travel advice for Uganda, Kenya, Nigeria and Mali advises that there is a general threat from terrorism**Medical facilities outside of Capital Cities, and particularly away from cities are limited***FCO advise against all travel to Borno State. There is also a  High Risk (4) threat of kidnapping across Nigeria and Maiduguri in particular**** Earthquake risk is (3) on Indian border with Pakistan and in Delhi***** Flash flooding can occur during the wet season in Nigeria; Eastern Uganda; and monsoon in North India.High RiskFor example: Abuja and Borno State in Nigeria; Mali; Kinshasa in DRC; parts of Kenya, including Nairobi; and the immediacte vicinity of the India-Pakistan border.For example, other project locations incl: Uganda (excluding Karamoja, which is not relevant to this project); Gujarat, Rajasthan, and Delhi in India (with exception of the area in immediate vicinity of the border between India and Pakistan where the Supplier is not required to travel).Dec-17LocationLow SUPPLEMENTARY ANNEXES Annex A1: DFID Theory of Change for DIBs A102 Annex A2: Initial Framework for Assessing Theory of Change for DIBs Initial framework for assessing the Theory of Change behind DIBs, developed during DFID evaluability assessment A103 Were(cid:9)(cid:9)/(cid:9)can(cid:9)deal-breakers(cid:9)/(cid:9)critical(cid:9)success(cid:9)factors(cid:9)identified(cid:9)early?(cid:9)What(cid:9)were(cid:9)they?Costs(cid:9)and(cid:9)cost(cid:9)drivers:(cid:9)(cid:9)What(cid:9)were(cid:9)the(cid:9)duration(cid:9)and(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stages?(cid:9)(cid:9)How(cid:9)were(cid:9)costs(cid:9)divided(cid:9)across(cid:9)the(cid:9)different(cid:9)participants?(cid:9)What(cid:9)factors(cid:9)drove(cid:9)the(cid:9)costs(cid:9)of(cid:9)the(cid:9)different(cid:9)stakeholders?(cid:9)Which(cid:9)costs(cid:9)show(cid:9)potential(cid:9)to(cid:9)decrease(cid:9)in(cid:9)future(cid:9)deals?(cid:9)(cid:9)What(cid:9)steps(cid:9)can(cid:9)be(cid:9)taken(cid:9)to(cid:9)reduce(cid:9)future(cid:9)costs?Comparison(cid:9)with(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)How(cid:9)do(cid:9)costs(cid:9)compare(cid:9)(higher(cid:9)or(cid:9)lower)(cid:9)with(cid:9)alternative(cid:9)funding(cid:9)mechanisms(cid:9)(for(cid:9)both(cid:9)provider(cid:9)and(cid:9)for(cid:9)funder/(cid:9)payors)?(cid:9)For(cid:9)which(cid:9)stages(cid:9)did(cid:9)the(cid:9)costs(cid:9)differ?Cost-effectiveness:How(cid:9)does(cid:9)the(cid:9)effectiveness(cid:9)of(cid:9)the(cid:9)DIB(cid:9)funded(cid:9)projects(cid:9)(ie,(cid:9)impact(cid:9)/(cid:9)cost)(cid:9)compare(cid:9)with(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)by(cid:9)different(cid:9)mechanisms?(cid:9)Additionality(cid:9)of(cid:9)funding:(cid:9)Was(cid:9)the(cid:9)funding(cid:9)for(cid:9)the(cid:9)DIB(cid:9)net(cid:9)new(cid:9)to(cid:9)development?(cid:9)Or(cid:9)does(cid:9)DIB(cid:9)funding(cid:9)shift(cid:9)existing(cid:9)resources(cid:9)to(cid:9)more(cid:9)effective(cid:9)uses?(cid:9)How(cid:9)was(cid:9)this(cid:9)judged?InputsProcessesOutputs(cid:9)/(cid:9)impactCost-effectiveness1.(cid:9)FeasibilityAppropriate(cid:9)projects:(cid:9)What(cid:9)are(cid:9)their(cid:9)attributes(cid:9)(eg,(cid:9)sector,(cid:9)problems(cid:9)/(cid:9)opportunities(cid:9)addressed,(cid:9)innovative(cid:9)or(cid:9)scaling(cid:9)up(cid:9)mature(cid:9)interventions,(cid:9)preventive,(cid:9)measurable(cid:9)baselines(cid:9)etc)?(cid:9)Funders(cid:9)/(cid:9)payors:(cid:9)(cid:9)How(cid:9)many?What(cid:9)are(cid:9)their(cid:9)goals(cid:9)and(cid:9)motivations?(cid:9)Was(cid:9)perceived(cid:9)transfer(cid:9)of(cid:9)risk(cid:9)a(cid:9)motivation?(cid:9)Were(cid:9)they(cid:9)easy(cid:9)/(cid:9)difficult(cid:9)to(cid:9)find(cid:9)/(cid:9)engage?(cid:9)Why?Providers:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)are(cid:9)they(cid:9)resource(cid:9)&(cid:9)capital(cid:9)constrained,(cid:9)are(cid:9)they(cid:9)used(cid:9)to(cid:9)PbR(cid:9)contracts,(cid:9)do(cid:9)they(cid:9)already(cid:9)have(cid:9)an(cid:9)appropriate(cid:9)monitoring(cid:9)system(cid:9)etc.)?Investors:(cid:9)What(cid:9)are(cid:9)their(cid:9)characteristics(cid:9)(eg,(cid:9)commercial(cid:9)or(cid:9)foundations,(cid:9)established(cid:9)or(cid:9)new(cid:9)to(cid:9)development,(cid:9)how(cid:9)many)?Intermediaries:(cid:9)Which(cid:9)intermediaries(cid:9)are(cid:9)involved(cid:9)What(cid:9)roles(cid:9)do(cid:9)they(cid:9)play?(cid:9)Who(cid:9)do(cid:9)they(cid:9)represent?(cid:9)How(cid:9)were(cid:9)they(cid:9)funded?Capacity-building:(cid:9)What,(cid:9)if(cid:9)any,(cid:9)support(cid:9)has(cid:9)been(cid:9)provided(cid:9)to(cid:9)help(cid:9)stakeholders(cid:9)prepare(cid:9)for(cid:9)the(cid:9)DIB?(cid:9)Has(cid:9)it(cid:9)been(cid:9)useful?Context:(cid:9)What(cid:9)contextual(cid:9)factors(cid:9)significantly(cid:9)influenced(cid:9)the(cid:9)development(cid:9)of(cid:9)the(cid:9)DIB?Estimates(cid:9)of(cid:9)impact:Was(cid:9)the(cid:9)intervention(cid:9)successful?(cid:9)Does(cid:9)it(cid:9)seem(cid:9)that(cid:9)the(cid:9)funding(cid:9)instrument(cid:9)played(cid:9)a(cid:9)role(cid:9)in(cid:9)whether(cid:9)or(cid:9)not(cid:9)it(cid:9)was(cid:9)(ie,(cid:9)via(cid:9)the(cid:9)mechanisms(cid:9)in(cid:9)(cid:9)3.(cid:9)Implementation)?Comparability(cid:9)to(cid:9)impact(cid:9)from(cid:9)using(cid:9)other(cid:9)funding(cid:9)instruments:(cid:9)Were(cid:9)the(cid:9)results(cid:9)different(cid:9)to(cid:9)past(cid:9)/(cid:9)similar(cid:9)projects(cid:9)funded(cid:9)using(cid:9)other(cid:9)instruments?Unintended(cid:9)outcomes:(cid:9)Were(cid:9)there(cid:9)any(cid:9)unintended(cid:9)outcomes,(cid:9)positive(cid:9)or(cid:9)negative?Engagement(cid:9)with(cid:9)beneficiaries:(cid:9)Did(cid:9)the(cid:9)DIBs(cid:9)create(cid:9)more(cid:9)or(cid:9)less(cid:9)engagement(cid:9)between(cid:9)beneficiaries(cid:9)and(cid:9)service(cid:9)providers?Sustainability:(cid:9)Are(cid:9)there(cid:9)reasons(cid:9)to(cid:9)believe(cid:9)any(cid:9)outcomes(cid:9)/(cid:9)impact(cid:9)achieved(cid:9)will(cid:9)be(cid:9)more(cid:9)or(cid:9)less(cid:9)sustainable(cid:9)than(cid:9)those(cid:9)achieved(cid:9)using(cid:9)other(cid:9)instruments?Repeatability:(cid:9)Would(cid:9)the(cid:9)various(cid:9)stakeholders(cid:9)participate(cid:9)in(cid:9)a(cid:9)similar(cid:9)instrument(cid:9)in(cid:9)the(cid:9)future?(cid:9)Under(cid:9)what(cid:9)conditions?What(cid:9)factor,(cid:9)if(cid:9)any,(cid:9)drove(cid:9)improvement?1)(cid:9)change(cid:9)in(cid:9)incentives(cid:9)(mgmt.(cid:9)and/or(cid:9)front-line)2)(cid:9)increased(cid:9)flexibility(cid:9)/(cid:9)autonomy3)(cid:9)support(cid:9)from(cid:9)active(cid:9)investorDid(cid:9)these(cid:9)or(cid:9)other(cid:9)factors(cid:9)increase(cid:9)focus(cid:9)on(cid:9)outcomes(cid:9)and(cid:9)delivery?Were(cid:9)investors(cid:9)&(cid:9)funders(cid:9)/(cid:9)payorsactive(cid:9)or(cid:9)passive(cid:9)in(cid:9)this(cid:9)stage?(cid:9)If(cid:9)active,(cid:9)did(cid:9)they(cid:9)add(cid:9)value?(cid:9)(cid:9)What(cid:9)were(cid:9)challenges?(cid:9)Were(cid:9)they(cid:9)overcome?(cid:9)If(cid:9)so,(cid:9)how?What(cid:9)factors(cid:9)were(cid:9)important(cid:9)for(cid:9)projects(cid:9)that(cid:9)did(cid:9)/(cid:9)did(cid:9)not(cid:9)proceed?(cid:9)2.(cid:9)Structuring(cid:9)the(cid:9)deal3.(cid:9)Implementation4.(cid:9)Evaluation(cid:9)and(cid:9)paymentsWhat(cid:9)measures(cid:9)&(cid:9)method(cid:9)were(cid:9)used(cid:9)to(cid:9)estimate(cid:9)impact?(cid:9)Were(cid:9)these(cid:9)appropriate(cid:9)(eg,(cid:9)were(cid:9)the(cid:9)measures(cid:9)good(cid:9)predictors(cid:9)of(cid:9)positive(cid:9)effects)?What(cid:9)were(cid:9)the(cid:9)timings(cid:9)of(cid:9)the(cid:9)payments(cid:9)(and(cid:9)investments)?(cid:9)Were(cid:9)outcome(cid:9)payments(cid:9)recycled(cid:9)as(cid:9)operating(cid:9)costs?What(cid:9)were(cid:9)challenges(cid:9)in(cid:9)validating(cid:9)the(cid:9)outcome(cid:9)measures(cid:9)(eg,(cid:9)data(cid:9)quality,(cid:9)collection(cid:9)capacity(cid:9)etc.)?How(cid:9)were(cid:9)external(cid:9)factors(cid:9)that(cid:9)influence(cid:9)outcomes(cid:9)addressed?Were(cid:9)repayment(cid:9)terms(cid:9)renegotiated?(cid:9)If(cid:9)so,(cid:9)why?\fAnnex D \u2013 DFID Indicative Programme Gantt Chart (subject to change) A104 DIBs Pilot Programme timelineJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDJFMAMJJASONDProgrammeBusiness CaseApproval of BCXProject Appraisal , Diligence, Approval (ICRC)Project Appraisal, Diligence, Approval (VE)DFID Annual ReviewsProject Completion ReviewDFID commissioned Evaluation Tentative Timeline for OutputsIssue TenderxSuppliers BiddingxBid evaluation & contractingxEvaluation Inception (4 weeks)xDIBs Design Phase Learning Report (QA)xxxxXMid-Term Evaluation Report (QA)XFinal Evaluation Report (QA)XAnnual Evidence/Learning ReportQuality Assurance of ToR, Design, OutputsICRCDesign (largely complete b4 DFID engaged)PbR Agreement negotiation/finalisationImplementationBuilding of new centres, training staff, testing efficiency measures in 8 centresOperationalisation of the new centresProject Progress ReportsLa Caixa Outcomes Payment (~\u00a30.88m on completion of building of centres)\u25caSER Outcomes Measurement & Payment (verification activities)NB: ICRC will produce monthly SER reports\u25caLearning Activities (no internal activities planned)VE DIBDesign Fnalisation & Contract negotiationOutcomes Verifier tender & designImplementationCohort 1dark red = targetting; light red = training and mentoringCash transfer verification & payment\u25ca\u25cagreen shows verification of initial seed transfer (larger portion); and second smaller supplementary seed transfer; with \u25ca showing donor payment $1 for every $ transferred.Cohort 2Cash transfer verification & payment\u25ca\u25caCohort 3Cash transfer verification & payment\u25ca\u25caCohort 4Cash transfer verification & payment\u25ca\u25caEndline Outcomes Measurement & Payment cohorts 1-4\u25caCohort 5Cash transfer verification & payment\u25ca\u25caCohort 6Cash transfer verification & payment\u25ca\u25caCohort 7Cash transfer verification & payment\u25ca\u25caEndline Outcomes Measurement (cohorts 5-7) & Payment (pooled result cohorts 1-7)\u25ca\u25caLearning Activities and Reports produced (\u2713)\u2713\u2713\u2713\u2713BAT Education DIBDesign of Education DIB IndiaxxxOutcome measurement instrument to be piloted in june/july, and baselines done in july or september)Implementation of Education DIB in IndiaOutcomes Measurement & PaymentsNB: We expect annual outcomes verification and annual results payments, but timing isn't confirmed\u25ca\u25ca\u25ca\u25caBAT Learning ActivitiesNB: Timing of learning activities & outputs are estimated, and will be confirmed later this yearResearch Report on BAT Education DIB\u2713\u2713Selection of areas of feasibility study\u25caFeasibility Reports for South Asia\u25caProof of Concept Reports for South Asia\u25caDIBs Expansion - Design?Stage 1Stage 2KeyPayments\u25caReports Produced\u271320232022201620172018201920202021We assume sustained service provision at centres, with maintained or increasing SER and replicated across ICRC PR programmeSome service providers will continue to deliver interventions in the schools after end of the programme.School year runs Sept -July.4 Years of schooling starting Sept 2018\fEnd of ToR Changes to the Terms of Reference Changes to the Terms of Reference were agreed during the inception phase, and set out in the inception report. No other changes have been \nmade during this research wave.", " The main changes and developments to the ToR are the following: 1.  A revision and development of the proposed evaluation questions, set out in section 2; 2.  A revision of the Theory of Change, set out in section 1.1.3.", " 3.  The  inclusion  of  annual  consultations  with  key  stakeholders  in  the  workplan,  to  enable  the  evaluation  team  to  keep  abreast  of \ndevelopments within the DIBs and ensure that relationships between the DIB stakeholders and the evaluation team remain strong. \nThese consultations will form the basis of the \u2018Keeping in Touch\u2019 reports in the years between the research waves.", " A105 Annex C: Bibliography Arena,  M.  Irene  Bengo,  Mario  Calderini  &  Veronica  Chiodo.  2016.  Social  Impact  Bonds: \nBlockbuster or Flash in a Pan?, International Journal of Public Administration, 39:12, 927-939.", " Azemati, H., Belinksy, M., GIlette, R., Liebman, J., Sellman, A., and Wyse, A. 2013. Social \nimpact Bonds: Lessons Learned so Far. Community Development Investment Review.", " Barr, J. and Christie, A. 2015. Improving the Practice of Value for Money Assessment. Centre \nfor Development Impact Practice Paper: Number 12 March 2015.", " Barr, J. and Christie, A. Better value for Money: An organising framework for management \nand measurement of VFM indicators. ITAD.", " Barder, O. (nd). Development Impact Bond Working Group. Center for Global Development. \nhttps://www.cgdev.org/working-group/development-impact-bond-working-group Bloomgarden,  D.,  Eddy,  M.,  Levey,  Z.  (2014).  Social  Impact  Bonds  and Education  in  Latin \nAmerica. GEMS Education Solutions.", " doing?", " Boggild, I and Gustafsson-Wright, E. 2017. Two Years in: How\u2019s the world\u2019s first developing \nbond \nhere: \nhttps://www.brookings.edu/blog/education-plus-development/2017/07/13/two-years-in-hows-\nthe-worlds-first-development-impact-bond-for-education-doing/ Development.", " Brookings: Education Available Plus Impact+  (2014).  Choosing  Social Bridges \nhttps://www.bigsocietycapital.com/sites/default/files/attachments/Choosing%20social%20imp\nact%20bonds%20Bridges%20report.pdf Impact  Bonds:  A  Practitioner\u2019s  Guide.", " British Asian Trust (nd). Social Impact Investing: the future of philanthropy?", " Cambridge  Education.  2015.  Evaluation  of  the  Pilot  Project  of  Results-Based  Aid  in  the \nEducation Sector in Ethiopia, available at: http://iati.dfid.gov.uk/iati_documents/5608531.pdf Cardno Emerging Markets and METIS Analytics (2014). The Case for DIBs: Inquiry into the \nrole  of  the  private  sector  in  promoting  economic  growth  and  reducing  poverty  in  the  Indo-\nPacific region.", " Carter, E., FitzGerald, C., Dixon, R., Economy, C., Hameed, T., and Airoldi, M. (2018) Building \nthe tools for public services to secure better outcomes: Collaboration, Prevention, Innovation, \nGovernment Outcomes Lab, University of Oxford, Blavatnik School of Government.", " Carter, C., Costanza, P., Goldsmith, B., Husain, S., and Menzies, D. (2016). Panel event on \nDIBs: \n(ODI).  \nOverseas \nhttps://www.odi.org/events/4380-development-impact-bonds-paying-success Development success.", " Institute paying for (2011).  DFID  and  CDC  announce  new  business  plan \nCDC \nhttp://www.cdcgroup.com/Media/News/DFID-and-CDC-announce-new-business-plan-for-\nCDC/ for  CDC.", " CDC (2017). Investment Policy for the period from 12 October 2017 to 31 December 2021.", " Center  for  Global  Development  and  Social  Finance  (2013).  Investing  in  Social  Outcomes: \nDIBs:  The  Report  of  the  Development  Impact  Bond  Working  Group.  Center  for  Global A106 Development and Social Finance. https://www.cgdev.org/sites/default/files/investing-in-social-\noutcomes-development-impact-bonds.pdf Center for Global Development and Social Finance (2014). DIBs Briefing Note.", " Chakravarty,  S.,  Lundberg,  M.,  Nikolov,  P.  and  Zenker,  J.  2016.  The  Role  of  Training \nPrograms  for  Youth  Employment  in  Nepal:  Impact  Evaluation  Report  on  the  Employment \nFund, World Bank Policy Research Working Paper 7656.", " Children\u2019s  Investment  Fund  Foundation,  (nd).  Education  Development  Impact  Bond. \nhttps://ciff.org/grant-portfolio/education-development-impact-bond/ Children\u2019s Investment Fund Foundation, (nd). The Educate Girls Development Impact Bond: \nA New Finance Model for International Development.", " Chinfatt, S. and Carson, M. 2017. Supplier Access to Pre-finance in PbR Contracts. Research \nStudy Report. Dalberg Intelligence.", " Clist, P., Dercon, S. 2014. 12 Principles for PbR (PbR) in International Development, (Working \npaper) Clist, P. 2017. Review of PbR in DFID: Establishing the Evidence Base.", " Clist, P. and Drew, R. (2015).  Evaluating DIBs. Department for International Development. \nhttp://r4d.dfid.gov.uk/pdf/outputs/misc_EcoDev/DIB_Study_Final_Report.pdf) Clist, P. and Verschoor, A. 2014. The Conceptual Basis of PbR, DFID.", " Eldridge, M., and TeKolste, R. (2016). Results-Based Financing Approaches: Observations \nInstitute. \nInternational \nfor \nhttp://pfs.urban.org/library/content/results-based-financing- Experiences.  Urban Success from Pay for Dear, A., Helbitz, A., Khabe, R., Lotan, R., Newman, J., Sims, G.C., Zaroulis, A. (2016). Social \nImpact Bonds: The Early Years. Social Finance.", " Deloitte. Undated. Social Impact Bonds in Canada: Investor Insights.", " DFID (nd). What works for PbR Mechanisms in DFID Programmes.", " DFID. 2011. DFID\u2019s Approach to Value for Money. 2011.", " DFID (2014). DFID\u2019s Evaluation Framework for PbR.", " DFID. 2016. Project Completion Report of Pilot Project of Results Based Aid in the Education \nSector in Ethiopia, available at: http://iati.dfid.gov.uk/iati_documents/5419380.odt DFID. 2016b. Annual Review of WASH Results Programme 2016 Annual Review, available \nat: http://iati.dfid.gov.uk/iati_documetns/5498698.odt DFID  (2017).  Economic  Development  Strategy:  prosperity,  poverty  and  meeting  global \nchallenges.", " Edminston,  D.  and  Nicholls,  A.  2018.  Social  Impact  Bonds:  The  Role  of  Private  Capital  in \nOutcome-Based Commissioning. Journal of Social Policy 47 (1): 57-76.", " Edmiston, Daniel and Nicholls, Alex (2017) Social Impact Bonds: The Role of Private Capital \nin Outcome-Based Commissioning. Journal of Social Policy.", " A107 Ernst, L. (2017). Using Human-Centred Design at DFID. Development Impact and You (DIY). \nhttp://diytoolkit.org/using-human-centred-design-at-dfid/ Evans, A. 2016. Results based financing in Zambia \u2013 an informal, unpublished annex, mimeo, \navailable at: https://www.researchgate.net/publication/308985858 Fleming, F. 2013. Evaluation Methods for Assessing Value for Money. Better Evaluation.", " Fraser,  A., Tan,  S.,  Largarde,  M.  and  Mays,  N.  (2018)  Narratives of  Promise,  narratives of \nCaution: A Review of the Literature on Social Impact Bonds. Social Policy & Administration. \nVol. 52(1): 4-28.", " Gorter AC et al. 2013. Evidence Review: Results-Based Financing of Maternal and Newborn \nHealth  Care  in  Low-  and  Lower-  middle-income  countries.  Study  commissioned  by  the \nGerman Federal Ministry for Economic Cooperation and Development.", " Grittner,  A.  2013.  Results-based  Financing.  Evidence from  performance-based financing  in \nthe health sector. Bonn: Deutsches Institut fuer Entwicklungspolitik.", " impact Gustafsson-Wright,  E.  and  Boggild-Jones,  I.  (2017).  Two  years  in:  How\u2019s  the  world\u2019s  first \nBrookings. \nfor \ndevelopment \nhttps://www.brookings.edu/blog/education-plus-development/2017/07/13/two-years-in-hows-\nthe-worlds-first-development-impact-bond-for-education-doing/ \nhttps://www.brookings.edu/events/year-two-results-of-the-worlds-first-development-impact-\nbond-for-education-dib/) education (webinar: doing?", " bond Gustafsson-Wright, E. and Boggild-Jones, I. (2018). Paying for social outcomes: A review of \nthe global impact bond market in 2017. Brookings. https://www.brookings.edu/blog/education-\nplus-development/2018/01/17/paying-for-social-outcomes-a-review-of-the-global-impact-\nbond-market-in-2017/ Gustafsson-Wright,  E.  Gardiner,  S.  and  Putcha,  V.  (2015).  The  potential  and  limitations  of \nimpact  bonds:  Lessons  from  the  first  five  years  of  experience  worldwide.  Brookings. \nhttps://www.brookings.edu/research/the-potential-and-limitations-of-impact-bonds-lessons-\nfrom-the-first-five-years-of-experience-worldwide/ Gustafsson-Wright, E., Boggild-Jones, I., Segell, D. and Durland, J. (2017). Impact bonds in \ndeveloping \nBrookings. \nlearning \nhttps://www.brookings.edu/research/impact-bonds-in-developing-countries-early-learning-\nfrom-the-field/ countries: Early field.", " from the Holden, J and Patch, J. 2017. The experience of PbR (PbR) on the Girls\u2019 Education Challenge \n(GEC) programmes: Does skin in the game improve the level of play?, Mimeo, available at: \nhttp://foresgiht.associates/wp-content/uploads/2017/01/2017.01.19-Skin-in-the-game-PbR-\non-the-GEC.-Final.pdf Honig, D. 2014. Navigation by Judgment: Organizational Autonomy and Country Context in \nthe Delivery of Foreign Aid.", " IMC Worldwide, Ideas to Impact, DFID, USAID and Rockfeller Foundation (2017). Innovating \nin development: Sharing learning, improving impact (Workshop report).", " Ibidem; Pereira, J; Villota, C. 2013. Hitting the target? Evaluating the effectiveness of results-\nbased approaches to aid. Brussels: EURODAD.", " A108 Instiglio  and  Thompson  Reuters  Foundation  (2014).  A  Legal  Road  Map  for  Social  Impact \nBonds  in  Developing  Countries.  http://www.instiglio.org/wp-content/uploads/2015/02/Legal-\nRoad-Map-for-SIBs-in-Developing-Countries.pdf Joy,  M.  and  Shields,  J.  (2013),  Social  Impact  Bonds:  The  next  phase  of  third  sector \nmarketization? Canadian Journal of Nonprofit and Social Economy Research, 4, 2: 39\u201355.", " KPMG. 2014. Evaluation of the Joint Development of the NSW Social Benefit Bonds Trial.", " Kandpal,  E.  2016.  Completed  Impact  Evaluations  and  Emerging  Lessons  from  the  Health \nResults \nat: \nhttps://www.rbfhealth.org/sites/rbf/files/IE%20and%20emerging%20lessons_Eeshani%20Ka\nndpal.pdf Innovation available Portfolio, Learning Trust Fund Khatib-Othman, H. 2016. Country Programmes: Strategic Issues, Report to the [GAVI] Board \n7-8  December  2016,  Appendix  B,  available  at  http://www.gavi.org/about/governance/gavi-\nboard/minutes/2016/7-dec/minutes/07a---country-programmes---strategic-issues/ King, J. 2017. Using Economic Methods Evaluatively. American Journal of Evaluation, Vol 38, \nissue 1, March 2017 King and OPM. 2018. The OPM Approach to Assessing value for Money: A Guide. Oxford \nPolicy Management Ltd.", " Lake,  R. W.  (2016).  The  subordination  of  urban  policy  in  the  time  of  financialization.  In  C. \nJohnson & J. DeFilippis (Eds.), Urban policy in the time of Obama (pp. 45\u201364). Minneapolis: \nUniversity of Minnesota Press.", " LAMP  Development.  The  Future  of  VFM:  A  consideration  of  the  challenges  and  potential \nsolutions for improving its measurement and application \u2013 a thought piece.", " Le Grand, J. 1995. Knights, Knaves or Pawns? Human Behaviour and Social Policy. Jnl Soc. \nPol., 26, 2, 149\u2013169.", " Liebman,  J.  and  Alina  Sellman,  A.  (2013).  Social  Impact  Bond  Guide  for  State  and  Local \nGovernments.  Social  Impact  Bond  Technical  Assistance  Lab,  Harvard  Kennedy  School. \nhttps://govlab.hks.harvard.edu/news/social-impact-bond-guide-state-and-local-governments Maier,  M.  and  Myer,  M.  (2017)  Social  Impact  Bonds  and  the  Perils  of  Aligned  Interests. \nInstitute  for  Nonprofit  Management,  WU  Vienna  University  of  Economics  and  Business, \nVienna.", " Maier, Florentine and Barbetta , Gian Paolo and Godina, Franka (2017) Paradoxes of Social \nImpact Bonds. Social Policy & Administration.", " Maximising Finance for Development \u2013 G20. DIBs debate about whether DIBs are about aid \neffectiveness or mobilising private finance, and how DIBs fit into the MFD debate.", " McEwan,  P.  2012.  Cost-effectiveness  analysis  of  education  and  health  interventions  in \ndeveloping countries. Journal of Development Effectiveness: Vol. 4, No. 2, June 2012, 189-\n213.", " McHugh, N., Sinclair, S., Roy, M., Huckfield, L., Donaldson, C. 2013. Social Impact Bonds: A \nWolf in Sheep\u2019s Clothing. Journal of Poverty and Social Justice. 21(3): 247-57.", " A109 Mulgan, G., Reeder, N., Aylott, M., Bo'sher, L. 2011. Social Impact Investment: the Challenge \nand Opportunity of Social Impact Bonds. The Young Foundation.", " NAO. 2017. Briefing on Social Impact Bonds April 2017 Norwegian Knowledge Centre for Health Services (NKCHS). 2008. An overview of research \non the effects of results-based financing. Oslo: NKCHS.", " Oroxom, R., Glassman, A., and McDonald, L. (2018). Structuring and Funding Development \nfor Health: Nine Lessons from Cameroon and Beyond. Policy Paper 117, Center for Global \nDevelopment. \nhttps://www.cgdev.org/sites/default/files/structuring-funding-development-\nimpact-bonds-for-health-nine-lessons.pdf Palladium and US Aid (2016). Pay for Results in Development: A Primer for Practitioners.", " Pearson, M. 2011. Results based aid and results based financing: What are they? Have they \ndelivered results? London: HLSP.", " Perakis, R. (2014). Two DIB Pilots Will Test New Development Partnerships. Center for Global \nDevelopment. \nhttps://www.cgdev.org/blog/two-dib-pilots-will-test-new-development-\npartnerships Perakis, R., and Savedoff, W. (2014). An Introduction to Cash on Delivery Aid for Funders. \nCGD  Note.  Center  for  Global  Development.  http://www.cgdev.org/publication/introduction-\ncash-delivery-aid-funders.", " Perakis, R., and Savedoff, W. (2015). Does Results-Based Aid Change Anything? Pecuniary \nInterests,  Attention,  Accountability  and  Discretion  in  Four  Case  Studies.  Policy  Paper  052. \nCenter  for  Global  Development.  http://www.cgdev.org/publication/does-results-based-aid-\nchange-anything-pecuniary-interests-attention-accountability-and.", " Perason,  M.  et  al.  2010.  Review  of  major  Results  Based  Aid  (RBA)  and  Results  Based \nFinancing (RBF) Schemes. London: Department for International Development.", " Perrin,  B.  2013.  Evaluation  of  PbR  (PbR):  Current  Approaches,  Future Needs:  Report  of  a \nStudy Commissioned by the Department for International Development.", " Sandefur,  J  and  Glassman,  A.  2015.  The  Political  Economy  of  Bad  Data:  Evidence  from \nAfrican Survey and Administrative Statistics, Journal of Development Studies, 51, (2), 116-\n132 Saldinger, \nDIBs \nhttps://www.devex.com/news/development-impact-bonds-gain-momentum-90591 momentum.", " (2017).", " gain A.", " DevEx.com.", " Sedlmayr, R. (2018). Paying for Poverty Alleviation.", " Sinclair, S. McHugh, N., Huckfield, L., Roy, M. and Donaldson, C. 2014. Social Impact Bonds: \nShifting the Boundaries of Citizenship. Social Policy Review 26.", " Social  Finance  (nd).  Impact  Bond  Global  Database.  Accessed  November  8,  2016. \nhttps://sibdatabase.socialfinance.org.uk/ Tan, S., Fraser, A., Giacomantonio, C., Kruithof, K., Sim, M., Lagarde, M., Disley, E., Rubin, \nJ.  and  Mays,  N.  2015.  An  Evaluation  of  Social  Impact  Bonds  in  Health  and  Social  Care, \nLondon:  PIRU,  London School  of  Hygiene  and Tropical  Medicine  and RAND  Europe.  See: A110 http://www.piru.ac.uk/assets/files/Trailblazer%20SIBs%20interim%20report%20March%202\n015,%20for%20publication%20on%20PIRU%20siteapril%20amendedpdf11may.pdf Tse, A. & Warner, M. (2018): The razor\u2019s edge: Social impact bonds and the financialization \nof early childhood services, Journal of Urban Affairs, UK Parliament (2014). The Future of UK Development Cooperation: Phase 1: Development \nFinance \nCommittee. \nhttps://publications.parliament.uk/pa/cm201314/cmselect/cmintdev/1255/125504.htm Development International - US Aid (nd). Investing for Impact, Capitalizing on the emerging landscape for global health \nfinancing.", " Upper Quartile. 2015. Evaluation of Results Based Aid in Rwandan Education, available at \nhttp://iati.dfid.gov.uk/iati_documents/5549076.pdf the  Results-Based  Financing  Programme Valadez, J., Jeffrey, C, Brant, T. Vargas, W and Pagano, M. 2015. Final Impact Assessment \nof \nfor  Northern  Uganda,  available  at \nhttps://www.gov.uk/governemnt/uplaods/system/uplaods/attachment_data/file/607579/Evalu\nation-of-Results-Based -Financing-Programme-for-Norther-Uganda.pdf Witter, S., Zaman, R., Scott, M. and Mistry, R. 2016. Evaluation of Delivering Reproductive \nHealth  Results  (DRHR)  through  non  state  providers,  MSI/PSI  Impact  Evaluation  Report, \nat: \nAvailable \nhttps://www.gov.uk/government/uploads/system/uploads/attachment_data/file/533669/Delive\nring-Reproductive-Health_Results-Non-State-Providers_Pakistan1.pdf A111 Annex D: EQUALs criteria mapped to report sections Ref  EQUALs Criteria 1. STRUCTURE AND CLARITY 1.1 1.2 1.3 1.4 The product is accessible to the intended audience (e.g. free of jargon, \nwritten in plain English, logical use of sections, appropriate use of tables, \ngraphs and diagrams). \nIt is clear who has carried out the evaluation. \nAn executive summary is included, and it can stand alone as an accurate \nsummary of the main product. \nThe annexes contain \u2013 at the least \u2013 the original TORs, the evaluation \nframework, a bibliography and a list of consultees.", " 1.5  Annexes increase the usefulness of the product.", " 1.6 Any departures from the original TOR been adequately explained and \njustified.", " 1.7  The product is of publishable quality.", " 2. CONTEXT 2.1 The product provides a relevant and sufficient description of the \nintervention to be evaluated. At the least, this should include detail on the \nintervention\u2019s anticipated impact, outcomes and outputs, target groups, \ntimescale, geographical coverage, and the extent to which the intervention \naimed to address issues of equity, poverty and exclusion.", " 2.4 2.3 2.2  The product describes the intervention logic and/or theory of change. \nThe product provides a relevant and sufficient description of the local, \nnational and/or international development context within which the \nintervention was operating. \nThe product identifies key linkages between the evaluated intervention and \nother relevant projects / programmes / donors. If no linkages are identified, \nthe product justifies why other projects / programmes / donors were not \nrelevant to the evaluation. \nThere is an assessment of the policy context for the intervention and this \nincludes reference to poverty reduction strategies, gender equality, \nenvironmental protection, and human rights. \nThe product describes the extent to which the intervention has been \nmanaged and delivered against Paris Declaration principles.", " 2.6 2.5 3. PURPOSE, SCOPE AND OBJECTIVES Corresponding \nSection n/a Disclaimer \nExecutive Summary Annex B, Annex E.1, \nAnnex C and Annex H. \nAnnexes \nAnnex B, \u201cChanges to \nthe ToR\u201d \nn/a Section 3 Section 1.1.3  \nSection 1.1 Annex E.4 and Annex \nE.7 Section 3.1 Annex E.7 The product describes what information is needed through the evaluation, \nand how that information will be used.", " The product describes whether the evaluation is for accountability and/or \nlearning purposes.", " Section 1.4, Annex \nE.1, E.2, E.3 and E.4, \nand Annex F \nSection 1.2 3.3  The product describes the target audience(s) for the evaluation.", " Section 1.2 and 2.2.3 A112 3.1 3.2 Ref  EQUALs Criteria 3.4  The product justifies the timing of the evaluation.", " 3.5 3.6 The product clearly outlines what aspects of the intervention are and are \nnot covered by the evaluation.  \nThe evaluation\u2019s objectives are specific and realistic. They are clearly \nrelated to the evaluation purpose.", " 4. EVALUATION METHODOLOGY AND DESIGN 4.1 The evaluation framework is clearly explained. It establishes the evaluation \nquestions, data sources and methods for data collection.", " The product describes and justifies which evaluation criteria are applied \n(e.g. OECD DAC). This includes discussion around which criteria were not \nrelevant for this evaluation. \nThe evaluation methods are described and justified. These methods are \nappropriate for addressing the evaluation questions. \nThe methodology is appropriate for assessing the cross-cutting issues of \ngender, poverty, human rights, HIV/AIDS, environment, anti-corruption, \ncapacity building, and power relations.", " The sampling strategy is described, and is appropriate. Primary and \nsecondary data sources are appropriate, adequate and reliable. Sample \nsizes are adequate.", " 4.6  The design provides for multiple lines of inquiry and/or triangulation of data.", " The methodology enables the collection and analysis of disaggregated data \nto show difference between groups. \nAny methodological limitations are acknowledged and their impact on the \nevaluation discussed. The limitations are acceptable and/or they are \nadequately addressed. \nAny departures from the TOR, inception phase and / or original evaluation \ndesign are adequately explained. \nThe product discusses any inherent imbalances or biases that interviews \nand other data collection may have created.", " The product describes how any bias has been overcome.", " 5. IMPLEMENTATION Corresponding \nSection  \nSection 1.4 \nSection 1.3 Sections 1.2 Section 2.1 and Annex \nE.1 \n \nSection 2.1 Annex E.1, E.2, E.3, \nE.4, E.5 and E.6 \nSection 5.4.2 and \nAnnex F Annex E2.3, E2.5, \nE6.2, E7.2; Annex C, \nAnnex H; Annex J and \nAnnex M \nSections 2.1 and \nAnnex E.1, E.2, E.3, \nE.4 \nSection 5.4.2 and \nAnnex F \nSection 2.3 Annex B, \u201cChanges to \nthe ToR\u201d \nSection 2.3 and Annex \nE.5 \nSection 2.3 and Annex \nE.5 Instruments were tested and validated (e.g. pre-testing of questionnaires). \nData was collected in an appropriate and respectful manner, taking into \naccount cultural, ethical and legal concerns. \nThere was an appropriate level of involvement from the various \nstakeholders in the design and implementation of the evaluation. \nThe evaluation process provided affected stakeholders with access to \nevaluation-related information in forms that respect people and honour \nconfidentiality.", " Annex E.5 \nAnnex E.5 Annex E.5 and E.7 Annex E.5, E.7 and \nE.9 4.2 4.3 4.4 4.5 4.7 4.8 4.9 4.10 4.11 5.1 5.2 5.3 5.4 A113 Ref  EQUALs Criteria 5.5  The evaluation process was transparent enough to ensure its legitimacy.", " Where primary stakeholders were not consulted due to the scope of the \nevaluation, the evaluation drew on relevant documentation and secondary \ndata sources were identified and referred to. \nAny summary or description of consultees takes into account ethical, \nprivacy and security concerns. (The document should only provide a \nsummary of number and level of staff interviewed, by organisation) \nTo what extent has the evaluation been implemented in accordance with \nParis Declaration principles? Have issues of country ownership and \nmanagement been addressed? To what extent has the evaluation used \ncountry systems? How far has the evaluation harmonised approaches with \nother donors? Has the evaluation contributed to building evaluation \ncapacity within partner countries?", " 6. ANALYSIS Corresponding \nSection  \nAnnexes E and Annex \nK \n Annexes H and J Annex H Annex E.7 6.1 Information is presented, analysed and interpreted systematically and \nlogically.", " 6.2  The analysis is presented against the evaluation questions and criteria.", " The evaluation is transparent about the sources and quality of information, \nand references or sources are provided. \nEvidence can be traced through the analysis and into findings and \nrecommendations. There is sufficient cross-referencing. \nThe analysis includes an appropriate reflection of the views of different \nstakeholders (reflecting diverse interests). \nThe analysis is disaggregated to show impact and outcomes on the \ndifferent stakeholder groups. \nThe analysis explores the cross-cutting issues of gender, poverty, human \nrights, HIV/AIDS, environment, anti-corruption, capacity building, and \npower relations.", " Sections 4, 5 and 6 Sections 4, 5 and 6 \nAnnexes C and H Sections 4, 5 and 6 Sections 4, 5 and 6 Sections 4, 5 and 6 Sections 4, 5 and 6 7. FINDINGS 7.1  Findings follow logically from the analysis. \n7.2  Findings address the evaluation questions and criteria. \n7.3  The relevance of the context (e.g. developmental, policy, institutional) is Sections 4, 5 and 6 \nSections 4, 5 and 6 \nSections 3, 4, 5 and 6 7.4  The evidence is clear and sufficiently triangulated. \n7.5  Findings are useful and they are presented in ways that are accessible to Sections 4, 5 and 6 \nSections 4, 5 and 6 7.6  Findings reflect diverse views and interests. If not, there is adequate Sections 4, 5 and 6 taken into account.", " different users.", " explanation for omissions.", " 7.7  There are appropriate and sufficient findings provided around the cross Sections 4, 5 and 6 cutting issues of gender, poverty, human rights, HIV/AIDS, environment, \nanti-corruption, capacity building, and power relations. \nIssues of attribution are considered.", " 7.8 \n7.9  Unintended and unexpected findings are identified.", " Section 4 \nSections 4, 5 and 6 A114 5.6 5.7 5.8 6.3 6.4 6.5 6.6 6.7 Corresponding \nSection Ref  EQUALs Criteria 8. RECOMMENDATIONS 8.1  Recommendations follow logically from the findings and evidence cited. \n8.2  They are relevant to the evaluation and targeted at the intended users. \nThey are prioritised and clearly presented, enabling individuals or \ndepartments to follow up on each specific recommendation.", " 8.3 9. LESSONS 9.1  Lessons contribute to general knowledge and they are useful.", " 9.2 9.3 9.4 Lessons are valid (i.e. they have not been generalised from single point \nfindings). \nLessons reflect the interests of different stakeholders, including different \nsexes. \nLessons are presented separately with a clear logical distinction between \nfindings, recommendations and lessons learned.", " 10. USEFULNESS Section 7 \nSection 7 \nSection 7 Section 6 \nSection 6 Section 6 Sections 4, 5, 6 and 7 10.1 10.2 10.3 10.4 The report addresses the needs of the TOR, and evaluation questions are \nadequately covered by the report. If not, departures from the TOR are \njustified. \nThe evaluation has been designed and managed to meet the information \nand decision-making needs of the intended users. \nStakeholders and end-users have been given opportunities to comment on \nthe draft findings, recommendations and lessons. The evaluation report \nreflects those comments and acknowledges disagreements. \nThere is a communications plan within the report. It suggests how \ndissemination of evaluation results could lead to improved accountability.", " n/a Annex E.7 Annexes E and K Annex E.9.3.3 11. INDEPENDENCE 11.1 Differences of opinion (within the evaluation team, or amongst stakeholders \nconsulted) are fully acknowledged in the report.", " Annex E.11 11.2  Any conflicts of interest are openly discussed.", " 11.3 11.4 The report indicates whether the evaluation team was able to work freely \nand without interference. \nInformation sources and their contributions were independent of other \nparties with an interest in the evaluation.", " Annex E.12 \nAnnex E.10 and E.12 Annex E.10 A115 Annex E: Evaluation methodology This Annex sets out the full evaluation methodology. The annex focuses on Research Wave \n1, but where appropriate, reference is also made to the following research waves.", " The Annex includes: E.1  Evaluation  Framework,  along  with  the  DIB  effect  indicators,  which  supplement  the \nevaluation framework This is followed by the three levels of research E.2 DIB-level research, including detail on E2.1 Data analysis E2.2 Document Review E2.3 DIB consultations and field visits E2.4 Use of process tracing E2.5 Research in comparator sites E2.6 Cost analysis E.3 Programme-level Research E.4 Sector-level Research E.5 Approach to data collection E.6 Analysis, Reporting and Dissemination E.7 Involvement of stakeholders E.7.1 Validation of findings E.7.2 Confidentiality E.7.3 Independence E.7.4 Differences of opinions E.7.5 Conflicts of interest and other limitations We then summarise the other areas of our evaluation methodology A116 Research Wave DIBs level research Programme \nlevel research Wider impact \nbond sector Methods w\ne i v\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nB\nD I s\ne\nt\ni\ns r\no\nt\na\nr\na\np\nm\no\nC s i l s\ny\na\nn\na t\ns\no\nC i s\ns\ny\na\nn\na l a\nt\na\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nD\nF\nD I i w\ne\nv\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nd e\nm\nm\na\nr\ng\no\nr\nP w\ne i v\ne\nr e\nr\nu\nt\na\nr\ne\nt\ni\nL s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc l r\ne\nd\no\nh\ne\nk\na\nt\nS s\np\no\nh\ns\nk\nr\no\nw i g\nn\nn\nr\na\ne\nL See DIB effect indicators \nset out in Annex E.", " x x x x x x x x E.1 Evaluation Framework Table E.1: Evaluation Framework Key evaluation \nquestions Relevance, efficiency, \neffectiveness (and additionality \ncross cutting) Indicators EQ1: Assess how \nthe DIB model \naffects the design, \ndelivery, \nperformance and \neffectiveness of \ndevelopment \ninterventions.", " Effectiveness37 To  what  extent  were  the  three  DIB \nprojects successful in realising their \naims,  outputs,  outcomes  and \nimpacts?", " To  what  extent  was  the  level  of \nsuccess  and  failure  due  to  the  DIB \nmodel - was the DIB model a small, \nmedium  or  large  driver  of  success \nand  was  it  at  all  critical  to  the \nprojects\u2019 overall performance?", " Did  the  DIB  model  provide  added \nvalue in relation to the cross-cutting \nissues  of  gender,  poverty,  human \nrights, HIV/AIDs, environment, anti-\ncorruption,  capacity  building  and \npower relations?", " 1\nW\nR 2\nW\nR 3\nW\nR x x x x 37 \u201cEffectiveness\u201d refers to the OECD DAC criteria of Effectiveness \u2013 A measure of the extent to which an aid activity attains (or is likely to attain) its objectives.", " A117 Research Wave DIBs level research Programme \nlevel research Wider impact \nbond sector Methods w\ne i v\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nB\nD I s\ne\nt\ni\ns r\no\nt\na\nr\na\np\nm\no\nC s i l s\ny\na\nn\na t\ns\no\nC i s\ns\ny\na\nn\na l a\nt\na\nD 1\nW\nR\nx 2\nW\nR\nx 3\nW\nR\nx s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nD\nF\nD I x i w\ne\nv\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nd e\nm\nm\na\nr\ng\no\nr\nP x w\ne i v\ne\nr e\nr\nu\nt\na\nr\ne\nt\ni\nL x s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc l r\ne\nd\no\nh\ne\nk\na\nt\nS s\np\no\nh\ns\nk\nr\no\nw i g\nn\nn\nr\na\ne\nL x x x x x x x x x x x x x x x x x x x x x x x Key evaluation \nquestions Relevance, efficiency, \neffectiveness (and additionality \ncross cutting) Indicators Where  was  the  DIB  model  most \neffective  -  was  its  greatest  value  in \nthe  design,  delivery, \nterms  of \ncost \ndevelopment, \nrelationship \neffectiveness, \ntime  efficiency  or \nimpact on beneficiaries?", " Comparisons To  what \nthe \nextent \neffectiveness  vary  across  the  three \nprojects and why?", " does does effectiveness \nthe \nHow \ncompare to other DIBs and funding \nmechanisms and why?", " Spillovers To what extent did stakeholders \ninvolved in the DIB use any of the \nworking practices of the model in \ntheir other work? To what extent did \ngood practice within the DIBs \nspread to other interventions or \norganisations?", " Extent  to  which  systems \nand \npractices \nimplemented  as  part  of \nproject  are  embedded \nwider \nthe \nacross \norganisation \nand/or \nsustained  once  the  DIB \nends A118 Does the increased evidence base \ndeveloped in the DIB enable the Funding accessed by the \nfrom \nprojects  resulting x x x x x x x Key evaluation \nquestions Relevance, efficiency, \neffectiveness (and additionality \ncross cutting) Indicators EQ 2: What \nimprovements can \nbe made to the \nprocess of \ndesigning and \nagreeing DIBs to \nincrease the \nmodel\u2019s benefits \nand reduce the \nassociated \ntransaction costs?", " projects to access additional \nfunding?", " evidence the \ndeveloped in the DIB base Efficiency What (if any) are the extra costs of \ndesigning  and  delivering  a  project \nusing a DIB model and how do they \nfunding \nto \ncompare \nmechanisms?", " other Where  are  the  extra  costs  most \nprevalent  and  what  specific  items \n(staff,  monitoring  procedures  etc.) \nhave  the  highest  costs?  Are  these \nextra  costs  mainly  found  in  the \ndesign or delivery stages?", " Do  the  extra  costs  represent  value \nfor  money  -  to  what  extent  do  they \nlead  to  additional  results,  impacts \nand benefits?", " Do any aspects to a DIB model (e.g. \ninvolving  an  investor,  undertaking \nverification of outcomes) shorten or \nextend the timeframes of projects?", " Who pays for these additional costs \nand to what extent do they see the \nbenefits?", " Additional costs of the \nimpact bond, \ndisaggregated where \npossible by:  \n\u2022 stage (design, set-up, \ndelivery, learning);  \n\u2022 actor who incurs this \ncost; and \n\u2022 type of cost (staff time, \nconsultancy and \nexpertise costs, and the \nrisk premium (return to \ninvestors, including \ninterest)  \nSavings in programme \ncosts (including staff \ntime) as a result of the \nimpact bond.  \nHow effectively has risk \nbeen transferred - what \nis the alignment of Research Wave DIBs level research Programme \nlevel research Wider impact \nbond sector w\ne i v\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nB\nD I s\ne\nt\ni\ns r\no\nt\na\nr\na\np\nm\no\nC i s\ns\ny\na\nn\na l a\nt\na\nD 1\nW\nR 2\nW\nR 3\nW\nR i w\ne\nv\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nd e\nm\nm\na\nr\ng\no\nr\nP w\ne i v\ne\nr e\nr\nu\nt\na\nr\ne\nt\ni\nL s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc l r\ne\nd\no\nh\ne\nk\na\nt\nS s\np\no\nh\ns\nk\nr\no\nw i g\nn\nn\nr\na\ne\nL Methods s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nD\nF\nD I s i l s\ny\na\nn\na t\ns\no\nC x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x A119 transferred risks with \nreturn?", " 1\nW\nR 2\nW\nR\nx 3\nW\nR\nx Research Wave DIBs level research Programme \nlevel research Wider impact \nbond sector Methods w\ne i v\ne\nr e\nr\nu\nt\na\nr\ne\nt\ni\nL\nx s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc l r\ne\nd\no\nh\ne\nk\na\nt\nS\nx s\np\no\nh\ns\nk\nr\no\nw i g\nn\nn\nr\na\ne\nL\nx w\ne i v\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nD\nx s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nB\nD\nx I i s\ns\ny\na\nn\na l a\nt\na\nD\nx s\ne\nt\ni\ns r\no\nt\na\nr\na\np\nm\no\nC s i l s\ny\na\nn\na t\ns\no\nC\nx x s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nD\nF\nD\nx I i w\ne\nv\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nd\nx e\nm\nm\na\nr\ng\no\nr\nP x x x x x x x x x x Key evaluation \nquestions Relevance, efficiency, \neffectiveness (and additionality \ncross cutting) Indicators Are there any inefficiencies in a DIB \nmodel  that  can  be  reduced  or  are \nthere  any  additional  costs  that  are \nunnecessary?", " Comparisons To what extent does the efficiency of \nthe  DIB  set  up  vary  between  the \nthree DIB projects and why?", " How does the efficiency compare to \nother DIBs and funding mechanisms \nand why?", " Level of transaction \ncosts of setting up a DIB \ncompare with the \naverage costs for other \nfunding mechanisms \n(e.g. fee-for-service \ncontracts) \nChanges in transaction \ncosts over time (as \nprojects start to learn \nfrom previous \nexperience) \nNumber of direct \nbeneficiaries with \nimproved outcomes as a \nresult of DFID funded \nDIB projects Relevance In  what  circumstances  are  DIBs \nrelevant  in  tackling  issues  in  the \ndevelopment context?", " Level of returns and \nprofit made by the x x x x x x x x x A120 Research Wave DIBs level research Programme \nlevel research Wider impact \nbond sector Methods w\ne i v\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nB\nD\nx I s\ne\nt\ni\ns r\no\nt\na\nr\na\np\nm\no\nC s i l s\ny\na\nn\na t\ns\no\nC i s\ns\ny\na\nn\na l a\nt\na\nD s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc\nD\nF\nD\nx I i w\ne\nv\ne\nr\n \nt\nn\ne\nm\nu\nc\no\nd\nx e\nm\nm\na\nr\ng\no\nr\nP w\ne i v\ne\nr e\nr\nu\nt\na\nr\ne\nt\ni\nL\nx s\nn\no\ni\nt\na\nt\nl\nu\ns\nn\no\nc l r\ne\nd\no\nh\ne\nk\na\nt\nS\nx s\np\no\nh\ns\nk\nr\no\nw i g\nn\nn\nr\na\ne\nL\nx 1\nW\nR 2\nW\nR 3\nW\nR\nx x x x x x x x x x x x x Key evaluation \nquestions Relevance, efficiency, \neffectiveness (and additionality \ncross cutting) Indicators What  social  issues,  target  groups, \ngeographies  and  project  scales  do \nDIBs fit best and have the greatest \nof impact?", " - is appropriate in \nAre \nDIBs \ndevelopment  contexts \nthe \nexistence of investors (and possible \nprofits),  payment  only  when  results \nare  made  and  strong  expectations \naround  measuring \noutcomes \nappropriate \nfor  donors  such  as \nDFID?", " To what extent are DIBs applicable \nto  DFID\u2019s  work  -  are  they  relevant \nacross  most,  some  or  a  few  of \nDFIDs priority result areas?", " investors and extent to \nwhich that influences \nfuture involvement in \nboth DIBs and \ndevelopment projects \nNumber of DFID \nsupported DIB projects \nwith improved cost-\neffectiveness ratio \ncompared with service \nproviders' own past \nperformance \nProportion of new DFID \nDIB instruments \ncommissioned that are \ninformed by \nrecommendations of \nDFID DIBs evaluation \nreports. \nNumber of new DFID \nprogrammes interacting \nwith DIBs guidance, \nevaluation findings and \nreports.", " A121 The table below provides a breakdown of the potential \u2018DIB effect\u2019, and the indicators we used \nwithin the DIBs and comparator sites to identify the extent to which these effects are present. \nThe potential \u2018DIB effect\u2019 is drawn from: \u2022  Programme Theory of Change \n\u2022  DFID DIB Business Case \n\u2022  Advantages and disadvantages identified during the literature review \n\u2022  Advantages and disadvantages (perceived or experienced) identified during inception phase consultations An initial set of DIB effects and indicators were provided in the Inception Report. These were \nrefined following RW1, to allow for a more nuanced description of the DIB effects.", " Table E.2: DIB effects and indicators Claimed DIB effect Indicator to measure presence of \u2018DIB effect\u2019 in DIBs and \ncomparator sites 1\nW\nR x x x 2\nW\nR 3\nW\nR \u2022  Extent to which investment capital is at risk \u2022  Extent to which outcome funders would have either \nfunded  the  project  at  all,  or  in  its  current  form,  if  it \nwere funded through a different mechanism \u2022  Scale  and  source  of  funding  (including  whether \nprivate financing), and where this funding would have \nbeen directed if it had not funded this project \u2022  Duration and \u2018security\u2019 of funding \n\u2022  Mobilization ratio: for every USD 1 of ODA mobilized USD x in private financing \u2022  Extent  that  supplier  pre-financing  was  required  for PbR contract \u2022  Opportunity  cost  of  using  own  funds  \u2013  i.e.  has  DIB \nfinancing allowed the organization to invest in other \nthings of: innovation); \u2022  new type  of intervention  altogether (radical \u2022  an  established  intervention  that  has  been  adapted (incremental innovation); or \u2022  an established intervention that has been applied to \na  new  context,  e.g.  location,  policy  area,  target \npopulation \u2022  Scale of project, in terms of delivery cost and number of beneficiaries \u2022  Extent and quality of external expertise Set up \u2022  Perceptions on rigour of design stage \n\u2022  Level of \u2018innovation\u2019 / risk in project delivery, in terms x \nx x x Delivery \u2022  Extent  to  which  delivery  decisions  are  made  to maximise outcomes x x x x have funder Claimed advantages \nTransfer  of  financial  risk \nto \nfrom  outcome \ninvestor \nFunding projects which would \nnot \nfunded \notherwise,  or  not  in  the  same \nguise (including scale) \nCrowd-in  private,  additional, \nupfront,  long-term,  stable  and \nfinancing,  which \nsecured \nbrings in additional finances to \nthe development sector been Shift focus to outcomes \nMore  innovative  services  (or \nlarger-scale \ninnovative \nservices) because: \n\u2022  providers have  more \nflexibility  and  autonomy \nto  deliver  what  they  feel \nwill achieve outcomes \u2022  Risk transfer from government/outcome \nto  service \nfunder  partly \nprovider  but  mainly \nto \ninvestor,  who  have  higher \nappetite for risk performance Drives \nmanagement \nGreater  accountability,  as \nimpact  bond  builds  leads  to \nculture  of  monitoring  and \nevaluation A122 Claimed DIB effect Indicator to measure presence of \u2018DIB effect\u2019 in DIBs and \ncomparator sites 1\nW\nR 2\nW\nR x 3\nW\nR x More  careful  and \nof \ndesign \ninterventions rigorous \nprogramme \u2022  Extent  to  which  a  service  provider  feels  more \nincentivised  to  offer  user-specific  supports  (the \nhuman touch element) \u2022  Level  of  flexibility  found  within  the  project  to  alter Monitoring project delivery and innovate \u2022  Extent to which service provider feels it can take risks \u2022  Extent to  which  service  provider feels it  has autonomy over delivery \u2022  Level  of  responsiveness  and  agility  of  partners  to deal with bottlenecks, issues and challenges \u2022  Extent and quality of external expertise \u2022  Rigour  of  monitoring  and  evaluation  systems \ndeveloped,  including  verification  of  outcomes  and \nduration of outcomes tracking \u2022  Transparency  of  outcomes  \u2013  i.e.  frequency  and quality of reporting internally and externally \u2022  Strength  of  performance  management  and measurement systems \u2022  Use  of  real  time  performance  information  to  inform ongoing delivery Sustained impact \u2022  Extent to which systems and practices implemented \nas  part  of  project  are  embedded  across  the  wider \norganisation and/or sustained once the DIB ends \n\u2022  Number of beneficiaries supported per GBP / FTE \n\u2022  Number of outcomes achieved per GBP / FTE x x \u2022  Number  and  type  of  providers  participating  in  PbR \ncontracts,  and  their  historic  experience  with  PbR \ncontracts \u2022  Level of unrestricted funding as % of overall value of x x x \u2022  Self-reported  strength  of  relationship  of  partners \nlevels  of  collaboration  and/or x x x PbR contract involved  and \ncoordination \u2022  Extent to which stakeholders believe the design to be \u2022  Demands of project design in terms of time and need complex for external expertise \u2022  Length of time it took to design and launch the project \n\u2022  Set up costs \n\u2022  Cost per outcome / beneficiary \n\u2022  Proportion of total cost of project going to front line \ndelivery  against  proportion  going \nto  project \ndevelopment and administration (including research \nand  data  verification,  and  project  and  funding \ncoordination and management) x x x x and more All of the above factors leading \nbeneficiaries \nto \nsupported, \nmore \noutcomes  achieved,  ultimately \nleading to more effective and \nefficient services \nMore \nproviders \nservice \nentering  the    PbR  market  due \nto transfer of risk Greater  collaboration  and/or \ncoordination \nbetween \nstakeholders  as  there  is  an \nalignment of interests \nClaimed disadvantages \nComplex to design Expensive \nimplement to  set  up  and A123 Claimed DIB effect Indicator to measure presence of \u2018DIB effect\u2019 in DIBs and \ncomparator sites Impact bonds create perverse \nincentives picking\u2019 \u2022  Profile  of  beneficiaries  and  evidence  of  \u2018cherry \u2022  Level,  quality,  range  and  duration  of  support,  and \nextent  to  which  decisions  around  these  have  been \naffected  by  the  contracting  model  (e.g.  leading  to \nparking) \u2022  Levels of morale amongst staff \n\u2022  Levels of staff turnover \u2022  Range and level of secondary outcomes achieved x x 1\nW\nR 2\nW\nR x 3\nW\nR x x x \u2022  Extent to which stakeholders perceive the project to x x x hold reputational and social risks The purpose of this level of research is to assess how the DIB mechanism has impacted on \nthe set up, delivery, performance and costs of each of the three DFID DIB pilots. To achieve \nthis, the team undertook the following tasks, which are detailed below: management \nPerformance \nculture  lowers  staff  morale \nand increases staff turnover \n\u2018Tunnel  vision\u2019:  Focus  on \nprimary  outcomes  comes  at \nthe  expense  of  secondary \noutcomes;  opportunities \nfor \nproject co-benefits are missed \nDIB creates  additional social \nrisks, \nand \nreputational \ndiminishing \nthe \nclaimed  advantages  (such  as \ninnovation) some  of E.2 DIB-level research \u2022  Data analysis \n\u2022  Document review \n\u2022  DIB consultation and field visits \n\u2022  Use of process tracing \n\u2022  Research in comparator sites \n\u2022  Cost analysis E.2.1.  Data Analysis (including DQA) As  part  of  the  evaluation  the  evaluation  team  aimed  to  gather  quantitative  data  on  the \nperformance of the DIBs, including progress in supporting beneficiaries, achieving outcomes \n(including  secondary  outcomes  and  the  extent  to  which  these  sustained)  and  outcome \npayments  and  returns  to  investors.  Due  to  the  early  stage  of  the  four  DIBs,  the  evaluation \nteam collected the planned figures, and the actual figures will be collected over the next two \nresearch waves, in order to assess the performance of DIBs against expectations.", " Annex E sets out the individual DIB level data collection and consultation plans in terms of the \nindicators that data will be collected against and the expected data sources (including both \nprogramme documentation, monitoring data and consultations).", " A124 As the evaluation team will be relying on data collected by the project themselves, the team \nhas assessed the quality of the monitoring and evaluation systems through our Data Quality \nAssessment (DQA) Checklist. These are set out in Annex G.", " E.2.2.  Document Review The evaluation team reviewed key documents related to each DIB to understand further the \nset up phase (See Annex H for list of documents reviewed).", " E.2.3.  DIB consultations and field visits The purpose of the consultations with stakeholders involved in the DIB projects is to identify \nhow the DIB mechanism is affecting the set up, delivery and performance of the project; and \nlessons learnt in implementing the DIB that could be applied to either later stages in the DIB, \nor future DIBs.", " The  table  below  sets  out  the  stakeholders  consulted  in  research  wave  1,  and  the  areas \ndiscussed. The precise areas discussed were tailored depending on the role of the interviewee \nwithin the DIB and the point of progress of each DIB, and sent in advance to DIB stakeholders.", " Table E.3: Stakeholder consultations in RW1 Stakeholder type Areas discussed: Wave 1 (Set up) Project  managers \n/ \nperformance managers /  \nintermediaries Service provider: Project \nmanagers Service \nService managers provider: funders / \nOutcome \ndonors  (including  DFID \nand other donors) Investors Outcomes \nagents verification Project \nevaluators \npartners level  process \nlearning / Progress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nView on DFID\u2019s role in the DIB \nReasons  for  getting  involved  in  project,  including  what  they  hope  to \nachieve and concerns \nProgress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nView on DFID\u2019s role in the DIB \nProgress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nReasons for getting involved in project, including what hope to achieve \nand concerns \nProgress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nView on DFID\u2019s role in the DIB \nReasons for getting involved in project, including what hope to achieve \nand concerns \nProgress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nView on DFID\u2019s role in the DIB \nProgress and lessons learnt in setting up project; what factors affected \nthis progress (including the DIB); and how things could be improved for \nthis DIB and future DIBs \nFindings from activity completed to date A125 The sampling strategy used was purposive. Given the focus on the set up phase, there was a \nlimited number of stakeholders involved, and random sampling was not considered necessary \nor appropriate. For the DIB-level research, for the most part, the evaluation team contacted all \nrelevant  stakeholders,  namely  investors,  service  providers,  outcome  funders,  performance \nmanagers  and  outcome  evaluators,  with  the  aim  of  gathering  and  comparing  different \nperspectives, and trying to avoid biases. On certain occasions, the team managed to interview \nmore than one representative from the same organisation, as their role within the DIB differed \nand this allowed us to collect more accurate information. Some stakeholders did not participate \nin  the  evaluation.  However,  the  team  has  tried  to  address  this  by  drawing  on  a  range  of \nprogramme  documentation,  and  triangulating  the  findings  and  data  from  the  existing \nstakeholder interviews.", " The  table  below  sets  out  the  number  of  organisations  interviewed,  and  the  number  of \norganisations per impact bond stakeholder category. By stakeholder group, we mean the key \nstakeholders involved in the impact bond model, including outcome funders, investors, service \nproviders, outcome evaluators and advisors/performance managers. The list of stakeholders \nper DIB is set out in section 3. This is summarised in the \u2018total\u2019 columns. The structures of the \nDIBs varied quite significantly. Hence, for example, there were 3 service providers in the QEI \nDIB, but only 1 in the other 3 DIBs.", " In parenthesis in this table under the \u2018interviewed\u2019 columns, we have included the number of \nindividuals interviewed. A full list of stakeholders interviewed is set out in Annex H. For the \nmost part, we sought to speak to all stakeholders, with the following exceptions: \u2022 \u2022 \u2022 In the  case  of  the  ICRC  HIB,  we  did  not  receive  responses  from  2  of  the  outcome \nfunders, nor from 1 investor.", " In the case of the QEI DIB, we were informed that the engagement of two outcome \nfunders was too recent unstable, and was advised to wait to consult with them during \nthe next research waves.", " In the case of the VE, two out of the three outcome funders were sampled. We will \nspeak to the third outcome funder over the next two research waves. Additionally, VE \nhas a consortium of investors, and a purposive sample was taken. The research team \ninterviewed  the  lead  investor  and  a  secondary  investor  recommended  by  the  lead \ninvestor.", " \u2022  Finally,  where  there  had  been  minimum  activity  on  the  part  of  the  outcome \nevaluators/verifiers,  we  decided  to  not  consult  with  them  during  this  first  research \nwave.", " Table E.3: Stakeholder consultations per DIB ICRC QEI VE Cameroon \nCataract d\ne\nw\ne\nv\nr\ne\nn t i I d\ne\nw\ne\nv\nr\ne\nn t i I l a t o\nT l a t o\nT l a t o\nT l a t o\nT d\ne\nw\ne\nv\nr\ne\nn t i I d\ne\nw\ne\nv\nr\ne\nn t i I A126 Outcome Funders \nInvestors 3 (4) \n1 (1) 5 \n2 3 (5) \n1 (3) 5 \n1 2 (3) \n2 (2) 3 \n- 3 (4) \n2 3 \n2 ICRC QEI VE PbR Comparator sites \nAdvisors / Intermediaries / \nPerformance Managers \nService Providers \nOther funders \nOutcome Evaluator \nDIB researchers 1 (2) \n1 (3) 1 (2) \n0 \n0 \n- n/a \n1 1 \n1 \n1 \n- 1 (2) \n3 (4) 3 (3) \n1 (2) \n1 (1) \n- n/a \n3 3 \n1 \n1 \n- 0 \n1 (4) 1 (4) \n- \n0 \n- Cameroon \nCataract n/a \n1 0 \n1(2) n/a \n1 1 \n- \n1 \n- 1 (2) \n- \n0 \n1 (1) 1 \n- \n1 \n1 Notes: The \u201cinterviewed\u201d column sets out the number of organisations interviewed, and in parenthesis, the number of individuals \ninterviewed (in certain organisations, we interviewed more than one individual). The \u201ctotal\u201d column sets out the total number of \norganisations within this stakeholder category.", " During research waves 2 and 3 the evaluation team will undertake a field visit to each DIB to \nconsult with local stakeholders face-to-face. These visits will be undertaken by members of \nthe central evaluation team and local researchers, who will assist in understanding the local \ncontext. A proposed list of consultations agreed with the DIBs is set out in the individual DIB \nlevel plans in Annex F.", " E.2.4.  Use of process tracing One of the most challenging aspects of the evaluation is to isolate the effect of the DIB on \nproject  performance  and  delivery  \u2013  the  \u2018DIB  effect\u2019.  There  is  a  substantial  range  of  factors \nexogenous to the DIB mechanism that could influence performance and delivery, particularly \nthe  national  and  local  economic,  social  and  political  context,  and  the  extent  to  which  this \nremains  stable  throughout  project  delivery.  Depending  on  the  metrics  used  to  measure \noutcomes,  these  external factors  may  confound the  intervention  effect  or DIB  effect.  Some \nstakeholders,  particularly  those  incentivised  to  grow  the  impact  bond  market  (such  as \ninvestors who wish to invest in more DIBs), may be inclined to exaggerate the \u2018DIB effect\u2019, and \nattribute  all  aspects  of  performance  and  delivery  to  the  DIB  mechanism.  Equally,  other \nstakeholders  (such  as  practitioners)  may  be  ideologically  opposed  to  the  mechanism,  and \ninclined  to  exaggerate  its  negative  effects.  Finally,  others  (such  as  local  organisations  and \nbeneficiaries) may be unaware of the DIB, and would attribute no aspects of performance and \ndelivery to the model. It is therefore important to implement a robust approach that identifies \nthe DIB effect in a structured and independent manner.", " Ecorys  has,  through  its  previous  impact  bond  evaluations,  developed  an  approach  for \nidentifying  the  DIB  effect.  This  involves  estimating  the  counterfactual  (what  would  have \nhappened  if  the  projects  were  delivered  through  alternative  funding  mechanisms)  by \nidentifying the differences between delivery of this project and other similar interventions, and \nusing process tracing to understand the extent to which these differences can be attributed to \nthe DIB. Process tracing is a qualitative research method for assessing causal inference within \nsmall-n studies. The method seeks to assess the causal chain that link independent variables \nand outcomes. The method recognises that there will not be one single factor that can explain \nwhy an outcome was achieved; instead it seeks to assess the relative contribution of different \nfactors. This approach, and how it was used in this evaluation, is detailed below.", " A127 This approach aligns with DFID\u2019s Evaluation Framework for PbR. The Framework notes the \nimportance  of  identifying  and  measuring  the  effects  of  PbR  and  proposes  this  is  done  by \nidentifying  and  testing  \u201cto  what  extent  expected  outcomes  are  caused  by  the  payment \napproach  and  how\u201d  and  \u201ccomparing  PBR  with  other  available  aid  instruments  to  establish \nappropriateness and value for money in different development interventions\u201d.", " 1.  Process  induction  and  creation  of  \u2018DIB  effect\u2019  indicators:  The  evaluation  team \nproduced  a  set  of  indicators  through  which  to  measure  the  outcomes  the  DIB \nmechanism is expected to achieve. This indicator set draws on the ToCs and has been \ndeveloped  in  consultation  with  DFID  and  stakeholders  from  the  DIB  projects  during \nWP1: Inception.", " 2.  Examine presence of indicators in DIB areas: We examined the extent to which the \nDIB effect indicators are present within the DIBs. We used both qualitative data (for \nexample, consultations with DIB stakeholders) and quantitative data (for example, the \nnumber of beneficiaries supported and outcomes achieved) to identify the indicators. \nWhilst this provides a structured approach for identifying the DIB effect, we also asked \nmore open-ended questions in relation to the impact of the DIB on project performance \nand delivery, in order to identify unintended factors outside of the programme ToC. We \nalso  examined  the  presence  of  these  indicators  in  other  impact  bonds  (through  a \nliterature review and consultations with stakeholders involved in other DIBs), to assess \nthe extent to which indicators hold true across multiple contexts.", " 3.  Examine presence of indicators in non-DIB areas: During WP2: DIB-level research we \nalso identified whether the DIB effect indicators are present within similar interventions \ndelivered  through  alternative  funding  mechanisms.  This  include  VE\u2019s  traditional \nprogramming; schools and geographies where the service providers of the QEI DIB \ndeliver  their  interventions  through  grant  funding;  the  generic  Physical  Rehabilitation \nProgramme  (PRP),  in  the  case  of  ICRC;  and  eye  care  hospitals  in  low-income \ncountries  funded  through  traditional  mechanisms,  in  the  case  of  the  Cameroon \nCataract. The analysis of the comparator sites was conducted through both primary \nresearch (for example, interviews  with DIB stakeholders who have been involved  in \nprevious or simultaneous similar interventions) and secondary research (for example, \nevaluations and research of similar interventions).", " We undertook the following activities to identify the presence of DIB effect indicators: \u2022  Consultations  with  stakeholders  involved  in  DIBs:  A  number  of  the  stakeholders \ninvolved  in  the  DIB  pilots  were  involved  in  similar  interventions  funded  through \nalternative  mechanisms.  We  asked  stakeholders  to  compare  the  delivery  and \nperformance of the intervention in the DIB to alternative funding mechanisms.", " \u2022  Analysing qualitative data from comparator sites: A number of the DIB effect indicators \nare qualitative in nature, and cannot be identified and analysed through quantitative \ndata (such as, for example, the level of flexibility found within the project, or the level \nof collaboration achieved between stakeholders). During the literature and programme \ndocument review we therefore identified the extent to which the qualitative DIB effect \nindicators were present in the comparator sites.", " A128 The  Ecorys  team  reviewed  the  evidence  from  comparator  sites  during  this  research  wave, \nwhich enabled the evaluation team to use the consultations to explore what might explain the \ndifferences between the DIB and non-DIB areas (including the use of the DIB), as part of the \nprocess verification approach. As illustrated in the conceptual framework, the analysis of the \ncomparator sites is important to illuminate whether the DIB mechanism provides advantages \n\u2013 in terms of effectiveness and efficiency - over alternative funding mechanisms used in similar \ninterventions.", " 4.  Analyse  difference  between  DIB  and  non-DIB  areas:  This  analysis  identifies  the \nelements  that  are specific  to  the  DIBs  and that  are not  present,  or are  present to a \nlesser  degree,  when  the  interventions  are  delivered  through  alternative  funding \nmechanisms.", " 5.  Process verification: The evaluation cannot assume that any differences between the \nDIB  and  non-DIB  areas can  be  attributed to the DIB  mechanism. We used  process \nverification to assess the extent to which the DIB mechanism contributed to the DIB \neffect indicators, relative to the other possible  explanations. This involved analysing \nthe qualitative and quantitative data to understand the relative contribution of different \nfactors on the outcomes, as well as holding structured discussions with stakeholders \nabout  their  own  interpretations  of  the  main  DIB  effects,  through  interviews  and \nworkshops.", " E.2.5.  Comparator sites In order to identify the DIB effect, one would ideally want to compare the DIBs with a similar \nprogramme not funded by a DIB, but through other funding mechanisms. In order to do this, \nwe  identified  two  forms  of  comparisons.  Firstly,  we  identified  similar  programmes  being \ndelivered by the same service providers funded by the DIDs, but which were funded under \ngrants.  Secondly,  we identified  programmes  working  in similar  sector  and  contexts, funded \nunder payment by results. The table below summarises the comparator sites: DIB \nICRC HIB QEI DIB VE DIB Grant funded programme \nPhysical Rehabilitation Programme, delivered \nby ICRC \nOne programme per service provider (three in \ntotal) \nCurrent grand-funded programme.", " PbR funded programme \nWorld Bank Global Partnership \non Output-Based Aid  \nGirls Education Challenge Helvetas livelihood programme Due to the late engagement of the Cameroon Cataract Bond, no comparator sites have yet \nbeen identified, although there are potential sites which are being discussed with stakeholders. \nAdditional work will be undertaken as part of the KiT review, in order to undertake comparative \nanalysis between the Cameroon Cataract Bond and its comparator sites. Further detail is set \nout below.", " Grant funded programmes As part of the inception phase, a list of parameters which would affect the comparability of \nprogrammes was developed based on discussion within the evaluation team and DFID. The \nevaluation team then worked with the service providers and intermediaries, in order to identify \npotential  comparator  sites,  and  assessed  the  similarity  to  our  impact  bonds  along  the \nparameters of: project purpose and objectives, service provider and processes used, countries \nof operation, context, time period, size of project, level of donor oversight/influence, payment A129 structure and availability of data and stakeholders. The comparator sites used for each DIB is \ndiscussed in further detail below: ICRC: As the centres are functioning within the broader Physical Rehabilitation Programme \n(PRP), one can find natural comparisons in the other ICRC centres running under the PRP. \nDuring RW1, we compared the HIB to the broader PRP. It may be that during the subsequent \nresearch waves it would be useful to focus on particular, comparable sites. This can include \neither historic comparisons, such as the centres providing historic data for the benchmarking \nof  the  outcome measure,  the  centres  where the efficiency  measures  are being  piloted  and \nother new centres. In order to understand how comparable the centres are, the evaluation can \ndraw  upon  ICRC\u2019s  analysis  on  the  different  factors  (such  as  ownership  of  centre,  location, \nlevel  of  ICRC  involvement  etc.)  which  are  considered  to  be  the  main  drivers  of  efficiency. \nMonitoring  and  evaluation  (M&E)  data  will  be  available  at  all  centres,  and  the  additional \nmeasures of efficiency will be available to different degrees for the other centres.", " QEI: The three NGO organisations are all expansions of programmes with existing evidence \nof their effectiveness (improvements in outcomes compared to a counterfactual). Therefore, \nwe worked with the service provider to compare data on performance, and qualitatively explore \nthe differences in how the project was set-up and developed, at what cost, and what was the \nworking relationship between the stakeholders involved, in areas affected by DIB and non-DIB \ncontracts.", " Village Enterprise: The programme has been running since 2013, under a traditional grant \nfunded  model,  and  currently  the  DIB  funds  30%  of  the  programme.  Hence,  potential \ncomparison sites include the historical programme, for which there is a RCT, and the current \nprogramme  currently  running  under  the  grant  funded  model.  Management  and  monitoring \ninformation  are  being  collected  for  both  the  DIB  and  non-DIB  elements  of  the  current \nprogramme. While the non-DIB element of the programme has a slightly different focus area, \nit  nonetheless  provides  a  useful  comparison  in  terms  of  understanding  any  changes  in \nprocesses and motivations.", " Our initial plan was to identify a separate, comparator site, and to first undertake interviews \nwith  a  stakeholder  involved  in  this  comparator  site.  However,  given  the  fact  that  we  were \nprovided contacts with the DIB stakeholders, and the fact that there were no comparator sites \nfully  comparable  to  the  DIB  funded  interventions,  it  was  more  practical  to  interview \nstakeholders with both experience of the comparator site, and the DIB funded programmes. \nInterviewees  were  able  to  reflect  and  compare  the  comparator  sites  and  DIB  funded \ninterventions. The consultations involved first discussing the comparator site, using the DIB \neffect indicators as a framework for discussion, before asking stakeholders to compare this \nwith  the  DIB  funded  programme.  In  doing  so,  we  sought  to  obtain  a  clearer  picture  of  the \neffects of the DIB funded mechanism.", " PbR Comparisons One PbR comparator site was identified per DIB. The criteria was PbR funded interventions \nworking  in  similar  sector,  and,  where  possible,  similar  geographies.  DFID  supported  with \nintroductions, and we undertook one interview per comparator site.", " The interview covered the successes and challenges to using PbR and the transaction costs \ninvolved in using PbR. The interview also covered the extent to which these were unique to A130 the sector and geographical location of operation. Finally, interviewees were also asked their \nopinion on the extent to which using a DIB would address the challenges of using PbR.", " The PbR funded programmes identified worked in similar sectors (VE DIB and QEI DIB) and \ncontexts (ICRC HIB), but differed along other dimensions. Hence, a focus of the interview was \non understanding the successes, challenges and costs to using the PbR, and the extent to \nwhich these were affected by the type of intervention and the context.", " E.2.6.  Cost analysis The objectives of the cost analysis are to: \u2022  Collect and analyse the costs of different stages; \n\u2022  Understand the extra costs of designing and delivering a project using a DIB model, and how this compares to other funding mechanisms; \u2022  Assess the extent to which these extra costs lead to additional results, impacts and benefits, and how efficiency compares to other DIBs and funding mechanisms; \u2022  Understand who pays for these additional costs and the extent to which they see the benefits; and \u2022  Consider the appropriateness of the outcome targets and payment mechanisms, which \nwill  affect  the  risk  and  return  transferred  between  the  different  impact  bond \nstakeholders Table E.4: Research Waves Focus Research \nWave \nResearch \nWave 1 Research \nWave 2 \u2022  Set up costs and any cost savings expected \n\u2022  Design of outcome target and payment mechanism, and alignment of risk and \u2022  Full costs of the programme, and cost savings \n\u2022  Outcome measures \n\u2022  Disaggregated data on outcomes \n\u2022  Qualitative data on outcomes and effects on equity arising from the impact bond funding mechanism Research \nWave 3 \u2022  As above  \n\u2022  Levels of returns and profit made by the investors, and service providers if return relevant The table below sets out the evaluation framework for VfM, summarising the approach to \neach of the 4Es.", " 4Es \nEconomy Table E.5: VfM Framework \nDefinition \nThe  cost  of  the  impact \nbond, \nof \ntop \nprogramming costs.", " on Detail \nDIBs  costs  (feasibility  study,  delivery,  design) \nfor all actors, compared with other DIBs, as well \nas PbR and grant funding mechanisms?", " A131 4Es \nEfficiency Definition \nAny  positive  or  negative \nchanges to efficiency as a \nresult of the impact bond.", " Effectiveness  Any  positive  or  negative \nchanges  to  effectiveness \nas  a  result  of  the  impact \nbond.  \nAny  positive  or  negative \nchanges  to  equity  as  a \nresult of the impact bond.", " Equity Detail \nAny savings in programming costs as a result \nof  the  impact  bond.  i.e.  lower  reporting/audit \ncosts.  \n \nHow effectively are the risks being transferred, \nand how well is this aligned with risk?  \nWhat  are  the  effects  on  outcomes  (including \nthose not captured by the outcome measure)  \nHow  well  are  the  programmes  fulfilling  their \ntargeting  strategy?  Are  there  certain  sub-\ngroups  that  are  not  being  reached?  The \napproach  to  equity  will  be  guided  by  the \nindividual programmes\u2019 targeting strategies, to \nunderstand  the  narrative  around  the  target \npopulation.  We  will  seek  to  understand  the \neffectiveness  of  the  targeting  strategy  of  the \nDIB, especially in terms of the hard to reach.", " The equity component of the VfM framework has a particular focus on assessing the cross-\ncutting issues of gender, poverty, human rights and power relations. In assessing the extent \nto which programmes are fulfilling their targeting strategy, we will review the beneficiary data \ncollected, with a particular focus on dimensions of gender, poverty and access. There is a real \nrisk when programmes are paid on outcomes, that there may be incentives to focus on those \nwho are easier to reach. Our assessment of equity will include an assessment to the extent to \nwhich the use of a DIB affected programme\u2019s focus and targeting strategy, and the extent to \nwhich different groups are reached, both in design and practice.", " The  VfM  indicator  framework set  out  by  Barr  and  Christie  (2014)  is  used  to organise  the \nproposed indicators. This provides clarity on the type of indicators we are using (monetary, \nquantitative and qualitative) and the measurement typology, in terms of the comparison to be \nused. The table below sets out the VfM indicators relevant for this research wave.", " 4Es Indicator Measurement typology Table E.6: VfM Indicators 1  Economy Indicator \ntypology \nMonetary the \nAdditional  costs  of \nimpact bond, disaggregated \nwhere possible by:  \n\u2022  stage  (design,  set-up, \ndelivery, learning);  \n\u2022  actor  who  incurs  this \u2022 cost; and \ntype  of  cost  (staff  time, \nand \nconsultancy Benchmark: Against other \nDIBs  (Total  costs,  and  as \n%  of  programme  cost), \nincluding  the  three  DIBs \nunder \nthe  programme. \nCompared  to  similar  PbR \nprogrammes.  \nChanges over time in new \nDIB projects.", " A132 4Es Indicator \ntypology Indicator Measurement typology Comparison: Between the \n3  centres  running  under \nthe  ICRC  HIB  and  the  4 \nservice  providers  running \nthe  BAT  DIB. \nunder \nChanges  between  years \nduring the delivery phase.", " expertise costs, and the \nrisk  premium  (return  to \ninvestors, \nincluding \ninterest) (Clist 2017).  \nThis  should  cover  the  full \ncost, including staff time not \ncharged, of all actors.  \nWhere possible, this will be \ndisaggregated by \u2018first time\u2019 \nwhich \nDIB \ncosts \nhypothetically \nwouldn\u2019t \nhave  to  be  incurred  again \nfor any subsequent DIBs.38 \nCost drivers to be analysed \nwhich \nto \nelements of the DIB are the \nmost \ntime-\nintensive/expensive.  \nprogramme \nSavings \ncosts  (including  staff  time) \nas  a  result  of  the  impact \nbond.", " understand in 2  Efficiency Monetary As above.", " transferred 3  Effectiveness  Qualitative  How  effectively  has  risk \n- \nbeen \ntransferred \nalignment  of \nrisks with return (in relation \nto  the  outcome  target  and \npayment  mechanism  of \nreturn  of \ninvestors  and \nservice provider).  \nWe  can  also  explore  the \nrange  of  potential  returns \nand capital at risk.", " Benchmark: Against other \nDIBs, including the 3 DIBs \nthe  programme. \nunder \nAgainst \ncommercial \ninvestments.  \nwith \nStandalone: \ninvestment \nreference \nto \napproaches \nin \nused \ncommercial  and  blended \nfinance.", " A costing structure is set out below, aligned to the 6 actors within an impact bond: Table E.7: Costing Structure Stakeholder  Changes in programme costs attributable to the impact bond  \nOutcome \nFunder \u2022  Staff time relating to set up of the DIB (additional or reduced set up \ntime compared to grant funded projects) \u2013 see below for staff time \nmonetisation \u2022  Staff time relating to delivery of the programme (additional or reduced) 38 The costing structure is set out in more detail below A133 Stakeholder  Changes in programme costs attributable to the impact bond Service \nProvider \u2022  Costs paid out, on top of the costs incurred by the service provider in delivery (i.e. the return paid out to the investors and/or service \nproviders) \u2022  Transaction costs incurred (payments to consultants and intermediary, \nlegal costs, set up costs) based on invoiced amount (assumed to be \nmarket value, where in-kind support is provided the market value \nshould be estimated) \u2022  Staff time relating to set up of the DIB (additional or reduced set up time compared to grant funded projects) \u2022  Staff time relating to delivery of the programme (additional or reduced), including M&E costs \u2022  Verification costs (staff time and invoiced)  \n\u2022  Other significant costs incurred as a result of the use of the impact bond Investor \u2022  Transaction costs and time should be captured within their return (costs \nto the outcome funder), so no additional costs included. However, costs \nadditional to those which would have been incurred for other \ninvestments are included.", " Verifier \u2022  This will form part of the programme delivery costs, so no additional costs to include.", " Intermediary \n/ Fiduciary Target \npopulation \u2022  Staff time relating to the service provided, if these are not charged to the service provider/outcome funder. To assess whether these \nrepresent fixed or recurrent costs.", " \u2022  Any additional costs needed to access the service (e.g. out of pocket \npayments, transportation costs), or in-kind delivery on the part of \nbeneficiaries or local government.", " In  order  to  try  and  identify  additional  costs  resulting  from  the  DIB,  the  team  was  primarily \nguided by discussions with stakeholders. The team probed using findings from the literature, \nand reviews of budgets (and comparison to other non-DIB budgets where available).", " For all costs, the team worked with stakeholders to estimate the proportion of costs that can \nbe seen as \u2018capital costs\u2019, or one-off costs related to the fact that the stakeholder is using a \nDIB for the first time, and recurring costs which would be incurred no matter how many DIBs \nhad been set up. It is important that where possible, in-kind costs and other costs not formally \ncharged are still included in the analysis.", " Where information was available, staff costs were calculated based on an estimate of time * \nrate, which will include: \u2022  Staff salaries \n\u2022  On-costs (including national insurance and pension costs to the employer) \n\u2022  Overhead costs, to account for rent and utility costs \n\u2022  Staff expenses, including travel and subsistence expenses.", " A134 For  investors,  verifiers  and  intermediaries,  costs  were  estimated  at  the  market  rate.  For \nexample, market day rates were used in the estimates.", " E.3 Programme-level Research The purpose of this level of research is to compare the findings on the individual DIBs, in order \nto  understand  further  how  the  DIB  effect  differs  (or  remains)  across  different  contexts. We \ncontextualised these findings within the wider DIB sector, and considered the implications of \nthe findings for both improving the DIB mechanism and how DFID could utilise the model in \nthe future. To achieve this, we undertook the following tasks: \u2022  DFID consultations \u2022  Programme document review \u2022  Learning workshops E.3.1.  DFID consultations The purpose of the consultations with DFID was to further understand the programme aims; \nDFID\u2019s perspective on the progress and success of the programme and its implications for the \nwider  DIB  landscape;  and  changes  to  relevant  DFID  strategies,  such  as  the  DIB  or  PbR \nStrategies. This information helped ensure the reports and recommendations are relevant and \nsituated within wider developments within DFID. We had consulted with the PbR team during \nthe  inception  phase,  and  consultations  with  DFID  in  this  research  wave  focused  on  the \nselection and structuring of the 3 DIBs under the pilot programme.", " E.3.2.  Programme document review We reviewed key programme-level documents, such as any internal reports written by DFID. \nAs  with  the  DFID  consultations,  this  ensured  the  evaluation  is  situated  within  wider \ndevelopments in DFID. The full list of documentation reviewed is set out in Annex H.", " We  have  already  reviewed  key  documents  as  part  of  the  inception  phase,  and  will  review \nfurther key documents during Research Waves 2 and 3.", " E.3.3.  Internal Learning workshops The  internal  workshop  brought  together  key  stakeholders  from  across  the  three  DFID  DIB \npilots and the Cameroon Cataract Bond. The purpose of this workshop was to focus on the \nsimilarities  and  differences  across  the  DIBs  and  what  might  explain  these  differences, \nincluding  the  DIB  effect.  The  evaluation  team  presented  the  main  effects  of  the  DIB \nmechanism and lessons learned in delivery, including how challenges can be overcome and \nhow the DIB mechanism can be improved for future DIBs. The presentation was used to spark \na discussion on the validity of these findings for the different DIBs, and additional perspectives \nand nuances across the range of DIBs present. Results from the learning workshop were used \nto  refine  the  evaluation  team\u2019s  analysis  and  findings,  and  have  been  incorporated  in  this \nevaluation report. Further detail is set out in Annex K.", " E.4 Sector-level Research E.4.1.  Literature review A135 The purpose of the literature review is to contextualise the findings from the programme within \nthe wider impact bond sector. The review focused predominantly on DIBs, but also included \nSIBs operating in low- and medium-income countries.", " We undertook the initial literature review as part of the inception phase (see Annex M). Given \nthe short intervening period between the inception report and this research wave, the literature \nreview  has  not  been  updated.  However,  additional,  relevant  literature  has  been  drawn  on \nduring  the  evaluation.  For  future  research  waves,  each  evaluation  report  will  include  a  full \nupdated literature review.", " E.4.2.  Other consultations In order to contextualise our findings within the wider sector, the evaluation is also interested \nin consulting other stakeholders within the sector, who had worked on other DIBs, including \nthose that had failed to launch, to understand if there are certain contexts not suitable for DIBs, \nor necessary conditions. The evaluation team worked with DFID and the DIB expert within the \nteam in order to approach other stakeholders, including DIB advisors, researchers, outcome \nfunders and service providers. The team managed to contact at least one stakeholder per DIB, \nincluding  outcome funders,  technical  advisors,  and  intermediaries.  A  snowballing  approach \nwas also taken whereby stakeholders would recommend other stakeholders for consultation. \nA full list of consultations is set out in Annex H. This allowed the evaluation team to develop a \nbroader understanding of the DIB wider sector, main issues and related challenges, from a \nrange of different perspectives.", " E.5 Approach to data collection The process of data collection took place between August and December 2018.", " Interview guides were created for different types of interviews, including stakeholders of the \nDIB  under  the  pilot  programme,  other  DIBs,  DIBs  that  did  not  launch,  DIB  advisors  and \nstakeholders providing their perspectives on multiple projects, and stakeholders working on \ncomparable PbR programmes. These interview guides were peer reviewed and included in \nthe inception report, so as to be refined based on feedback from DFID. The guides were then \nupdated  throughout  the process,  based  on feedback from  interviewees,  and tailored to the \nspecific DIB and stakeholder group.", " Data  was  collected  in  an  appropriate  and  respectful  manner,  taking  into  account  cultural, \nethical and legal concerns. Given the focus of research wave 1 on the set up and delivery of \nthe DIB, interviews were undertaken with outcome funders, intermediaries/advisors, service \nproviders, investors and outcome verifiers only. No interviews were undertaken directly with \nbeneficiaries. Interviews were undertaken with individuals from different backgrounds, and the \nevaluation  team  liaised  with  \u2018gatekeepers\u2019  in  terms  of  the  best  way  to  undertake  these \ninterviews. For example, in the case of the QEI DIB, the evaluators worked closely with BAT, \nDalberg and MSDF in order to understand how best to conduct interviews with the India based \nservice  providers.  Data  was  collected  with  due  consideration  to  ethical  concerns.  The \nevaluation took a participatory, collaborative process, working closely with DIB stakeholders \nin order to tailor the evaluation process. For each DIB, the evaluation worked closely with the \nDIB stakeholders in order to finalise the particular research approach and information required. \nThe evaluation team considered how best to communicate research findings to participants, A136 in order to actively engage participants with findings and implications. This is discussed further \nin the next section. Data was collected in compliance with GDPR.", " In the next two research waves, we envisage speaking directly with beneficiaries. We will work \nclosely  with  our  peer  reviewer,  our  local  researchers  and  the  service  providers,  in  order  to \nensure our research is conducted in an ethically appropriate manner.", " Privacy  and  security  concerns  were  taken  into  account  during  the  consultations,  as \ninterviewees were ensured confidentiality in the treatment of interview data and their informed \nconsent  was  obtained  for  the  recording,  use  and  storage  of  the  interview  material.  The \ninterviewees  were  given  the  opportunity  to  review  and  if  necessary  rectify  the  findings \npresented in the individual DIB case studies.", " For certain stakeholders, the evaluation team also drew upon the introductions and support of \nother stakeholders. This intermediation facilitated the interview process, and ensured it was \nconducted in a transparent and respectful manner. For example, for the QEI DIB, the team \nasked Dalberg, the performance manager working closely with the Indian service providers, \nto make introductions and share advice on the best ways to engage with the service providers. \nIn  terms  of  the  sector  level  research,  the  evaluation  team  was  introduced  to  relevant \nstakeholders by DFID, DIB experts within the team, or DIB advisors.", " E.6 Analysis, Reporting and Dissemination The purpose of the analysis, reporting and dissemination phase is to analyse the findings from \nacross the evaluation and share these with external stakeholders through a variety of outputs. \nWe discuss the analysis, reporting and dissemination steps in further detail below.", " E.6.1.  Analysis The evaluation generated a variety of qualitative and quantitative evidence, which provided \nmultiple  lines  of  enquiry  and  enabled  the  triangulation  of  different  data  sources,  including \nliterature and document review, consultations with DIB and DFID stakeholders, cost analysis \nand  research  on  comparator  sites.  To  ensure  detailed  and  consistent  analysis  a  clearly \nstructured  approach  to  the  analysis  is  essential.  The  recommended  analytical  stages  and \ntasks are as follows.", " For  the  qualitative  analysis,  this  has  been  organised  into  two  distinct  phases  -  data \nmanagement and data interpretation39. The evaluation team drew upon the topic guides and \nearly stages of fieldwork to develop a framework of themes and sub-themes organised around \nthe key research questions. This has been reviewed as the fieldwork progressed. The data \nfrom the transcripts and field notes will be summarised and synthesised under the headings \nand sub headings within the Evaluation Framework.", " The subsequent data interpretation stage involved synthesising findings across the multiple \nsets of interview respondents and case study areas, searching for similarities and differences \nor any other patterns occurring in the data according to key variables.", " The  findings  from  the  qualitative  analysis  were  triangulated  with  the  findings  from  the \nquantitative  analysis,  which  was  described  above.  The  two  sets  of  data  were  examined  to 39 Ritchie, J., and Lewis, J. (2013) Qualitative Research Practice, SAGE, Sections 8-9.", " A137 assess the extent to which the findings are complementary. Where findings between the data \nsets  contradicted  each  other,  each  data  set  was  further  interrogated  to  examine  possible \nexplanations. We have held debriefings during the analysis with all team members, including \nthe  external  experts,  to  support  in  this  analysis  stage.  As  mentioned  in  Methodological \nconsiderations, we adopted process tracing to specifically analyse the effect of the DIB on the \ndelivery and performance of the services.", " The findings from the qualitative and quantitative data were then examined alongside the cost \ndata to gain an overall assessment of the cost effectiveness of the DIBs. Analysis took place \nat three levels, focusing firstly on the individual DIBs; bringing this together to analyse progress \nat a programme level; and finally considering the implications for the wider DIB sector.", " The evaluation team also undertook sub-analysis to disaggregate the data to show differences \nbetween groups. The team examined the extent to which key findings differ between the three \nDIBs,  and  whether  different  stakeholder  groups  have  different  experiences  of  the  DIB \nmechanism.", " E.6.1.1  Robustness of Findings To  ensure  that  analysis  is  undertaken  consistently,  the  Analytical  Lead  and  Team  Leader \nquality  assured  interview  notes  and  findings.  Furthermore,  detailed  research  guides  and \nbriefings  were  provided  to  all  researchers,  and  regular  catch-ups  were  planned  to  ensure \nemerging  issues  were  discussed  in  a  timely  fashion.  Finally,  the  same  researcher  led  the \nresearch in the DIB and non-DIB programme, which ensured the consultations around the DIB \neffect indicators were delivered consistently.", " To assess the robustness of findings, the following assessments undertaken as part of the \nprocess tracing were key: \u2022  Assessing the reliability of data sources, including their potential limitations and biases; \u2022  Assessing the strength of evidence for each DIB effect. Where data was more limited or where there was disagreement between stakeholders, this is discussed.", " The Lead Analyst and Team Leader provided technical support on this.", " A138 E.6.2.  Reporting E.6.2.1  Evaluation reports This  forms  evaluation  report  1,  which  includes  early  feedback  on  the  set-up  of  the  DIBs \n(including an estimate of set-up costs) and recommendations for expanding and improving the \nDIB programme and these DIB mechanisms This is also complemented by specific case study reports focusing on each of the three DIBs \n(See Annex A).", " E.6.2.2  Annual briefings We will meet with the DFID team and Evaluation Steering Group to provide an annual briefing \non the evaluation progress to date. This will include the latest evaluation findings; areas of \nfocus for the upcoming research wave; and reflections on the effectiveness of the evaluation \nmethodology, and any suggested amends.", " E.6.3.  Dissemination E.6.3.1  External workshops Following the publication of the evaluation report, an external workshop will be planned which \nwill bring stakeholders from across the DIB sector. The purpose would be twofold: firstly, to \nbring  learning  into  the  programme  and  to  understand  the  DIB  effect  and  lessons  learnt  in \ndelivery in other DIBs to contextualise the programme evaluation findings; secondly to share \nlearning  out  of  the  programme;  to  share  lessons  from  the  programme  and  consider  the \nimplications for the wider sector.", " E.6.3.2  Learning outputs Furthermore, following the publication of the evaluation report, we will produce short stand-\nalone  \u2018lessons  learnt\u2019/\u2019how  tos\u2019/\u2019top  tips\u2019,  focusing  on  specific  learning  themes  that  will  be \nuseful for DFID and the wider sector. The evaluation team discussed possible themes with \nDIFD and the DIBs during the inception phase, and the main area of interest was in the extent \nto which the DIB mechanism has impacted on set up and delivery. We proposed that for RW1, \nthe learning outputs focus on Top Tips in designing DIB structures such as: outcome metrics, \nverification,  pricing,  contracting,  origination,  involving  stakeholders  and  governance.  The \nresearch tools were structured to capture information to feed into the learning outputs.", " E.6.3.3  Communication Plan In the inception report, we undertook a stakeholder analysis, which categorised stakeholders \ninto  primary  users  (DFID),  secondary  users  (stakeholders  involved  in  the  pilot  DIBs)  and \ntertiary users (those involved in other DIBs or SIBs or considering implementation of DIBs or \nSIBs).", " Our  communications  plan  is  set  out  in  the  table  below  summarising  the  reporting  and \ndissemination activities and outputs. Further details of the communications strategy, including \nthe types of communications outputs envisaged, are included in the Inception Report.", " A139 Table E.8: Communication Plan Phase \nWave 1 Period \nJuly \nFebruary \n2019 \u2013 Focus \nProcess of designing and launching \nthe DFID DIB pilot projects Wave 2 April-\nNovember \n2020 Emerging  lessons  from  the  DFID \nDIBs  pilot  projects,  and  evidence \ngenerated by other DIBs.", " Wave 3 April  2022 \nMarch \n2023 the  DIBs  and the \nLegacy  of \nprogramme, including the extent to \nwhich  outcomes  and  DIBs  were \nsustained.", " Internal and external workshop Communication Activities \n\u2022  Case study on each DIB \n\u2022  Report \n\u2022 \n\u2022  Annual briefing \n\u2022  Learning outputs (2-3) \n\u2022  Case study on each DIB \n\u2022  Report \n\u2022 \n\u2022  Annual briefing \n\u2022  Learning outputs (2-3) \n\u2022  Case study on each DIB \n\u2022  Report \n\u2022 \n\u2022  Annual briefing \n\u2022  Learning outputs (2-3) \n\u2022  Annual briefings Internal and external workshop Internal and external workshop Keeping \nin touch 2019  and \n2021 Annual  update  on  the  progress  of \nthe DIBs.", " The target audience groups for the communication activities are as follows: Phase Primary  users:  DFID \nstakeholders users: Secondary \nStakeholders \ninvolved  in  the  pilot \nDIBs Tertiary users: those \ninterested \nin  DIBs \nand/or SIBs Case studies \nReports \nInternal workshops \nExternal Workshops \nAnnual Briefing \nLearnings outputs \uf070\uf020\n\uf070 \n\uf070 \n \n\uf070 \n\uf070 E.7 Involvement of stakeholders \uf070 \n\uf070 \n\uf070 \n \n\uf070 \n\uf070 \uf070 \uf070 \n \n\uf070 The evaluation has been designed and managed to meet the information and decision-making \nneeds of the intended users. Discussions were carried out with DFID and stakeholders of the \npilot DIBs in order to inform the approach and needs of stakeholders, as part of the inception \nphase.  DFID  is  also  coordinating  the  evaluation  stakeholder  group  for  this  purpose. \nAdditionally, during this first research wave, the evaluation team has held bi-weekly catch up \ncalls  with  DFID,  to  inform  DFID  of  emerging  issues  and  to  ensure  DFID  input  in  the \nimplementation of the evaluation. The scope of the evaluation and individual DIB level plans, \nin  terms  of  data  to  be  shared  and  consultations  to  be  undertaken  over  the  course  of  the A140 evaluation, have been discussed and agreed with the DIB level stakeholders. The individual \nDIB level plans are set out in Annex E.", " The  evaluation  process  has  been  set  out  transparently.  The  inception  report  has  been \npublished  and  clearly  sets  out  the  scope  and  proposed  approach  to  the  evaluation.  Each \ninterview began with a clear explanation of the research process, aims, and objectives. This \nincluded  an  explanation  of  how  collected  data  would  have  been  used,  and  in  what  form. \nInterviewees were then provided with an opportunity to ask questions.", " The balance of primary and secondary sources varied across the different levels of research. \nFor the DIBs under the scope of the evaluation, extensive primary research was conducted, \nas  well  as  review  of  relevant  secondary  documentation,  such  as  M&E  protocols,  business \ncases, minutes and appraisal documents. For the sector level review, there was a blend of \nprimary  and  secondary  sources  reviewed.  Often,  only  one  or  two  stakeholders  were \ninterviewed per DIB, and a review of programme documentation enabled the evaluation team \nto triangulate findings and obtain a broader perspective on the DIB. The methods and quality \nof data collection of the M&E data collected for the DIBs under the scope of the evaluation \nhas been reviewed using our Data Quality Assessment checklist, set out in Annex G.", " Additionally, the evaluation has sought to support the harmonisation of approaches used in \nthe DIB/SIB sector. The evaluation has drawn on the following frameworks and approaches, \nin order to better support the synthesis of evaluation findings and learning across the sector: \u2022  The  evaluation  is  taking  a  harmonised  approach  by  using  the  same  evaluation \napproach, and synthesising findings for the 3 DIBs under DFID\u2019s pilot programme, as \nwell as the Cameroon Cataract Bond; \u2022  The evaluation team is undertaking a range of sector level consultations and attending \nsector events, such as conferences and working groups, in order to keep abreast of \nemerging learning and findings; \u2022  The  DIB  effect  model  builds  on  DFID\u2019s  PbR  evaluation  framework,  to  facilitate consolidation of learning; \u2022  Our findings have been aligned broadly with the Brookings Institutes\u2019 issue areas as \nset  out  in  Gustafsson-Wright  et  al\u2019s  (2017)  early  findings  report  and  builds  on  their \nfindings; \u2022  The  framework  for  categorising  DIBs  builds  on  the  work  undertaken  by  GOLab  at Oxford, and other key efforts to categorise DIBs; \u2022  The  evaluation  categorises  the  other  DIBs  consulted  in  terms  of  the  stages  of development as set out with Gustafsson-Wright et al\u2019s (2017) deal book; \u2022  The process tracing approach builds on a tested approach used by Ecorys for other SIBs evaluations, which enables cross-sector learning; and \u2022  For  the  DIBs  under  the  scope  of  the  evaluation,  we  have  drawn  on  relevant  and \nexisting studies, such as BOND\u2019s report on lessons learned from the Girls Education \nChallenge40 and the CGD paper on lessons from the Cameroon Cataract Bond.", " 40 https://www.bond.org.uk/resources/does-skin-in-the-game-improve-the-level-of-play A141 The approach is also guided by the principles of the Paris Declaration41. Low- and medium-\nincome  countries  must  lead  and  manage  their  own  development  if  aid  is  to  contribute  to \nsustainable development.", " In line with the Paris Declaration, the evaluation is aiming to avoid duplicating data collection \nand  learning  activities,  by  leveraging  data  and  learning  outputs,  in  order  to  synthesise \nevidence. The need to generate an independent and unbiased perspective is being balanced \nwith the need to ensure that the evaluation team builds on data already generated. As such, \nthe evaluation relies on data collected by the service providers. We have undertaken an initial \nassessment  of  this  data  in  the  Data  Quality  Assessments.  The  Paris  Declaration  also \nhighlights  the  need  to  develop  better  tools  and  systems  to  measure  impact,  and  we  have \nfactored  in  time  to  support  the  DIB  projects  to  improve  their  measurement  systems,  if \nnecessary.  Furthermore,  the  evaluation  team  is  committed  to  building  evaluation  capacity \nwithin partner countries. The evaluation team includes experts from the countries where the \nDIBs are in operation. The experts provide valuable context and input into the evaluation.", " Finally, an important part of understanding the effects of using DIBs includes consideration of \nthe sustainability of the intervention and mechanisms, and the extent to which there may be \npotential for take up by the national government. This will be a focus of research waves 2 and \n3, and we have planned for extensive consultations with the relevant government officials.", " E.7.1.  Validation of findings Stakeholders and end-users have been given opportunities to comment on the draft findings, \nrecommendations  and  lessons.  The  evaluation  report  reflects  those  comments  and \nacknowledges areas of disagreement. Interviewees have been given the opportunity to ask \nquestions, review and rectify emerging findings, when needed. Draft findings were presented \nat a stakeholder workshop held in December. A summary of the discussions and how these \nhave affected the evaluation findings is set out in Annex K. The feedback will also be used to \norient and structure the next wave of research. Additionally, individual DIB case studies were \nreviewed and fact checked by the relevant stakeholders, and used to refine the case studies.", " E.7.2.  Confidentiality The  evaluation  process  provides  information  in  ways  that  honours  confidentiality.  The \nevaluation team obtained interviewees\u2019 informed consent for the treatment of interview data \nand  use  of  programme  documents.  Security  and  privacy  concerns  have  been  taken  into \naccount in storing, using and reporting this information.  Data has been stored in a secured \nfolder  on  Ecorys\u2019s  drive,  which  is  only  accessible  to  members  of  the  research  team.  No \nsensitive  or  confidential  information  has  been  shared  via  email.  We  have  anonymised \nopinions, and have only included a record of the number and positions of staff interviewed, in \norder to avoid identification issues, particularly around sensitive topics.", " E.7.3.  Independence It  is  important  that  the  central  evaluation  remains  independent  and  credible.  In  reviewing \navailable data, we investigated how the data was collected and verified to assess quality. This 41 http://www.oecd.org/dac/effectiveness/parisdeclarationandaccraagendaforaction.htm A142 involved  providing  advice,  guidance  and  a  QA  role  to  ensure  the  evidence  is  sufficiently \nreliable.", " Whilst the evaluation team includes external technical experts, it is also important that the final \nconclusions  are  reached  independently  by  the  evaluation  team.  The  role  of  the  external \nexperts  has  been  to  act  in  an  advisory  capacity,  but  the  report  and  its  findings  have  been \nwritten by the evaluation team.", " E.7.4.  Differences of opinions Differences of opinions arising from the consultations are set out in the Analysis and Findings \nsections  in  sections  4,  5  and  6.  Annex  K  also  summarises  the  findings  from  the  learning \nworkshop, the feedback received and discussion, as well as referencing to how these were \naddressed in the report. The learning workshop offered a further opportunity openly to discuss \nand verify emerging findings, so as to complement any information missing and incorporate \nstakeholders\u2019 opinions and feedback.", " E.7.5.  Conflicts of interest and other limitations No conflicts of interest were identified, and the evaluation team were able to work freely and \nwithout  interference.  Each  consultation  was  conducted  by  a  lead  analyst  who  was  then \nresponsible for the analysis and the reporting of the information gathered through interviews \nand  document  review.  All  key  informant  interviews  were  conducted  under  conditions  of \nconfidentiality.", " The  impact  bond  space  is  a  small  one,  and  undoubtedly  information  sources  and  their \ncontributions are not completely independent of other parties with an interest in the evaluation. \nWe have sought to address this by triangulating findings between different respondents and \nother sources of information, and by disaggregating findings by type of respondent, and role \nin the DIB.", " A143 Annex F: Individual DIB level plans The tables below set out the DIB-level evaluation plans. These have been discussed with the stakeholders across the four DIBs.", " The three tables below set out the proposed consultations, VfM and other data to be collected from each DIB. We have also set out, where \nrelevant, which research wave and to which stakeholders the data request relates to, and whether the data is also requested for the identified \ncomparison programme. This will be confirmed as part of the research wave.", " Stakeholder type RW2  RW3 ICRC Village Enterprise QEI Cataract Table F.1 : Proposed consultations managers Project \nperformance  managers \nintermediaries / \n/ x x n/a provider:  Project x x Service \nmanagers/service \nmanagers/practitioners Outcome \n/  donors \nfunders \n(including  DFID  and  other \ndonors) x x Investors Outcomes verification agents \nprocess \nlevel \nProject \nevaluators / learning partners \nNational \ndistrict/local \ngovernments and x x \nx x x x \nx x Instiglio (Project Manager, \nProcess Learning lead, \nCEO, Financial Model \nDeveloper) \nDirector of MEL; Kenya \nand Uganda country \nDirector, CEO, COO DFID, USAID, the \nAnonymous Donor Group of private family \nfoundations and SV2, via \nImpactAssets \nIDInsight \nInstiglio TBC Dalberg \n(Performance \nmanager) Volta Capital (bond \nmanager) Gyan Shala, SARD, \nKaiyvala Education \nFoundation The Magrabi \nFoundation British Asian Trust, \nTata Trusts, MSDF, \nComic Relief, USB OF The Fred Hollows \nFoundation, Conrad \nN. Hilton Foundation \nand Sightsavers \nOPIC and Netri \nFoundation Gray Matters India \nN/A AEDES \nN/A N/A Regional \ngovernments in the \nstates where the \nservice providers are \noperating PRP Lead, Director of \nFinance, HIB Head, Staff at \nthe 3 HIB centres and \nidentified comparison \ncentres \nGovernments of \nSwitzerland, Belgium, UK \nand Italy, and La Caixa \nFoundation \nMunich Re, Lombard Odier \npension fund, charitable \nfoundations and others  \nPhilanthropy Associates \nN/A Local Governments in Mali, \nDRC, and Nigeria A144 Stakeholder type RW2  RW3 ICRC Village Enterprise Local  organisations  that  work \nwith the project \nAdvisors (designers) Service users / beneficiaries x x x x x x Ministry of Health in \ncountries of operation \nKOIS TBC N/A QEI N/A Dalberg Sample  of  users \nin  new \nICRC centres, and the 8 pilot \ncentres.", " Sample  of  participating \nhouseholds  in  Kenya  and \nUganda Sample  of \npeople \nin \ntreatment schools young \nthe Cataract N/A Volta Capital \nAravind Foundation \nSample  of  patients  in \nhospital  in  Cameroon \nfrom  middle  and  low \nincome backgrounds RW1   RW2  RW3   Comparison \nprogrammes x x x All \nstakeholders Stakeholder ICRC VE QEI Cataract this \nis VE  have \nstated \nthat \ndata \navailable \nand  can \nbe shared \nwith us.", " Possibility \nto  get  the \nfrom \ndata \nVolta  but \nmight  not \nbe \nvery \ndetailed Table F.2: Value for Money data Indicator 1  Additional  costs  of  the  impact  bond, disaggregated where possible by:  \n\u2022 (design,  set-up,  delivery, stage \nlearning); \u2022  actor who incurs this cost; and \n\u2022 type of cost (staff time, consultancy \nand  expertise  costs,  and  the  risk \npremium \ninvestors, \n(return \nincluding interest).", " to \u2022  This  should  cover  the  full  cost, \nincluding staff time not charged, of all \nactors.", " \u2022  Where  possible, this  will  be \ndisaggregated  by  \u2018first  time\u2019  DIB \ncosts  which  hypothetically  wouldn\u2019t \nhave  to  be  incurred  again  for  any \nsubsequent DIBs.", " \u2022  Cost  drivers to  be  analysed to \nunderstand  which  elements  of  the \nDIB \ntime-\nare \nintensive/expensive.", " the  most A145 x Indicator Stakeholder ICRC VE QEI Cataract 2  Savings  in  programme  costs  (including \nstaff time) as a result of the impact bond.", " RW1   RW2  RW3   Comparison \nprogrammes x x x 3  How effectively has risk been transferred \n-  alignment  of  transferred  risks  with \nreturn (in relation  to the outcome target \nand  payment  mechanism  of  return  of \ninvestors and service provider).  \nRange of potential returns and capital at \nrisk.", " 4  Level  of  returns  and  profit  made  by  the investors.", " 5  Outcome measure.", " Other  intended  outcomes  as  set  out  in \nthe M&E framework.", " x x x x 6  Difference in: x x x \u2022  Quality of outcomes \n\u2022  Sustainability of outcomes \n\u2022  Organisation \nperformance \n(spillovers) approach to \nmanagement \u2022  Positive  and  negative  unintended effects to (with reference 7  %  of  participants  in  the  different  sub-\ntargeting groups \nstrategy).  \nTargeting  costs  if  relevant  (with  the \nassumption that targeting costs increase \nwhen trying to access the hard to reach) \n8  Change in targeting approach based on \nthe identified effects of the impact bond.", " Service \nprovider;  \noutcome \nfunder All \nstakeholders Service \nprovider \nService \nprovider Service \nprovider Service \nprovider Service \nprovider Unlikely  to \nbe  able  to \nreceive \nthis \ninformation Likely \nto \nbe  largely \nqualitative \ndata Unlikely  to  be \nable \nto \nreceive \ndisaggregated \ninformation on \nparticipants Unlikely  to  be \nable \nto \nreceive \ndisaggregated \ninformation on \nparticipants x x x x x x A146 Targeting \ncosts  will \nbe \ndifficult to \nobtain Indicator Stakeholder ICRC VE QEI Cataract RW1   RW2  RW3   Comparison \nprogrammes Different effects of the intervention on the \ndifferent sub-groups.", " RW1   RW2  RW3   Stakeholder ICRC VE QEI Cataract x x x Service \nprovider Quarterly \nreports Table F.3: Other data Data type M&E data \n(Beneficiary \nnumbers \nand \noutcomes) Examples \nof relevant \nreports Internal \nprogress \nreports; \nProject \nmonitoring \nreports \nreceived \nfrom  each \nDIB partner; \nSummary of \nbeneficiary \nfeedback Outcome \nverification \nreports \n(baseline \nand \nendline) Comp\narison \nprogra\nmmes \nx How this data will \nbe used To  understand \nthe \nstatus and success of \nthe  programme,  and \nto  compare  the  DIB \nfunded  programmes \nsimilar \nwith  other \n(where \nprogrammes \nsimilar  M&E  data  are \ncollected).", " Outcome  verification \ndata  will  be  used  to \nthe \nunderstand \nreturns  payable.  The \ndata  can  also  be \ncompared against the \nother  outcome  data, \nto  understand \nthe \nextent  to  which these \nare \ncorrelated \n(improvement  in  the \ntarget  outcome  but \nworsening \nacross \nother  outcomes  may A147 Outcome \nVerification x x x Service \nprovider Verification \nreports a \nquarterly \nreport \nfrom \nThe \nMagrabi \nFoundati\non that \nincludes \nall the \nmetrics \nfor the \nhospital \nVolta \nhas \nalready \nprovided Village \nEnterprise \n(quarterly \nreport) \nincludes all the \nmetrics for the \nproject IDInsight has \noutcome \nverification \nreport for each \ncohort (the first \nof these \nreports has \nalready been \nreceived) BAT / \nDfID \n(a \nquarterly \nreport \nthat \nincludes \nall the \nmetrics \nfor the \nproject) Gray \nMatters \nIndia \n(we \nhave \nalready \nreceived \nthe \nbaseline \nreport) Data type Examples \nof relevant \nreports How this data will \nbe used Comp\narison \nprogra\nmmes Learning \nActivities Internal  and \nexternal \nlearning \nreports Progress \nreports Investment \nreturns  \nOutcome \npayments perverse suggest \nincentives).  \nbe \nLearning  will \nacross \ncompared \nDIBs \nand \ncontextualised  within \nthe \nfrom \nlearning \nother impact bonds.   \nTo  understand  how \nthe  DIB  performs \nagainst targets.", " To  better  understand \nthe  set  up  process, \nand  key  challenges \nand enablers.", " RW1   RW2  RW3   Stakeholder ICRC VE QEI Cataract x x x x Service \nprovider Learning \nreports x x x x x x x Service \nprovider \nService \nprovider \nAll \nstakeholders Quarterly \nreports \nQuarterly \nreports  \nStakeholders \nalready \nprovided Instiglio \nprocess review \n(the first of \nthese reports \nhas already \nbeen provided) \nVE biannual \ninterim reports \nAs above VE, Instiglio \nalready \nprovided UBSOF BAT UBSOF, \nBAT Volta \nand \nFred \nHollows \nhave \nalready \nprovided Programme \ndesign \ndocuments; \nBusiness \nand \nfinancial \ncases; \nmemos \nexplaining \ndecisions  to \nfund  each \npilot  DIB; \nrecords  of \nproject \nappraisal \nprocess, \nnegotiations \nand \ndecisions Data \nsupporting \nset \nphase up A148 Annex G: Data Quality Assessment As explained in the Methodology section, at the start of each research wave the evaluation team \nwill review the evidence provided prior to any consultations or field visits, and assess the quality \nof the data through Ecorys\u2019 Data Quality Assessment Checklist. This will enable the team to plan \nquestions to clarify the evidence sources and quality of data collection and to work with the DIB \nprojects to identify potential gaps and limitations affecting the evaluation. These findings can then \nbe used to support the DIB projects to improve their measure systems if necessary, or to re-focus \nEcorys\u2019  primary  research  on  areas  not  sufficiently  covered  through  local  data  collection  and \nlearning  activities.  The  tables  below  illustrate  the  Data  Quality  Assessment  Checklist  that  the \nteam conducted for each of the four DIBs under study, as part of RW1. The data being assessed \nrelates  to  the  monitoring  and  evaluation  data,  which  goes  beyond  the  outcome  assessment \ndata,.We intend to draw on this data as part of the next two research waves.", " G.1 ICRC Question Yes/No Comments Validity \u2013 Data should clearly and adequate represent the intended results Indicator and collection methods \nare clear and sensible.", " TBD.", " Indicators are sensible Yes, based on centre records.", " Clear  guidelines  in  instructions \nper  cell.  Some  cells  are \nensure \nprotected \nconsistent.", " etc., to Does  the  information  collected  measure \nwhat it is supposed to measure?", " Do  results  collected  fall  within  a  plausible \nrange?", " Is there reasonable assurance that the data \ncollection  methods  being  used  do  not \nproduce  systematically  biased  data  (e.g. \nconsistently over- or under-counting)?", " Yes Yes Yes Are sound research methods being used to \ncollect the data?", " Yes Yes When  the  same  data  collection  method  is \nused  to  measure/observe  the  same  thing \nmultiple  times,  is  the  same  result  produced \neach time?", " Are  data  collection  and  analysis  methods \ndocumented  in  writing  and  being  used  to \nensure  the  same  procedures  are  followed \neach time?", " Reliability  \u2013  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis \nmethods over time.", " Yes As above.", " Timeliness-  data  should  be  available  at  a  useful  frequency,  should  be  current,  and  should  be \ntimely enough to influence management decision-making.", " Are  data  available  frequently  enough  to \ninform program management decisions?", " Are  the  data  reported  the  most  current \npractically available?", " Yes Yes Yes, done monthly.", " Yes, done monthly.", " A149 Question Are  the  data  reported  as  soon  as  possible \nafter collection?", " Yes/No Yes Comments Precision \u2013 data have a sufficient level of detail to permit management decision-making; e.g. the \nmargin of error is less than the anticipated change.", " Is the margin of error less than the expected \nchange being measured?", " N/A Has the margin of error been reported along \nwith  the  data?  (Only  applicable  to  results \nobtained through statistical samples) Is the data collection method/tool being used \nto collect the data fine-tuned or exact enough \nto  register  the  expected  change?  (e.g.  a \nyardstick may not be a precise enough tool \nto measure a change of a few mm).", " N/A Yes All  users  calculated  and  not \ncalculated  based  on  a  sample \nbasis.", " N/A.", " Service  users  and  equipment \nprovided \nNo \nestimates used.", " calculated.", " Integrity \u2013 data collected should have safeguards to minimize the risk of transcription error or \ndata manipulation.", " Are  procedures  or  safeguards  in  place  to \nminimize data transcription errors?", " Yes Is there independence in key data collection, \nmanagement and assessment procedures?", " Are  mechanisms \nunauthorized changes to the data?", " in  place to  prevent Yes Data checks included  within the \nspreadsheet to minimise error.", " Not sure.", " To ask for our centres.", " Template \npassword.", " secured with Summary: Based on the assessment relative to the five standards, the quality of ICRC\u2019s PRP \nprogramme seems to be of good quality. For the centres whose data we will be relying on, we will \nask additional questions as to whether there is independence in key data collection, management \nand assessment.", " G.2 QEI Question Yes/No Comments Validity \u2013 Data should clearly and adequate represent the intended results Does the information collected measure what \nit is supposed to measure?", " Yes Do  results  collected  fall  within  a  plausible \nrange?", " Yes Is there reasonable assurance that the data \ncollection  methods  being  used  do  not \nproduce  systematically  biased  data  (e.g. \nconsistently over- or under-counting)?", " Yes Information is collected against a variety \nof indicators relevant to the outcomes of \ninterest.", " The  number  of  schools  selected  to \ncollect  data  shall  ensure  adequate \naccuracy \nvariance \ncomponents  between  and  within \nschools.", " estimating in Stratification by school size, urban/rural \nlocation and school type ensures that all \nparts  of  the  population  are  included  in \nthe sample.", " A150 Question Are sound research methods being used to \ncollect the data?", " Yes Yes/No Comments through Dalberg  collects  data \nfield \nobservations  to  intervention  schools, \nindividual  and  group  interviews  with \nschool  staff,  and  in  person  interviews \nwith service provider leadership as well \nas program staff.", " Reliability  \u2013  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis \nmethods over time.", " TBD When  the  same  data  collection  method  is \nused  to  measure/observe  the  same  thing \nmultiple  times,  is  the  same  result  produced \neach time?", " Are  data  collection  and  analysis  methods \ndocumented  in  writing  and  being  used  to \nensure  the  same  procedures  are  followed \neach time?", " Yes Service  providers  need  to  report  data \ncollection  and  analysis  procedures  in \nquarterly reports provided by Dalberg.", " Timeliness-  data  should  be  available  at  a  useful  frequency,  should  be  current,  and  should  be \ntimely enough to influence management decision-making.", " Are  data  available  frequently  enough  to \ninform program management decisions?", " Yes Using  the  performance  management \nsystem established by Dalberg, service \nproviders  enter  and  report  data  on  a \nvariety  of  indicators  on  a  quarterly \nthis \nbasis.  Dalberg \ninformation \ncourse \nto \ncorrection measures.", " then \npropose uses This  is  what  Dalberg  recommend,  but \nthe  first  quarterly  report  related  to  the \nimplementation  phase  has  not  been \npublished yet.", " Data  are  reported  on  a  quarterly  basis \nby service providers, and this feeds into \nDalberg quarterly reports.", " Are  the  data  reported  the  most  current \npractically available?", " TBD Are  the  data  reported  as  soon  as  possible \nafter collection?", " Yes Precision \u2013 data have a sufficient level of detail to permit management decision-making; e.g. the \nmargin of error is less than the anticipated change.", " Is the margin of error less than the expected \nchange being measured?", " Has the margin of error been reported along \nwith  the  data?  (Only  applicable  to  results \nobtained through statistical samples) Is the data collection method/tool being used \nto collect the data fine-tuned or exact enough \nto  register  the  expected  change?  (e.g.  a \nyardstick may  not be a precise enough tool \nto measure a change of a few mm).", " TBD TBD Yes Indicators  are  sensible  and  based  on \nhistoric  data  related  to  the  sector  and \nthe specific service providers.", " Integrity \u2013 data collected should have safeguards to minimize the risk of transcription error or \ndata manipulation.", " A151 Question Are  procedures  or  safeguards  in  place  to \nminimize data transcription errors?", " Is there independence in key data collection, \nmanagement and assessment procedures?", " Yes Yes/No TBD Comments Data  is  collected  by  service  providers \nagainst  the  performance  management \nframework  provided  by  Dalberg,  which \ncomplements with other data collection \nmethods  and  independently  assesses \ndata.", " Triangulation  of  data  sources  and \nindependent \nprevent \nunauthorized changes.", " assessment Are  mechanisms \nunauthorized changes to the data?", " in  place to  prevent Yes Summary: Based on the assessment relative to the five standards, the quality of data of the \nQEI  DIB  seems  adequate.  In  terms  of  the  performance management  and  outcome  evaluation \ndata collection, the information is adequate for the purpose of the measurement; collected and \nprocessed  according  to  clear  and  rigorous  procedures  and  through  a  variety  of  methods;  and \ncollected in a timely fashion used to improve the intervention. There are a few areas which will be \nfollowed up by the evaluation team during the next research wave.", " G.3 Village Enterprise Question Yes/No Comments Validity \u2013 Data should clearly and adequate represent the intended results Does the information collected measure \nwhat it is supposed to measure?", " Do results collected fall within a plausible \nrange?", " Is there reasonable assurance that the data \ncollection methods being used do not \nproduce systematically biased data (e.g. \nconsistently over- or under-counting)?", " Yes Yes Yes Are sound research methods being used to \ncollect the data?", " Yes Indicator and collection methods are \nclear and sensible.", " TBD.", " Indicators are sensible and based on \nhistoric successful RCT. VE routinely \nmonitors all five aspects of program \nimplementation \u2013 targeting, business \ntraining, savings groups, business \nformation, and mentoring. Village \nEnterprise\u2019s monitoring and evaluation \nstaff continuously monitor data synced \ninto the database for accuracy.", " Yes. Village Enterprise staff collects \ndata using android devices equipped \nwith TaroWorks, a suite of mobile data \ncollection tools built on the Salesforce \nplatform. Use of TaroWorks facilitates \nremote data collection through offline \ndata entry in areas without mobile or \nWiFi signal.", " Reliability \u2013 Data should reflect stable and consistent data collection processes and analysis \nmethods over time.", " A152 Question When the same data collection method is \nused to measure/observe the same thing \nmultiple times, is the same result produced \neach time?", " Are data collection and analysis methods \ndocumented in writing and being used to \nensure the same procedures are followed \neach time?", " Yes/No Comments Yes Yes, see above.", " Yes As above.", " Timeliness- data should be available at a useful frequency, should be current, and should be \ntimely enough to influence management decision-making.", " Are data available frequently enough to \ninform program management decisions?", " Yes Monitoring and evaluation staff routinely \nmonitor completeness of training \nattendance, assign monitoring spot-\nchecks, and report results to relevant \nstaff.", " Precision \u2013 data have a sufficient level of detail to permit management decision-making; e.g. \nthe margin of error is less than the anticipated change.", " Are the data reported the most current \npractically available?", " Are the data reported as soon as possible \nafter collection?", " No Yes Is the margin of error less than the expected \nchange being measured?", " Has the margin of error been reported along \nwith the data? (Only applicable to results \nobtained through statistical samples) Is the data collection method/tool being used \nto collect the data fine-tuned or exact \nenough to register the expected change? \n(e.g. a yardstick may not be a precise \nenough tool to measure a change of a few \nmm).", " TBD.", " TBD.", " TBD.", " Integrity \u2013 data collected should have safeguards to minimize the risk of transcription error or \ndata manipulation.", " Are procedures or safeguards in place to \nminimize data transcription errors?", " Is there independence in key data collection, \nmanagement and assessment procedures?", " Yes No Are mechanisms in place to prevent \nunauthorized changes to the data?", " Data checks included within the \nspreadsheet to minimise error.", " Data collected by Village Enterprise.", " Not sure.", " Summary: Based on the assessment relative to the five standards, the quality of Village \nEnterprises\u2019 graduation model programme data seems to be of good quality. For the centres \nwhose data we will be relying on, the evaluation team will ask additional questions as to \nwhether there is independence in key data collection, management and assessment.", " A153 G.4 Cameroon Cataract Bond Question Yes/No Comments Validity \u2013 Data should clearly and adequate represent the intended results Does the information collected measure what \nit is supposed to measure?", " Yes Do  results  collected  fall  within  a  plausible \nrange?", " Yes The  performance  metrics  against \nwhich  the  performance  of  MICEI  is \nevaluated  are  consistent  with \nthe \noutcome of interest.", " The  Monitoring  Cataract  Surgical \nOutcomes  (MCSO)  software  is  used \nwith  all  cataract  patients.  The \nEquityTool  questionnaire  will  be \nadministered  to  all  cataract  patients \noperated  at  MICEI,  or \nto  a \nrepresentative sample.", " to  certify The process of independently verifying \nMICEI\u2019s data will consist of three main \ncomponents \nthe \ninformation provided by MICEI is a true \nand \nthe  hospital \nperformance  in  meeting  the  Cataract \nBond targets.", " fair  account  of that review, include \nData  collection  methods \ndocument \nthe  Monitoring \nCataract  Surgical  Outcomes  (MCSO) \nsoftware to report on the number and \nquality outcomes of cataract surgeries, \nand  the  Equity  Tool  to  evaluate  the \nwealth  status  of  cataract  patients \ntreated at MICEI.", " The procedure and forms used by the \nverification agent for measuring visual \nacuity  is  strictly  the  same  as  the  one \nused  by  the  hospital  staff  in  order  to \nreduce the inter-observer variation.", " Procedures  to  be  followed  to  collect \nand  analyse  are  documented  in  M&E \nprotocol.", " Is  there  reasonable  assurance  that  the  data \ncollection methods being used do not produce \nsystematically  biased  data  (e.g.  consistently \nover- or under-counting)?", " Yes Are  sound  research  methods  being  used  to \ncollect the data?", " Yes When  the  same  data  collection  method  is \nused  to  measure/observe  the  same  thing \nmultiple  times,  is  the  same  result  produced \neach time?", " Are  data  collection  and  analysis  methods \ndocumented  in  writing  and  being  used  to \nensure  the  same  procedures  are  followed \neach time?", " Yes Yes Reliability  \u2013  Data  should  reflect  stable  and  consistent  data  collection  processes  and  analysis \nmethods over time.", " Timeliness-  data  should  be  available  at  a  useful  frequency,  should  be  current,  and  should  be \ntimely enough to influence management decision-making.", " Are data available frequently enough to inform \nprogram management decisions?", " Yes The hospital will be required to provide \nfinancial  statements  and  progress \nreports  on  achieving  the  Cataract \ntargets  on  a \nBond  performance \nquarterly  basis.  This  will  enable  the A154 Question Are  the  data  reported  the  most  current \npractically available?", " Are  the  data  reported  as  soon  as  possible \nafter collection?", " TBD Yes Yes/No Comments Cataract  Bond \nidentify \ninefficiencies  and  course  correct  in  a \ntimely manner.", " team to Precision \u2013 data have a sufficient level of detail to permit management decision-making; e.g. the \nmargin of error is less than the anticipated change.", " Is the margin of error less than the expected \nchange being measured?", " Has the margin of error been reported along \nwith  the  data?  (Only  applicable  to  results \nobtained through statistical samples) Is the data collection method/tool being used \nto collect the data fine-tuned or exact enough \nto  register  the  expected  change?  (e.g.  a \nyardstick may not be a precise enough tool to \nmeasure a change of a few mm).", " TBD Yes Yes on achieving Quarterly  progress  reports  including \ncataract \nupdate \nperformance  are  delivered  within  45 \ndays  after  the  end  of  each  fiscal \nquarter.", " Discrepancies  between  reported  and \nverified  data  are  reported  in  CCBP \nPilot Verification Report It - \nThe  MCSO  software  was \ndeveloped by the International Center \nfor  Eye  Health  (ICEH)  at  the  London \nSchool  of  Hygiene  and  Tropical \nMedicine \nis  a \nin  London. \ncomputerised  system  for  data  entry \nand  analysis  designed  for  monitoring \nand  evaluation  of  visual  outcome  of \ncataract  surgeries  and  produces \nvarious  standard  reports,  graphs  and \nlists of patients due for follow-up. \nTool  was \n- \ndeveloped \nto  enable  development \norganisations  to  assess  the  wealth  of \nthe  beneficiaries  of  their  programs. \nThe EquityTool is an easy to use and \neasy to  interpret  and is a  measure of \nrelative  wealth.  The  tool  is  country-\nspecific  and  consist  of  a \nlist  of \nquestions  on \nrespondent  dwelling \ncharacteristics  and  ownership  of \ndurable assets.", " Equity The from  each form  related Data \nto \nsurgeries performed has to be entered \ninto a computer. The data entry facility \nthe  Cataract  Surgery  Record \nof Integrity \u2013 data collected should have safeguards to minimize the risk of transcription error or \ndata manipulation.", " Are  procedures  or  safeguards  in  place  to \nminimize data transcription errors?", " Yes A155 Question Yes/No Comments Is there independence in key data collection, \nmanagement and assessment procedures?", " Yes Are  mechanisms \nunauthorized changes to the data?", " in  place to  prevent Yes package  has  a  number  of  in-build \nchecks to avoid data entry errors Data  is  collected  and  managed  by \nMICEI, and independently validated by \nAEDES.", " will triangulate records,  and  other AEDES \nthe \nperformance  data  provided  by  MICEI \nagainst a number of sources available \nat  the  hospital  including,  patient  files, \nmedical  registers,  cash  book,  drugs \nrelevant \nuse \ndocuments  or  registers.  The  M&E \nAuditor will also conduct onsite visits at \nand/or \nMICEI \ncommunity verification on a sample of \npatients.  Any  discrepancies \nfound \nbetween  the  data  recorded  by  the \nhospital and the verification agent will \nneed  to  be  further  investigated  on  a \ncase-by-case basis.", " Telephonic and Summary:  Based on  the assessment relative  to the five standards,  we can  conclude that  the \noverall quality of the data is good. The information used to measure whether the outcomes are \nmet  is  collected  by  the  MICEI  hospital  in  a  systematic  and  unbiased  manner,  according  to \nprocedures that are clearly specified in the M&E protocol. The tools used to collect and analyse \ndata  are  easy  to  use  and  to  interpret,  and  fine-tuned  to  register  changes  in  visual  acuity  and \npatients\u2019 relative wealth. Data collection at the hospital will be paper-based initially but hospital \nmanagement  is  planning  to  move  gradually  to  an  electronic  data  collection  system,  so  as  to \nminimise transcription errors. Finally, data collected by MICEI is being validated by two different \nentities, based on a variety of methods.", " A156 Annex H: Consultees and Sources reviewed Consultees DIBS \nICRC QEI Cataract Bond KOIS Organisation \nICRC Role \nHead of the HIB \nPartner,  Principal  \u2013  Impact  investing,  Senior \nAssociate \nExecutive Director, Capital Relief Transactions \nProgramme  Officer,  Federal  Department  of \nForeign  Affairs  FDFA,  Swiss  Agency  for \nDevelopment and Cooperation SDC \nGovernment of Belgium  Advisor, Development Cooperation Government \nSwitzerland Munich Re of DFID World Bank GPOBA \nUBSOF \nUBSOF \nUBSOF \nDalberg \nBAT \nBAT \nMSDF \nComic Relief \nComic Relief \nTata Trust \nTata Trust \nGMI \nGyanShala \nKaivalya \nSARD \nEducateGirls \nDFID \nDFID \nDFID GEC \nAEF/ \nFoundation \nThe Magrabi Foundation \nConrad \nHilton \nN. \nFoundation \nConrad \nFoundation \nSightsavers \nThe \nFred \nFoundation \nOPIC \nNetri Foundation \nVolta \nVolta \nCGD The  Magrabi Hollows Hilton N.", " Programme  manager  and  Development \nImpact Bonds Adviser \nPbR comparator site \nInvestor \nInvestor \nInvestor \nPerformance manager \nIntermediary \nIntermediary \nOutcome funder \nOutcome funder \nOutcome funder \nKnowledge partner \nKnowledge partner \nOutcome evaluator \nService provider \nService provider \nService provider \nTechnical advisor \nFunder/technical advisor \nFunder/technical advisor \nPbR Comparator site \nImplementer Investor/Implementer \nOutcome funder Outcome funder Outcome funder \nOutcome funder Investor \nInvestor \nDIB performance manager \nDIB performance manager \nDIB researchers A157 DIBS \nVillage Enterprise   Village Enterprise \nVillage Enterprise Organisation Village Enterprise \nVillage Enterprise \nInstiglio \nInstiglio Instiglio Role \nVE Kenya Country Director \nVE  Director  of  Monitoring,  Evaluation  and \nLearning \nDirector of Institutional Giving  \nVE Chief Operating Officer \nInstiglio \u2013 Leading the Process Evaluation \nCEO  \u2013  and  designer  of  DIB;  part  of  design \nprocess \nInstiglio  Project  Manager;  managed  design \nprocess and kept record of discussions \nInstiglio- developed financial model for DIB Instiglio \nThe Anonymous Donor  Co-designed the DIB model \nCo-designed the DIB model \nDFID \nCo-designed the DIB model \nDFID \nLead Investor \nAnon \nSecondary investor \nBridge Fund \n \n \n \n \nNepal Employment Fund  PbR Comparator site \nSocial Finance UK Cardano Development Convenor/manager Contributed to DIB design and development as \ntechnical advisor Service provider \nOutcome funder Technical \ndevelopment) and performance manager advisor design (DIB and Technical advisor (DIB design) Outcome funder (DIB advisor Technical \nand \ndevelopment),  sub-contractor  of  the  project \nimplementation agency \n \nTechnical advisor (transaction management) design Volta Technical advisor (transaction management) Convergence Commissioned a feasibility study to KOIS Social Finance UK Technical advisor (DIB design) Cameroon \nKangaroo  Mother \nCare DIB \nCook  and  Clean \nDIB \nEducate Girls DIB  Educate Girls \n(Rajasthan) \nIndia \nMaternal \nand \nNewborn  Health \nDIB \nMozambique \nMalaria DIB \nPalestine \n(West \nBank  and  Gaza) \nEmployment DIB World Bank Palladium USAID Volta Social Finance Volta South  Africa  ECD \nBond \nImpact \nInnovation  Fund  \u2013 \nSocial \nDevelopment \nSouth  Africa  ECD \nImpact \nBond \nInnovation  Fund  - \nHealth \nSyrian  Refugees \nEmployment DIB \nUganda  Sleeping \nSickness DIB \nSources reviewed A158 DIBs ICRC QEI Cameroon \nCataract Document PRP HIB Efficiency Improvement Measures Project Final Execution Version of PHII PBR Agreement Signed by DFID (26/7/2017) Benchmark Data (5/8/2017) Q&A with DFID Verification agreement signed between ICRC and Philanthropy associates Final Detailed presentation 20/4/17 20/4/2017 Final ICRC HIB Program Description Final  Initial  Verification  Report  by  Philanthropy  Associates  confirming \nbaseline SER as 33.87 HIB Social Investor Presentation ICRC SER Ratio and how it compares to number of beneficiaries PHII Summary of the transaction Email KOIS/DFID discussion (17/5/17) 1st ORCM presentation, February 2018 1st Quarterly Status Update Jul - Sept 2017 \n2nd PHII Quarterly Status Update Oct - Dec 2017 3rd PHII Quarterly Status Update Jan - Mar 18 Agenda meeting 2018/2/27 PHII Summary of the transaction \u2013 updated 29/11/18 \n4th and 5th PHII Quarterly Status Update (Apr-Jun and July-Sept 2018) \nAnnex to HIB Report Phase 1 Netherlands Instructions on PRP data collection (Assort MSR 2015 instruction final and \nAssort MSR 2017 template 20 centres) BAT India Technical Assistance Grant Proposal (December 2017) \nBritish Asian Trust - DIB Quarterly DFID Report (April-June 2018) \nEducation DIB Fund Financial model (June 2018) \nEducation  DIB  Performance  Management  Annual  Report  Template  (April \n2018) \nEducation DIB performance Management Overview Document (April 2018) \nGray  Matters.  Proposal  for  Outcome  Evaluator  Education  DIB  Fund \n(February 2018) \nGyanShala Education DIB Proposal Revised (2018) \nKEF Education DIB Proposal (2018) \nSARD Education DIB Proposal (2018) \nService  Providers\u2019  Proposals  Consolidated  Summary  Document  (March \n2018) \n020818 Schedule 5 ME Verification Protocol updated \nCCBP Pilot Verification Report 2018 07 27 \nLegal Structure \nONGOING REPORT_aug2018 \u2013 quality A159 DIBs Document \nOPIC Q2  2018_Cataract Loan Reporting_Final \nSteerCo Cataract Bond Report - July 2018.Final \nCataract Bond 2 page Summary_FINAL \nCataract Bond presentation \nCataract Bond_FAQs_July 2016 \nCataract Bond Monitoring & Evaluation Protocol \nCameroon Cataract Performance Bond Application_FINAL \nCCPB HF Grant application addendum \nValue for Money data compiled by Volta \nVillage Enterprise DIB Design Memo, Nov 2017; Instiglio \nGDI Activity Proposal for Village Enterprise \nVillage Enterprise DIB Process Review, July 2018; Instiglio \nPaying for Poverty Alleviation; Richard Sedlmayer \nCSAE Working Paper, Cash-Plus Poverty Impacts of Transfer based \nintervention alleviation (RCT into Village Enterprise traditional model) \n \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field. \nSave the Children (2018). Investing in Maternal and Child Health: \nDevelopment Impact Bonds. \n \nCook and Clean Development Impact Bond Concept Note.", " Educate Girls (2018). Driving Quality at Scale. Implementing the world\u2019s first \ndevelopment impact bond in education. \nIDinsight (2018). Final Evaluation Report \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field. \nSave the Children (2018). Investing in Maternal and Child Health: \nDevelopment Impact Bonds. \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field. \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field. \nWorld Bank Implementation Completion and Results Report (December \n2016) \nWorld Bank Project Appraisal Document (November 2015) \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field.", " Village \nEnterprise Cameroon \nKangaroo \nMother Care Cook  and  Clean \nDIB \nEducate \nDIB Girls India (Rajasthan) \nand \nMaternal \nNewborn  Health \nDIB Mozambique \nMalaria DIB \nPalestine  (West \nBank  and  Gaza) \nEmployment DIB Africa \nSouth \nECD \nImpact \nBond  Innovation \nFund  \u2013  Social \nDevelopment A160 Document \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field.", " Convergence/KOIS Feasibility study (September 2017). \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field. \nGustafsson-Wright et al. (2017). Impact Bonds in developing countries: \nearly learnings from the field.", " DIBs \nAfrica \nSouth \nECD \nImpact \nBond  Innovation \nFund - Health \nSyrian  Refugee \nEmployment DIB Uganda \nSleeping \nSickness DIB A161 Annex I: Framework for categorising DIBs The  table  below  sets  out  a  synthesis  of  the  ways  in  which  DIBs  can  be  categorised.  The \ncharacteristic is briefly described, and the different possible configurations presented. In some \ncases, a distinction has been made between those which appear to be more \u2018textbook\u2019 impact \nbond (indicated in red), in contrast to configurations more similar to a PbR or grant(indicated in \ngreen). In other cases, no such distinction has been made, and different configurations have been \nset out in black. The characteristics and possible configurations set out in the table draw on Carter \net al (2017), Arena et al (2016) and Gustafsson-Wright et al (2017).", " Characteristic Description \u2022 \u2022 - \u2018Textbook\u2019 SIB / more like \nPbR or grant  \n- Different configurations Reference Intermediary  /  service  provider  / \ninvestor / outcome funder Arena  et  al \n(2018) Grant  received  /  Loan  or  self-\nfunded of the new programme / \nTotally \nexpansion \nexisting \nprogramme of a service provider \n/ implementation of a programme \nalready proven successful Arena  et  al \n(2016) focuses \nof on \nContract \nspecific \nachievement \noutcomes  /  contract  involves  a \nspecific \nwell-defined \nintervention.", " and Arena  et  al \n(2016)/  Carter \net al (2018) is of of on the Level \ninnovation Nature \npromoter/designer Design phase \u2013 identifying interventions  \nLead \ndesigning \nintervention  \nFunding \nfor \ndesign  and  set \nup phase Whether  a  grant  was \nprovided,  or  this  phase \nwas  self-funded  by  the \nactors involved  \nThe \nthe \nfeatures  of \nintervention,  and  whether \ntotally  new,  an \nit \nexpansion  of  an  existing \nprogramme  or  involves  a \nprogramme \nwhose \nprinciples \nunderpinning \nhave already been tested \nthe \nExtent \na \ncontract \nspecific  and  well-defined \nintervention  and  service \nprovider, \nspecific \nor \noutcomes  which  enables \nto \nservice \nproviders \norganise  work  as \nthey \nprefer.  \nIdentifying metrics and structuring payments \nNature \npayment \noutcomes Level of outcome \norientation  and \nflexibility  versus \nspecific \nintervention \ndefined to  which \ninvolves of Were  payments  made \nsquarely  for  outcomes  or \nwas some payment made \nfor inputs or activities? \nRisk  borne  by  private \ninvestors  or  distributed \namong  different  actors \nthrough  capital  protection 100% payment on outcome / \npart  payment  for  activities  or \nmilestones  \n \nFull risk on investors / presence \nof  capital  protection  measures  / \nsharing \nof \npresence \narrangements risk Carter  et  al \n(2018) Arena  et  al \n(2016)/  Carter \net al (2018) Nature  of  capital \nused \nfund \nto \nservices A162 Characteristic Description \u2022 \u2022 - \u2018Textbook\u2019 SIB / more like \nPbR or grant  \n- Different configurations Reference measures and risk sharing \narrangements Identifying and selecting stakeholders \nSocial  intent  of \nservice providers \nSocial  intent  of \ninvestors \nStructuring the vehicle and developing the operating model \nType of contract Are the service providers / \ninvestors  a  charity  or \ncompany  without  explicit \nsocial values?", " Strong / Weak Commercial / Social Intermediated Direct (OF and service provider) \n/ \nand \ninvestors);  Managed  (OF  and \nintermediary)  \nStrong / Weak (OF of Strength \nperformance \nmanagement \nsystem on Lead \nmanaging \nperformance \nGovernance arrangements and level of involvement of stakeholders: \nOutcome funder  Role  of High / Low Intermediary / service provider Investor High / Low Typologies  of  structure \ndepending on which actor \nhas  the  contract  with  the \noutcome funder.  \nHow  hands  on  are  the \nother  stakeholders? \nIs \ndedicated \na \nthere \nperformance \nmanagement function?  \nWho  takes  the  lead  in \nperformance \nmanagement?", " the  outcome \nfunder  /  investor  toward \nservice  providers  and  its \nlevel  of  control  over  the \norganisations  involved  in \nthe SIB Carter  et  al \n(2016) Gustafsson-\nWright  et  al \n(2017) Carter  et  al \n2018 Gustafsson-\nWright  et  al \n(2017) Arena  et  al \n(2016) Measuring impact \nValidation \nof \noutcome metrics used Methodology \nto \nestimate  the  outcome  of \nthe programme Validated  administrative  data  / \nquasi-\nexperimental \nexperimental methods or Gustafsson-\nWright  et  al \n(2017) A163 Annex J: DIBs reviewed as part of programme level consultations As part of our programme level data collection, the evaluation team interviewed a range of stakeholders, involved in other DIBs in various \nstages of development, and reviewed a range of deal and proposal documentation. Our interviews were undertaken in the second half of \n2018, and the table summarises the information we have for these 9 DIBs, in terms of the objective of the DIB, the stage of development, \nthe stakeholders involved, the structure of the DIB and the value of the DIB. It must be noted that the majority of these DIBs are under \nnegotiation, and the table below reflects the information provided to the evaluation team during the consultation, and may be already out of \ndate by the time of this report\u2019s publication.", " No DIB Objective Stakeholders involved Structure Value Development \nStage Late-stage \ndesign 1  Cameroon \nKangaroo \nMother \nCare DIB The DIB aims to roll out \nKMC practices in up to 9 \nhospitals across four or five \nregions in Cameroon \u2013 in \nview of further expansions \nto hospitals nationwide - to \nimprove low birth weight \n(LBW) infant health. The \nimpact bond structure is \nused because it provides \nstrong \nincentives to test and \noptimize an innovative \ntrain-the-trainer KMC \nscaling model.", " Payment terms: TBC \nOutcome metric: To be \nfinalized. Likely to include: \na) number of hospitals \nattaining quality KMC \nprerequisites; b) number of \ninfants receiving quality \nKMC services; c) number \nor % of infants achieving \ntarget nutritional \nstatus/weight at 40 weeks \ngestational age and/or at \nfollow-up. \nRange of returns: TBC The planned operating \nbudget USD 2.1 million, \nto be spent over three-to \nfour years. Total outcome \ncommitment of USD 2.8 \nmillion. Upfront capital \ncommitment: USD 3.0 \nmillion (pre-capital \nrecycling). \nAdditional grants \n(covering feasibility study, \nbaseline data study, DIB \ndesign and structuring, \ndata systems design, \nlegal advice) USD 1 \nmillion.", " Outcome Funder: Ministry of \nPublic Health, Cameroon (via the \nGlobal Financing Facility) and \nNutrition International (NI). \nInvestors: Grand Challenges \nCanada (GCC). \nService providers: Kangaroo \nFoundation Cameroon and \nLaquintinie Hospital. \nIntermediary: The MaRS Centre \nfor Impact Investing and Social \nFinance UK. \nTechnical assistance providers: \nKangaroo Foundation Colombia \n(leading KMC trainer); UNICEF \nCameroon; IDinsight (conducted \ninitial baseline data study); \nMorrison Foerster and Miller \nThomson (international co-legal A164 No DIB Objective Stakeholders involved Structure Value Development \nStage 2  Cook and \nClean DIB The DIB aims to increase \nthe number of clean cook \nstoves in use by at least \n50,000 over the lifetime of \nthe bond. By doing so, it \naims to pursue health, \ngender and environmental \nobjectives and contribute to \nthe Sustainable \nDevelopment Goals.", " Educate \nGirls DIB Complete.", " The India Educate Girls \nDIB aims to enrol out-of-\nschool girls and improve \nboth girls\u2019 and boys\u2019 literacy \nin English, Loan amounts will range \nfrom USD 0.5 million to \nUSD 2 million per \nenterprise.", " counsel); Cameroon-based legal \ncounsel; in-country public health \nconsultant; data systems provider. \nOutcome evaluator: TBC Outcome Funder: TBC \nInvestors: Likely to be BIX \ncapital. Shell Foundation and \nDFID funded the DIB set-up; IFC \nfunds the data gathering for the \ncertification process with support \nfrom the Ministry of Finance in \nJapan. \nService providers: Mimi-Moto \n(cookstove producer); Emerging \nCooking Solutions (ECS, seller of \nMimi-Moto cookstoves). Apart \nfrom ECS, Cardano will select \none more social enterprise. \nIntermediary: Cardano \nDevelopment.  \nTechnical assistance providers: \nBaker McKenzie (pro-bono legal \nadviser). \nOutcome evaluator: TBC Outcome Funder: Children \nInvestment Fund Foundation.  \nInvestors: UBS Optimus \nFoundation.  \nService providers: Educate \nGirls.", " A165 Payment terms: The \ninvestor will provide a loan \nto the enterprise after \nconducting its due diligence \nand gaining the approval of \nits independent investment \ncommittee. Each enterprise \nwill be provided a separate \nloan, and each loan will be \nbased on the agreements \nbetween the enterprises \nand the eventual buyers of \nthe certificates. \nOutcome metric: Health, \ngender equality and \nenvironmental outcomes to \nbe measured and certified \naccording to the Gold \nStandard for the Global \nGoals on an annual basis.   \nRange of returns: TBC Payment terms: UBSOF \ndisburses 50% of \ninvestment principal to \nEducate Girls in \n2015 and 50% in 2016; \nCIFF will disburse one No DIB Objective Stakeholders involved Structure Value Development \nStage Intermediary: Instiglio. \nPerformance Manager:  N/A \nTechnical assistance providers: \nNone \nOutcome evaluator:  IDinsight.", " Hindi, and Math by funding \nEducate Girls\u2019 intervention \nin Rajasthan, India. The \nimpact bond structure is \nused because of the focus \nof results and related \nflexibility of the intervention, \nand because of the \npossibility to unlock new \nfunding streams.", " outcome payment of USD \n0\u2013USD 412,000 to UBSOF \nin 2018 \nOutcome metric: 1) \nEnrolment outcomes (20% \nof outcome payment): \nnumber of girls on school \nrosters in grades 2-8 in the \ntreatment group over 3 \nyears; 2) Learning \noutcomes (80% of outcome \npayment): Annual Status of \nEducation Report (ASER) \nmeasures basic literacy in \nHindi, basic literacy in \nEnglish, and basic \nnumeracy. \nRange of returns: Target \nIRR = 10%, max IRR = \n15%; UBSOF pays \nincentive to Educate Girls \nequal to 32% of its payment \nabove principal Implementatio\nn.", " India \n(Rajasthan\n) Maternal \nand \nNewborn \nHealth DIB The bond is intended to \nimprove and standardize \nthe quality of maternal care \nin Rajasthan\u2019s private \nhealthcare facilities. The \nDIB implementing partners \nwill guide the targeted \nprivate healthcare facilities Outcome Funder: USAID and \nMerck for Mothers. MOU with the \nRajasthan State Ministry of Health \nto invest in, and scale-up, the \npartnership if the pilot program is \ndeemed successful by the \nindependent evaluator. \nInvestors: UBS Optimus Payment terms: Six-\nmonthly payment to \ninvestors, with USD 4,500 \nfor each facility at \nprogressive stage and \nremainder USD 13,500 for \nfacilities that reach Joint \nQuality Standard (JQS) Projected total investment \nof USD 9 million, USD 1 \nmillion of which is set \naside for results \nverification. UBS Optimus \nFoundation will provide \n80% of the USD 4 million \nupfront working capital A166 Development \nStage No DIB Objective Stakeholders involved Structure Value through quality \nimprovements and the \napplication process to be \naccredited through the \ngovernment-approved \nhealthcare facility \ncertification process.", " Foundation. \nService providers: Washington-\nbased Population Services \nInternational (PSI) and the \nRajasthan NGO Hindustan Latex \nFamily Planning Promotion Trust \n(HLFPPT). \nIntermediary:  Palladium. \nPerformance manager: \nPalladium.  \nTechnical assistance providers: \nReed Smith (Pro bono legal counsel);  \nOutcome evaluator: \nMathematica Policy Research \n(MPR).", " needed for the project. \nThe remaining 20% of \ninvestment capital will be \nprovided by Palladium, \nPSI, and the HLFPPT. \nUSAID and Merck for \nMothers will collectively \nprovide up to USD 8 \nmillion in outcome funds.", " during that period. No \nguarantee \u2013 payments are \nper facility ready for \naccreditation hence full loss \nis not possible. \nOutcome metric: Full \npayment is readiness for \nJQS - >=50% of the total \npoints available in each \nNABH section (10 in total) \nAND 100% of at least 70% \n(11) of the applicable \nFOGSI standards (16 in \ntotal). Progressive metric \n(25% of full outcome \npayment ie USD 4,500) is \n>=30% of the total points \navailable in each NABH \nsection (10 in total) AND \n100% of at least 40% (6) of \nany of the applicable \nFOGSI standards (16 in \ntotal). \nRange of returns: 8% \nannualized for UBSOF and \n15% return possible for the \nimplementation partnership \nof Palladium, PSI and \nHLFPPT.", " A167 No DIB Objective Stakeholders involved Structure Value Development \nStage Late-stage \ndesign - Failed \nto launch.", " 5  Mozambiq\nue Malaria \nDIB Mozambique has the 6th \nhighest malaria burden \nglobally, but faces a 62% \nmalaria funding deficit. The \nDIB would support an \nIndoor Residual Spraying \nprevention programme that \nshould reduce by 60% the \nnumber of cases reported.", " Upfront capital \ncommitment was USD 4 \nmillion, first close at USD \n2 million.", " Outcome Funder: Goodbye \nMalaria, underwritten by Nandos \nand other corporates.  \nInvestors: not defined.  \nService providers: Lubombo \nSpatial Development Initiative \n(LDSI) II.  \nIntermediary:  D. Capital \nPartners. \nTechnical assistance providers: \nthe University of Pretoria, the \nMedical Research Council, and \nthe National Malaria Control \nProgramme within the Ministry of \nHealth (Mozambique). \nOutcome evaluator: TBC Payment terms: The \npayment would have been \nmade as a bullet payment \nat the end of the third year \nof bond implementation, \nbased on the achievement \nof expected outcomes. \nOutcome metric: 60% \nreduction in the prevalence \nand incidence of malaria \ncases, compared to \nbaseline rates at year 1. \nIncidence of malaria cases \nbased on prevalence \ntesting done at sentinel \nsites in each district.   \nRange of returns: The \nmaximum potential loss of \ninvestment for investors \nwould have been 30%, and \nthe maximum return 0.05%.", " Late-stage \ndesign.", " Palestine \n(West \nBank and \nGaza) \nEmployme\nnt DIB The DIB aims to tackle \nyouth unemployment by \nfostering closer \ncollaboration between the \nprivate sector and training \nand education providers, so \nas to enhance the skills of \nthe Palestinian workforce in \na more market-driven way.", " Outcome Funder: World Bank \nGroup.  \nInvestors: Five investors from \nChile, Palestine, Holland, Britain \nand Switzerland, which have not \nsigned yet. \nService providers: 2-4 \nPalestinian, not-for profit service \nproviders to be selected per cycle, Payment terms: The \ninvestors will form a SPV \nfor the flow of funds, with a \nDIB manager (contracted \nby the PIA) who will \nmanage the SPV funds on \nbehalf of the investors.  \nOutcome metric: Likely to \nbe a mixture of training The outcome funds are \nUSD 5 million. The DIB is \npart of a wider World \nBank project called \nFinance for Jobs, a larger \ninitiative to create \nemployment in West Bank \nand Gaza.", " A168 Development \nStage No DIB Objective Stakeholders involved Structure Value outputs and employment \noutcomes \nRange of returns: N/A depending on the size of the \nconsortium.  \nIntermediary: Private \nimplementation agency (PIA) \nresponsible for daily management \nand for subcontracting technical \nadvisors). \nTechnical assistance providers: \nSocial Finance (set up and design \nphase); Emily Gustafsson-Wright \nof Brookings Institution \n(evaluation framework).", " Outcome Funder: Department of \nSocial Development and ApexHi \nCharitable Trust. \nInvestors: Could be the \nSyndicate of the Foundation for \nCommunity Work, an Institutional \ninvestor or a philanthropist. \nService providers: Foundation \nfor Community Work. \nIntermediary: D. Capital Partners  \nPerformance manager: \nmothers2mothers \nTechnical assistance providers: \nSocial Finance UK \nOutcome evaluator: TBC A169 Late-stage \ndesign South \nAfrica ECD \nImpact \nBond \nInnovation \nFund \u2013 \nSocial \nDevelopm\nent The DIB aims to improve \ndevelopmental outcomes in \n3-5 year old children in low-\nincome communities by \nfunding non-centre based \nearly learning interventions \nin Western Cape in South \nAfrica. The impact bond \nstructure is used because \nof the rigorous performance \nmanagement and the \npossibility to align public \nand private sector \noutcomes funding.", " Payment terms: Payment \nattached to recruitment and \nretention targets: every 6 \nmonths. \nPayment attached to \nattendance and \ndevelopment assessment: \nonce a year. \nOutcome metric: \nRecruitment and retention, \nattendance, development \nassessment score \nRange of returns: The \nmaximum return on \ninvestment is capped at \n16% The outcome funds are \nUSD 2.2 million, split \nbetween the two outcome \nfunders. The upfront \ncapital commitment is of \nUSD 1.1 million across \ntwo impact bonds (social \ndevelopment and health, \nsee below). The total \npotential outcome \npayment could reach USD \n3.6 million. Additional \ngrants accrue to USD \n111,000.", " South \nAfrica ECD \nImpact The DIB aims to improve \nhealth, nutrition and \ndevelopmental outcomes of Late-stage \ndesign Outcome Funder: Provincial \nDepartment of Health and \nDiscovery Fund.", " Payment terms: Payment \nis realised every six months \nwith relation to recruitment Outcome funds USD 1.38 \nmillion, split between the \ntwo outcome funders.", " No DIB Objective Stakeholders involved Structure Value Bond \nInnovation \nFund \u2013 \nHealth Development \nStage pregnant women and \nchildren from 0-2 years by \nfunding home and \ncommunity based \ninterventions in the \nWestern Cape in South \nAfrica. The impact bond \nstructure is used because \nof the rigorous performance \nmanagement and \nmechanism with which to \nalign public and private \nsector outcomes funding.", " Investors: Could be the \nSyndicate of the Foundation for \nCommunity Work, an Institutional \ninvestor or a philanthropist. \nService providers: TBC \nIntermediary: D. Capital Partners \nPerformance manager: \nmothers2mothers  \nTechnical assistance providers: \nSocial Finance UK \nOutcome evaluator: TBC Upfront capital \ncommitment USD 1.1 \nmillion across 2 impact \nbonds (social \ndevelopment and health), \nthe total potential outcome \npayment of which could \nreach USD 3.6 million. \nAdditional grants: USD \n110,000.", " outcomes, and once a year \nwith relation to all the other \noutcomes.  \nOutcome metric: \nRecruitment; mother child \nunit: antenatal care (ANC) \naccess, reduction in \nmaternal alcohol \nconsumption (RMAC), \nprevention of mother \nto child transmission of \nHIV, birth-weight (BW); 0-1 \nyears: exclusive breast \nfeeding \n(EBF), weight for age, \nprevention of HIV \ntransmission, prevention \nand treatment of TB; 1-2 \nyears: height for age, \nimmunization, prevention \nand treatment of TB, \nprimary caregiver \nassessment. \nRange of returns: \nInvestment rate of return is \ncapped at 16%.", " A170 No DIB Objective Stakeholders involved Structure Value Payment terms: TBC \nOutcome metric:  TBC \nRange of returns: TBC Outcome funds USD 10-\n30 million (anticipated).", " Development \nStage Early-stage \ndesign Outcome Funder: TBC \nInvestors: TBC \nService providers: Two service \nproviders have been shortlisted, \ndelivering employment and \nentrepreneurship interventions. \nFour potential service providers \nhave been identified and are \nfollowing detailed due diligence. \nIntermediary: KOIS Invest \n(feasibility study)  \nTechnical assistance providers: \nTBC Outcome Funder: N/A \nInvestors: N/A \nService providers: N/A \nIntermediary:  Social Finance UK \nTechnical assistance providers: \nN/A Late-stage \ndesign \u2013 failed \nto launch, \npending \navailability of \noutcome \nfunding Syrian \nRefugee \nEmployme\nnt DIB 10  Uganda Sleeping \nSickness \nDIB The multi-country DIB \nintends to improve the \nwelfare of Syrian refugees \nand vulnerable host \npopulations by funding job \nmarket integration and \naccess to livelihoods \ninterventions in the Middle \nEast. This includes \nemployment and \nentrepreneurship \ninterventions.", " The aim of the DIB is to \nprevent two deadly strains \nof Sleeping Sickness from \noverlapping in Northern \nUganda. A successful pilot \nwas implemented in \n2014/15, in which 20,000 \ncattle were treated for \nSleeping Sickness. The \nintervention model also \nincludes a behaviour \nchange component to \nensure that farmers spray \ntheir cattle effectively to \nprevent the spread of \nSleeping Sickness and \nimprove cattle health.", " A171 N/A Payment terms: N/A \nOutcome metric:  Audited \ndelivery of the mass \ntreatment intervention and \nstatistically significant \nreductions in the T. brucei \ns.l. parasite among the \ncattle population in target \nareas. \nRange of returns: N/A Annex K: Learning workshop note The learning workshop offered the evaluation team the opportunity to test emerging findings with \nthe key evaluation stakeholders. A complete list of the participants is set out in the table below.", " DIB QEI DIB ICRC HIB VE DIB Stakeholder Group Organisation Outcome convenor Risk investor Performance manager Service provider Service provider Service provider Advisor Risk investor Intermediary BAT UBSOF Dalberg KEF Gyanshala ICRC KOIS Munich Re Instiglio Cameroon Cataract \nDIB Outcome Funder Outcome Funder Advisor Hilton foundation Sightsavers Volta Service Provider The Magrabi Foundation The main aims of the workshops were to present Ecorys\u2019s emerging findings on the DIB effect \nand  the  main  lessons  learnt  in  the  delivery  of  the  DIBs,  so  as  to  compare  differences  and \nsimilarities across DIBs, and test these findings.", " Emerging findings in terms of how the DIB affected the interventions funded by the DIBs were \ndiscussed. The DIB effects table set out in the inception report was used to frame findings. The \nkey lessons learned were also presented.", " Participants\u2019 comments were useful in informing and refining the evaluation team\u2019s analysis. A \nbrief summary of the discussion, and how they have been incorporated into the report, is set out \nbelow.", " In terms of the DIB effect table: \u2022 \u2022 In terms of the transfer of risk effects, one investor commented that it was important that \nservice providers share some of the risk, and have \u2018skin in the game\u2019. However, another \nintermediary commented that this may not always be possible, and that reputational risk \nand potential upside can also serve the same purpose. This has been incorporated into \nour analysis, and the consideration of risk sharing broadened to include both financial risk, \nreputational risk and potential upside (See section 4).", " In terms of the financing and funding effects, it was noted that new types of investors were \nattracted  by  the  outcomes  focus  of  the  DIB,  as  well  as  the  possibility  to  recycle \ninvestments.  On  the  other  hand,  another  stakeholder  commented  that  it  seemed  to  be \nrather  the  novelty  of  the  tool  that  was  attracting  stakeholders.  The  importance  of A172 distinguishing between the DIB effect and the \u2018novelty\u2019 effect, and how the fact that the \nDIBs are in a pilot phase may affect findings, has been taken onboard, and reflected in \nthe analysis in section 4.", " \u2022 In  terms  of  the  design  effects,  the  discussion  raised  the  importance  of  distinguishing \nbetween solution and process innovation, as well as innovation in the design of the DIB \nitself.  It  was  noted  that  DIBs,  by  design,  may  not  be  the  most  appropriate  funding \nmechanism  for  solution  innovation.  One  DIB  stakeholder  highlighted  that  a  substantial \ninnovation is the involvement of the private sector in the humanitarian sector, thanks to \nthe  DIB  mechanism.  The  importance  of  distinguishing  between  different  types  of \ninnovation is reflected in the updated DIB effect table set out in Annex E.1.", " \u2022  An effect highlighted but not included in the presentation was the signalling effect that can \nbe produced by successful DIBs. This can signal to the government the feasibility of using \nimpact  bonds  to  solve  this  social  problem,  and  creates  the  potential  for  the  DIB \ntransitioning to a SIB. This is not an effect that can be expected to emerge during the set \nup phase, and will be reviewed in the following research waves.", " In terms of the lessons learned: \u2022  Discussion focused on the importance of clearly defining roles and responsibilities at the \noutset, and take advantage of the process of co-design that characterises the DIB, in terms \nof shared experience and shared learning.", " \u2022 In terms of the additional costs of the DIB mechanism, it is still unclear whether they are \nspecific to the DIB, or rather to the fact that it is a new financing mechanism. The VfM \nsection of the report (see section 5) seeks to contribute to developing this understanding.", " \u2022  The role of the private sector was discussed, in terms of bringing expertise in analysing \nrisks. Several stakeholders commented that a key role for the investor can be to bring in \nmore  rigorous  scrutiny  of  the  design  of  the  intervention,  the  theory  of  change  and  the \nevidence base, which can be used to improve the intervention design. It was also noted \nthat expertise within the commercial sector in terms of assessing and pricing risk could be \nused to support a better process in terms of designing and pricing outcome metrics.", " A173 Annex L: VfM Analysis \u2013 Supporting Evidence This sub-section provides additional detail on the value for money (VfM) analysis undertaken on \nthe four DIBs, in terms of detail on the cost drivers across the four DIBs and the extent to which \ncosts can be considered \u2018first DIB\u2019 costs.", " L.1 ICRC Stakeholders from the ICRC HIB said some but by far not all of the additional costs were due to \nbeing a first time deal.", " The main cost drivers for this HIB were: \u2022  establishing the legal framework for the DIB to be possible (including the PbR contracts \nand the investment agreement), while aligning the interests and constraints of the \nvarious parties.", " \u2022 identifying  outcome funders  which  took  longer  than  expected.  The  number  of  outcome \nfunders lengthened the time it took to negotiate outcome metrics and pricing.", " L.2  Quality Education India DIB The UBS Financial modelling design document (2016) estimates that a quarter of additional costs \nabsorbed by outcome funders/investors were one-off costs as opposed to ongoing. However, it \nshould be noted that not all one-off costs have been estimated.", " The main cost drivers for this DIB were: \u2022  Number  of  organisations  to  involve  and  coordinate  (increasing  costs):  The \ninvolvement of multiple outcome funders and service providers, as well organisations with \nspecific  roles  in  performance  management  and  outcome  verification,  has  required \nsubstantial  coordination  to  engage  people  in  the  process  and  work  out  their  roles  and \nresponsibilities  .", " \u2022  Fundraising/engaging multiple outcome funders (increasing costs): BAT had to raise \nthe remaining amount after MSDF committed to USD 4 million. BAT spent a significant \namount of time doing this, which caused delays in the set-up after the design phase had \nfinished.", " \u2022  Press and marketing (increasing costs): Service providers commented on the number of meetings they have had to engage with, as a result of being involved in a DIB.", " \u2022  Key stakeholders (i.e. UBS and Dalberg) were involved in Educate Girls, a previous \nDIB  in  education  (reducing  costs):  UBSOF  stakeholders  confirmed  that  the  legal \nprocess was shorter in the current DIB, completed in six months, compared to two years \nin the Educate Girls, which they felt was evidence of increasing efficiency from reusing \ntools and applying learning from this previous experience.", " \u2022  Costs  to  the  stakeholders  varied  depending  on  when  the  stakeholder  engaged \n(reducing  cost):  Comic  Relief  engaged  quite  late  in  the  process  so  they  thought  the A174 demands of the project were reduced. For example, once they had \u2018got up to speed\u2019 on \nthe documents, most of the agreements had already taken place so the set-up for them \nwas less than in other projects.", " L.3  VE DIB Stakeholders from both Village Enterprise and Instiglio commented on the increased cost at the \ndesign and set-up stage of the DIB. This was largely in the form of staff time as well as consultancy \nand expertise costs which appeared to be significantly more than their usual programming costs \nunder grant funded mechanisms.", " The main cost drivers for this DIB were: \u2022  The responsibility for raising the investment finance was that of the service provider (VE).", " The process took a considerable amount of staff time and capacity.", " \u2022  This was the first DIB that the service provider was involved with. In order to understand \nthe  process  fully  they  needed  both  legal  and  accounting  consultancy  support  to  help \nstructure the finance and set up the SPV. Stakeholders from VE said this was a lot of work \nand time (over 100 hours of legal and accountancy support).", " Stakeholders  from  VE  and  Instiglio  were  confident  that  these  costs  were  largely  \u2018one-off\u2019  and \nfuture DIBs will not incur the same level of staff time and consultancy work required.", " Instiglio (as the intermediary) also conducted a process review into the launch of the DIB which \nlooked into costs. The key cost drivers identified were: \u2022  Service provider selection: Additional costs (particularly in staff time) were incurred at this stage.", " \u2022  Outcome funder engagement: Time was spent engaging with foundations that did not \nresult in any commitments to provide outcomes funding. Furthermore, engaging multiple \noutcome funders at different times created inefficiency.", " \u2022  Design: It was stated that negotiations lacked clear protocols for ensuring the right levels \nof inclusivity, which increased the amount of time it took, and therefore staff time required, \nto finalise the design of the DIB in all stakeholders.", " \u2022  Trustee  selection:  This  process  also  took  longer  as  conversations  with  trustee candidates began without a clear agreement on the function of the trustee.", " \u2022  Contracting: There was a poor understanding of outcome funders\u2019 procurement burden which delayed the start of the contracting process \u2022  Financier  engagement:  Given  VE\u2019s  limited  experience  and  infrastructure  to  do  this,  it took considerably longer.", " Stakeholders did not attribute these costs to the DIB model specially. Rather these were seen to \nbe largely \u2018first time\u2019 costs with the learning from this process to be used to ensure fewer costs \nare incurred in future DIBs.", " L.4  Cameroon Cataract Bond The outcome funders considered that some of the costs, especially staff time, were a one off as \na result of it being their first DIB. They also considered that all technical advice would be incurred A175 in future DIBs, but were not entirely sure to what extent due to this being their first DIB. Investors \nconsidered all costs would be incurred in future DIBS as well.", " The main cost drivers identified were: \u2022  Time it took to set up the DIB, find investors and several iterations of term negotiations. \nInvestor with demanding due diligence increased the costs for the outcome funder in \n\u2022 \nterms of legal advice.", " A176 Annex M: Literature Review The objective of the literature review is to contextualise the findings emerging from the DIBs pilot \nprogramme with those from the wider impact bond sector. The review focuses predominantly on \nDIBs, but also draws on findings of SIBs operating in low and medium income countries, and SIBs \nand PbR more broadly. The main areas of focus of the literature review are the two evaluation \nquestions, as well as approaches used to evaluate DIBs. The literature review is structured as \nfollows: \u2022  Section 1 explores the ways in which the DIB model is hypothesised to affect interventions. \n\u2022  Section  2  explores  the  theoretical  basis  for  DIBs  and  PbR,  which  then  leads  to  a \ndiscussion  on  potential  limitations  of  the  DIB  model,  criteria  necessary  for  DIBs  to  be \nsuccessful and contexts where DIBs seem to be well suited, concluding with a summary \nof the conceptual underpinning of impact bonds and critiques.", " \u2022  Section 3 reviews the evidence base mapped to the hypothesised effects of DIBs set out against the framework used above.", " \u2022  Section 4 summarises the key recommendations around how to design and agree DIBs \nto  increase  the  model\u2019s  benefits  and  reduce  the  associated  transaction  costs,  and \nrecommendations for scaling DIBs.", " \u2022  Section 5 concludes with a summary of the challenges to evaluating impact bonds, and approaches that have been used.", " a.  Hypothesised effects of DIBs The literature posits a range of effects DIBs could potentially have on programmes. In order to \norganise the different factors, the framework presented in the DFID PbR Evaluation Framework \n(see below) is used.42 The framework is split into three parts. Inputs (INP1), Processes (P1-P4) and Impacts (IMP1-5). \nIt  is  important  to  note  that  the  DIB  effect  can  be  considered  both  in  terms  of  the  individual Figure M.1: Framework for synthesising evaluation evidence 42 The framework draws upon papers by Clist and Drew (2015) and Clist and Verschoor (2014).", " A177 programmes being run, but also broader sector-wide effects, for example, ways of working and \nprogramme design and selection. We consider the  DIB  effects  on  both these levels.  Also,  the \nframework is supplemented with the team\u2019s addition of INP2, which captures the stakeholders \nproviding finance to programmes delivering social value. The rest of this sub-section sets out the \nhypotheses  by  which  DIBs  affect  programmes  based  around  the  input,  process  and  impact \nelements.", " The sources consulted are set out in the table below: Table M.1: Sources consulted Title Detail CGD and Social Finance 2013 Gustafsson-Wright  et  al  2015.  The  potential  and \nlimitations of impact bonds: lessons from the first \nfive years of experience worldwide.", " Gustafsson-Wright  et  al  2015.  Impact  bonds  in \ndeveloping countries: Early learning from the field.", " Center \nfor  Global  Development  and  Social \nFinance.  2013.  Investing  in  Social  Outcomes: \nDIBs Supplier  Access  to  Prefinance  in  PbR  (Chinfatt \nand Carson 2017) Three  key  ways  in  which  the  impact  bond  is \nexpected to lead change 10 claimed benefits of impact bonds The  \u2018Deal  Book\u2019  categorising  all  impact  bonds  in \nmiddle  and  low  income  countries.  Each  DIB  is \nassessed against a list of justifications for using the \nDIB / reason(s) existing financing was/is inadequate 6 case studies presented, including where DIB can \nadd value.", " 7 benefits and 6 limitations based on consultations.", " Oroxom et al Brookings. 2018. Nine Lessons from \nCameroon and Beyond.", " Three-part coordination problem linked to three key \njustifications for using DIB.", " SIBS 2018 presentation  \nCardno and Metis Analytics. 2014.", " Sedlmayr,  R. \nAlleviation Discussion Paper.", " (2018).  Paying USAID Investing for Impact (n.d.) 6 ways in which an impact bond adds value.", " 7 perceived advantages of DIBs/SIBs for  Poverty 3 difficulties and limitations of PbR Investing for Impact paper setting out spectrum of \nglobal  health  financing  and  new  opportunities  and \nadvantages of different models for  Global  Development  and  Social \nCentre \nFinance  (2013).  Report  of  the  Development \nImpact Bond Working Group 3 advantages of DIBs Instiglio43 Introduction to impact bonds \u2013 5 benefits.", " DFID PbR Evaluation Framework. 2014.", " theories  of  change 4 \nfor  PbR  based  on \npresentations and discussions at the PbR: Theory \nto  Evidence  Workshop,  21  November  2014, \nLondon.", " 43 http://www.instiglio.org/en/impact-bonds/ A178 Inputs Below, the main ways DIBs are hypothesised to affect programmes are set out against the input \nand process elements of the framework set out in Figure M.1 above.", " Donors,  investors  and  other  stakeholders  provide  the  support  needed  to  design,  develop  and \nintroduce programmes using DIBs This includes stakeholders cooperating in ways to maximise their comparative advantage.", " \u2022 Investors  are  better  than  donors  at  picking  investments  with  the  highest  potential  to \ndeliver outcomes. This also forces market discipline to the design of impact bonds, as \ninvestors are unlikely to back strategies which cannot demonstrate success. A stronger \nand more rigorous evidence base is needed to support business cases, which incentivises \nbetter and increased evidence collection and impact evaluation.", " \u2022  DIB  model  offers  a  clear  management  and  governance  structure  bringing  actors \ntogether,  to  address  large-scale  and  complex  interventions  that  require  successful \nstakeholder coordination. This can spill over into better stakeholder coordination beyond \nthe specific DIB.", " \u2022  The DIB model allows the design of tailored incentive structures, which can vary the risk \nsharing  profile  and  reward  structure  between  actors  to  fit  the  context  and  targeted \noutcomes, and ensure that incentives are aligned.  \nInvestors  have  strong  incentive  to  monitor  performance;  they  bring  private  sector \napproaches, and are better able to control and manage risks when compared to traditional \ndonors. This leads to investors (directly or through an intermediary) driving efficient and \neffective service delivery.", " \u2022 Donors, investors and other stakeholders provide the capital needed to deliver programmes which \nprovide social value This includes donors, investors and other stakeholders being able to finance these programmes, \nespecially where the use of the DIB mechanism enables stakeholders to do so, or on a larger \nscale.", " \u2022  DIBs  can  mobilise  private  funding  that  can  be  combined  with  public  funding.  These \nsources of funding can be used to cover a capital gap/market failure \u2013 for example: i) \npreventive services; ii) interventions that can add value to society but where the outcome \nfunders might not be willing or able to fund directly (due to the lack of certainty around \noutcomes/levels of risk); iii) Where a service provider can deliver on a PbR contract but \ndoes  not  have  the  upfront  finance  to  do  so,  or  needs  capacity  development.  The \nmobilising of additional funding can be used to achieve scale for proven interventions for \nwhich outcomes are clearly measurable.", " \u2022  DIBs can also reduce the risk for outcome funders, as funders only pay when outcomes \nare achieved. Political accountability can make it difficult for donors to provide public funds \nin advance for risky programmes, and this can make it possible for donors to fund these \nprogrammes.  This  means  donors  can  fund  risky  projects  that  can  satisfy  the  public \nexpectation of accountability. Limited budgets can be spent on what works.", " Process Outcome funders focus on results and not inputs \u2022  Outcome funders can be more hands-off as they do not need to hold providers to account \nfor  inputs/outputs  (provided  they  can  accept  certain  non-transferable  risks  such  as A179 reputational  risk,  political  risk  etc.)  This  can  minimise  administrative  processes  and \nworkload for outcome funders.", " DIBs create incentives for service providers to focus on producing desired results \u2022  Service  providers  have  the  incentive  to  be  result-focused,  which  can  incentivise  the \nestablishment or improvement of performance management systems. This can generate \na culture of results, together with rigorous measurement and evidence-based monitoring \nand evaluation. This can spill over to other programmes not funded by the DIB and build \na culture of M&E and course correction. (it is noted that a related theory suggests that it \nis increased attention, rather than the pecuniary interest, which may motivate change)  \n\u2022  Service providers may be more incentivised to target populations that face the greatest \nneeds, as this is often where the greatest gains (social and financial) are to be had.", " There is greater innovation and flexibility in approaches to delivering services \u2022  DIBs may improve quality by providing the service provider with autonomy and flexibility \nin  implementation,  to  adapt  the  intervention  to  changing  needs,  and  increasing  the \nchances  of  achieving  the  desired  outcomes. This  may facilitate shorter feedback loops \nand better course correction and innovation.", " Programme implementation improves and is more effective \u2022 Investors  have  strong  incentive  to  monitor  performance;  they  bring  private  sector \napproaches, and are better able to control and manage risks when compared to traditional \ndonors. This leads to investors (directly or through an intermediary) driving efficient and \neffective service delivery.", " Impact Expected outcomes are produced\u2026more effectively than with other approaches\u2026more efficiently \nthan with other approaches\u2026 \u2022  With the focus on results and not inputs, this also enables a market for impact bonds, for \nexample  through  outcome  funds,  which  can  be  used  to  increase  competition  in  the \ndelivery of target outcomes and drive down costs.", " \u2022  As DIBs incentivise outcome delivery for a fixed price, it also produces incentives towards \ncost  control  and  intervention  effectiveness.  This  can  lead  to  greater  efficiency \n(increasing  output  or  decreasing  costs)  and  maintaining  of  quality  if  the  appropriate \nincentives are set up.  \nIf outcome funders are less focused on inputs, this may mean that service providers have \nlighter reporting requirements, which can reduce costs.", " \u2022 With additional unintended positive outcomes\u2026and without unintended consequences\u2026in ways \nthat generate learning for use of DIBs in other countries \u2022  As  social  outcomes  take  time  to materialise,  and  service  providers  require  time  to test \ndifferent approaches and adapt, this could create incentives for outcome funders to fund \nprogrammes  over  a  longer  period  of  time.  This  can  lead  to  a  better  sustainability  of \noutcomes.", " \u2022  Outcome verification can lead to greater transparency around the impact of the funding and the service providers\u2019 work, and correspondingly, improved accountability.", " A180 The  summary  above seeks  to  set  out  a  comprehensive list  of the  many  ways  in  which impact \nbonds are hypothesised to have a positive effect on programmes. However, in reality, the aims \nof  using  impact  bonds  vary  for  different  stakeholders,  as  will  the  relative  importance  of  these \nbenefits. Box 1 below sets out a summary of a recent consultation with stakeholders, concerning \ntheir main objectives in using impact bonds.", " Box 4: Stakeholders' objectives in using impact bonds The  recently  established  Impact  Bonds  Working  Group  brings  together  a  range  of \norganisations  interested  in  growing  the  impact  bond  sector.  Members  were  surveyed  to \nunderstand the objectives sought with the use of impact bonds.", " Over 50% of members expressed that the primary objective is to increase the effectiveness \nof  their  organisation\u2019s  funding,  to  access  private  sector  finance,  and  to  allow  for  more \ninnovation in service delivery.", " Over a third of members see impact bonds as a way to make local government spending \nmore effective, and nearly half of members see impact bonds as a way to engage private \nsector  know-how  and  expertise.  Several  members  commented  that  impact  bonds  have \nhelped  transform  the  way  they  used  data  to  course  correct  and  improve  results  on  the \nground.", " Other objectives sought by members with the design of impact bonds included: i) to create \nbetter models for diaspora philanthropy; ii) to create a platform that allows a bridge for low-\nincome/transition countries to go from aid-dependent economies to investment-partnership \nopportunities; and iii) to advance the robustness and fidelity of impacts of poverty alleviation \nprogramming at scale.", " M.1.1 Theoretical Basis, Criteria, Suitable Contexts for Effective use of PbR and \nimpact bonds and Critiques In this section we highlight some considerations and theories from the literature that need to be \nborne in mind when developing and launching impact bonds.", " M.1.2 Theoretical Basis Exploring  the  theoretical  basis  for  PbR  and  DIBs  is  important  to  understand  the  potential \nlimitations  of  using  impact  bonds,  as  well  as  the  factors  necessary  for  its  successful \nimplementation.", " The  theory  behind  PbR  relies  on  the  assumption  that  PbR  creates  stronger  incentives  for \nimplementers to undertake desired actions and also imposes greater risk. The trade-off for the \ndonor  is  between  the  positive  gains  resulting  from  the  use  of  this  mechanism,  versus  the  risk \npremium  paid  out  (Clist  and  Verschoor,  2014).  As  such,  the  extent  that  expected  benefits  are \nrealised depends on a number of principles (Clist and Dercon 2014). The principles most relevant \nto impact bonds are set out in Table 29 below: A181 Table M.2: Impact bond principles Quality of the performance measure Principle Alignment Observability of effort Control Risk aversion and risk transfer Distortion and gaming Additional \nverification costs transaction,  contractual  and Requirement  for  PbR  to  be  more  effective  than \nregular contracts Performance  measure  needs  to  be  correlated  with  the \nunderlying  outcome  of \ninterest  before  and  after \nincentivisation.", " There  can  be  incomplete  alignment  between  outcome \nfunders and service providers in terms of incentives and \ngoals.  If  the  service  provider  is  always  incentivised  to \ndeliver  the  target  outcomes,  the  payments  by  results \nwould not change incentives, and as such there would \nbe no expected gains in efficiency or effectiveness. For \nimproved  performance,  the  incentive  needs  to  lead  to \nbetter alignment of incentives and aims, and the service \nprovider needs to be able to effect changes. The service \nprovider  also  needs  i)  a  level  of  autonomy,  and  ii)  the \ncapacity and skills to improve delivery.", " Effort  should  not  be  easily  observed,  otherwise  the \ncontract could be based on this instead.", " Service  providers  have  significant  control  over  the \noutcomes.  This  may  be  weaker  in  contexts  of  policy \nuncertainty  and  high  risk.  Otherwise,  the  service \nprovider or investor may not be willing to take on this risk \nif there is too much out of their control.", " risk transferred  needs The  amount  of \nto  be \ncommensurate  with  the  risk  premium  paid.  Different \nactors will have different levels of risk aversion, and this \nmay affect the risk premium and the pool of interested \nactors.  Determining  the  appropriate  risk  and  reward \nstructure  (pricing  and  outcomes)  to  get  the  incentives \nright can be difficult.", " Service  providers  do  not  or  cannot  game  the  system, \nand incentives are not distorted so that actions important \nfor the underlying goal but not measured by the outcome \nmeasure are ignored (i.e. tunnel vision). There may be \ntension  between  this  principle  and  the  alignment \nprinciple.", " Additional costs need to be offset by other benefits, such \nas  increased  outcomes  or  efficiency  gains  (including \nreduced staff time or transaction costs).", " Challenges  to  secure  financing,  access  the  capital \nmarket,  or  donor  requirements  are  not  much  reduced \nfrom regular contracts, can further increase costs and, \ncorrespondingly,  the  risk  of  foregoing  the  expected \nefficiency or effectiveness gains.", " These principles highlight the requirements for PbR to be more effective than other contracts, and \nalso the potential limitations and weaknesses should these requirements not be met. Additionally, \nClist  and  Drew  (2015)  argue  that  there  are  two  additional  requirements  for  DIBs  to  be  more \neffective than other contract arrangements: A182 \u2022  For DIBs to be cost effective, the risk premium paid out by outcome funders needs to be \nless than the gains in effectiveness. Clist and Drew (2015) also argue that risk transfer \nshould not necessarily be a rationale for DIBs, as donors such as DFID are involved in a \nnumber  of  diversified  projects.  As  it  already  has  a  diversified  risk  profile,  transferral  of \ndelivery risk will not be efficient, unless it leads to higher programme efficiency. The idea \nis that it would be more efficient for DFID to accept the risk of failure or non-delivery across \nall its programmes, rather than pay out a risk premium on all of these projects. However, \nthis is from a pure cost-efficiency perspective, and does not take into account reputational \nrisks for donors; \u2022  The additional benefits of DIBs (when compared to PbR contracts), relies on the fact that \nthe outcome funder can outsource the selection of investible opportunities to the investor. \nClist and Drew (2015) argue that if the outcome funder thinks it has an obligation to specify \nwho  the  investor,  service  provider,  intermediary  and  verification  provider  in  the  impact \nbond should be and how they should function, then the benefits of DIBs will be foregone, \nand a PbR contract should be used instead.", " M.1.3  Criteria This  sub-section  explores  the  main  criteria  set  out  within  the  literature  as  necessary  for  the \neffective  use  of  an  impact  bond.  Echoing  some  of  the  principles  above,  they  can  broadly  be \nconsolidated into five criteria: Analysis  of  the  SIB  evidence  seems  to  suggest  four  necessary  criteria  for  an  impact  bond  to \nlaunch.", " 1.  Collective Leadership: \u2022  Strategic (between members of the leadership team);  \n\u2022  Organisational (between these leaders and their internal stakeholders)  \n\u2022  Environmental (between the team and organisation\u2019s external environment and outside stakeholders) (Gustafsson-Wright and Gardiner, 2016).", " 2.  Clear  outcomes  \u2013  measurable  outcomes  and  linked  to  overall  objective  of  the  intervention (Gustafsson-Wright et al., 2015; Gustafsson-Wright and Gardiner, 2016).", " 3.  Shared understanding of the policy \u2018problem\u2019 and sufficient evidence for the intervention so that it is credible or knowledge-based.", " 4.  Data to build up a business case, including data on the eligible cohort and outcomes likely to be achieved.", " Additionally, a fifth criteria is suggested as particularly relevant for DIBs: 5.  Appropriate political and legal context, to enable the legal structure and contracting, and to \nreduce  risks  of  corruption  in  procurement,  outcome  payment  design  or  evaluation  at  a \nreasonable level.44 44 http://www.instiglio.org/en/impact-bonds/ A183 M.1.4  Suitable Contexts This  sub-section  summarises  the  literature  on  the  contexts  to  which  impact  bonds  are  best \nsuited. There is more debate in this area. This is because of slightly different, and often conflicting, \ntheories and experiences of how impact bonds work. Further evidence generation is needed to \ntest these different theories.", " The advice is consistent in that DIBs are best suited for where there is a market failure, that is, \na  lack  of  funding  or  capacity  to  deliver  interventions  or  services  that  lead  to  societal  value \n(Gustafsson-Wright  and  Gardiner,  2015;  USAID,  nd).  This  may  include  situations  where \nstakeholders are not working together, as impact bonds can facilitate their coordination (Social \nFinance, 2018).", " There is less evidence on the sectors that may be suited for impact bonds, Gustafsson-Wright \nand Gardiner (2015) suggest that future impact bonds will include a wider range of interventions \nin  early  child  development,  health,  housing,  and  water  and  sanitation.  Health  is  a  particularly \npromising area, given the potential for high future returns, both social and economic. The paper \nalso suggests that services that cater to particularly undeserved or marginalised populations and \nthose that improve existing services may be a further growth area.", " There is conflicting advice on the level of evidence needed, and linked to this, on the level of \npotential innovation. On the one hand, some suggest that impact bonds work best when there \nis  a  lack  of  knowledge  about  the  most  effective  intervention  model,  when  there  is  insufficient \nimpact evidence, or when suppliers are willing to test new approaches (Gustafsson-Wright and \nGardiner, 2015) and can benefit from innovation and accountability (Bloomgarden et al., 2014; \nGustafsson-Wright et al., 2015). On the other hand, CGDev (2013), Bloomgarden et al. (2014), \nand  Gustafsson-Wright  et  al.  (2015)  suggest  that  key  factors  are  that  there  are  \u2018proven,  cost-\neffective,  evidence-based  interventions  that  can  be  implemented\u2019  and  evidence  of  success  in \nachieving outcomes.", " This raises three important points: \u2022  There needs to be a balance between risk that needs to be transferred for the risk premium \nto be worthwhile, and risk that the investor is happy to take on. There needs to be sufficient \nevidence of intervention impact to attract the investor risk appetite.", " \u2022  Secondly,  as  Gustafsson-Wright  et  al.  (2015:  43)  note,  how  innovative  something  is \ndepends on what it is being compared to. A broader definition of innovation means that \n\u2018an intervention can be considered innovative if it has never been implemented at all, with \na given population, in a particular service delivery setting, by a particular service provider, \nin  a  geographical  area,  or  in  combination  with  other  interventions.\u2019  The  right  level  of \n\u2018innovation\u2019 or level of unknown in terms of balance between being new but proven can \nbe selected to correspond with the risk appetite of the investor.", " \u2022  Lastly,  there  may  be  different  categories  of  impact  bonds,  with  different  levels  of \ninnovation.  Dear  et  al  (2016)  categorises  a  range  of  SIBs  along  the  innovation/scale \nspectrum as set out in Table M.3: Categorisation of SIBs by level of innovationbelow: A184 Table M.3: Categorisation of SIBs by level of innovation Projects focused on Measurement Example SIB Innovation Non-experimental Youth Engagement Fund Building Evidence or Peterborough Quasi-experimental \nexperimental Replication,  drawing  on  an \nestablished evidence base Against  a  counterfactual \nfurther build evidence to Child-parent Center Model Simpler methodology Essex Social Impact Bond Scaling,  using  established, \nhighly \nevidence-based \ninterventions Not all hypothesised effects or principles may be relevant for all impact bonds. Both Gustafsson-\nWright  et  al.  (2017)  and  CGDev (2013)  include case  studies  that  are  analysed  in  terms  of the \njustifications  for  using  the  impact  bond  and  where  the  impact  bond  is  thought  to  add  value. \nDifferent case studies had slightly different combinations of these factors. Similarly, the DFID PbR \nevaluation framework (2014) highlights the importance of tailoring theories of change to individual \nDIBs.", " The evidence base for impact bonds is still emerging. It may be that different design features and \nfocus  areas  work  best  in  different  combinations  and  contexts,  leading  to  different  possible \noutcomes of impact bonds. This is something suggested by Clist (2017). He mentions two \u2018sweet \nspots\u2019  of  PbR,  each  with  a  specific  combination  of  factors  which  make  the  PbR  instrument \neffective.", " The two categories are \u2018Big\u2019 PbR and \u2018Small\u2019 PbR. \u2018Big\u2019 and \u2018small\u2019 refer to the scale and costs of \nimplementation, as well as to the level of risk transfer and return. Clist (2017) proposes that the \nrequirements  for  these  two  categories  of  PbR  differ,  as  a  result  of  the  different  theories \nunderpinning their operation.", " \u2022 \u2022 \u2018Big\u2019  PbR  requires  excellent  measures  (that  is,  highly  correlated  with  the  underlying \nobjective of the programme, which may require difficult of expensive data collection and \nverification).  It  also  requires  high  incentives  and  a  longer  term  timeframe  to  allow  for \ncourse-correction and innovation in service delivery. The theory of change relies on the \nincentivisation of outcomes and pecuniary interest, which drives the service provider to \ninnovate, or what Clist (2017) terms \u2018recipient discretion\u2019. To allow for the autonomy of the \nservice  provider,  requirements  such  as  reporting  of  financial  inputs  to  pre-agreed \nparameters or burdensome requirements to seek funder approval for course correction is \ndangerous and can stifle innovation.  \nIn  contrast,  \u2018Small\u2019  PbR  requires  lower  incentives  and  reasonable  quality  measures. \nStandard donor procedures and oversight is less harmful. The main theory of how change \nis effected, is the service providers\u2019 increased attention and focus on outcomes.", " Clist  and  Drew  (2015)  contrasts  the  piloting  of  the  Ugandan  sleeping  sickness  DIB  with  the \nRajasthan  DIB.  The  Rajasthan  DIB  was  designed  to  be  smaller  in  terms  of  scale,  risk  and \ninnovation, though with relative autonomy as it was about scaling up a proven intervention within \na relatively short timeframe and low cost, in contrast to the Ugandan sleeping sickness DIB which \nwas  completely  new  and  untested.  This  is  an  area  that  could  be  further  explored  in  future A185 evaluations.  Learning  on  how  DIBs  should  be  structured  in  different  contexts,  and  the  likely \noutcomes  in  different  scenarios  will  be  important  for  improving  the  designing  and  agreeing  on \nfuture DIBs.", " M.1.5  Conceptual Underpinning of Impact Bonds and Critiques Based  on  a  systematic  literature  review,  Fraser  et  al  (2016)  identify  that  the  conceptual \nunderpinning of impact bonds relies on two narratives: a public sector reform narrative emerging \nfrom theories of public management, and a private financial sector reform narrative emerging from \ntheories of social entrepreneurship. The two narratives underpin the two main benefits argued by \nproponents  \u2013  that  impact  bonds  bring  rigour  to  social  services  and  attract  private  finance  to \naddress  social  problems  (Warner  2013).  Similarly,  the  critiques  of  impact  bonds  are  framed \naround  broader  critiques  of  new  public  management  and finacialisation of  public  services,  the \nassociated perverse incentives resulting from these arrangements and doubts about the extent \nto  which  impact  bonds  can  deliver  on  its  promises  and  provide  value  for  money  (Carter  et  al, \n2018). The next few sub-sections discuss each in turn.", " \u2018Managerialism\u2019 and \u2018financialisation\u2019 of public services This critique of impact bonds see them not as neutral instruments, but as the latest phase of new \npublic  management  and  quasi-market  theory  (Joy  and  Shields  2013;  Le  Grand  1995),  with \nimplications for the control and accountability of services and involving limited consideration of \ncitizens\u2019  rights  and  entitlements  (McHugh  et  al  2013;  Sinclair  et  al  2014).  The  values  of  the \n\u2018market\u2019 and of social provision are seen as fundamentally different (McHugh et al 2017). Four \nsub-points are considered below: \u2022  Firstly, the financialisation of social provision is a political issue affecting social rights. \u2018The \nmonetisation of policy goals\u2026 transforms substantive social outcomes from the status of \nends  in  themselves  to  a  means  for  reducing  government  spending  and  producing  a \nfinancial return for investors\u2019 (Lake 2016:57), and the status of service users is changed \nfrom a citizen with rights to a commodity which can be processed for profit (Sinclair et al \n2014). Furthermore, the use of an impact bond may lead to the prioritisation of policies \nwhich generate a cost saving, instead of policies and provision prioritised by citizens or \nlinked to statutory rights.", " \u2022  Secondly, use of impact bonds and the requirement of a measurable outcome metric may \npromote  narrow  conceptions  of  programme  design,  constraining  possible,  fundable \nsolutions  to  those  that  generate  high  returns,  which  can  be  captured  in  a  performance \nmanagement framework. The move to a narrow conception of outcomes means that that \nimpact bonds undermine systemic issues. For example, Cooper et al (2016) note that a \nSIB working on homelessness failed to address systemic issues, and instead relied on an \nunderstanding of a homeless person as a failed individual. This more narrow view also \nhas implications for the sustainability of results. Also, benefits achieved in one area may \nbe transferred as costs to another area, outside the scope of what is covered by the SIB \noutcome metrics (Warner 2013).", " \u2022  Thirdly, McHugh et al (2013) and Sinclair et al (2014) note that many SIB guides (Centre \nfor  Social  Impact  Bonds,  Audit  Commission  and  the  Cabinet  office)  recommend \noutsourcing funding, service delivery and the responsibility for selecting a provider. The \nrationale is that it is reasonable for investors or intermediaries to influence how the project A186 is  delivered  and  to  terminate  the  project  in  the  event  of  sustained  under-performance, \ngiven that they are taking on the risk. The implicit assumption is that the provision of the \nservice should be accountable to those who pay for it rather than those who use it, which \nis problematic for accountability to service users / beneficiaries.", " M.1.6  Perverse incentives This  critique  of  impact  bonds  focuses  on  the  perverse  incentives  generated  by  the  use  of  an \nimpact bond. While impact bond proponents often speak of the alignment of interests, Maier and \nMyer (2017) explore the potential perils of impact bonds aligning interests among key actors. The \nauthors caution against the \u2018illusion that all these interests can be easily aligned without displacing \nor neglecting some of them\u2019, and the misguided notion that it is possible to merge these interests \ninto a complete contract.", " \u2022  Firstly, the interests of the service provider and investor overlap. Both are incentivised to \nreach  the  outcome  targets,  because  they  bear  the  reputational  and  financial  risk, \nrespectively. Hence, service providers may focus on those easier to reach, or on short-\nterm activities to trigger payments. Both actors may be incentivised to design easier to \nachieve outcome targets. The outcome funder is a key counterbalance to these interests, \nand ensure that pressure for success thresholds are ambitious and repayment conditions \nare at least at the risk-return rate of funding alternative (i.e. at market level). The outcome \nfunder  plays  a  crucial  role  in  protecting  the  interests  of  beneficiaries.  This  may  be \nproblematic  in  cases  where  outcome  funders  cede  control  over  all  aspects,  including \ngrantee selection and evaluation of outcomes to private investors, for example, in the case \nof the Peterborough SIB (Warner 2013).", " \u2022  Secondly, all actors may collude in decisions on funding conditions to the disadvantage of \ntaxpayers.  In  order  to  assess  the  cost  efficiency  of  impact  bonds,  it  is  important  that \noutcome funders are neutral and choose a funding instrument only on the basis of value \nfor money and contribution to desired outcomes. If outcome funders have strategic and \npolitical interests in investing in impact bonds, this distorts the balance of interests, and \nmay mean that the impact bond is used even in cases where it does not provide greater \nvalue  when  compared  to  other  funding  mechanisms,  or  where  impact  bonds  are \nsubsidised without providing greater value for the taxpayer. This may be the case because \nimpact bonds have bipartisan appeal, and can be supported by both those supportive of \nincreased welfare spending and those which are interested increasing the marketisation \nof service provision.", " To  date,  the  SIB  market  has  been  heavily  subsidised45.  In  fact,  no  SIBs  have  been  launched \nwithout subsidy. Also, the UK SIBs funded by the UK central government are primarily focused \non activities that the government is not funding use other models, so in these cases, SIBs are in \ncompetition with nothing. However, it is unclear what mechanism and criteria have been used to 45 Subsidies can be channelled through development of the model or of individual SIBs, de-risking of investments (for \nexample by \u2018guaranteeing\u2019 certain values) and subsidies for outcomes.", " A187 judge whether SIBs work better than other funding models. This may negatively impact on the \nvalue for money provided by the impact bond and the associated subsidies.4647 M.1.7 Impact bonds are difficult and costly to design and implement Critics  of  impact  bonds  point  out  that  impact  bonds  are  difficult  and  costly  to  design  and \nimplement, and do not generate benefits that justify the additional costs. For example, Tse and \nWarner (2018) note that SIBs that only pay for their current costs and do not involve consideration \nfor sustainability are not worth the transaction cost or interest rate. Tan et al (2015) find that many \nof the savings in SIB schemes are hypothetical rather than real cost reductions. Calculations of \nsavings are challenging and hard to attribute, in the absence of experimental impact evaluations.", " Secondly, the popularity of impact bonds have been attributed to their \u2018chameleonic\u2019 state, which \ncan be many things to many people. Some of the claims are paradoxical, and may affect the value \nfor money of impact bonds (Maier et al 2017).", " \u2022  The first claim is that impact bonds allow for evidence-based flexibility. Maier et al (2017) \nnote that there are three main arguments used to address this paradox. Firstly, a more \nflexible understanding of \u2018evidence-based\u2019 is used; secondly, flexibility is used to regard \nthe  financial  model  but  not  the  intervention  itself;  thirdly,  the  flexibility  rests  with  the \nintermediary,  but  the  service  provider  has  limited  flexibility  and  implements  a  clearly \ndefined evidence-based intervention. The extent to which these three models of operation \naffect the hypothesised effects of an impact bond will affect the value for money of this \nfunding mechanism.", " \u2022  The  second  claim  and  paradox  is  cost-effective  risk  transfer.  Impact  bonds  have  high \ntransaction costs and risk premiums. Risk transfer comes at a cost, and total costs for the \noutcome  funder  will  only  be  reduced  if  they  are  able  to  strike  preferential  deals,  as \ninvestors  require  compensation  for  their  taking  on  of  this  risk.  A  conceptual  paper  by \nGiacomantonio  (2017)  builds  a  rational  choice  framework  and  argues  that  SIBs  are \nunlikely to be both rational choices on the part of governments and attractive to investors \ninterested in financial returns. There are five arguments used to argue this: Introducing philanthropic funding; o  Presenting governments and service providers as more risk-averse than investors;  \no \no  Pointing out additional positive effects of impact bonds;  \no  Arguing that the relatively high transaction costs of impact bonds are transitory \no  Arguing that impact bonds increase the overall amount of funding going to good \ncauses.  However,  impact  bonds  do  not  represent  new  funding,  and  in  reality \ndisplaces  funding,  unless  prevention/remedial  cost  savings  pay  for  the  impact \nbond (Department of Budget and Finance 2013).", " 46 Social Impact Bonds: An overview of the global market for commissioners and policymakers \n47 To assess the VfM of these subsidies and funds, one would need to assess the extent to which these subsidies and \nfunds are i. encouraging stakeholders to develop new approaches to delivery; ii. leading outcome funders, providers or \nintermediaries to choose the impact bond funding mechanism rather than an alternative; iii. Causing investors to invest \nin impact bonds when they otherwise would not have done.", " A188 M.1.8  What is the evidence base, and what does it say about the DIB effect and the cost effectiveness of impact bonds?", " This sub-section sets out the evidence base on DIBs. As very few DIBs have been launched, the \nliterature  review  also  draws  upon  the  evidence  base  related  to  SIBs  and  PbR,  though  the \nevidence base on the impact elements of PbR is still very thin (Clist 2017). It must be noted that \nthe SIB context will be different from the DIB context, and the emerging evidence will have to be \ntested for  its  applicability  to the  DIB  setting.  Furthermore,  while a number  of the  hypothesised \neffects of DIBs contracts overlap with those of PbR contracts, there remain some differences. For \nexample, DIBs are hypothesised to address some of the limitations of PbR such as access to \ncapital  as  well  as  risk  aversion  (as  investors  are  potentially  less  risk  averse  than  service \nproviders).", " We set out the evidence against the framework introduced in Figure 14. Evidence on DIBs, SIBs \nand PbR seem to fall naturally into two categories: 1.  Reviews  to  synthesise  learning  across  multiple  SIBs,  generally  consultative  exercises, where relevant stakeholders have been invited to feed in their opinions; and 2.  Evaluations  seeking  to  identify  the  impact  of  the  intervention  and/or  the  effect  of  the payment instrument (Drew and Clist, 2015).", " Generally, the consultative reviews provide stronger evidence for the inputs and process, while \nthe (limited) evaluations assessing the DIB effect provide evidence for the impact element.", " There appears to be more evidence around the process rather than impact parts of the framework. \nThis  may  be  due  to  the  fact  that  there  have  been  more  evaluations  and  reviews  based  on \ninterviews  and  online  surveys  of  existing  impact  bonds  and  PbR  contracts  (for  example \nGustafsson-Wright et al (2015) and CBO evaluations48. Where there are evaluations on specific \nimpact bonds or PbR programmes, only a minority focus specifically on the effect of the funding \ninstrument.", " b.  Input M.2.1  Donors, investors and other stakeholders provide support needed to design, develop and introduce programmes using DIBs Investors better at picking investments: Limited evidence. As the impact bond market is still \nnascent, impact bonds have tended to be designed with heavy involvement from all stakeholders. \nThere is not yet a strong market for impact bonds.", " Market discipline to the design of impact bonds: In terms of mobilisation of private funding, \nSIBs have generally generated reasonable returns (Social Finance 2018). However, it is unclear \nwhether reasonable returns are the result of strong design of programmes, or targets linked to \nresults set too low. As a market grows around impact bonds, there should be better information \non the assessment of the commensurability of risks and returns.", " Collaboration: There is some indication that stakeholders are interested in collaboration. In a \nconsultation with investors in Canada (Deloitte, undated), the vast majority of respondents were 48 For further information see: https://www.biglotteryfund.org.uk/research/social-investment/publications A189 interested in the idea of an impact bond, and wanted to co-invest as part of a consortium in order \nto share capital commitments, due diligence, governance, and learning as well as to allow for risk \nreduction.", " Furthermore, Gustafsson-Wright et al\u2019s (2015) review found that there were some good examples \nof  collaboration  in  SIBs.  For  example,  there  are  good  examples  in  the  UK  where  SIBs  have \nbrought very different partners together as funders all interested in achieving similar outcomes \n(such as the local authority, schools and philanthropists as outcome funders in the West London \nZone SIB, or different government departments in the Youth Engagement Fund).", " M.2.1  Donors, investors and other stakeholders provide the capital needed to deliver programmes which provide social value Mobilising  private  funding:    Gustafsson-Wright  et  al  (2015:  37)  found  that  additional  capital \nfrom traditional private actors has been limited, as this would require \u2018a different analytic mindset \nand  acceptance  of  credit  approval\u2019.  However,  it  has  led  to  an  increase  in  social  financing  by \nphilanthropic actors.", " Scale: Gustafsson-et al (2015) found from a review of SIBs that scale was achieved in certain \ntarget populations, but not as a whole.", " Risk transfer: A key learning has been that while the funder\u2019s risk has been reduced to some \ndegree as payments are only made if it works, the funder is subject to new risks through increased \nexposure, risk of demonstrated failure or paying too much. (Social Finance, 2018; Gustafsson-\nWright et al, 2015). Also, it is not clear how risky the SIBs are, and as such, the level of risks \ntransferred. Four types of new risks arising from use of the SIBs are cited: execution risk, or the \ndelivery of interventions in a new context; measurement risk related to how good the outcome \nmeasure is relative to the ultimate goal; basis risk, or that is, additional costs of using the SIB not \noffset by savings; and unintended consequences (Mulgan 2010; Gustafsson-Wright et al, 2015).", " It is important to note that the extent to which funds are additional depends on perspective. While \nthere is no net change in available funding, it can be seen as an additional source of funding, to \nthe extent that it enables commissioning which would not have happened, or the extent to which \nit  facilitates  additional  innovation.  Whether  funds  represent  \u2018additionality\u2019  depends  on  the \nperspective of stakeholders.", " M.2.2  Process Outcome funders focus on results and not inputs Hands-off  nature  of  outcome  funders:  The  evidence  is  mixed  in  this  area.  Some  outcome \nfunders  cited  the  motivation  for  using  impact  bonds  as  the  possibility  of  circumventing  rigid \ngovernment  budget  silos  and  procurement  processes  and  the  ability  to  overcome  politics \n(Gustafsson-Wright  et  al  2015).  Other  stakeholders  felt  that  thinking  about  procurement  and \nprovision of social services had changed, with service providers now being selected on the ability \nto deliver outcomes. The London Rough Sleepers SIB is a good example where service providers \nfelt outcome funders had stepped back and focused on results over inputs.", " However,  Boggild-Jones  and  Gustafsson-Wright (2017) found that taking  \u2018a  step  back\u2019  can  be \nchallenging for outcome funders, especially if they have expertise in an area. A shift in culture A190 may be needed inside these organisations. Similarly, in DFID PbR systems, there was an ongoing \ntension  between  the  desired  flexibility/adaptability  and  compliance  with  procurement  policy. \nHolden and Patch (2017) found that in the Girls Education Challenge Fund, there was very little \nadaptation  in  programmes,  and  service  providers  cited  the  time-consuming  nature  of  making \namendments  to  milestones,  outputs  and  budgets.  A  tension  may  be  due  to  the  fact  that  PbR \nprojects are expected to comply with standard procedures for grant funding while at the same \ntime be more innovative than traditional grant funded projects (Clist 2017).", " DIBs create incentives for service providers to focus on producing desired results Result Focus: This seems to be an area well supported by the evidence so far.", " \u2022  A  KPMG  evaluation  of  the  New  South  Wales  Social  Benefit  Bonds  in  2014  found  that \nincreased attention on and understanding of programmes outcomes and how to measure \nthem produced positive outcomes for NGOs and government.", " \u2022  The CBO SIB outcome fund evaluation found that most stakeholders are of the view that this has been the case.", " \u2022  SIBs have been cited as changing delivery culture (Social Finance 2018) \n\u2022 In the DFID funded Zambian HRITF RBF, one health worker noted that the \u2018attitude has \nreally changed, people used to come late for work, now everyone is on time. We were \ndoing shortcuts, but not we are doing full procedures.\u2019 (Evans, 2016) \u2022  Holden and Patch (2017) found that in the Girls Education Challenge PbR programmes, overall focus on learning outcomes and rigorous measurement was very positive.", " As set out in the alignment principle of PbR, PbR may be only beneficial when incentives were \nnot initially aligned: \u2022  Holden and Patch (2017) found that GEC staff were already very motivated to achieve \noutcomes before the introduction of the payment incentive. Similarly, Rwanda was already \nfocused  on  increasing  enrolment  before  the  introduction  of  the  RBA  (Upper  Quartile, \n2015).", " Also, it may be not the pecuniary interest, but the very attention on the outcome measured which \nleads to increased outcome focus.", " \u2022  Evans (2016) argues that it was not pecuniary interest in Zambia, but being recognised in \na context where workers feel undervalued which led to a positive effect. Similarly, reward \nfor  performance  was  cited  as  a  positive  motivator  in  Ethiopia  and  Afghanistan  (DFID \n2016).", " There are some exceptions to the positive incentivisation of service providers, and the reasons \nfor this have been explored in evaluations.", " One hypothesis is that measures can fail to incentivise recipients if they are too complex relative \nto the incentive size. This seems to be the case for certain Health Results Innovation Trust Fund \n(HRITF) PbR agreements (Kandpal 2016), NGOs (Holden and Patch, 2017) and governments \n(Cambridge Education, 2015 and Upper Quartile, 2015). Measures can also fail to incentivise if \nthe incentives are too low, agreements too short or outside of the recipient\u2019s control (such that \nthe recipient has no incentive to try). Clist (2017) notes that a common theme for projects with A191 poor  performance  is  incentives  which  are  insufficient,  in  comparison  to  the  programme\u2019s \ncomplexity and duration, and perverse incentives to prioritise the short term over the long term.", " This seems to be supported by the success stories as well. Where PbR worked best and provided \nthe  strongest  evidence  of  success  was  where  incentives  were  also  largest,  including  HRITF\u2019s \nprogramme in the Misiones province (where incentives were largest); Employment Fund in Nepal \nwhere  organisations  responded  to  the  incentive  to  increase  employment,  not  just  training;  the \nUganda RBF health project, where incentivised quality of care increased.", " More incentivised to focus on target populations:   Evidence from the Employment Fund in \nNepal  (Chakravarty  et  al,  2016)  suggested that  specific targets for the  hard  to reach, such  as \ngreater payments for disadvantaged groups discouraged cherry picking and more focus on the \nhard to reach populations.", " There is greater innovation and flexibility in approaches to delivering services M.2.3 Innovation and flexibility There are two levels of innovation we should consider - innovation in design of the programme, \nand innovation in delivery (e.g. performance management / course correction / adaptation).", " In terms of innovation in design, Edmiston and Nicholls (2017) found that a substantial number of \nthose interviews with experience of SIBs felt that the use of SIBs did support the development of \nexperimental and innovative service interventions, which was made possible by the fact that social \ninvestors were taking on the social risk, in exchange for potential financial returns. On the other \nhand, Gustafsson\u2013Wright et al (2015) found that in the landscape of SIBs, none of the 38 were \ninnovative in the traditional sense, but a number were innovative in the sense that they trialled \ninterventions in new locations or contexts. This is likely due to the risk appetite of investors. For \nexample, an evaluation undertaken by KPMG 2014 found that the use of SIBs was considered to \nhave been an exercise in innovation in a number of areas including financing, contracting and \nmeasurement, but seemed to be a contradiction between service innovation and developing a \nbond with a sound evidence base.", " The evidence on the extent to which PbR and impact bonds have driven adaptation is mixed. In \nthe  UK  there  are  multiple  examples  where  the  programme  has  adapted  in  order  to  ensure \noutcomes are maximised. This was the case in the Peterborough SIB, Ways to Wellness SIB and \nYouth  Engagement  Fund.  However,  Gustafsson-Wright  et  al  (2015)  found  that  few  deals  had \nactually reported using data to make course adjustments along the way. Similarly, Holden and \nPatch  (2017:7)  undertook  a  review  of  the  Girls  Education  Challenge  which  was  partially  PbR \nfunded and found that \u2018a consistent view emerging from the study is that PbR did not incentivise \ninnovation  and  adaptation  during  delivery,  and  more  likely  had  the  opposite  effect,  leading \norganisations to be more risk-averse\u2019.", " Capacity for the service provider to adapt and innovate during delivery is likely to be impacted by \nthe amount of autonomy granted to them. For example, Honig (2014) found that autonomy was \nnot linked to PbR contracts in World Bank projects. Course correction may also require longer \ntimeframes for feedback loops to materialise. Upper quartile (2015) found that in the Big Results \nNow! Education project in Tanzania, the service provider felt there was a mismatch between the \ntimeframe agreed and the necessary timeframe to really deliver change.", " A192 Programme implementation improves and is more effective The  Health  Trailblazers  review  (Tan  et  al,  2015)  noted  the  benefits  of  SIBs  instilling  \u2018market \ndiscipline\u201d in the VCSE49 sector, covering elements of both better business planning and improved \ncontact management. Gustafsson-Wright et al (2015) also found that some stakeholders noted \nthat the broader M&E culture had improved, leading to spillover to other projects. One caveat is \nthat  this  seems  to  depend  very  much  on  the  actors,  and  the  extent  to which  they  are  already \nwanting to improve.", " In terms of the hypothesised benefits of private sector input in improving delivery, Gustafsson-\nWright  et  al  (2015)  found  that  it  depends  on  how  deals  are  structured  (whether  merged, \nintermediated  or  direct). It  also  depends  on  the fidelity  to the  model  in terms  of  who  plays  the \nperformance management role (whether it was investor, intermediary, outcome funder, or none \nof the above), and the role of the intermediary in supporting course corrections.", " M.2.4 Impact Expected outcomes are produced more effectively / efficiently than with other approaches More effective outcomes: The evidence in this area has been the weakest, due to the limited \nnumber of evaluations seeking to identify the instrument effect and the challenge of establishing \ncomparative baselines.", " An independent review of four SIBs by Daniel Edmiston and Alex Nicholls (2018) argued that, on \ncurrent  evidence,  a  SIB  model  was  no  more  effective  than  other  forms  of  outcome  based \ncommissioning  and  PbR.  While  interviewees  noted  that  private  sector  investor  involvement  in \nSIBs did lead to greater degrees of oversight and accountability, it is unclear that this facilitated \nservice  innovation  that  would  not  otherwise  have  been  present  through  other  funding  models \n(Edmiston and Nicholls 2017). In terms of PbR, the evidence is mixed.", " Some reviews have found that RBF can improve the quality of services (Gorter 2013) and that \ncontracting  out  health  services  can  increase  access  and  use  (Perrin  2013).  Evaluation  of  the \nUganda RBF project in health (Valadez et al, 2015) compared a RBF project to an input-based \nalternative. While quality of care was a concern across the board, RBFs region achieved 50% of \navailable  performance  points  compared  with  traditionally  financed  control  regions  which  only \nachieved 20%. However, more evidence is needed to understand the causal mechanisms, and \nhow RBF led to the better performance observed.", " However,  Perrin\u2019s  (2013:  5)  review  of  the  PbR  evidence  base  concluded  that  \u2018there  is  limited \nevidence  that  PbR  approaches  offer  value-added  vis-\u00e0-vis  other  modalities\u2019.  A  number  of \nevaluations50 find PbR has no significant effect. Some hypotheses for why this may be the case \nare  that  the  incentives  may  have  been  too  low-powered,  or  because  the  recipient  had  limited \nability  to  affect  the  outcome  (Afghanistan  HRITF  project  discussed  in  Kandpal,  2016).  Seven \nevaluations of the HRITF which attempt to evaluate the PbR mechanism and not just the PbR \nprojects find that while outcome indicators have shown steady improvements, impact evaluations \nhave shown mixed results (DFID, 2016h).", " 49 Voluntary, community and social enterprise (VCSE) organisations and social investors \n50 Reproductive health in Pakistan (Witter et al, 2016), RBA in Ethiopia (Cambridge Education, 2015) and Rwanda \n(Upper Quartile, 2015), Sierra Leone\u2019s Budget support program.", " A193 Efficiency: It was thought that costs would decline as transactions increased in size, but in reality \nsize has been limited by the counterparty. It is argued that single transactions cannot be efficient, \nbut what is needed is a market approach (Social Finance, 2018). Evidence that calls into question \nthe efficiency argument of impact bonds include: \u2022  While there is optimism that verification should be cheaper than alterative systems and \nlead to benefits of better information, generally verification is felt to be a substantial cost \nwith few redeeming benefits (Clist 2017).", " \u2022  Early evidence highlights that RBF mechanisms  are not always easy to implement and \nhave been associated with implementation failures that result in less effective programs. \nIt is not clear whether this is a result of use of PbR, or because PbR is still in an early \nstage (Clist 2017).", " \u2022  While  PbR  was  hypothesised  to  be  administratively  easy  to  manage  and  to  allow  for \nreduction in the pressure associated with contract management, in reality, management \nprojects have been more complex and required more time than expected (Clist 2017).", " Cashable  savings:  A  review  delivered  by  Azemati  et  al  (2013)  found  that,  based  on  the  SIB \nexperience in the US, there was little evidence that interventions truly pay for themselves. This \ncould be related to the fact that PbR projects seem to generally be subject to expectations of both \nbeing innovative and following? standard procedures for traditional aid modalities. (Clist 2017) Impact Bonds Market which increases competition and drives down costs: There is limited \nevidence on this point, as the impact bonds market is still nascent.", " Unintended consequences: In the SIB sphere, the service provider survey undertaken for the \nCBO evaluation 2017 update report suggests that the outcomes-focused culture can also have \nadverse effects. Service providers reported that the second main negative impact of SIBs was \nthat the increased pressure to achieve outcomes affects staff morale and leads to higher levels \nof staff turnover. Furthermore, in the Zimbabwe, HRITF staff reported more likely to suffer burnout \n(Kandpal 2016).", " In addition, Ecorys\u2019s evaluations have seen some evidence of the \u2018perverse incentives\u2019. These \nare  often  associated  with  outcomes  based  commissioning,  primarily  \u2018cherry  picking\u2019  (where \nservices target beneficiaries easiest to reach/turn around as opposed to the hardest to reach) and \n\u2018parking\u2019 (where beneficiaries are left on programmes but not supported, either because it is clear \nthey will not achieve any outcomes or because the provider gets paid for having beneficiaries on \nthe programme).", " In the PbR sphere, literature reviews have found that RBF health programmes tend to focus on \neasier to measure outcomes (such as number of vaccinations). Outcomes such as health systems \nstrengthening  tend  to  be  harder  to  measure  (Grittner  2013;  NKCHS  2008).  Holden  and  Patch \n(2017, p. 36) noted that some programme staff in the field felt there were perverse incentives from \nPbR, to prioritise short term over long term, and sometimes felt pressure from headquarter staff. \nIn a WASH Results project, some suppliers neglected the most important but incentivised longer-\nterm elements (DFID, 2016b).", " Furthermore, there  is evidence  that  the quality  of the  measure  reduces  once  it  is  incentivised. \nSandefur and Glassman (2015) found that in the GAVI programme, once reliable self-reported A194 administrative data became unreliable once incentivised. This was assessed through triangulation \nwith the demographic health scores. Furthermore, the review found that GAVI had little effect on \nnon-performing  countries,  and  had  no  positive  effect  on  immunisation  results,  and  hence  was \nessentially disbursing too much money to already well-performing countries.", " On the  other  hand,  Clist\u2019s  (2017)  review  of  DFID  PbR  evaluations  to assess cherry  picking  or \ngaming, find that in a vast majority of cases, there was no evidence of any problems. HRITF\u2019s \nZimbabwe (Kandpal 2016) identified that none of the non-incentivised services showed a decline \nin  the  number  of  cases  treated,  as  would  be  expected  if  the  incentives  had  affected  these \nservices.", " Sustainability  of  services:  It  was  theorised  that  demonstrated  impact  of  SIBs  would  lead  to \nscaling of models, but no UK SIB has been continued at the end of its contract (Social Finance \n2018). The strongest argument for sustainability seems to be the use of multi-year contracting, \nwhich could provide more continuous and reliable service. However, there is little evidence in this \narea at the moment (Gustafsson et al 2015).", " Transparency and accountability: There is limited evidence to date that beneficiaries and other \nstakeholders have used the verified outcome data in order to demand better services and drive \naccountability. However, the extent to which verified outcome data has been shared and validated \nwith beneficiaries will be important to explore.", " M.2.5  What are the key recommendations around improvements to designing and agreeing DIBs to increase the model\u2019s benefits and reduce the associated \ntransaction costs?", " This sub-section first explores the challenges of designing impact bonds, before setting out the \nkey recommendations raised to improve the designing and agreeing of DIBs, recommendations \non developing outcome metrics and a pricing structure and finally recommendations targeted to \nspecific stakeholders.", " M.2.6  Challenges The experience to date has raised many challenges with launching and delivering DIBs. A recent \nsurvey conducted by the Impact Bonds Working Group of its members noted the following main \nchallenges faced by teams designing impact bonds51: 51 https://www.dropbox.com/s/ccfixil4cgtgq79/Mid-term%20Progress%20Report_June8%272018.pdf?dl=0 A195 Table M.4: Challenges of designing impact bonds Challenge Examples Institutional barriers Legal or procurement Nature of deals Deals are too time-consuming Budgeting Unease with investor earning a return Availability of human resources Deals are too expensive Deals are too small No good deals have been presented Informational and technological barriers Difficulties accessing data on target population Inability to measure desired outcomes Impact bond instrument Lack of evidence of effectiveness of instrument Lack of awareness/understanding of instrument Lack of co-funders / outcome funders / co-investors c.  Recommendations In this section we include some of the key recommendations raised to improve the designing and \nagreeing of DIBs. We firstly provide a broad set of recommendations, before including specific \nrecommendations for different actors, and finally provide recommendations on scaling DIBs.", " M.3.1  Recommendations for implementing DIBs 1.  Identifying  appropriate  service  providers  with  implementation  capacity  is  critical.  The \nservice  provider  must  have  the  capacity  to  carry  out  the  impact  bond  activities  and  be \nopen to change (Gustafsson-Wright et al., 2017; Oroxom et al., 2018) 2.  Engaging investors since the beginning, to ensure they are comfortable with the metrics \nand risk-return profile of the investment. However, there are pros and cons to the order in \nwhich  investors  and  outcome  funders  are  approached  (Gustafsson-Wright  et  al.,  2017; \nOroxom et al., 2018) 3.  Not  underestimating  the  resources  needed  to  launch  an  impact  bond  (Oroxom  et  al., \n2018).  It  is  complex,  challenging  and  expensive  to  structure;  it  can  require  intensive \npreparation  time  and  transaction  costs,  as  well  as  good  collaboration  between \nstakeholders; and contracting an impact bond can be constrained by legal issues. While \ndonors and outcome funders are building the architecture to support the operations, work-\naround solutions in the interim can complicate things (Palladium and USAID, 2016).", " 4.  Clarifying everyone\u2019s priorities and roles (Oroxom et al., 2018).", " A196 5.  Surveying  the  investor  market  before announcing  the  bond  and  strategically  timing  the announcement of the bond (Oroxom et al., 2018).", " 6.  Convincing organisations to pivot toward financing DIBs. More work needs to be done in \nthis sense, as champions are critical within the impact bond space (Oroxom et al., 2018).", " 7.  Some of the data needed to develop new DIB proposals are either not available or of poor \nquality. For example, figures on guarantees or interest rates may be difficult to find, and \nsometimes only accessible to intermediary organisations, which have a special financial \nlicense. (Oroxom et al., 2018). Furthermore, due to lack of historical data and precedent \ntransactions in pricing, negotiation is required (CGDev, 2013).", " 8.  Requiring funders and providers to embrace a new way of doing business (Palladium and 9.  Structuring  contracts  in  a  way  that  allows  them  to  respond  to  unforeseen  changes US Aid, 2016).", " (Gustafsson-Wright et al., 2017) 10. The  impact  bond  market  is  not  yet  well  developed.  Impact  bonds  are  currently  illiquid. \nDifferent  investors  with  different  levels  of  social/commercial  investing  motivations  and \ndifferent risk appetites will seek different risk profiles or returns. Setting up a market or \npool of outcome funders can increase the options in terms of level of risk transfer to suit \ndifferent stakeholders (CGDev, 2013; Gustafsson-Wright et al., 2015).", " M.3.2  Recommendations for scaling DIBs As has been referenced in this review, there is a view that DIBs need to operate on a larger scale \nin order for them to be reduce relative transaction costs and be efficient. For DIBs to reach scale, \nCGDev (2013) has opined that a mature market is needed, which includes 1) a robust supply of \ninvestors;  2)  confident  demand  from  outcome  funders,  and  3)  market  infrastructure,  which \nfacilitates investors and outcome funders working together.", " Potential approaches which could bring together funding from multiple actors and create scale \ninclude outcomes funds. Outcomes funds would finance multiple outcomes-based contracts on \nthe same areas. Outcomes rate cards would allow the outcome funder to set prices for certain \noutcomes,  and  then  contract  with  service  providers  to  achieve  this.  (Gustafsson-Wright  et  al., \n2017) One potential limitation for an outcome fund, is the difficulty of setting incentives so that a \nbroad spectrum of actors is incentivised (Clist 2017).", " CGDev (2013) recommended that to stimulate a market for DIBs: 1.  Donors  should  establish  a  DIB  outcomes  Fund  and  investors  should  establish  DIB Investment Funds.", " 2.  DIB parties will have to accept the high transaction costs of early DIBs, and foundations should consider subsiding these costs.", " 3.  DIB parties should invest in learning about this new approach, and a DIB community of practice set up to share and accelerate learning.", " 4.  DIBs  should  be  open  by  design,  and  donors  and foundations  to  lead on establishing  a research data protocol.", " A197 Gustafsson-Wright et al.\u2019s (2017) recommendations largely echo these ones, with four additional \nrecommendations to grow and develop the impact bond sector: 1.  Expand the evidence base, so that organisations with the capacity to deliver results can be selected.", " 2.  Build capacity of service providers.", " 3.  Educate potential outcome funders and investors.", " 4.  Support legislation.", " The impact working group recently undertook a survey of its members as to the main barriers to \nscale, and potential of some of these proposed solutions. Those rated with the most potential to \naddress a number of barriers included: \u2022 \u2022 \u2022 \u2022 In  terms  of  paying  for  outcomes  at  scale:  single  and  multi-payer  outcome  fund, \ncommissioning platforms and co-funding facility In terms of stimulating outcomes based investment: Single Impact Bond investment(s) In terms of building impact bond market capacity: building government and intermediary \ncapacity In terms of data: codified knowledge, standardised contracts and processes and impact \nbond centre of excellence.", " d.  What approaches have been used to evaluate impact bonds? What are the main challenges and solutions?", " In  some  of  the  DIB  literature,  \u2018evaluation\u2019  has  been  used  when  discussing  verification  of \noutcomes. However, here we focus primarily on process or impact evaluation, which goes beyond \nthe assessment of the outcome measures.", " This section first analyses the strengths and weaknesses of existing evaluation approaches and \nevidence.  The  section  then  moves  to  approaches  used  to  assessing  VfM  and  approaches  to \nevaluation before concluding with how the evaluation will use a framework to synthesise evidence.", " Strengths and weaknesses of existing evaluation approaches and evidence The table below, excerpted from Clist and Drew (2015:27), sets out the strengths and weaknesses \nof existing evidence and evaluation approaches and methods related to impact bonds.", " A198 Figure  M.2:  Strengths  and weaknesses  of  existing  evidence  and  evaluation  approaches \nand methods related to SIBs and DIBs (Drew and Clist 2015:27) A199 M.4.1  Assessing VfM In terms  of  VfM,  Clist\u2019s  (2017)  review  of  PbR  projects  and  VfM  assessments found that many \nevaluations dealt with entire projects, and hence did not undertake PbR specific VfM calculations. \nPerrin\u2019s (2013) review of evaluations of PbR also noted that PbR evaluations could benefit from \nan increased focus on impact and value for money; there has been limited attention to the cost \neffectiveness  of  PbR  approaches,  in  comparison  with  other  approaches.  As  there  was  no \nconsideration of the added value of the PbR element, the correlation/causality link is unclear. In \nsome  examples,  it  was unclear  whether  PbR  is  rewarding  successful  programmes  or  creating \nthem. It is important that VfM assessment of PbR/impact bond funded projects aims to understand \nthe added value of the funding mechanism, and not to solely rely on outcome measures (Clist, \n2017).", " M.4.2  Approaches to evaluation While  experimental  approaches  will  be  valuable  for  generating  comparisons  between \ninterventions  funded  by  DIBs  versus  other  funding  mechanisms,  there  would  need  to  be  a \nreasonable  number  of  groups  or  clusters  to  generate  power.  In  reality,  this  is  unlikely  to  be \nfeasible. Quasi-experimental methods can be used, either by matching clusters or by allocating \nclusters based on numerical criteria. Finally, when using non-experimental approaches, there can \nbe problems with using a historical baseline. However, this can be combined with using theory- based methods of evaluation, by gaining a deep understanding of how an intervention is expected \nto  produce  change,  and  then  collecting  data  to  support  or  refute  that  theory  (Clist  and  Drew, \n2015). DFID\u2019s PbR Evaluation Framework (2014:6) also notes the importance of identifying the \n\u2018logical  steps  by  which  a  PbR  mechanism  will  lead  to,  or  improve,  outcomes,  in  the  particular \ncontext of the programme\u2019, and reflecting on the \u2018theory of change of PbR, as a subset of the \nbroader theory of change of the intervention\u2019 will support effective evaluation.", " M.4.3 Framework for synthesising evidence Finally,  Clist  and  Drew  (2015)  suggest  designing  evaluations  around  a  common  evaluation \nframework,  conducting  real-time  synthesis  and  undertaking  periodic  synthesis  exercises.  This \nframework has been used to frame the understanding of the hypothesised effects of impact bonds \nand the evidence generated to date. The evaluation\u2019s approach of contextualising the evaluation \nfindings in the wider DIB sector will aim to facilitate real-time synthesis of learning.", " A200 Annex N: List of Acronyms Acronym Definition AFD \nBAT \nBEH \nBPS \nBSG \nCBO \nCEA \nCIFF \nDAC \nDCMS \nDFAT \nDFID \nDIB \nEMT \nEQUALS \nESRC \nGAVI \nGDI \nGEC \nGEFA \nGSRU \nHIB \nHRTIF \nHSE \nICRC \nIDB \nIFI \nKiT \nKPI \nLLC \nLOUD \nM&E \nMEL \nMRS \nNGO \nNORAD \nOECD \nORCM \nPbR \nPHII \nPRP \nPSD \nRBA \nRBF \nRCT \nSARD \nSDC \nSECO Agence Fran\u00e7aise de D\u00e9veloppement / French Development Agency \nBritish Asian Trust  \nBusiness Engagement Hub \nBritish Psychological Society   \nBusiness Saving Groups \nCommunity Based Organisation \nCost Effective Analysis \nChildren\u2019s Investment Fund Foundation \nDevelopment Assistance Committee of the OECD \nDepartment for Culture Media and Sports (UK) \nDepartment for Foreign Affairs and Trade (Australia) \nDepartment for International Development (UK Aid) \nDevelopment Impact Bond \nEvaluation Management Team \nEvaluation Quality Assurance and Learning Services \nEconomic and Social Research Council   \nGlobal Vaccine Alliance \nGlobal Support Development Initiative \nGirls Education Challenge \nGlobal Evaluation Framework Agreement \nGovernment Social Research Unit \nHumanitarian Impact Bond \nHealth Results Innovation Trust Fund \nHealth and Safety Executive \nInternational Committee of the Red Cross \nInter-American Development Bank \nIntergovernmental Financial Institutions \nKeeping in Touch \nKey Performance Indicator \nLimited Liability Company  \nLOUD SIB Model \nMonitoring and Evaluation \nMonitoring, Evaluation and Learning \nMarket Research Society \nNon-Governmental Organisation \nNorwegian Agency for Development Cooperation \nOrganisation for Economic Cooperation and Development \nOperating Review Committee Meeting \nPayment-by-Results \nInternational Committee of the Red Cross Programme for \nPhysical Rehabilitation Programme \nPrivate Sector Department \nResult Based Aid \nResults Based Financing \nRandomised Control Trial \nSociety for All Round Development \nSwiss Agency for Development and Cooperation \nState Secretariat for Economic Affairs A201 Acronym Definition Staff Efficiency Ratio \nSocial Impact Bond \nSpecial Purpose Vehicle  \nSocial Research Association \nTheory of Change \nTerms of"]}, {"paper_id": "#17192", "title": "Evaluation Of Kindergarten Readiness In Five Child-Parent Centers: Report For 2014-15", "paragraphs": [" Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15 April 2016 Prepared for: \nIFF Pay For Success I, LLC \n333 S. Wabash Avenue, Suite 2800 \nChicago, Illinois 60604 \nAttention: Matthew J. Roth, Chief Operating Officer \nE-mail: mroth@iff.org Copy to: \nDLA Piper LLP (US) \n203 N. LaSalle Street, Suite 1900 \nChicago, Illinois 60601 \nAttention: Richard F. Klawiter, Esq. \nE-mail: Richard.klawiter@dlapiper.com Prepared by: SRI International \nErika Gaylor \nTraci Kutaka \nKate Ferguson  \nCyndi Williamson \nXin Wei \nDonna Spiker Revised June 2019 to correct an error on p. 9 \n \n \n \n \nSuggested citation: \nGaylor, E., Kutaka, T., Ferguson, K., Williamson, C., Wei, X., & Spiker, D. (2016). Evaluation of Kindergarten Readiness in Five Child-Parent Centers: Report for 2014-15. Prepared for IFF Pay for Success I, LLC. Menlo Park, CA. SRI International.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 ii Contents List of Exhibits .............................................................................................................................. iv Background ................................................................................................................................... 1 CPC Program Model ..................................................................................................................... 1 CPC Model Description ............................................................................................................. 1 Expected Outcomes from the CPC Program Model .................................................................. 4 Impact on School Readiness ................................................................................................. 4 Impact on Third-Grade Reading and Literacy ........................................................................ 4 Impact on Special Education Use .......................................................................................... 5 Chicago PFS Project (SIB-CPC Project) ................................................................................... 5 Evaluation Design ......................................................................................................................... 6 Analysis Approach ..................................................................................................................... 8 Sample Included in the Cohort 1 analysis ............................................................................. 8 Measuring Kindergarten Readiness .................................................................................... 11 Calculating Impact on Kindergarten Readiness ................................................................... 12 Results ........................................................................................................................................ 12 Discussion ................................................................................................................................... 14 References .................................................................................................................................. 17 Appendices ................................................................................................................................. 19 Appendix A: Chicago Child-Parent Center Social Impact Bond Evaluation Plan .................. A-1 Appendix B: Timing of Cohorts .............................................................................................. B-1 Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iii List of Exhibits Exhibit Exhibit 1. CPC Program Model Components ................................................................................ 3\u00a0\nExhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by  \nExclusion Criteria ........................................................................................................................ 10\u00a0\nExhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains ....... 13\u00a0\nExhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain ............... 13 Page Child-Parent Center Evaluation: Report for 2014-15                  April 2016 iv Background The Child-Parent Center (CPC) model, one of the longest-running early childhood intervention models in the United States, has produced some of the most  robust long-term outcomes for children\u2019s academic and social outcomes (Reynolds, 2000; Reynolds & Temple, 2008). Beginning in January 2012, as part of a U.S. Department of Education Investing in Innovation (i3) grant to the University of Minnesota, the city of Chicago and Chicago Public Schools (CPS) received funding to (1) increase the number of children who could attend existing CPC sites, and (2) increase the number of CPC programs by adding 16 new sites.", " The Social Impact Bond (SIB) (also referred to as Pay for Success) is a funding mechanism where private businesses support programs that are expected to have a high return on investment. Beginning in 2014-15, the IFF Pay for Success project funded additional CPC preschool slots at six CPS schools. In 2015-16, two additional sites (identified by CPS and approved by the city of Chicago) were added to the PFS project. SRI International (SRI) has been hired to conduct the evaluation of the child outcomes for this project referred to as the \u201cSIB-CPC project\u201d. The project anticipates serving four cohorts of preschool children across the eight sites over four school years\u2014 Cohort 1: 2014-15, Cohort 2: 2015-16, Cohort 3: 2016-17, and Cohort 4: 2017-18.", " This first SRI project report describes the kindergarten readiness outcomes of the first cohort of children in the SIB-CPC project. First, we briefly describe the CPC program and its expansion efforts using SIB funding, including evidence about the impacts of the CPC program model on children\u2019s school readiness and school achievement.", " Second, we describe how the SIB-CPC program is being evaluated. Third, we present the extent to which the SIB-CPC program goals have been achieved for the kindergarten readiness outcomes for Cohort 1.", " CPC Program Model CPC Model Description CPC programs seek to promote school readiness, parent involvement, and early learning that, in turn, will translate into long-term benefits with regards to academic achievement, higher graduation rates, and career success. The CPC model is unique Child-Parent Center Evaluation: Report for 2014-15                  April 2016 in that it is designed to (1) provide full- or part-time high-quality preschool experiences for three- and four-year old children, and (2) combine those educational experiences with family support services and parent engagement activities. The services for children and families offered by CPC sites are intended to be delivered in a coordinated and synergistic way across the preschool to third grade continuum.", " Indeed, the CPCs emphasize the provision of comprehensive services and parental involvement\u2014program features that are considered to be strongly associated with program quality (Reynolds & Hayakawa, 2011; Reynolds, Magnuson, & Ou, 2010). A typical CPC site includes the components listed in Exhibit 1.", " The CPC Program model components are explained more fully at https://humancapitalrc.org/midwest-cpc/cpc-resources (Human Capital Research Collaborative, 2015). For this report, the components listed in Exhibit 1 are taken from the draft evaluation plan in the SIB-CPC expansion agreement (see Chicago Child- Parent Center Social Impact Bond Evaluation Plan, December 2, 2014, in Appendix A, pp. 9-11). Note that the CPC model as conceptualized in the current SIB expansion project primarily focuses on providing high-quality preschool education, engaging parents in their child\u2019s education through a parent resource teacher (PRT) provided at the child\u2019s preschool, and promoting continuity and stability from pre-K through the primary grades. Because the focus for the SIB-CPC project is on providing preschool programming, SRI\u2019s evaluation has been designed to measure the impact of the preschool components on children\u2019s short- and long-term outcomes.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components Effective Learning Experiences \uf0b7  Offer Pre-K classes that are limited to 34 children for half-day classrooms (two \nsessions of 17 children each) and have a minimum of 2 teaching staff. Full day \nclassrooms, if available, will be limited to 20 children per session.", " \uf0b7  Provide highly qualified educational staff that will provide the classroom instruction and parent engagement activities. For example, classroom teachers are certified \nwith a bachelor\u2019s degree (or higher). Overall, program staff must adhere to the \nrequirements set forth by the CPS Talent office, in accordance with collective \nbargaining unit agreements, and state regulations. Any changes in CPS education \nand certification requirements will be complied with.", " \uf0b7  Use data to drive instruction by effectively documenting the organization and implementation of instructional practices to monitor quality and adherence to the \nProgram, which is completed by all Program staff where appropriate.", " \uf0b7  Program staff meet with parents over the course of each school year to review their child\u2019s progress and discuss parent program opportunities with the Parent \nResource Teacher (PRT).", " Aligned Curriculum \uf0b7 Implement a CPS District curriculum and formative assessment that is aligned to \nstandards, domains of learning, assessments, and learning activities.", " \uf0b7  Collaborate with the PRT and classroom teachers to ensure that opportunities to \nengage families in student learning are available, appropriate, and aligned to the \nprogram and parents\u2019 needs.", " \uf0b7  CPS and, most specifically, the Office of Early Childhood Education provides \nmeaningful professional development and ongoing coaching and feedback for \nteachers, aides, and other staff members that facilitates high-quality instructional \npractices.", " Parent Involvement and Engagement \uf0b7  Engage a PRT and School-Community Representative (SCR) to work closely with \nthe Head Teacher and Liaisons to maintain a consistently supportive parent \nprogram.", " \uf0b7  Encourage parents to sign a CPC school-home agreement at the start of the school year outlining a plan for fostering learning at home and participating in CPC \nactivities.", " \uf0b7  Offer and engage families in monthly activities. PRTs create and distribute a monthly parent involvement calendar, and conduct parent/teacher conferences \nover the year to review progress in the parent program.", " \uf0b7  Provide a resource room dedicated to parent and family activities through Kindergarten when possible.", " \uf0b7  Provide culturally responsive learning opportunities for families that provide flexibility for families\u2019 needs and schedules.", " Collaborative Leadership Team \uf0b7  Engage a Program leadership team that includes the Head Teacher, Parent Resource Teacher, and School-Community Representative.", " \uf0b7  Meet regularly, under the direction of the Principal, to discuss operations and best practices within the CPC.", " \uf0b7  Meet regularly, under the direction of the OECE Management Team, with staff from across sites to share challenges, experiences, and best practices, and make \nfrequent on-site visits to monitor quality and effectiveness to the Program.", " \uf0b7  Establish meaningful partnerships with community providers to strengthen service delivery and enlist local universities in training opportunities.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Exhibit 1. CPC Program Model Components (concluded) Continuity and Stability \uf0b7  CPC Pre-K classrooms are co-located in the same building as Kindergarten classrooms, when possible, to promote familiarity and integration for students as \nthey transition to Kindergarten.", " \uf0b7  Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from \nPre-K through the primary grades.", " \uf0b7  Provide a part-time Kindergarten aide when funding is available to support the transition into Kindergarten.  \nProfessional Development System \uf0b7  Offer ongoing professional development opportunities on current trends and needs \nin early childhood education classrooms, through the Office of Early Childhood \nEducation and the CPC leadership teams, including topics such as quality \ncurriculum and instruction, data-driven instruction, learning environment, social and \nemotional needs, and parent engagement.", " \uf0b7  Meet regularly and create professional learning communities to review ways to support their instruction in the classroom and with other teachers.", " Source: Chicago Child-Parent Center Social Impact Bond Evaluation Plan, dated December 2, 2014, \nincluded in Appendix A, pp. 9-11.", " Expected Outcomes from the CPC Program Model IMPACT ON SCHOOL READINESS Early research on CPC showed significant positive effects on children\u2019s kindergarten readiness, with 47% of children who received CPC preschool considered ready for kindergarten compared with 28% of children who did not receive any preschool (Reynolds, 1995; Reynolds, Temple, Robertson, & Mann, 2002). Examination of a more recent cohort of CPC participants indicated that they had significantly higher scores on a measure of language proficiency at the end of the program compared with children enrolled in other publicly funded preschool programs (Reynolds, 2002).", " IMPACT ON THIRD-GRADE READING AND LITERACY The Chicago Longitudinal Study (CLS) followed children over time using administrative records to examine attendance, achievement, and graduation rates in CPC participants compared with children who did not attend CPC preschool. One study found a significant positive impact on third-grade reading achievement for pre-K to third-grade participants (.53 standard deviation) compared with participants who attended CPC only for pre-K and kindergarten (Reynolds, 1994). Smaller studies of high-quality preschool interventions have found similar impacts on later school achievement compared with a no-preschool control group (e.g., Abecedarian study: Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Campbell, Raey, Pungello, Sparling, & Miller-Johnson, 2002; Perry preschool project: Belfield, Nores, Barnett, & Schweinhart, 2006).", " IMPACT ON SPECIAL EDUCATION USE The same long-term CLS study (described in the previous section) showed that extended CPC participation (defined as 4 to 6 years) resulted in reductions in the use of special education. For children 6 to 18 years, CPC participants had an average rate of special education placement of 14.4% compared with 24.6% for children in the comparison group (who did not receive CPC preschool intervention), indicating that CPC participants had a 41% lower rate of special education placement (Reynolds, Temple, & Ou, 2003). This finding is consistent with another analysis using the CLS sample that compared the average rates of special education placement over time for children who had attended a CPC preschool to those of children who attended a full- day non-CPC kindergarten classroom (special education placement rates of 12.5% versus 18.4%, respectively) (Conyers, Reynolds, & Ou, 2003). A more recent study of North Carolina\u2019s current state-funded preschool program used statewide population- level data over time (1995 to 2010) to show that third-grade special education rates were reduced by as much as 39% for children who participated in the preschool program,even after taking into account a variety of child and family risk factors, types of special education categories, and funding levels that varied by year (Muschkin, Ladd, & Dodge, 2015). Other reviews of a variety of preschool program models also report reductions in special education placement as one of the many cost savings results from participation in high-quality preschool programs like the CPC model (Karoly et al., 1998; Lynch, 2007).", " In summary, positive impacts on kindergarten readiness, third-grade reading achievement, and special education placements have been cited extensively to demonstrate the short- and long-term benefits for the individual child and savings for society that come from investing in early childhood education. These studies were used as the basis for identifying the selected outcomes in the current study and for calculating the repayments that will be made in the Chicago SIB-CPC project.", " Chicago PFS Project (SIB-CPC Project) During 2014-15, the SIB expansion of the CPC model involved funding for part-day or full-day CPC preschool at five sites. A sixth site was converted to a CPC model Child-Parent Center Evaluation: Report for 2014-15                  April 2016 beginning in January 2015. Because it did not operate for a full year, this site is not part of the evaluation for 2014-15. The five sites that participated in the evaluation for 2014-15 were already implementing the CPC program prior to the SIB-CPC expansion.1 The SIB funding expanded the capacity of these five sites to provide preschool to an additional 156 three- and four-year olds.2 The funding paid for the hiring of an additional teacher and teacher assistant at each site as well as enhanced resources and instructional materials to implement the CPC model. The CPC program typically serves both three- and four-year olds; sometimes in mixed-age classrooms. Thus, the funding provided by investors was used to provide CPC preschool and enhanced services to both three- and four-year olds.", " In the second year (2015-16) of the SIB-CPC project, two additional sites, identified by CPS and approved by the city of Chicago, were added to the six 2014-15 SIB- CPC sites. The project anticipates that four cohorts of children will be served across the eight sites, identified by the school year in which children begin preschool (cohort 1: 2014-15, cohort 2: 2015-16, cohort 3: 2016-17, cohort 4: 2017-18) (see Appendix B for grade levels of children in the four cohorts across years.) Evaluation Design SIB and PFS initiatives typically involve an independent evaluator to help determine whether the outcomes have been achieved. Because government only pays when outcomes are achieved rather than for activities, the focus of the evaluation is on measuring the outcomes of the individuals participating in the initiative.", " SRI is conducting the independent evaluation of the outcomes of the SIB-CPC expansion project for three primary child outcomes. SRI developed the evaluation methodology building on a draft design written by a team that included the Harvard Social Impact Bonds Technical Assistance Lab. The project also will include an oversight committee comprised of early education and research experts. The evaluation team has been charged with independently documenting the outcomes- based performance measures of the initiative. This kind of evaluation is not intended \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n1 Note that three sites had been providing CPC services since 2012 at the start of the i3 federal grant \nand two had been providing CPC services since 2013 when the original sites from the 1970s were \nmerged with the current site.  \n2 The sixth site opened up 6 new CPC classrooms for expansion of the CPC model to an additional 218 \nthree- and four-year olds. Again, this site is not included in the 2014-15 evaluation as the site was not \nopen for long enough to provide adequate dosage of CPC preschool.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 to test the impact of the CPC model against other preschool models; rather it is tracking the outcomes of the participating children against specific performance standards. Three performance questions are being addressed in the evaluation.", " (1)  What is the rate of kindergarten readiness in children participating in the SIB- CPC sites as defined by performance on the Teaching Strategies (TS) Gold instrument (completed by teachers in the spring of preschool before a child enters kindergarten)?", " (2)  What is the rate of third-grade literacy as defined by performance in meeting or exceeding grade-level performance on the state or district-administered third-grade assessment in reading?", " (3)  What is the rate at which students are identified with special education needs and placed in special education services (starting in kindergarten) compared with a matched-comparison group of children?", " Kindergarten readiness is being measured in the spring of preschool for CPC participants (as described below), and third-grade literacy will be measured in the spring of third grade following the administration of required state achievement tests.", " SRI will begin measuring special education placement in kindergarten and continue each year until spring 2020 (note that in spring 2020, cohort 1 will reach the fourth grade; cohort 2 will reach the third grade; cohort 3 will reach the second grade; and cohort 4 will reach the first grade).3 The evaluation of the SIB-CPC project is using two different designs to track the primary outcomes, a descriptive study for the kindergarten readiness and third-grade literacy outcomes and a quasi-experimental design for the special education outcomes (first to fourth grades). Specifically, for the kindergarten readiness and third-grade literacy outcomes, there will be no comparison group for evaluating the outcomes and calculating the subsequent repayment.  For these two primary outcomes, the outcomes will be based on the intervention group only and payments will be calculated using outcomes relative to national standards. For the kindergarten readiness and literacy outcomes, a decision was made in the planning phase that these outcomes had normative information so that children\u2019s performance on the measure could be used to identify whether they were performing at or above 3 SRI\u2019s involvement in the evaluation is currently scheduled to end in Fall 2020.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 normative trends. It was decided to use this kind of standard rather than compare performance with a comparison group of children.  In addition, the kindergarten outcome measure is not available for children with no preschool experience, given that the kindergarten readiness measure is collected during the spring of Pre-K in Chicago Public Schools.", " For special education outcomes (first to fourth grades), children are identified as receiving the intervention (defined here as attendance in a CPC preschool classroom) in the year they are in preschool and then are matched to children with similar demographic characteristics but who did not attend any type of preschool in CPS.", " This \u201cno Pre-K\u201d comparison group will be identified when the children are in kindergarten for each of the four Cohorts. Specifically, the evaluators will create a no Pre-K comparison group for each cohort of intervention children using propensity score matching processes.", " Analysis Approach SAMPLE INCLUDED IN THE COHORT 1 ANALYSIS Children were included in the intervention cohort if they attended one of the five SIB- CPC sites, were enrolled in either a full- or half-day pre-K classroom, were not identified as having a severe disability, were income-eligible (i.e., eligible to receive free or reduced-price lunch), and were at least four years old in September 2014.", " Additionally, a child needed to have attended a CPC pre-K classroom for at least 66% of the days (not consecutively) in a given school year\u2014a percentage considered a sufficient amount or \u201cdose\u201d of the intervention to affect child outcomes.", " Children with a severe disability were excluded because the project is based on the hypothesis that high-quality early childhood education will prevent children at risk for developing delays or mild disabilities from needing special education services at later ages. Early childhood education and intervention also may reduce the need for children with mild delays or speech and language impairments in preschool from needing additional special education services in kindergarten and beyond. The project does not expect to prevent children with severe disabilities or needs from receiving special education services. Children were categorized as having no disability, a mild disability, or severe disability based on a priori decisions in the planning and evaluation design phase. A severe disability could include autism, Child-Parent Center Evaluation: Report for 2014-15                  April 2016 specific learning disability, deaf-blindness, deafness, hearing impairment, orthopedic impairment, other health impairment, traumatic brain injury, visual impairment, and multiple disabilities. A mild disability could include developmental delay, speech and language impairment, specific learning disability, and accommodations or modifications for children with no other disability (mild or severe).4 Additionally, children were excluded from the intervention cohort if they were in a separate classroom for special education students.", " The cohort used to determine kindergarten readiness included children from the five sites that were already providing the CPC model to three- and four-year olds.", " Inclusion of all eligible four year olds in this group increases the sample size for the study to provide a more reliable and valid assessment of kindergarten readiness at these five sites. At the end of the year, administrative enrollment data showed that 653 three- and four-year old children were attending preschool at these five sites (267 three-year olds; 386 four-year olds). SIB expansion funding covered the costs of providing CPC preschool for 156 of these 653 children. Of note, all of the children across all classrooms received the full CPC model. That is, the experience of all four year olds enrolled in these CPCs is similar with a common curriculum, professional development, and parent engagement aligned through monthly Collaborative Leadership Training by all CPCs, including high-quality preschool and family support services and parent engagement activities. Thus, the evaluation does not distinguish between SIB funding and other CPC funding sources.", " SRI\u2019s evaluation is focused on kindergarten readiness as the first outcome and therefore focuses on examining the outcomes of children in each cohort who are at least four years old in September of their preschool year and then tracking outcomes beginning at the end of preschool, before children start kindergarten the following year.", " SRI requested a data export of all students ever enrolled as grade PK5 (the CPS designation for four-year-olds in preschool) in the five sites at any time in the 2014-15 school year. Overall, 449 PK students were ever enrolled at one of the five sites 4 In an earlier version of this report, the \u201cmild\u201d disability category was incorrectly described as including \nSPL, DD, and ED. This description has been corrected; the results of analyses have not changed. \n5 PK is the designation CPS assigns to students enrolled in 4-year-old preschool. Students in three-year-\nold preschool are designated PE, and are not included in the evaluation.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 during 2014-15.6 Across the total sample of 449 PK children attending one of the 5 sites in 2014-15, 328 or 73% met all of the eligibility criteria. The consort diagram in Exhibit 2 illustrates the exclusions from the original sample of 449 PK children ever enrolled in one of the 5 sites that resulted in the final sample of 328 children included in the analytic sample for this Cohort 1 (2014-15).", " Exhibit 2. Participating Sample of Cohort I Children Attending CPC Sites, by Exclusion Criteria 449 Children Attending CPC \nin grade PK One\u2010fifth\u00a0(20%)\u00a0did\u00a0not\u00a0attend\u00a0at\u00a0least\u00a066%\u00a0of\u00a0\ndays 3%\u00a0had\u00a0severe\u00a0disability\u00a0or\u00a0were\u00a0in\u00a0a\u00a0\nseparate\u00a0classroom\u00a0for\u00a0special\u00a0education\u00a0\nstudents Less\u00a0than\u00a01%\u00a0were\u00a0too\u00a0young\u00a0(i.e.,\u00a0under\u00a04\u00a0\nyears\u00a0old\u00a0in\u00a0September\u00a02014) A\u00a0small\u00a0percentage\u00a0(2%)\u00a0were\u00a0not\u00a0eligible\u00a0for\u00a0\nfree\u2010 or\u00a0reduced\u2010price\u00a0lunch\u00a0or\u00a0were\u00a0denied\u00a0\nbecause\u00a0of\u00a0insufficient\u00a0documentation A\u00a0small\u00a0percentage\u00a0(2%)\u00a0were\u00a0removed\u00a0for\u00a0\ntwo\u00a0or\u00a0more\u00a0of\u00a0the\u00a0above\u00a0reasons 328 Children Met Eligibility Criteria The remaining 328 children became the SIB-CPC Cohort 1 (2014-15). As seen in Exhibit 2, meeting the attendance criteria was the biggest challenge, with approximately 80% of the 449 PK children ever enrolled in the five sites attending for 66% of the days. The SIB-CPC cohort is defined as meeting the eligibility criteria above and will become the cohort to be tracked for outcomes in kindergarten and in later grades. This cohort also will be used to identify a matched-comparison group of 6 The number of children ever enrolled is different than enrollment estimates at any given point in the \nyear. As children left a site, new children were enrolled. The 449 includes all children ever enrolled \nduring the 2014-15 year. Based on enrollment in May/June 2015, CPS reported that 386 four year old \nchildren were enrolled at the five sites at the end of the year.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 and in later grades.", " children in kindergarten for comparing special education outcomes in kindergarten The 328 students in Cohort 1 had the following characteristics: \uf0b7  Half of the children were male (51%).", " \uf0b7  Two-thirds of the children (68%) were identified as Hispanic and one-third (29%) were identified as African-American. Fewer than 2% of the children were identified as Caucasian and the remaining 2% were identified as Asian or multiracial.", " \uf0b7  About one-tenth (11%) of the children attending the five sites had an identified mild developmental delay or disability or an identified 504 plan that described modifications and accommodations (e.g., an extra set of textbooks, home instruction, a tape recorder or keyboard for taking notes) that they needed to perform at the same level as their peers.", " \uf0b7  About one-third (35%) were enrolled in full-day with the remainder enrolled in half- day Pre-K classrooms.", " This final cohort included for the Year 1 analysis (n = 328) was similar to the total sample of PK children (n = 449) in regard to the following characteristics: gender, and disability. However, when we compared the 121 who did not meet the eligibility criteria to the 328 that did, we found that the children who were included (n = 328) were significantly more likely to be Hispanic and significantly more likely to speak Spanish compared with the children who were excluded (n = 121) (p < .001).", " MEASURING KINDERGARTEN READINESS Kindergarten readiness was examined using Teaching Strategies (TS) GOLD\u2122 scores from the spring before the child entered kindergarten.7 TS GOLD\u2122 is a teacher-reported measure of young children\u2019s skills across six developmental domains, including: literacy, language, mathematics, cognitive development, socio- emotional well-being, and physical health. This measure is being used because it was the only available child assessment data that CPS routinely collects and was therefore selected as the measure of kindergarten readiness by the SIB planning 7 Teaching Strategies GOLD\u2122 assessment was developed to be used as a formative assessment tool \nto monitor children\u2019s skills while attending a child care or preschool program so teachers can adjust their \ninstructional strategies depending on how children are progressing on a variety of skills and behaviors. \nTS GOLD\u2122 was not developed as a measure of kindergarten readiness.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 team.8  It is used routinely in the CPS preschool programs and there is no CPS-wide measure of kindergarten readiness that is completed about children as they are entering kindergarten in the fall of the school year. The metric for kindergarten readiness is the percentage of children who are performing \u201cat\u201d or \u201cabove\u201d national trends across at least five of these six domains.9 Put another way, a child is determined to be ready for kindergarten if he or she is rated by the teacher as demonstrating levels of skill or knowledge that are expected for a child at a particular age\u2014the reference point for such expectations come from the observed abilities of other children from a representative sample of same-aged peers in the United States.", " We categorized children as kindergarten ready on each domain by the criterion of meeting or exceeding the 50% percentile on the standard score for that domain using scores from the most recently published technical manual (Lambert, Kim, & Burts, 2014a). Then, we calculated the percentage of children who met this criterion on five of six domains.10 CALCULATING IMPACT ON KINDERGARTEN READINESS Every child who scored \u201cat\u201d or \u201cabove\u201d the national norm on at least five of the six domains in the spring of their preschool year was categorized as \u201ckindergarten ready.\u201d Results This section discusses the results for the first cohort of SIB-CPC children (Cohort 1).", " The TS GOLD\u2122 Spring 2015 data were missing for three11 of the 328 children, resulting in a final analytic sample for this outcome of 325 children (99% of the 328 children), which we used to calculate kindergarten readiness.", " 8 The methodology involved in SIB projects relies on use of available administrative data rather than \nadditional data collection to evaluate outcomes.  \n9 There are no available data on which domains of the TS GOLD\u2122 assessment to use to reliably and \nvalidly determine kindergarten readiness. The decision to define kindergarten readiness as performing at \nor above national trends on five of six domains (and not four of six) aligns with the National Research \nCouncil\u2019s definition of school readiness which includes age-level skills across multiple domains (National \nResearch Council, 2008). The threshold of 5 of 6 domains also takes into account that a child may not \nmeet a standard for all 6 domains, especially in the spring of preschool, as these skills are emerging \nduring this time period.  \n10 Teacher-reported assessments have some unknown sources of variability and the GOLD assessment \nis no different.  Research on the GOLD assessment indicates that between 17% and 25% of the \nvariance in scale scores is accounted for by unmeasured differences between classroom and teachers, \nincluding rater effects (Lambert, Kim, and Burts, 2014b). \n11 These children were missing data either because they were no longer enrolled in the spring (n = 2) or \ntheir GOLD assessment was incomplete (n = 1).", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Of those 325 children, 59% (58.77%) were considered to be ready for kindergarten, where \u201creadiness\u201d was defined as scoring at or above the 50th percentile on at least five of the following six domains: literacy, language, mathematics, cognitive development, socio-emotional well-being, and physical health. One-tenth (11%) of the 325 children did not score at or above the 50th percentile for any domain, with 3% meeting the criteria for only one domain, 7% for two domains, 11% for three domains, and 9% for four domains (see Exhibits 3 and 4). Additionally, children who attended full-day CPC preschool had higher rates of kindergarten readiness (67%) compared to children who attended half-day CPC preschool (55%).", " Exhibit 3. Percent of Cohort I Children Meeting Kindergarten Readiness Across Domains Number of domains meeting or \nexceeding the 50th percentile Percent Exhibit 4. Percent of Cohort I Children Meeting Kindergarten Readiness, by Domain Domain Percent Cognitive Language Literacy Math Physical Social-emotional 11% 3% 7% 11% 9% 10% 49% 80% 64% 72% 78% 58% 77% Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Discussion Socio-demographic risk factors\u2014the most extensively studied of which is poverty\u2014 are associated with variability in skill development, as well as differential growth in later academic achievement. Early childhood programs potentially mitigate the risks endemic to children from disadvantaged backgrounds, with studies showing that the strongest positive short- and long-term outcomes result from intensive and comprehensive programs targeting low-income children (Burger, 2010; Institute for Research on Poverty, 1997; Reynolds et al., 2010). Prior studies highlight early childhood as a critical and sensitive period for the development of brain architecture and neurochemistry (e.g., Knudsen, Heckman, Cameron, & Shonkoff, 2006) and subsequent academic and socio-emotional well-being (Shonkoff & Phillips, 2000).", " Indeed, possessing cognitive and socio-emotional skills at kindergarten entry has been linked to enhanced learning and performance down the academic pipeline (e.g., Duncan et al., 2007).", " In reporting the extent to which the CPC program has been successful at preparing children for kindergarten, comparisons may be instructive with respect to the degree our research findings agree with what we would expect from one year of preschool.", " We structure our discussion by reflecting on three guiding questions. First, do any data from TS GOLD\u2122 (our outcome measure) indicate whether the proportion of children who are kindergarten-ready in this project, is typical for the population we are studying? Second, to what extent are our findings similar to those of other CPC and CLS data? Third, to what extent are our findings similar to the ECLS-B12 or ECLS-K data for the general population and for children from low-income families?", " For the first contrast, does evidence exist that will allow us to verify the extent to which the TS GOLD\u2122 accurately measures the kindergarten readiness domains?", " Kim, Lambert, and Burts (2013) recently published data that provide empirical evidence supporting the validity for the TS GOLD\u2122 domains and learning objectives for typically developing children, as well as English-language learners and for those children identified with special needs or disabilities. In other words, this observation- based teacher rating evaluation measures the construct domains in the same way \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n12 ECLS-B and ECLS-K are two contemporary longitudinal datasets that draw from a nationally \nrepresentative sample; both collected direct assessments of children\u2019s skills at kindergarten entry (cf. \nHair, Halle, Terry-Humen, Lavelle, & Calkins, 2006; Lee, Zhai, Brooks-Gunn, Han, & Waldfogel, 2014).", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016 across various subgroups of children 3 to 5 years old. Next, Lambert, Kim, and Burts (2014b) established the external validity of the instrument by examining whether teacher ratings of child development and learning were associated with child demographic characteristics in expected directions. For example, children with identified disabilities started behind their typically developing peers and developed at a slower rate.", " More recently, Reynolds and colleagues (2014) published data in a peer-reviewed journal showing that 80.9% of children attending full-day CPC classrooms (n = 409) and 58.7% of children attending part-day CPC classrooms (n = 573) were considered kindergarten-ready when kindergarten-readiness was defined as meeting the national norm on four of the six TS GOLD\u2122 subdomains. Additionally, full-day participants demonstrated higher average levels of skill mastery in the subdomains of language, mathematics, socio-emotional development, and physical health (but not for literacy and cognitive development). Reynolds and colleagues (2014) report a higher proportion of children who are kindergarten ready, but use a less stringent standard for \u201creadiness,\u201d i.e., a threshold of four compared with five; five was the standard for the current evaluation. If we had used that standard of 4 of 6 domains, an additional 9% would meet that kindergarten readiness criteria, for a total of 68% (Exhibit 3). The independent evaluator decided prior to the analysis to use the more stringent standard of 5 of 6 domains to represent kindergarten readiness.", " The CPC model, integrated into the CPS system since its inception in 1967, has been systematically evaluated for its impact on child and family outcomes. A notable by- product of the CPC program\u2019s efforts is the CLS, which has supported researchers\u2019 efforts to develop a deeper understanding of the \u201cactive\u201d ingredients of early dual- generation interventions and early childhood interventions more generally. Using data from the CLS, Reynolds (1995) suggests that children who attended any form of preschool (e.g., full- versus half-day; a 1-year versus a 2-year program) outperformed those children who did not attend preschool in regard to measures of cognitive readiness at kindergarten entry. Specifically, analysis of the one of the original CPC cohorts (i.e., children attending kindergarten in the 1985-86 school year) showed that 44% of children who attended a CPC for 1 year were considered ready for kindergarten, compared with 28% for children who had no preschool (Arthur Reynolds, personal communication, February 25, 2015). These differences in Child-Parent Center Evaluation: Report for 2014-15                  April 2016 2003).", " achievement remained significant until third grade, reappeared in fourth grade, increasing in magnitude until children exited the study in the sixth grade. Additionally, preschool participants also had consistently lower cumulative rates of grade retention and special education placement up through the sixth grade. Indeed, ongoing evaluation efforts of the CPC program by Reynolds and colleagues have continued to document the positive effect of preschool participation on the cognitive aspects of kindergarten-readiness and early grade achievement (relative to children who did not attend preschool) over the years (e.g., Reynolds & Temple, 1998; Reynolds et al., Finally, data from the contemporary, nationally representative sample of ECLS-K children and using calculations that are similar to those of this report, indicate rates of school or kindergarten readiness that are typically less than 50% for children from economically disadvantaged households (Isaacs, 2012). In comparison, the same report showed that 75% of children from more economically advantaged households (i.e., moderate to high income households) were considered ready for kindergarten.", " Together these findings suggest a large number of children who attended a SIB-CPC for preschool were assessed by their teachers as ready for kindergarten based on the assessment tool used. Given that this is not an experimental design, we cannot make causal attributions.", " The year 2 report will include kindergarten readiness outcomes for children participating in Cohort 2. It will also include data examining special education placement rates in kindergarten for Cohort 1 compared with rates of special education placement in a matched-comparison sample of children who did not attend any preschool in CPS.", " Child-Parent Center Evaluation: Report for 2014-15                  April 2016", " Appendices Appendix A:  Chicago Child-Parent Center Social Impact Bond Evaluation Plan Appendix B:  Timing of Cohorts Child-Parent Center Evaluation: Report for 2014-15                  April 2016 Appendix A: Chicago Child-Parent Center Social \nImpact Bond Evaluation Plan Social Impact Bond Report November 2015 A-1 Chicago Child-Parent  Center  Social Impact  Bond \nEvaluation  Plan December 2, 2014 Table of Contents I.", " INTRODUCTION AND STUDY  OBJECTIVES II.", " STUDY  POPULATION \na.  Eligible population -  Treatment group i.  Year  1  contingency  for CPC Treatment Group, b.  Eligible population -  No Pre-K Comparison group \nc.  Eligible population -  Other CPS Pre-K Comparison group \nd.  Exclusions RECRUITMENT  PROCEDURES \na.  CPS Pre-K recruitment process III.", " IV.", " INTERVENTION  AND  OUTCOMES \na.  Defining  the intervention \nb.  Defining  compliance with the treatment \nc.  Defining  Primary Impact Outcomes i.  Special Education Utilization outcome metric \nii.  Kindergarten Readiness outcome metric \niii.  Third Grade Literacy outcome outcomes \nd.  Defining  Performance  Improvement  Questions i.  Attendance \nii.  Dosage \niii.  Social/Emotional  learning \niv.  Transition to Kindergarten \nv.  Subgroup analyses V.", " DATA  COLLECTION \na.  Student data \nb.  Neighborhood  data \nc.  School data \nd.  Data security VI.", " STUDY DESIGN & OVERVIEW OF ANALYSIS a.  Propensity  score matching protocol \nb.  Checking for covariate balance between  groups \nc.  Matching methodology  remedies \nd.  Calculating mobility \ne.  Calculating effect  size for  Special Education  utilization i.  Calculating payments for  Special Education  utilization f.  Calculating effect  size for Kindergarten  Readiness \ng.  Calculating effect  size for Third Grade literacy \nh.  Investigating highly unexpected  outcomes VII.  APPENDICES INTRODUCTION  AND STUDY  OBJECTIVES The purpose of this document is to describe the methodology to be used to evaluate the impact of \nthe Child Parent Center (CPC) Social Impact Bond (SIB) expansion on three primary  impact \noutcomes: Special Education Utilization, Kindergarten Readiness, and Third Grade Literacy. \nThis document also describes additional research questions that the Evaluator will seek to \nexplore in collaboration  with CPS to help the CPCs improve their performance.  This \nmethodology  will be developed  in conjunction  with CPS and other experts in the early  education \nfield.", " Participants in the CPC program (the Treatment Group) will be compared to groups of matched \ncomparison  students who did not have a CPC experience through the use of a propensity  score \nmatching algorithm. One comparison group will consist of children who did not attend any  form \nof CPS Pre-K (No Pre-K comparison group). Another group will consist of children  who \nattended some other type of CPS pre-K program, such as Head Start or Pre-School  for All (Other \nPre-K comparison group).", " Payments based on Special Education utilization for the SIB project will be calculated using the \ndifference  in outcomes between the Treatment group and the No Pre-K comparison group.", " Payments based on Kindergarten Readiness and Third Grade literacy will be calculated  using \noutcomes of the treatment group relative to national  standards The Other CPS Pre-K comparison group will be used for  sensitivity analyses and for  addressing \nother research questions not related to payment triggers.", " For the purposes of calculating payments owed as part of the SIB transaction, impacts will \nestimated using the total population  of eligible students at SIB CPC sites, and then scaled to \nreflect the actual number of seats funded  by the Lenders. We will adjust  the scaling  factors \nannually to reflect  observed mobility trends.", " The primary impact outcome questions are as follows: 1.  What is the impact of the CPC program on the rate at which students need an IEP? \n2.  What is the impact of the CPC program on Kindergarten Readiness as defined  by performance  on the TS Gold instrument (completed by teachers at the end of preschool)?", " 3.  What is the impact of the CPC program on Third Grade literacy  as defined  by performance  on the CPS 3r  grade assessment?", " In addition to these impact outcome questions, this evaluation will also seek to answer \nqualitative research questions that will help improve the performance  of the program going \nforward  unrelated to the Pay for  Success calculations. These research questions will be \ndeveloped more fully  in conjunction  with CPS and other experts in the early education field, and \nwill only be pursued subject to additional external funding.  The questions may include: 1.  How do the primary impact outcomes vary by key subgroups, including gender, race, \nprior pre-school attendance, English language learner status, and potentially  other \nsubgroups?", " 2.  How is the CPC program impacting attendance in Pre-K? How does attendance vary by \nsite? How does attendance vary compare to other CPS Pre-K programs? Are there \npolicies in place at specific  sites that could be driving improved  attendance?", " 3.  How does the CPC program  support a transition to Kindergarten? What sites are better at \nretaining children from  Pre-K to K, both within their host school and within the entire \ndistrict? Where do children who transfer  within CPS go and why? Are there  different \nimpact outcomes for students who have less mobility?", " 4.  How successful  is the CPC program at improving social-emotional  learning outcomes \n(defined  by the social-emotional  components of the TS Gold instrument) compared to \nchildren enrolled in other CPS pre-K programs?", " 5.  How successful  is the CPC model at engaging parents? What strategies are the most \neffective  at encouraging parental engagement? What strategies appear to have the \ngreatest impact on children's outcomes?", " This document will  serve as a template for how the evaluation will be conducted. The Evaluator \nwill draft  a final Evaluation Plan to be approved by CPS, the City, the Project Coordinator with \nApproval of the Lender Committee (such term being defined  herein as such term is defined  in the \nLoan Documents of the Lenders)  using this document as a framework.  No changes to payment \nterms or payment terminology  will be made.", " STUDY  POPULATION Eligible Population -  Treatment  Group \nThe Treatment Group in this study will consist of four-year-olds1  who are attending Pre-K at any \nof the CPC SIB sites, in full  day or half day programs, who at any point during the school year \nare eligible for the National  School Lunch Program (NSLP).", " 1 The inlention is to identify  children  in the \"age cycle four\" year -  the year prior to when they are planning to attend Kindergarten. At the time of \nthe drafting  of this document, this was defined  by CPS as attaining age four on or before  September  1st. This age identification  protocol  may be \nadapted  as necessary to capture these children.", " In the first year of the program, the following  sites will be considered CPC SIB sites: \u2022  De Diego \n\u2022  Melody \n\u2022  Peck \n\u2022  Thomas \n\u2022  Fiske \n\u2022  Hanson Park In the second year of the program, two additional  sites, identified  by CPS and approved  by the \nCity, will be added to the list of CPC SIB sites in addition to the sites listed above. If SIB \nfunding  in future years is used to add classrooms at additional schools as part of this project, \nthose schools can be considered  CPC SIB sites as well. If SIB funding  is removed from  one of \nthe above sites, that site will no longer be considered a CPC SIB site.", " A child may enter the program based on CPS age elibility criteria. For the 2014/15 school year, \nthis entailed being age 4 as of September 1st.", " All four-year-olds  at CPC SIB sites, including children attending full-day  classes, will be \nincluded in the treatment group, subject to the exclusions listed below.", " In the first year of the program, we anticipate that 374 new slots for  four-year-olds  will be \ncreated through the SIB program. In the second year of the program, we anticipate that we will \ncreate an additional 408 new slots for four-year-olds  in addition to maintaining funding  for the \noriginal 374. In the third year of the program, we anticipate that we will maintain the 782 new \nslots that were created in years one and two. In the fourth year of the project, we expect to \nprovide funding  for at least 680 slots. Overall at CPC SIB sites, we anticipate that approximately \n840 four-year-olds  will be served per year once the program is operating at scale, with 782 of \nthose positions funded  by the SIB. The new slot amounts will be finalized  prior to the launch of \neach new cohort.", " Year  1  contingency  for CPC Treatment  Group \nDue to the timing of the contracting, some of the new classrooms to be added in the 2014/15 \nschool year will not be ready to serve children until the school year has already begun. Five of \nthe Year  1  CPC SIB Sites where we will be adding additional classrooms (De Diego, Melody, \nPeck, Thomas, and Fiske) have been operating as a CPC for a year or more. As a result, they \nhave an established  leadership team, trained and experienced teachers, and fully  outfitted \nclassrooms.", " To ensure that the children being tracked are receiving a sufficient  dosage of the CPC program, \nfor Year  1  only we will restrict the Treatment group eligibility to children who are enrolled in \none of these five established  CPC SIB sites, in a classroom that was already  established as of \nSeptember 2n  2014 (the start of the 2014/15 school year). CPS will proceed with opening the \nnew classrooms once all contractual  issues have been resolved, but the children who are enrolled \nin those classrooms (including children at Hanson Park, the new CPC for  Year  1) will not be \nincluded in the outcome calculations for the purposes of determining payments. This will allow \nCPS leeway to identify  and train high quality teachers, and mitigate the risk that the outcomes \n(or underlying characteristics) of children who enroll in a CPC Pre-K after  the start of the year \nare different  from those of their peers who enrolled at the start of the year. The outcomes of these \nlate-enrollees can be used as a unique sub-group, but will not factor  into any calculations that \ndetermine payment amounts.", " It is anticipated that the sample size of eligible four-year-olds  in existing classrooms at existing \nCPC SIB sites will be at least 300 students. As with future  analyses, when calculating payments \nthis number will be scaled to reflect the actual number of slots funded  by the Lenders as part of \nthis initiative.", " Eligible Population -  No Pre-K Comparison Group \nThe No Pre-K Comparison Group in this study will be identified  via a propensity  score matching \nalgorithm that pulls from  a pool of eligible No Pre-K children districtwide. The pool of eligible \nNo Pre-K children will include all children who meet the following  criteria: \u2022  Are enrolled in a CPS Kindergarten program, excluding: o  Charter schools \no  Schools currently operating a CPC, as part of the SIB program or otherwise \no  Magnet and Selective Enrollment  Schools \no  Schools that serve exclusively  a special education  population \u2022  Are five years of age as of September 1st \n\u2022  Did not attend a CPS Pre-K program in the school year prior to beginning  Kindergarten \n\u2022  Did not attend a Head Start program  funded  through the City of Chicago \n\u2022  Are eligible for NSLP at any point during the school year A child will be considered to have attended  a Pre-K program if that child attended  10 days or \nmore of a city funded  pre-school program, or any days at any CPC site over the course of the \nschool year. Days need not have been attended  consecutively.", " The No Pre-K Comparison  group will be identified  the year that their matched Treatment cohort \nbegins Kindergarten to ensure that children within both groups are on the same age cycle.", " Eligible Population -  Other CPS Pre-K Comparison  Group The Other CPS Pre-K Comparison  Group in this study will be identified  via a propensity  score \nmatching algorithm that pulls from  a pool of eligible children who attended other forms of CPS \npre-K within the district. The pool of eligible Other CPS Pre-K children will include children \nwho meet the following  criteria: \u2022  Are enrolled in a CPS Pre-K program, excluding: o  Charter schools \no  Schools currently operating a CPC, as part of the SIB program or otherwise \no  Magnet and Selective Enrollment  Schools \no  Schools that serve exclusively  a special education  population \u2022  Are four years of age as of September 1st. \n\u2022  Are eligible for NSLP at any point during the school year The Other CPS Pre-K Comparison group will be identified  the same year that their matched \nTreatment cohort begins pre-school to ensure that children within both groups are on the same \nage cycle. This group will only be identified  subject to available external  funding Exclusions for payment calculations \nThe hypothesis is that the CPC program will have the biggest impact on children who are \ndeemed at risk for poor school performance  and achievement, but who lack a severe or \nsignificant  disability. Without additional  support, many of these children may end up being \ndiagnosed with a mild learning disability, emotional  disturbance, or developmental  delay \n(including speech/language  impairment). For these children, additional  support in the classroom \nand at home can help ensure that they stay on track developmentally  with their peers, avoiding \nthe need for years of special education  services.", " The same impact is not expected for children with severe disabilities (identified  in preschool or \nat a later date), and it is also not expected that a preschool  intervention would meet the needs of \nthe child without the benefit  special education  services, nor would that be appropriate or within \nthe parameters of a child's right to a free  and appropriate education. To ensure that children  have \naccess to the supports they need based on a clinical evaluation, if a child at any point during the \ncourse of the study is diagnosed with a severe disability, he or she will be removed  from  the \nstudy group during the year that the disability is added to the child's IEP onward. The \npreliminary  list of severe disabilities, with input from the Independent Evaluator, may be as \nfollows: \n\u2022 \n\u2022  deaf-blindness \n\u2022  deafness \n\u2022  hearing impairment \n\u2022  orthopedic  impairment \n\u2022  other health  impairment autism traumatic brain  injury \u2022 \n\u2022  visual  impairment \n\u2022  multiply  disabled\" \n\u2022 \n\u2022 intellectual  disability \nstudents placed into self-contained  classrooms for children with special needs This list may be adapted at the discretion of the Evaluator with approval  from  CPS, the City, the \nProject Coordinator, and the Approval of the Lender Committee.", " RECRUITMENT PROCEEDURES \nChildren are identified  for enrollment under the Chicago: Ready to Learn! application process. A \ntimeline of application, placement, registration, and enrollment of children for the 2014/15 \nschool year is provided below; this will also serve as an illustrative plan for how the process will \noccur in future  years: Action Description Parents obtain information  about \npotential programs through \nchicagoearlyleaming.org. \ncps.edu/readytolearn and the Chicago: \nReady to Learn! hotline. Parents apply \nat application  centers across the city \nfor preschool under two application \nrounds.3 The first round is held during \nthe month of March - April and the \nsecond round is held during the month \nof May-June. Parents can choose up to \nthree schools.", " Parents are offered  a placement in a \nschool and/or are placed on a waiting \nlist. \nChildren placed in a preschool \nprogram  or on a waiting list are put \ninto schools' Program Management in April and June 2014 Chicago: Ready to \nLearn! Application \nRounds  1  & 2 May and July/August 2014 Placement \" Intended to represent students with multiple severe disabilities 3 For a complete list of application  centers, see \nhttp://cps.edu/Schools/EarlvChildhood/Documents/ApplicationSites  SY14  15.pdf or \nhttp://cps.edu/readytoleam.Every  CPC also is capable of accepting applications  directly.", " June through  September \n2014 Registration September 2014 Enrollment September 2014 onward Rolling enrollment the CPS SIM IMPACT system.", " Parents accept or decline placement. \nSchools notify  parents of registration \ndates and times. \nSchools indicate parents'  acceptance \nor decline of placement in Program \nManagement and move registered \nchildren into the classroom \nHomerooms for  IMPACT. \nTeachers complete the registration \npacket with families  for all new \nstudents. \nClerks enter identifying  additional \ninformation  into the IMPACT system.", " Children are enrolled upon attendance \non the first  day of school.", " Schools continue to enroll  students \nthroughout the school year as slots \nopen up due to attrition, new  funding, \netc. Staff conduct additional  outreach \nin communities with lower than \nexpected enrollment to help fill all the \nslots. This includes additional ad \nspots, flyers, and community events. \nThese children will only be included \nfor evaluation purposes if they meet \nthe dosage and eligibility  requirements \noutlined in this document.", " INTERVENTION  AND  OUTCOMES Defining  the Intervention \nThe CPC SIB intervention will provide one year of half-day  CPC Pre-K to four-year-olds  at CPC \nSIB sites. The key components of the CPC model are as follows: Effective  Learning  Experiences \u2022  Offer  Pre-K classes that are limited to 34 children for half-day  classrooms (two sessions \nof  17 children each) and have a minimum of 2 teaching staff. Full day classrooms, if \navailable, will be limited to 20 children per session.", " \u2022  Provide highly qualified  educational  staff that will provide the classroom instruction and parent engagement activities. For example, classroom teachers are certified  with a \nbachelor's degree (or higher). Overall, program staff must adhere to the requirements set \nforth by the CPS Talent office,  in accordance with collective bargaining unit agreements, \nand state regulations.  Any changes in CPS education and certification  requirements will \nbe complied with.", " \u2022  Use data to drive instruction by effectively  documenting the organization and implementation  of instructional practices to monitor quality and adherence to the \nProgram, which is completed by all Program staff where appropriate.", " \u2022  Program staff meet with parents over the course of each school year to review their \nchild's progress and discuss parent program opportunities with the Parent Resource \nTeacher (PRT).", " Aligned  Curriculum \u2022 Implement a CPS District curriculum and formative  assessment that is aligned to \nstandards, domains of learning, assessments, and learning activities.", " \u2022  Collaborate with the PRT and classroom teachers to ensure that opportunities to engage \nfamilies  in student learning are available, appropriate and aligned to the program and \nparents' needs.", " \u2022  CPS and, most specifically,  the Office  of Early Childhood  Education provides meaningful  professional  development and ongoing coaching and feedback  for teachers, \naides, and other staff members that facilitates  high-quality  instructional practices.", " Parent Involvement and Engagement \u2022  Engage a PRT and School-Community  Representative  (SCR) to work closely with the \nHead Teacher and Liaisons to maintain a consistently  supportive parent program. \n\u2022  Encourage parents to sign a CPC school-home agreement at the start of the school year \noutlining a plan for fostering  learning at home and participating  in CPC activities. \n\u2022  Offer  and engage families  in monthly  activities. PRTs create and distribute a monthly \nparent involvement calendar, and conduct parent/teacher  conferences  over the year to \nreview progress in the parent program.", " \u2022  Provide a resource room dedicated to parent and family  activities through  Kindergarten \u2022  Provide culturally responsive learning opportunities for families  that provide  flexibility when possible.", " for families'  needs and schedules.", " Collaborative Leadership  Team \u2022  Engage a Program leadership team that includes the Head Teacher, Parent Resource Teacher, and School-Community  Representative.", " \u2022  Meet regularly, under the direction  of the Principal to discuss operations and best practices within the CPC.", " \u2022  Meet regularly, under the direction of the OECE Management Team, with staff  from across sites to share challenges, experiences, and best practices and makes frequent  on-\nsite visits to monitor quality and effectiveness  to the Program.", " \u2022  Establish meaningful  partnerships with community providers to strengthen  service delivery and enlist local universities in training opportunities.", " Continuity and Stability \u2022  CPC Pre-K classrooms are co-located  in the same building as Kindergarten classrooms, \nwhen possible, to promote familiarity  and integration for  students as they transition to \nKindergarten.", " \u2022  Provide a structure of communication, planning, and joint activities, under the direction of the principal, Leadership team and OECE Management Team, from  Pre-K through the \nprimary grades. \nProvide a part-time Kindergarten  aide when funding  is available to support the transition \ninto Kindergarten.", " Professional Development System \u2022  Offer  ongoing professional  development  opportunities on current trends and needs in early childhood education classrooms, through the Office  of Early Childhood  Education \nand the CPC leadership teams, including topics such as quality curriculum and \ninstruction, data driven instruction, learning environment, social and emotional needs, \nand parent engagement.", " \u2022  Meet regularly and create professional  learning communities to review ways to support their instruction in the classroom and with other teachers.", " Defining  Sufficient  Dosage \nEnrollment and attendance fluctuate  throughout the year, with substantial  changes during the \nearly weeks of the school year. As a result, some of the children who start the year in a given \nclassroom may not be the same children who end the year in that classroom. This may be due to \nfor a variety of reasons such as mobility, a change in parents'  schedules/ability  to bring their \nchildren to school, or admission  to a closer/more desirable program off of a waitlist later in the \nschool year.", " To ensure that CPC SIB children and families  are receiving a minimum  sufficient  dosage of the \nCPC program, we will restrict analyses to children who attend a certain minimum cutoff  of days. \nThe Evaluator will examine historical data from  CPS and other districts to determine trends in attendance  and  identify  a cutoff  that  sufficiently  indicates  that a child  has  received  enough  of the \nprogram  for  us to expect  to  see an  impact.  We are temporarily  placing  this cutoff  at 66%  of \nschool  days  in a given  school  year;  children  who  attended  fewer  than  66% of  days  during  their \nPre-K  year will be  omitted  from  the primary  analyses.", " The Evaluator  may  add  additional  criteria based  on an analysis  of enrollment  and  attendance \ndata with the approval  of  CPS, the  City,  and  the  Project  Coordinator  and  Approval  of the  Lender \nCommittee.", " Similarly, for  the No  Pre-K  Comparison  group, we will  limit the primary  analysis  sample  to \neligible No  Pre-K  children  who  attend  at least  66% of school  days  in a given  school  year.  If a \nchild  at any  point during  the  Kindergarten  year  attends  a school  operating  a CPC program,  that \nchild  will be omitted  from  primary  analyses.", " Defining  Primary  Impact  Outcomes Special Education  Utilization \nThe primary  Special  Education  utilization  outcome  will  be defined  as a binary  indicator  of \nwhether  or not a student has  a CPS-issued  Individualized  Education  Plan  (IEP)  in a given  year. \nThis  will be  a data point provided  as part of the  regular data collection  points by  CPS.  As \ndescribed  above,  if a student has  a diagnosis  on  his or her IEP of a severe  disability, that  student \nwill  be removed  from  the  study  pool  for  the primary  analyses. This indicator  will  be  collected \nannually  ever year  Kindergarten  through  6 th  grade.", " Kindergarten  Readiness \nCPS  uses the  Teaching  Strategies  Gold  (TS  Gold)  instrument  in all their  Pre-K  classrooms  to \ntrack the development  of children.  Based  on  teacher  observations,  TS  Gold  measures  the \nprogress  of children  in domains  such  as socio-emotional,  physical,  language,  literacy,  and \ncognitive  development.", " The TS Gold  instrument  is utilized  nationally  in Head  Start programs  and  some  publicly-funded \npreschool  programs. The primary  outcome  metric  for  Kindergarten  Readiness  will  be the  share \nof children  which  are performing  at or  above  the national  trends  across  at least  five  out  of the \nfollowing  six domains: Literacy,  Language,  Math,  Cognitive  Development,  Socio-Emotional, \nPhysical  health.", " Third Grade  Literacy \nCurrently,  CPS  is planning  to adopt  the PARCC  standardized  exam.  Treatment  group  children \nwill be measured  relative to national  percentile  rankings  on this test or the  accepted  District \nassessment  administered  for  3 rd grade. In following  with  Lesnick  et al  (2010)  , every  child See http://www.chapinhall.org/sites/default/files/Reading_on_Grade_Level_l  11710.pdf reading at or above the 25l  percentile on the English Language Arts/Literacy  portion of the \nspring sitting of the PARCC test will be deemed to be reading at grade level. Any child reading \nat or above the 75  percentile nationally  will be deemed to be reading above grade level. Any \nchild reading below the 251  percentile will be deemed to be reading below grade level.", " At the time of drafting  this analysis, the PARCC test has yet to be officially  implemented  in CPS \nschools. Given the uncertainty  of performance  on this test and how its outcomes will compare to \npast tests taken by CPS students, the evaluator may suggest amendments to the definition  of \nreading \"on grade level\" that could include utilizing a different  test or metric. Any  modifications \nmust be made prior to the first  cohort starting Third Grade, and must be approved by CPS, the \nCity, the Project  Coordinator, and Approved by the Lender Committee.", " Defining  Performance  Improvement  Questions \nThe details of these questions will be developed  in conjunction  with CPS and other partners over \nthe 2014/15 school year. These analyses will be specified  in full  prior to the start of any data \ncollection or analyses. These analyses will not affect  the methodology  or results of the primary \nimpact outcomes, and will only be pursued subject to additional philanthropic or other  funding.", " DATA  COLLECTION Student data \nStudent data will be provided to the Evaluator by CPS. Pursuant to the data sharing agreement3, \nCPS will strip sensitive individual identifiers  and replace them with an anonymous student ID. \nThe key variables CPS will provide are: \u2022  Student ID \n\u2022  CPS School ID of school currently enrolled in \n\u2022  Date of Birth (or birth month & year) \n\u2022  Days attended to date \nIEP status \n\u2022 \n\u2022 \nIEP diagnoses \n\u2022  Reported  race \n\u2022  Reported  ethnicity \n\u2022  Free/reduced price lunch  eligibility \n\u2022  ZIP code of residence \n\u2022  Fall and Spring TS Gold scores (if applicable) \n\u2022  Any available variables on parental  education \n\u2022  Other variables deemed appropriate by the Evaluator and CPS for the purposes of creating a better propensity  score match 5 This data sharing agreement will be included as an appendix to this plan pending negotiation  and drafting  between \nCPS and the Evaluator.", " Data will be collected on an annual basis on the based on the last school day in June which is \nreported for accuracy  in the beginning of July. This may be adjusted  based on discussions \nbetween the Evaluator and CPS to reflect the earliest date that all the necessary data would be \navailable.", " Neighborhood  data \nThe Evaluator will pull neighborhood  data from  publicly available census data, such as the \nAmerican Community  Survey 5-year  averages, which break out characteristics by zip code. \nNeighborhood  data include: Neighborhood % of population in poverty \nNeighborhood  % of population that are single mothers \nNeighborhood  % of population that is Black \nNeighborhood  % of population that is Hispanic \nNeighborhood  % of population  employed \nNeighborhood  crime statistics \nNeighborhood  health  indicators The Evaluator will update the neighborhood  data file when creating a new cohort of matched \ngroups.", " School data \nData on school level characteristics will be provided by CPS, including: \u2022  CPS School ID \n\u2022  Total student body population \n\u2022  % Free/RP lunch \n\u2022  % Black \n\u2022  % Hispanic \n\u2022  School-wide attendance rate from  the 2013/14 school year \n\u2022  School Rating (Levels  1, 2, or 3) from the 2013/14 school year7 These data, except for attendance and the school rating, will be updated annually. Attendance \nand rating data from  SY2013/14 (or the closest assessment prior to SY2013/14) will remain fixed \nto reflect  the fact that the presence of a CPC may improve attendance and the school rating over \ntime, which could affect  the matching algorithm for later cohorts. The Evaluator may adjust  this \nprotocol  if extraneous events such as school closures, new leadership, or expansive new \nprograms are added at individual  schools or system wide that could contribute to imbalanced \nmatches.", " Data Security 6 Crime stats and health  indicators subject  to availability  of data. It may be possible to pull data from a Chapin  Hall \nneighborhood  analysis. These covariates may be omitted  if it proves too difficult  or costly to obtain them.", " 7 All these data are publicly available online at http://www.cps.edu/schools/find  a  school/pages/findaschool.aspx. \nSchool rating is based on the CPS Performance  Policy which  is used to rate CPS schools. A Level  1  rating is \n\"excellent\",  a Level 2 rating is \"good\" and a Level 3 rating  is \"low\".", " A data sharing agreement between CPS and the Independent Evaluator will define  the \nparameters for sharing data required under this agreement.", " STUDY DESIGN  & OVERVIEW  OF  ANALYSES Propensity  score Matching Protocol \nComparison group students will be selected using a propensity  score matching technique. \nIndividuals from the treatment group will be matched to up to two individuals from  the No Pre-K \nComparison group and up to two individuals from  the Other CPS Pre-K Comparison group. \nMatching will be conducted with replacement to allow comparison individuals to be matched \nmore than once.", " To create the Treatment Group in school year r, the Evaluator will receive the data collected on \nthe last day of June of school year / from  CPS of all four-year-olds  who attended a SIB CPC in \nschool year t up to the date of the data collection. The data collected  and shared will contain all \nthe student data elements listed above. After  screening for eligibility as described above and \nremoving ineligible students from  the sample, the Evaluator will use students' ZIP codes to \nmerge on neighborhood data, and students'  school IDs to merge on school characteristics. \nNeighborhood data will be collected from a reliable source such as Chapin Hall. This will create \na de-identified  student-level file that contains student-level characteristics, characteristics of that \nstudent's neighborhood of residence, and characteristics of that student's  school.", " To create the No CPS Pre-K pool to be used for matching to the Treatment cohort in school year \n/, the Evaluator will receive a data dump on the last day of June of school year t+\\  from  CPS of \nall five or six-year-olds who attended a CPS Kindergarten  in school year t+\\  up to the date of the \ndata dump. The data dump will contain all the student data elements listed above.  After \nscreening for eligibility as described above and removing ineligible students from  the sample, the \nEvaluator will use ZIP code data to merge on neighborhood  data, and school ID data to merge on \nschool characteristics.", " To create the Other CPS Pre-K pool to be used for matching to the Treatment cohort in school \nyear t, the Evaluator will receive a data dump on the last day of June of school year / from  CPS \nof all four-year  olds who attended  a CPS Pre-K program other than CPC in school year t up to \nthe date of the data dump. The data dump will contain all the student data elements listed above. \nAfter  screening for eligibility  as described above and removing ineligible students from  the \nsample, the Evaluator will use ZIP code data to merge on neighborhood  data, and school ID data \nto merge on school  characteristics.", " To create the matched No Pre-K Comparison group, the Evaluator will append the Treatment \nGroup dataset and the No Pre-K Comparison pool dataset, creating an indicator to identify  which \nchildren are members of the Treatment group. The Evaluator will then run a probit model using \nthe treatment indicator as the dependent variable and the following  variables as  independent \nvariables: \u2022  Race binary  indicators \n\u2022  Ethnicity binary  indicators \u2022  Gender (\"Male\" binary  indicator) \n\u2022  Parental education (subject to availability) \n\u2022  Language spoken at home binaries \n\u2022  Neighborhood  % poverty \n\u2022  Neighborhood  % single mothers \n\u2022  Neighborhood  % by race \n\u2022  Neighborhood  % by ethnicity \n\u2022  Neighborhood  % employed \n\u2022  Neighborhood  crime rates (subject to availability) \n\u2022  Neighborhood health indicators (subject to availability) \n\u2022  Total student population of school currently  attending \n\u2022  % Free/RP lunch at school currently  attending \n\u2022  Racial composition of school currently  attending \n\u2022  Ethnicity composition of school currently  attending \n\u2022  School-wide attendance rate from  the 2013/14 school year \n\u2022  School Rating binaries from  the 2013/14 school year Using the results of this model, the Evaluator will predict a propensity score based on a student's \nobserved characteristics. This score effectively  represents the likelihood that a child, given his \nindividual, neighborhood,  and school level characteristics, would be in the Treatment group.", " The Evaluator will use a nearest-neighbor matching algorithm  to identify  the two closest \nmatches based on propensity  score for each Treatment group observation, with replacement.", " Individuals from  either the Treatment group or Comparison pool who are not matched will be \ndropped.", " The remaining students from  the Comparison pool who were matched will become the No Pre-K \nComparison group for the remainder of the study. Comparison group students will receive a \nfrequency  weight equal to the number of times they were matched. Note that as a result, the \nComparison group should contain approximately two times as many unique individuals as the \nTreatment group.", " The same protocol will be used to identify  the Other CPS Pre-K Comparison group, replacing the \nNo CPS Pre-K Comparison pool with the Other CPS Pre-K Comparison pool.", " A unique set of comparison  groups will be created for each Treatment cohort (see Appendix for a \ncohort timing chart).", " Checking for covariate balance between  groups \nOnce the comparison groups have been identified,  the Evaluator will check for balance between \nthe groups across matching demographics. The Evaluator will choose appropriate methods to \ncheck for balance, including but not limited to normalized differences  and t-tests of mean values \nof covariates between groups.  If the Evaluator determines that there is imbalance in covariates By way of example, see \"nnmatch\" stata command between groups, the Evaluator may choose to pursue a Matching Methodology Remedy as \ndescribed below. The decision to pursue a remedy will be at the discretion of the Evaluator, \ntaking into account the fact that with many matching variables and a p-value cutoff  of .05, \napproximately  1  in 20 variables could have a statistically  significant  difference  by random \nchance alone. The evaluator will consider the magnitude of the difference  and the relative \nimportance of the unbalanced variable(s) in question, placing particular attention to the \nindividual-level  race and gender indicators, the home language indicators, the neighborhood \npoverty  indicators, and the school rating indicators.", " Matching Methodology  Remedies \nIn the event that the Evaluator deems that the propensity  score matching algorithm has produced \nan inadequate match, the Evaluator may make modifications  to the matching methodology. This \ncould include introducing a caliper to ensure that certain variables are matched to within a \nnarrow range (or matched exactly), adding or subtracting additional covariates, increasing or \ndecreasing the number of matches, or other techniques deemed rigorous and appropriate by the \nEvaluator.", " The Evaluator may also explore utilizing a set of comparison schools to limit the comparison \npool. In this methodology, the Evaluator would identify  a set of comparison  schools that match \nthe SIB CPC sites, identifying  one to three schools for each site. The Evaluator would use a \nsimilar propensity  score matching protocol, using school level characteristics, to identify  these \nschools. From those comparison schools, the Evaluator would then perform  a student-level \npropensity score match using a comparable methodology to the one described above. The \nEvaluator will then check for covariate balance to see if this produces better match results.", " Once the Evaluator identifies  a suitable comparison group that they deem to be well-matched  on \ncovariates, the Evaluator will present the match results, describing any changes that were made \nto the matching algorithm, which must be approved by CPS, the City, the Project  Coordinator \nand Approved  by the Lender Committee. The Evaluator should endeavor to use a similar \nmatching protocol from year to year.", " Calculating mobility  factor \nThe theory behind the financing  component of the SIB project  is that providing the  upfront \nintervention of high quality Pre-K can produce savings to CPS downstream through reduced \nSpecial Education  utilization among the students served. For CPS to realize these savings, \nhowever, those students must remain in the CPS school district. If a student leaves the district, \nCPS would realize no savings from  the fact that the intervention may have helped that that \nstudent catch up to his peers and prevented him from  acquiring an IEP.", " As a result, the Evaluator will calculate a Mobility  Factor for each cohort that will represent the \nshare of the original cohort that is still enrolled in a CPS school in a given year. This will be used \nto adjust  the payment amounts to better reflect  savings realized by CPS.", " To calculate mobility, every year Kindergarten through 6l  grade the Evaluator will determine \nwhat share of the original children  in a given group from  the first year of observation are still enrolled in any CPS school. To do this, every year the Evaluator will send CPS a list of all the \nstudent IDs of the original group. CPS will match these IDs to their current enrollment  database \nto determine which students were enrolled in a CPS school at any point in that school year. CPS \nwill then return a dataset to the Evaluator indicating which student IDs are enrolled in a CPS \nschool that year. The Mobility Factor will be defined  as: /\u2014# of  original  students currently enrolled in any CPS schooW of  students  originally enrolled in the  group By way of example, assume 500 Treatment group students were identified  for the 2014/15 \ncohort. In SY2015/16, the Evaluator sends a list of these student IDs to CPS, who informs the \nevaluator that 460 of them are still enrolled at a CPS school. The cumulative mobility for that \nyear would be  1  - 460/500 = .08. In SY2016/17, the Evaluator sends the original list of student \nIDs to CPS again, who informs the evaluator that 440 of them are still enrolled at a CPS school. \nThe cumulative mobility for SY2016/17 would be  1  -  440/500 = .12.", " For grades 7  through  12  , the Evaluator will impute a marginal mobility rate by averaging the \nincremental annual increase in the Mobility Factor over the last three years.  Every year, the \nEvaluator will impute a new Mobility Factor based on the average imputed marginal mobility \nrate. See Appendix B for a full  example using hypothetical  data.", " Calculating effect  size for Special Education  utilization \nTo calculate the impact on Special Education utilization, the Evaluator will calculate the Average \nEffect  Size per Person, which will then be scaled to reflect  the number of seats funded  by the \nLenders for the purposes of calculating payments. This will allow the Evaluator to utilize all the \ndata available, increasing sample sizes and precision of estimates.", " To calculate this, the Evaluator will use the following  equation: AESPi,t=  SPEDC,i,t-  SPEDT,i,t where AESPu is the Average Effect  Size per Person for cohort i in year /,  S P E D QU is equal to \nthe average of a binary indicator of Special Education utilization among the No CPS Pre-K \nComparison  group for cohort /' in year t and SPEDjjj  is the average of a binary indicator of \nSpecial Education utilization  among the Treatment group for cohort / in year t. At the discretion \nof the Evaluator and with approval  from  CPS, the City, the Project  Coordinator, and the \nApproval of the Lender Committee, the Evaluator may regression-adjust  this estimate to help \naccount for any differences  in covariates between the Treatment group and the Comparison \ngroup.", " 9 The Evaluator  may revise the methodology  for  averaging  the mobility  rate  if they  determine  that the  current \nmethodology  includes  a grade  breakpoint  year that could  result  in abnormally  high  mobility  out of the district.  This \nmethodology  must be finalized  before  the first  cohort  reaches  6th grade.", " 1!", " Special Education outcomes will be calculated annually every year Kindergarten through 61 \ngrade. Outcomes will be calculated  separately for each cohort. Based  on conversations with \nspecial education experts and reviewing existing CPS data, we believe that the vast majority  of \nchildren who have a disability  will be identified  by the end of 6th grade. As a result, after  the 6th \ngrade effect  size has been calculated, we will average the effect  size over the last three years (4th, \n5  and 6  grades) and lock in that average rate for the purposes of calculating payments in \ngrades 7  through  12th. This lock-in rate will be calculated  separately  for each Treatment  cohort. \nThe Evaluator may propose changes to this lock-in methodology  in the event that the Evaluator \ndetermines that this methodology produces skewed results. Any modifications  must be approved \nby CPS, the City, the Project  Coordinator, and Approved by the Lender  Committee.", " Calculating payments for  Special Education utilization \nTo determine the size of Special Education payments owed in a given year for a given treatment \ngroup cohort, the Evaluator will multiply the Special Education Average Effect  Size per Person \nfor such cohort by the base cohort size multiplied by the  1  minus the cumulative mobility rate for \nthat year. This will determine the Total Number of Special Education  Slots Avoided  for a given \ncohort in a given year: Total Number of  Special  Education  Slots  Avoided=AESR,t* BCSi*  (l-MFi,t) where AESPij is the Average Effect  Size per Person for cohort i in year t, BCSi is the base cohort \nsize for cohort /, and MFU  is the cumulative mobility rate for cohort /' in year t.", " The base cohort sizes are based on the number of seats actually  funded  by investors. It is \nanticipated that the base cohort sizes will be as follows10: Cohort \nYear Base Cohort \nSize 2014/15 2015/16 2016/17 2017/18 Year Savings \nRate The Total Number of Special  Education  Slots Avoided will then be multiplied  by the Annual \nSavings Rate to determine the Special Education Payments owed for a given cohort in a given \nyear. Negative payments will be rounded to zero. The Annual  Savings Rate starts at a base of \n$9,100 in 2015 and grows  1% annually. The table below provides the rates through 2030: Note that  actual sample sizes used for calculating effect  sizes may be larger or smaller than the  number  of seats funded.", " 2015 \n2016 \n2017 \n2018 \n2019 \n2020 \n2021 \n2022 \n2023 \n2024 \n2025 \n2026 \n2027 \n2028 \n2029 \n2030 $9,100 $9,191 \n$9,283 \n$9,376 $9,469 \n$9,564 $9,660 \n$9,756 \n$9,854 $9,953 \n$10,052 \n$10,153 \n$10,254 \n$10,357 \n$10,460 \n$10,565 If applicable, the Special Education Payments from  each cohort will be summed to produce the \nTotal Special Education Payment owed by CPS for that year. These calculations will be reported \nto the Project Coordinator for the purposes of triggering payments to the Project Coordinator to \nbe used to repay the lenders.", " Payments for  Special Education will be made every year K -  12th for each Treatment cohort.", " Calculating effect  size for  Kindergarten Readiness \nAs part of the annual data pull, the Evaluator will receive spring TS Gold scores for  Treatment \ngroup students. TS Gold regularly publishes a set of averages that reflect  how children have \nscored nationally  on TS Gold assessment  sub-categories, broken  out by the time of the test and \nthe age in months of the child. Students will be classified  as \"meeting the national norms\" for a \nsub-category  if they  score at or above the national mean spring score for that category  for \nchildren in their age band.11 The Evaluator will use the most up to date tables available.", " Every child who scores at or above the national norm on at least five of the six subcategories in \nspring of their four-year-old  pre-school year will be deemed \"Kindergarten  Ready.\" To calculate \nthe Kindergarten  Readiness payment, the Evaluator will calculate the share of the Treatment \ngroup students deemed Kindergarten  Ready. The Evaluator will then multiply this number by the \nbase cohort size, multiplied by cumulative mobility  from  the Kindergarten year of a given \ncohort. This will determine the Total Number of Kindergarten Ready  Children for a given \ncohort. The Evaluator will then multiply this number by the payment rate of $2,900 to determine \nthe total Kindergarten  Readiness payments owed by the City for that cohort.", " See tables 5-14 of  https://www.kl2.wa.us/assessment/pubdocs/GOLDTechnicalManual2ndEditionLambert2.pdf for a list of the score thresholds.", " Calculating effect  size for  Third Grade Literacy \nCPS is currently transitioning to the PARCC exam. As a result, the exact methodology  for \ncalculating Third Grade Literacy may have to be adapted pending observation of how the test is \nbeing administered, scored, etc. In particular, in the event that data suggests that fewer than 50% \nof students are scoring above the 25th percentile, the Evaluator will propose a new protocol or \ntest for determining Third Grade Literacy that better captures the performance  of students. The \nEvaluator will propose a final  protocol for approval by CPS, the City, and the Project \nCoordinator with Approval  of the Lender Committee prior to the start of the 2018/19 school year \n-  the year the first cohort begins 3rd grade. A draft  protocol is below: As part of the annual data pull, the Evaluator will receive 3r  grade spring PARCC scores for \nTreatment group students. The PARCC test is administered  nationally, and as a result the \noutcomes of Treatment students can be compared to national averages. Students will be \nclassified  as \"reading at or above grade level\" if they score at or above the 25  percentile on the \nEnglish Language Arts/Literacy portions of the PARCC exam.", " To calculate the Third Grade Literacy payment, the Evaluator will calculate the share of the \nTreatment group students deemed to be reading \"at or above grade level\". The Evaluator will \nthen multiply this number by the base cohort size, multiplied by cumulative mobility  from  the \nThird Grade year of a given cohort. This will determine the Total Number of Third Grade \nChildren Reading at Grade Level for a given cohort. The Evaluator will then multiply this \nnumber by the payment rate of $750 to determine the total Third Grade Literacy payments owed \nby the City for that cohort.", " Investigating Highly Unexpected  Outcomes \nThe results of this evaluation will govern the flow of millions of dollars of payments. While it is \nthe full  intention of all parties to accept the results of the evaluation, in the event that a highly \nirregular outcome is achieved, a mechanism must be in place to validate the findings and  confirm \nthat they are due to the impact of the program, and not a flaw  in the analysis or evaluation \ndesign. The Evaluator will have complete discretion to decide if and when a validation of the \nfindings may be necessary, but the following  events will serve as guiding principles that could \nsuggest that a validation may be warranted: \u2022  The difference  in Special  Education Utilization rates between the Treatment group and \nNo Pre-K comparison group is negative or not statistically  different  from  zero (p-value \n<.05) for any cohort in any year after  Kindergarten \u2022  The No Pre-K comparison group Special Education  Utilization  rate is more than 2.5 times the Treatment group Special Education  Utilization rate for any cohort in any year \nafter  Kindergarten \u2022  An irregular pattern from  one year to the next in Special  Education utilization  for a given \ngroup, defined  as utilization  shrinking by more than  two percentage points for a given \ngroup, or increasing by more than seven percentage points \u2022  A larger impact observed when comparing a Treatment group cohort to its corresponding Other CPS Pre-K Comparison group any year after  1st grade.", " The Evaluator will determine the appropriate techniques and mechanisms to employ to  confirm \nthe cause of the irregularity, which could include handchecking code, checking for  continued \nbalance in the treatment and comparison groups, and looking for policy changes within  specific \nschools or system-wide that could have affected  outcomes.", " If the Evaluator finds  a mechanical error, the results will be recalculated using the correction. If \nthe Evaluator finds a methodological flaw, the Evaluator may propose a remedy to the evaluation \nplan to mitigate the inconsistency  in future  years. However, the results will not be recalculated \nfor that year or any other past year. Changes to the plan must be approved by CPS, the City, and \nthe Project Coordinator, and Approved by the Lender Committee.", " APPENDIX  A: TIMING  OF  COHORTS 1st 5th .'\u25a0  Mobility  rates cohort X Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 2 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 3 Treatment Other  CPS PK Comparison No  CPS PK Comparison Cohort 4 Treatment Other  CPS PK Comparison No  CPS PK Comparison 2nd 2nd Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK Identified  and \nenroll In   CPS K Identified  and enroll  InCPCPK Identified  & enroll In other   CPS  PK 3rd 3rd 3rd 2nd 2nd 1st 4th 4th 4th 3rd 3rd 3rd 2nd 2nd 2nd Identified  and enroll  In CPS K Identified  and enroll  InCPCPK Identified  8. enroll In other   CPS  PK Identified  and enroll  In  CPS K 4th-6th avg. SPED & \nMobility  fates locked 4th-6th avg. SPED & locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th avg. SPED ,  & Mobility  rates locked 4th-6th avg. SPED 5th & Mobility  rates locked 5th 5th 4 t h.", " 3rd 3rd 3rd 2nd 2nd 2nd 5th 5th 4th 4th 3rd 3rd 3rd 4th-6th  avg. SPED 5th & Mobility  rates locked 4th-6th avg. SPED & Mobility  rates locked 4th-6th  avg. SPED & Mobility  rates locked 5th 5th 4th 4th 4th-6th  avg.  SPED & Mobility  rates locked 4th-6th avg. SPED locked 4th-6th avg. SPED & Mobility  rates locked 5th & Mobility  rates 5th APPENDIX  B: SAMPLE  MOBILITY  CALCULATIONS  USING  SIMULATED  DATA Sample Mobility  Calculations  Using Hypothetical  Data Grade Original \nEnrollment Students still enrolled at a CPS school Cumulative \nMobility Marginal \nMobility School \nYear 2014 \n2015 \n2016 \n2017 \n2018 \n2019 \n2020 \n2021 School \nYear \n2022 \n2023 2024 \n2025 \n2026 \n2027 PK K 1st 2nd 3rd 4th 5th 6th Grade \n7th \n8th 9th \n10th \n11th \n12th 500 \n500 \n500 \n500 \n500 \n500 \n500 \n500 500 \n500 \n500 \n500 460 \n440 \n415 \n405 \n390 \n378 \n365 \n353 316 \n304 \n291 \n279 .08 \n.12 \n.17 \n.19 \n.22 \n.244 \n.27 \n.294 Imputed \nCumulative \nMobility \n.319 \n.343 .368 \n.393 \n.417 \n.442 .08 .04 .05 .02 .03 .024 .026 .024 .025 .025 .025 .025 .025 .025 .025 Imputed \nMarginal \nMobility Imputed average marginal mobility for future  calculations: Original \nEnrollment \n500 \n500 Imputed Students still enrolled at a \nCPS school \n341 \n328 Appendix B: Timing of Cohorts Cohort 1 Intervention Comparison Cohort 2 Intervention Comparison Cohort 3 Intervention Comparison Cohort 4 Intervention Comparison School year:  2014-15 2015-16 2016-17 2017-18 2018-19 2019-20* 2020-21 2021-22 2022-23 2023-24 2024-25 CPC PreK No PreK K K CPC PreK No PreK 1st 1st K K 2nd 2nd 1st 1st K K CPC PreK No PreK 3rd 3rd 2nd 2nd 1st 1st K K CPC PreK No PreK 4th 4th 3rd 3rd 2nd 2nd 1st 1st 5th 5th 4th 4th 3rd 3rd 2nd 2nd 6th 6th 5th 5th 4th 4th 3rd 3rd 6th 6th 5th 5th 4th 4th 6th 6th 5th 5th 6th 6th * SRI evaluation ends on Dec 1, 2020.", " Social Impact Bond Report November 2015 B-1"]}, {"paper_id": "#17725", "title": "Educate Girls Development Impact Bond: Final Evaluation Report.", "paragraphs": [" Educate Girls Development Impact Bond \nFinal Evaluation Report 10 June 2018 \n \nIn Partnership with: Children\u2019s Investment Fund Foundation, Educate Girls, Instiglio, and UBS Optimus Foundation Table of Contents 1.  Executive Summary .................................................................................................... 4 2.  Outcome I: Learning Gains ....................................................................................... 5 Methodology ...................................................................................................................... 5 Findings ............................................................................................................................. 7 3.  Outcome II: Enrollment of Out-of-School Girls .................................................. 11 Methodology .................................................................................................................... 11 Findings ........................................................................................................................... 12 4.  Conclusion ................................................................................................................. 12 References ......................................................................................................................... 13 Appendix ........................................................................................................................... 14 About the Author IDinsight  helps  clients  generate  and  use  rigorous  evidence  to  improve  social  impact. \nDepending on client needs, we help diagnose social sector challenges, design and test potential \nsolutions,  and  operationalize  those  solutions  found  to  be  most  impactful.  We  believe  that \nclient-centered, rigorous, and responsive evaluation is essential to help managers maximize \nprogram impact.", " About the Educate Girls Development Impact Bond The  Educate  Girls  Development  Impact  Bond  (EG  DIB)  is  a  joint  project  between  the \nChildren\u2019s  Investment  Fund  Foundation  (CIFF),  Educate  Girls  (EG),  the  UBS  Optimus \nFoundation,  Instiglio,  and  IDinsight  (collectively,  the  \u201cWorking  Group\u201d)  to  provide  and \nimprove education for girls in rural India. UBS Optimus, acting as the investor, financed EG\u2019s \nproject  implementation,  while  CIFF  will  pay  for  educational  outcomes  as  evaluated  by \nIDinsight. Instiglio is managing the project.", " A Note on Grade and Student Cohort Labels Over  the  course  of  the  three-year  evaluation,  IDinsight  tracked  five  different  grades  of \nstudents  as  they  progressed  through  school.  At  Baseline,  we  assessed  students  in grades  1 \nthrough 5. In each subsequent Endline, we assessed students who were then in grades 3, 4, \nand 5 (the target grades for Educate Girls\u2019 programming). Since a student\u2019s grade changes year \nto year, student cohort labels can be ambiguous; for instance, \u201cGrade 3\u201d could refer to three \ndifferent cohorts of students in the evaluation (students who were 3rd graders in Year 1, Year \n2 or Year 3 of the evaluation). To remove this ambiguity, in this report we refer to student \ncohorts according to their grade in Year 1, unless explicitly noted otherwise. We attach the \n\u201cY1\u201d suffix to grade labels to remind the reader of this convention. For instance, \u201c2Y1\u201d refers \nto students in grade 2 during the first year of the evaluation, who had progressed to grade 3 \nin Year 2 and grade 4 in Year 3.", " Table  1 shows how each  cohort progressed  through school  during  the  evaluation  and  how \nmany years students in the treatment group were potentially exposed to EG programming. \nGray cells indicate when the cohort was assessed by IDinsight.", " Table 1: Student Cohorts During the Evaluation Student cohort label  Grade level at each year of evaluation Years of exposure to \nEG program Baseline Y1 \nEndline Y2 \nEndline Y3 \nEndline Grade 1Y1 Grade 2Y1 Grade 3Y1 Grade 4Y1 Grade 5Y1 1. Executive Summary In this report, we present the results of IDinsight\u2019s three-year impact evaluation of Educate \nGirls\u2019 program in Bhilwara District in Rajasthan, India. The two outcomes described in this \nreport \u2013 learning gains of students enrolled in grades 3-5 and enrollment of out-of-school girls \n\u2013 will determine the payments in the Educate Girls Development Impact Bond.1 Educate Girls \nsurpassed the DIB targets for both learning gains and enrollment.", " Outcome 1: Learning gains By  the  end  of  Year  3,  students  in  treatment  villages  gained  an  additional  8,940  ASER \nlearning levels relative to students in control villages, representing 160% of the final target.2 Methodology: IDinsight conducted a three-year clustered randomized controlled trial in which \nwe  compared  students  in  schools  where  EG  operated  with students  in  control schools.  We \nassessed  students  on  basic  literacy  and  math  competencies  using  the  Annual  Status  of \nEducation Report (ASER) testing tool; a student\u2019s score on ASER determined her \u201clearning \nlevel,\u201d which is scored out of 16 points and forms the basis of the learning metric.  \nResults: On average, students in EG schools gained an additional 1.08 ASER levels compared \nto  students  in  control  schools  (p  <  0.01).  Differences  in  aggregate  learning  gains  between \ntreatment and control schools were much greater in Year 3 (+6,045 learning levels) than in Year \n2 (+1,434 levels) or in Year 1 (+1,461 levels).3 Outcome 2: Enrollment By the end of Year 3, Educate Girls enrolled 92% of all 837 eligible out-of-school girls in \ntreatment villages,4 representing 116% of the final target for enrollments.  \nMethodology:  IDinsight  used  a  simple  pre-post  comparison  to  verify  enrollments  of  out-of-\nschool girls in treatment villages. Due to the cost of conducting a census of all households, the \nWorking Group decided against estimating enrollments in control villages. Thus, unlike the \nlearning estimates, the enrollment estimates do not reflect a causal effect of EG\u2019s program.   \nResults:  Educate  Girls  enrolled  155  girls  in  Year  3,  or  19%  of  all  eligible  out-of-school  girls \nidentified since the start of the evaluation. Including the 613 enrollments from Years 1 and 2, \nEducate Girls enrolled a total of 768 out of 837 eligible out-of-school girls.", " Table 1: Summary of EG\u2019s performance against DIB targets Outcome Methodology Target Final Result Aggregate \nlearning gains for \nall students in \ngrades 3-5 Clustered \n(village-level) \nrandomized \ncontrolled trial +5,592 ASER \nlearning levels \nabove control \ngroup gains +8,940 ASER \nlearning levels \nabove control \ngroup gains Pre-post \ncomparison 79% of all eligible \nout-of-school \ngirls 92% of all eligible \nout-of-school \ngirls enrolled Enrollment of \nout-of-school \ngirls Performance as \nPercent of Target 160% 116% 1 Approximately 80% of the outcome payments are based on changes in learning levels. Approximately 20% are \nbased on changes in enrollment of out-of-school girls. See the Evaluation Design Memo for a full description of \nhow payments will be calculated. \n2 This target was revised down from 6,664 to 5,592 by the Working Group in Year 1. EG would have surpassed the \noriginal target by 34%. \n3 This is a slight change from the result reported in the Year 2 Endline report (1,314 learning for Year 2, 1,498 for \nYear 1), reflecting updates to the data made in Year 3 as per Appendix 14. \n4 At the beginning of the 2017-2018 academic year, Educate Girls identified 90 additional out-of-school girls along \nwith 88 girls already on the list who had left the area of program coverage. This resulted in a final population of \n837 out-of-school girls eligible for enrollment.", " 2. Outcome I: Learning Gains Methodology \nIDinsight conducted a three-year randomized controlled trial, clustered at the village level, to \nestimate learning gains attributable to EG\u2019s program.5 Sampling and Randomization \nThe evaluation was conducted in 332 schools across 282 villages in rural Rajasthan, which were \nselected according to the process outlined in Figure 1, below.", " Figure 1: Sampling and Randomization Protocol Note:  *  Village  and  school  eligibility  criteria  are  based  on  data  in  the  2014-15  DISE  database  unless  otherwise \nindicated. ** Students are considered assessed if at least one Endline score is available.", " Our  study  population  consists  of  all  students  who  were  enrolled  in  treatment  or  control \nschools  at  Baseline  as  well  as  students  who  enrolled  during  the  evaluation.6  In  the  results 5 See the Evaluation Design Memo for a full treatment of the methodology. \n6 For students in grades 1 and 2 at Baseline, we attempted to assess all students in the population. For students in \ngrades 3, 4, and 5 at Baseline, due to budgetary constraints we assessed a random sample of 69% of students in the \npopulation,  stratified  by  gender  and  grade.  During  analysis  we  apply  appropriate  sample  weights  to  these \nstudents\u2019 outcomes to recover population-level learning gains. For example, if 60% of eligible students in a school-\ngrade-gender cohort were sampled, then their learning gains are multiplied by 1/60% = 1.66 in the final analysis. \nIf 100% of eligible students in a school-grade-gender cohort were sampled (as with all grade 1 and 2 students), then \ntheir learning gains were multiplied by 1/100% = 1 in the final analysis.", " section below, we present average and aggregate results for the full sample of students unless \notherwise indicated. In the appendix, to provide points of comparison with previous reports, \nwe also present results separately for students present at Baseline (also called \u201cType I-III\u201d in \nthe Design Memo) and students absent at Baseline (\u201cType IV-V\u201d).7  If students were absent \nfrom school on the day of the assessment then we assessed them at home.8 We separately report learning gains of newly enrolled girls from EG\u2019s out-of-school girl lists, \nwhich are included in aggregate learning gains calculations and DIB payments. Since we did \nnot  collect  comparable  data  in  control  villages,  we  exclude  these  girls  from  the  average \ntreatment effect results.", " The third and final Endline was conducted between February 2 and February 28, 2018 and is \ndescribed in Appendix 3. Please refer to the Year 1 and Year 2 reports for further details on \ndata collection in those years.", " Student Assessments \nLearning  gains  were  measured  using  the  Annual  Status  of  Education  Report  (ASER) \nassessment tool (see Table 2 below and Appendix 18). The ASER assessment consists of three \nsections: Hindi, Math, and English. Each section consists of 5 levels (and a possible score of 1 \nto 5 points). IDinsight added one additional level to the Hindi section (\u201cStory Plus\u201d) to reduce \n\u201cceiling effects,\u201d in which the highest score on a section underestimates a student\u2019s true ability. \nThe  highest possible  total score  on  this  assessment  is  thus  16 points  (5  +  5 +  6);  the  lowest \npossible score is 3 points (1 + 1 + 1).", " Table 2: Learning Levels as Measured by ASER Level \n1 \n2 \n3 \n4 \n5 \n6 Hindi \nBeginner \nLetters \nWords \nParagraph \nStory 1 \nStory Plus Math \nBeginner \nNumbers 1-10 \nNumbers 11-99 \nSubtraction \nDivision \n\u2014 English \nBeginner \nCapital letters \nLowercase letters \nWords \nSentences \n\u2014 Calculating Learning Gains \nThe change in learning levels for each student is calculated by subtracting his or her total score \nat Baseline from his or her total score at Endline,9 with the following caveats: \u2022  Baseline scores for students in treatment and control schools who were not present at \nBaseline are imputed to be the lowest score possible (a score of 3) and any additional \nlearning levels achieved by those students at Endline are assumed to be gains.", " \u2022  Students with no Endline score from any round are not included in the analysis (466 students).", " 7 While secondary to the full sample results, we believe that distinguishing between students present at Baseline \nversus absent at Baseline is a useful robustness check. Students who were present at Baseline form a consistent \nsample throughout the three-year evaluation and are thus comparable between treatment and control schools. On \nthe other hand, students who were absent at Baseline are composed of both students who were absent but enrolled \nat  Baseline  and  students  who  enrolled  in  schools  later.  Since  EG\u2019s programming  includes  enrollment  activities, \nstudents who were absent at Baseline are not directly comparable between treatment and control schools, limiting \nour ability to make causal claims about their learning gains. \n8 Due to cost and logistical constraints we did not assess students in grade 5 at Baseline who were absent on the \nday of the assessment and had graduated out of the program after Year 1. Per the Working Group\u2019s decision in \nYear 2, the learning gains of these students were imputed based on the learning gains of students in grade 5 who \nwere present on the day of the Baseline assessment. \n9 This is a difference-in-differences estimator. For more information, see the Evaluation Design Memo.", " \u2022  For students who were assessed during multiple Endlines (for example, students who \nwere in grade 3 during Endline Year 1, grade 4 during Endline Year 2, and grade 5 \nduring Endline Year 5), only the final Endline score is counted.10 \u2022  We apply sampling weights to each group of students according to the proportion of students selected for assessment from this group.", " Findings We present both average treatment effects and aggregate treatment effects.11 Average treatment \neffects are the difference in average learning gains between treatment and control students,12 \nand  are  particularly  useful  for  understanding  the  magnitude  of  the  program\u2019s  impact  and \ncomparing it to other interventions. Aggregate treatment effects are calculated by adding up \nlearning  gains  of  all  students  in  treatment  schools  and  subtracting  learning  gains  of  all \nstudents in control schools, and therefore account for differences in the number of students in \ntreatment and control schools due to EG\u2019s enrollment activities and other factors.13 The final \nDevelopment Impact Bond payments are based on aggregate treatment effects.", " Learning Gains against the DIB Target \nStudents in EG schools gained on average an additional 1.08 ASER learning levels compared \nto students in control schools (p < 0.01).14 Learning gains for students in EG schools are 28% \nor  0.31  standard  deviations  larger  than  gains  for  students  in  control  schools,  comparing \nfavorably with primary school programs aimed at improving test scores in rural India.15 With these large learning gains, EG exceeded the three-year DIB aggregate treatment effect \ntarget.  By  the  end  of  the  three-year  program, students  in  treatment  villages  had  gained  an \nadditional 8,940 learning levels relative to students in control villages, representing 160% of \nthe  final  target  of  5,592.  Figure  2 shows  year-to-year  growth  in  the  difference  in  aggregate \nlearning  gains  between  treatment  and  control  students,  with  more  than  two-thirds  of  the \ndifference occurring in year 3.", " 10 33 students who should have graduated out of the program were retained. We assessed these students during \ntheir additional retention year and use their final score to calculate learning gains. \n11  We  present  average  and  aggregate  results  for  the  full  sample  of  students  unless  otherwise  indicated.  In  the \nappendix, to provide points of comparison with previous reports, we also present results separately for students \npresent at Baseline (also called \u201cType I-III\u201d in the Design Memo) and students absent at Baseline (\u201cType IV-V\u201d). \nWe separately report learning gains of newly enrolled girls from EG\u2019s out-of-school girl lists, which are included \nin aggregate learning gains calculations and DIB payments. Since we did not collect comparable data in control \nvillages, we exclude these girls from the average treatment effect results. \n12 Technically, we control for Baseline learning levels in a linear regression rather than subtracting Baseline learning \nlevels from Endline learning levels. \n13  By  using  aggregate  treatment  effects  as  the  DIB  payment metric,  EG  was incentivized  to  enroll  out-of-school \nstudents even if their learning levels were very low and would bring down the school average.  \n14  The  difference  in  learning  gains  is  statistically  significant  at  the  1%  level.  This  means  that  the  probability  of \nobserving this difference due to random chance, if the treatment effect is zero, is less than 1%. Since this probability \nis  very  low,  we  reject  the  null  hypothesis  that  the  gains  in  learning  levels  were  equal  in  program  and  control \nvillages. Due to randomization we can reasonably expect that, on average, the only difference between students in \ntreatment villages and students in control villages is that the former have been exposed to Educate Girls\u2019 program. \nBalance checks presented in the Baseline report show that there are no statistically significant differences between \nthe control and treatment groups across any of the variables collected. \n15 According to an evidence review of education evaluations in developing countries conducted by the Abdul Latif \nJameel Poverty Action Lab, an increase in test scores of less than 0.1 SD is typically considered to be a small effect, \nwhile an increase of more than 0.3 SD is considered a large effect, and an increase of more than 0.5 SD a very large \neffect. Among the programs included in the evidence review, the Balsakhi Program, a remedial tutoring education \nintervention implemented in schools in Vadodara and Mumbai, may be the most similar to EG\u2019s program. In that \nevaluation, the Balsakhi program increased average test scores by 0.28 standard deviation (Banerjee et al. 2007). \nThe  same  evaluation  found  no  discernible  impact  of  reducing  class  sizes  on  test  scores.  Other  evaluations  of \nprimary school programs in rural India have found effects on math and language test scores ranging from 0.16 to \n0.47 standard deviations (e.g. Duflo, Hanna, and Ryan 2012; Muralidharan and Sundaraman 2012; Banerjee et al. \n2007).", " These differences resulted from a combination of increased learning and increased enrollment \nin treatment schools, though relatively more from learning. By the end of Year 3, our study \npopulation included 7,318 students in treatment schools and 6,786 students in control schools, \nreflecting a modest increase in enrollment due to EG\u2019s program. The majority of this difference \ncan  be  explained  by  the  42116  out-of-school  girls  EG  reported  enrolling  in  grades  3  to  5  in \ntreatment  schools  during  the  study.  Excluding  learning  gains  among  these  newly-enrolled \ngirls,  students  in  treatment  schools  gained  7,719  more  learning  levels  than  their  peers  in \ncontrol  schools,  representing  86%  of  the  difference  in  aggregate  learning  gains  between \ntreatment and control schools.", " Appendices  10  and 11 provide  additional  detail about  how  aggregate  learning gains  break \ndown across grade and student type.", " Figure 2: Aggregate Learning Gains (Treatment-Control) by Year Learning Gains by Cohort \nTreatment  effects  vary  across  grades  and  years.  Figure  3  shows  average  learning  gains  for \ntreatment and control students by grade at Baseline.17 Grade 1Y1 refers to students who were \nin grade 1 at Baseline, Grade 2Y1 to students who were in grade 2 at Baseline, and so forth. \nEach year, EG\u2019s program targeted students in grades 3-5. Hence, students in Grade 1Y1 entered \nthe program for the first time in Year 3, and Grade 5Y1 students exited the program after the \nfirst year.  Students in Grade 3Y1 were the only cohort to receive the program for all three years.", " Figure 3 provides two major insights. First, program impact increases with years of program \nexposure. Students in Grade 3Y1, who were exposed to EG\u2019s programming for all three years, \nhad the largest learning gains of any cohort. Second, EG\u2019s intervention in Year 3 was far more 16  While  girls  enrolled  from  the  list  of  eligible  out-of-school  girls  were  counted  towards  the  enrollment  target \nregardless of their grade, their learning gains were only assessed if they were in grades 3-5 at the time of one of the \nEndline surveys.  \n17 Figure 3 omits students who were absent at Baseline since these students were only assessed during the Year 2 \nand Year 3 Endlines. In Appendix 6, 7, and 11 we present final results for all student types.", " effective  than  in  previous  years.18  Students  who  participated  in  the  program  in  Year  3 \nbenefitted 2-3 times more than their peers who had aged out of the program prior to Year 3. \nTreatment students in Grades 2Y1 and 3Y1 grew an astonishing 79% more during the final year \nof the program than their peers in control schools.", " Figure 3: Average Learning Levels by Cohort Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Lines omit students absent at baseline (since they do not have a baseline \nscore), though average treatment effects (ATEs) include all students. ATEs denote the difference in average learning \ngains between students in program schools and students in control schools.", " Figure 4 shows the effect of EG\u2019s program on learning gains by project year for each grade \ntargeted by EG (grades 3-5). Hence, each bar denotes the additional learning gains achieved \nin  program  schools  within  that  year  compared  with  gains  among  comparable  students  in \ncontrol  schools.  For  instance,  the  first  bar  shows  the  difference  in  average  learning  gains \n(+0.18) for students in Grade 3 during the first year of the program (2015-16, corresponding to \ncohort  3Y1),  and  the  second  bar  shows  the  difference  in  average  learning  gains  (+0.09)  for \nstudents in Grade 3 during the second year of the program (2016-17, corresponding to cohort \n2Y1).", " Across  all  grades,  the  one-year  effects  of  the  program  in  Year  3  far  exceed  the  effects  in \nprevious years. The difference is greatest for students in Grade 3: whereas the program did \nnot have  a statistically significant  effect  on  learning  gains  for  Grade  3  students  in  previous \nyears, in the final year of the program Grade 3 students made gains comparable to older peers.", " 18  The  structure  of  the  Development  Impact  Bond  gave  EG  the  flexibility  to  revise  its  teaching  intervention \nthroughout the three-year project.", " Figure 4: One-Year Average Treatment Effects by Grade and Year Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Range bars denote 95% confidence intervals. Since we did not assess students \nat the beginning of grade 3 in Year 2 and 3, we calculate grade 3 treatment effects using baseline scores for those \ncohorts. The one-year comparison therefore assumes that any treatment effects for these cohorts occurred during \nGrade  3  only.  The  yearly  average  treatment  effects  for  each  cohort  do  not  sum  exactly  to  the  overall  average \ntreatment effect for that cohort since the yearly average treatment effects do not account for students who have \ndropped out or have been retained.", " Learning gains by subject, gender, and geography \nFigure  5  shows  average  learning  gains  for  all  students  by  subject  and  treatment  status. \nProgram impacts were concentrated in Math and English, where the treatment effects were \napproximately 3 times larger than in Hindi.19 Appendix 12 further shows that students with \nlow baseline scores, especially in Math and English, benefitted the most from EG\u2019s program.", " As in previous years, average treatment effects were larger for students in Bijoliya block than \nfor students in Mandalgarh and Jahajphur. Girls benefitted slightly more than boys (+1.13 vs. \n+1.04).", " 19 Appendix 7 shows treatment effects separately for students present at Baseline and absent at Baseline.", " Figure 5: Average Learning Gains by Subject and Treatment Status Note: * p < 0.1, ** p < 0.05, *** p < 0.01. Average treatment effects (ATEs) denote the mean difference in learning \ngains  between  students  in program  schools and  students  in  control schools.  Range bars  denote  95% confidence \nintervals. The figure includes data from all Endlines. For a subject-wise analysis of average treatment for Year 3, \nsee Appendix 8.", " 3. Outcome II: Enrollment of Out-of-School Girls Methodology Educate Girls compiled and maintained a census of out-of-school girls in treatment villages, \nwhich  IDinsight  validated  each  year  according  to  the  process  shown  in  Figure  6.  Due  to \nbudgetary constraints, the DIB Working Group decided not to conduct a parallel census of \nout-of-school girls in control villages. As a result, we cannot rule out the possibility that other \nfactors besides the Educate Girls program influenced enrollment in treatment villages.", " Figure 6: Enrollment Verification Process To validate enrollment each year, IDinsight surveyors visited each school in which a girl was \nreported enrolled and presented the headmaster with a form that included the girl\u2019s name, \ncaste,  age,  and  father\u2019s  name.  Headmasters  were  requested  to  verify  this  information  by \nsigning the IDinsight form as well as by showing surveyors the register.", " Findings Figure 7 shows the results of this validation exercise. Including the enrollments from Year 1 \nand Year 2, EG enrolled 768 out-of-school girls, representing 92% of the 837 eligible20 out-of-\nschool girls.21 EG exceeded the enrollment target of 79% by 13 percentage points, or 16%.", " Figure 7: Enrollments of Out-of-School Girls by Year Note: Percentages refer to the percent of enrolled girls relative to the Year 3 target of 837 eligible girls. The list of \neligible out-of-school girls was updated each year to include newly-eligible girls and exclude newly-ineligible \ngirls.", " 4.  Conclusion Educate Girls exceeded the 3-year DIB targets in both learning and enrollment. Students in \nprogram  villages  gained  an  additional  8,940  ASER  learning  levels  relative  to  comparable \nstudents  in  control  villages,  surpassing  the  learning  target  set  by  the  Development  Impact \nBond  by  60%.  The  effects  of  Educate  Girls\u2019  program  on  learning  gains  were  large  and \nstatistically significant over the three-year program: Students in EG schools gained on average \nan additional 1.08 learning levels, or 28%, compared to students in control schools.", " Learning gains were higher for treatment students than for control students across all grades \nand subjects, with relatively higher gains in Math and English than in Hindi and relatively \nlarger treatment effects among students who were exposed to the program for more years. \nEG\u2019s program in Year 3 was particularly effective in increasing test scores.", " By  the  end  of  the  three-year  project,  Educate  Girls  had  enrolled  768  out-of-school  girls, \nrepresenting 92% of all identified out-of-school school girls eligible for enrollment. Educate \nGirls thus exceeded the enrollment target of 79% by 16%.", " 20 Girls are eligible for enrollment if they are between 7 and 14 years old, live in treatment villages, and have not \npreviously been reported enrolled by Educate Girls. \n21  In  Year 3  EG  reported  enrolling 155  girls,  including four  girls whose  enrollment in  the  Rajasthan State Open \nSchool (RSOS) will be verified in July 2018. IDinsight was able to verify 148 of the Year 3 enrollments for an error \nrate of 2%, well below the threshold of 10%. Hence, all 155 girls reported by EG are counted towards the target.", " Appendix Appendix 1: Description of Educate Girls\u2019 Intervention Enrollment \nEducate Girls delivers a comprehensive community intervention to enroll girls into school. \nThis intervention includes identification of out-of-school girls through door-to-door surveys, \nexplanation of the value of schooling to their parents and to the community, and multi-\nchannel engagement with households with unenrolled girls. Educate Girls also uses multiple \ninterventions to improve school attendance and prevent drop-outs, such as frequent parent \ncounselling sessions and working with School Management Committees to improve school \ninfrastructure. It also identifies girls who have dropped out and works with the community \nto re-enroll them into school.  \n \nLearning \nEducate Girls trained volunteers to deliver a child-centric curriculum one to five times a week \nto boys and girls in Grades 3-5. Volunteers were often drawn from the villages in which they \nworked.  They  were  incentivized  with  a  small  number  of  skill  and  career  development \nopportunities,  such  as  free  English  classes  and  the  possibility  of  being  hired  by  EG  in  the \nfuture. \n \nIn Year 3, EG rolled out a new curriculum called \u201cGyan Ka Pitara\u201d (\u201cKnowledge Box\u201d). As \npart  of  this  new  curriculum,  EG  increased  the  number  of  teaching  sessions  per  day  and \nconducted  home  visits  to  reach  students  who  were  frequently  absent  from  school  or  who \nneeded  remedial  tutoring.  In  addition  to  the  thrice  yearly  rounds  of  student  assessments \nconducted  previously  in  Years  1  and  2,  EG  conducted  three  additional  rounds  of  ASER \nassessments in Year 3. These additional assessments led EG to identify areas of improvement, \nwhich  informed  adjustments  to  the  clustering  of  schools  for  program  implementation,  the \ntraining  of  volunteers,  and  the  content  of  remedial  classes.  School  teachers  were  also  more \ninvolved in programming in Year 3 through school meetings and block review meetings.", " Appendix 2: Description of Student Types The  Evaluation Design  Memo  outlines  five student  types,  which  together make  up  the  full \npopulation of students assessed in the evaluation. The interpretation of student types slightly \ndeviates  from  what  is  suggested  in  the  Evaluation  Design  Memo,22  but  was  held  constant \nthroughout the three Endline data collection exercises and analyses.", " Consolidated \nStudent Group Student \nType Status at Baseline Status at Endline Students \nPresent at \nBaseline Students \nAbsent at \nBaseline Type I Enrolled in Grades 1-5 Enrolled, present at school, assessed Type II Enrolled in Grades 1-5 Enrolled, absent at school, assessed Type III Enrolled in Grades 1-5 Not assessed (enrolled or \nunenrolled, present or absent)23 Type IV Absent or unenrolled Enrolled, present at school, assessed Type V Absent or unenrolled Enrolled, absent at school, \nassessed24 Newly \nEnrolled Girls -- Unenrolled Enrolled by EG; present or absent Appendix 3: Data Collection for the Year 3 Endline IDinsight conducted the third and final Endline between February 2 to February 28, 2018, \naccording to the following protocol: \u2022 IDinsight visited a total of 32525 schools.26 \u2022  Out of a sample of 8,237 students (4,211 in treatment, 4,026 in control), we successfully \nassessed 7,655 students in grades 3-5, or 93% of all sampled students (92% in treatment, \n93% in control).", " \u2022  We also assessed 198 newly enrolled girls in grades 3-5, representing 73% of the newly- enrolled girl population of 272 girls eligible for assessment in Year 3.27 \u2022  74% of students were assessed at the school while 26% were assessed at their home. In \nthe majority of cases in which we were not able to assess a child at their home, it was 22 For example, Type III students are considered to be students who drop out from the sample (i.e., their last Endline \nscore is not available) rather than students who dropped out of school.  \n23 Most students not assessed at Endline are students who dropped out from school and permanently or temporarily \nmigrated. However,  students  enrolled in  school were also  sometimes unable  to  be  assessed  (for  example,  if  the \nchild was ill or the child or family did not consent to being assessed).  \n24 Some Type IV/V students may not have been assessed during their last Endline. As with Type III students, we \ninclude their latest available score in the calculation of learning gains. \n25  There  were  332  schools  in  the  original  sample.  In  two  cases,  treatment  and  control  schools  merged.  Per  the \nWorking Group\u2019s decision, IDinsight dropped schools affected by treatment/control merges from the sample (a \ntotal of four  schools).  There  were  three  other  in-sample merge  cases  (treatment  school  closed  and merged with \nanother treatment school or control school closed and merged with control school), which reduced the number of \nschools to be visited by an additional three schools to 325 schools. In these in-sample merge cases, IDinsight found \nand surveyed the affected students at home or at their new school. For more information on how school merge \ncases were dealt with, please refer to Appendix 16. \n26 In keeping with the pairwise matching design described in the Baseline Report, students in control villages were \nin most cases assessed in the same week and by the same surveyors as their treatment equivalents to reduce time \nand surveyor effects. \n27 Many girls enrolled by EG dropped out again and/or permanently migrated, making it harder for surveyors to \nassess them.", " because  the  family  had  moved  temporarily  or  permanently  to  areas  too  far  for \nsurveyors to reach.28 \u2022  Children were presented with paper copies of the ASER assessment and their answers \nwere recorded on smartphones via the SurveyCTO electronic data collection interface \nused  in  the  Baseline  and  previous  Endline  assessments.  Information  about  school \ninfrastructure and staffing was collected from the headmaster or head teacher in each \nschool or by direct observation.", " Appendix 4: Descriptive Student Statistics Variable Average \n(All) Std. Dev. \n(All) Average \nTreatment Average \nControl p-Value of \nDifference 0.18 0.39 0.12 0.24 0.11 0.41 0.49 0.35 0.44 0.39 1.39 0.5 1.74 0.5 1.69 1.05 0.5 1.52 0.5 1.31 0.5 2.13 3.1 0.48 8.19 0.49 2.66 2.36 1.89 1.98 0.5 7.42 0.51 - - - 3.08 0.48 8.21 0.49 2.71 2.43 1.93 1.93 0.54 7.4 0.52 - - - 0.48 0.9 0.64 0.69 0.5 0.11 0.47 0.19 0.3 0.76 0.61 - - - 3.09 0.48 8.2 0.49 2.68 2.4 1.91 1.95 0.52 7.41 0.51 2.92 0.51 9.59 Grade (1-5) SC or ST caste \n(fraction of total) Age Female (fraction of \ntotal) Hindi Level (1-6) Math Level (1-5) English Level (1-5) English Word \nComprehension \n(fraction answering \ncorrectly) English Sentence \nComprehension \n(fraction answering \ncorrectly) Grade (1-5) SC or ST caste \n(fraction of total) Female (fraction of \ntotal) Grade (1-5) SC or ST caste \n(fraction of total) Age Children \nPresent at \nBaseline Children \nAbsent At \nBaseline Age Newly \nEnrolled \nGirls Note: The p-value indicates the likelihood of the difference in means between treatment and control being this large \n(or larger) by random chance if the difference in means was zero. Age and grade of students absent at Baseline and \nnewly enrolled girls were imputed by subtracting the number of years passed since Baseline. For example, an 8-\nyear-old child in grade 3 during Year 3 Endline is shown as a 6-year-old child in grade 1 in this table.", " 28 If available, we use the most recent assessment of these children for the calculation of learning gains.", " Appendix 5: Average Treatment Effects as ASER Levels and Standardized Effects Grade at \nBaseline Years of exposure \nto EG program Average learning gains Treatment \nstudents Control \nstudents Difference Difference \n(std effects) p-Value 1 \n2 \n3 \n4 \n5 1 \n2 \n3 \n2 \n1 Total 5.97 \n6.76 \n6.13 \n3.59 \n1.32 4.96 4.59 \n5.40 \n4.43 \n3.06 \n0.84 3.88 1.38 \n1.35 \n1.71 \n0.52 \n0.48 1.08 0.46 \n0.41 \n0.50 \n0.16 \n0.28 0.31 1.36 \n1.23 \n1.72 \n0.39 \n\u2013 1.26 <0.01 \n<0.01 \n<0.01 \n<0.01 \n<0.01 <0.01 <0.01 \n<0.01 \n<0.01 \n0.36 \n\u2013 <0.01 Note: Treatment effects are presented as raw differences in scores and as standardized effect sizes. Standardized \ndifferences are calculated by subtracting the control mean and dividing by the control standard deviation for each \ngrade.  Standardized  effects reflect  the magnitude of  gains  in  the  treatment  group  relative  to  the  distribution  of \nlearning gains and are useful for benchmarking treatment effects against impact estimates from outside programs. \nThe p-value indicates the likelihood of the difference in means between treatment and control being this large (or \nlarger) by random chance if the treatment effect was zero.", " Appendix 6: Average Treatment Effects by Baseline Grade and Student Type All students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Grade 1 \n2 \n3 \n4 \n5 Total 1.38 \n1.35 \n1.71 \n0.52 \n0.48 1.08 <0.01 \n<0.01 \n<0.01 \n<0.01 \n<0.01 <0.01 1.44 \n1.55 \n1.70 \n0.69 \n0.48 1.07 <0.01 \n<0.01 \n<0.01 \n<0.01 \n<0.01 <0.01 Note: \u201cDifference\u201d shows the raw difference in learning gains between students in treatment villages and students \nin  control  villages  (treatment-control).  The  p-value  indicates  the  likelihood  of  the  difference  in  means  between \ntreatment and control being this large (or larger) by random chance if the treatment effect was zero.", " Appendix 7: Average Treatment Effects by Subject and Student Type Subject All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value Hindi \nMath \nEnglish Total 0.14 \n0.44 \n0.50 1.08 0.14 \n0.45 \n0.48 1.07 <0.01 \n<0.01 \n<0.01 <0.01 0.19 \n0.49 \n0.58 1.26 0.17 \n0.00 \n0.00 0.00 Note: The table shows subject-wise average treatment effects for all students. \u201cDifference\u201d shows the raw difference \nin learning gains between students in treatment villages and students in control villages (treatment - control). The \np-value indicates the likelihood of the difference in means between treatment and control being this large (or larger) \nby random chance if the treatment effect was zero.", " 0.03 \n<0.01 \n<0.01 <0.01 Appendix 8: Average Treatment Effects by Subject and Student Type for Year 3 Subject Hindi \nMath \nEnglish Total All Students Present at Baseline Absent at Baseline Difference p-Value Difference p-Value Difference p-Value 0.20 \n0.59 \n0.68 1.47 0.04 \n<0.01 \n<0.01 <0.01 0.21 \n0.66 \n0.72 1.59 0.02 \n<0.01 \n<0.01 <0.01 0.21 \n0.53 \n0.65 1.39 0.13 \n<0.01 \n<0.01 <0.01 Note: The table shows subject-wise average treatment effects for students assessed in the Year 3 Endline (students \nin  Grades  1Y1,  2Y1,  and  3Y1).  \u201cDifference\u201d  shows  the  raw  difference  in  learning  gains  between  students  in \ntreatment villages and students in control villages (treatment - control). The p-value indicates the likelihood of the \ndifference in means between treatment and control being this large (or larger) by random chance if the treatment \neffect was zero.", " Appendix 9: Total Aggregate Learning Gains from Baseline for All Student Types By Year 1 Endline By Year 2 Endline By Year 3 Endline Total \nShare of Target (5,592) 1,461 26% 2,895 52% 8,940 160% Note: Results by Year 1 and Year 2 slightly deviate from the results reported after the Year 2 Endline (2,812 learning \nby Year 2, 1,498 by Year 1), reflecting updates made in Year 3 as per Appendix 14.", " Appendix 10: Aggregate Learning Gains by Baseline Grade, Year, and Type Grade \nat Baseline Year 1 Difference \nfrom Baseline Year 2 Difference  \nfrom Baseline Year 3 Difference  \nfrom Baseline Present at Baseline, Types I-III 1 \n2 \n3 \n4 \n5 \nTotal 1 \n2 \n3 \n4 \n5 \nTotal 1 \n2 \n3 \n4 \n5 \nTotal 237 \n400 \n549 \n1,186 \u2013 \n\u2013 \n\u2013 \n- 93 \n81 \n101 \n275 Absent at Baseline, Types IV-V Newly Enrolled Girls 162 \n642 \n949 \n \n2302 -245 \n64 \n31 \n96 \n-54 \u2013 \n130 \n178 \n238 \n \n647 856 \n877 \n1905 \n \n \n5136 920 \n583 \n938 \n \n \n2583 227 \n254 \n401 \n \n \n1221 Note: Scores in bolded text represent the cohort\u2019s final score. While the total aggregate gains are consistent, the sub-\naggregate gains of some student types may differ by one learning gain from the numbers reported in Appendix 10 \ntext due to rounding weighted gains at different steps of the calculation. Appendix 10 represents the final result.", " Appendix 11: Breakdown of Learning Gains from Baseline by Grade and Type Grade at Baseline Present at Baseline, \nAssessed at Endline \nType I-II Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Population: Assessed in Y1 but not Y2 or Y3 Sampled: Assessed in Y1 but not Y2 or Y3 Present at Baseline, \nNot Assessed \nat Endline \n(Type III) Average treatment effect (Y1) p-Value (Y1) Aggregate gains (Y1) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Absent at Baseline, \nAssessed at Endline \n(Type IV/V) p-Value (Y2) Aggregate gains (Y2) Population Sampled Average treatment effect p-Value Aggregate gains Population Sampled Absent at Baseline, \nNot Assessed \nat Endline \n(Type IV/V) Population: Assessed in Y2 but not Y3 Sampled: Assessed in Y2 but not Y3 Average treatment effect (Y2) Newly Enrolled \nGirls p-Value Aggregate gains (Y3) Population/Sampled Assessed in Y1 but not Y2 or Y3 Average treatment effect (Y1) Aggregate Gains (Y1) Assessed in Y2 but not Y3 Average treatment effect (Y2) Aggregate Gains (Y2) Assessed in Y3 Average treatment effect (Y3) Aggregate Gains (Y3) <0.01 <0.01 <0.01 <0.01 <0.01 1.44 1.58 1.36 4.73 1.54 0.00 1.22 -0.34 0.91 -68 1.20 3.28 1.74 -0.14 0.69 -7 1.64 0.06 1.72 0.57 0.10 -15 1.63 4.96 0.75 0.71 0.23 0.39 0.36 3.01 1.00 6.00 0.48 \u2013 \u2013 \u2013 \u2013 1.94 All -83 850 \n8940 <0.01 <0.01 <0.01 TOTAL Aggregate Gains Note: The calculated learning gains in this table represent the final result. The sub-aggregate gains of some student types may \ndiffer  by one  learning  gain  in  other  tables  due  to  rounding  weighted gains at  different  steps  of  the  calculation  of  aggregate \nlearning gains. In Year 1, the Working Group decided to impute learning gains for students not present at Baseline in grade 5 \nsince they were not included in the Year 1 sample and would have graduated from the program in Year 2. The Working Group \nagreed to err on the side of overestimating learning gains for this group by assuming that the effect of Educate Girls\u2019 program \non students not assessed at Baseline in grade 5 were the same as the effect on students assessed at Baseline.", " Appendix 12: Sub-Group Analysis by Caste, Grade, Block, and Baseline Scores Subgroup Difference Average \n(Treatment) \n5.90 Average \n(Control) \n4.58 p-Value of \nDifference \n<0.01 3.86 \n3.88 3.78 3.98 3.77 3.35 \n4.22 3.77 1.86 1.91 \n1.50 1.06 \n0.60 \n-0.16 1.36 0.91 0.62 0.26 -0.33 1.19 0.97 0.48 0.13 -0.30 2.94 1.31 0.91 \n1.26 0.86 1.04 1.13 1.62 \n0.78 1.14 0.15 0.16 \n0.36 0.10 \n0.01 \n-0.10 0.46 0.49 0.42 0.16 0.20 0.57 0.42 0.35 0.37 0.07 1.07 <0.01 \n<0.01 <0.01 <0.01 <0.01 <0.01 \n<0.01 <0.01 0.09 <0.01 \n0.06 0.08 \n0.73 \n0.03 <0.01 <0.01 <0.01 0.02 <0.01 <0.01 <0.01 <0.01 0.03 0.63 <0.01 Caste Category Gender Block Hindi Score \nat Baseline Math Score \nat Baseline English Score \nat Baseline Total General OBC \nSC ST Boy Girl Bijoliya \nJahajpur Mandalgarh 2 \n3 4 \n5 \n6 4.78 \n5.14 4.64 5.02 4.90 4.98 \n5.00 4.91 2.01 2.07 \n1.86 1.17 \n0.61 \n-0.25 1.82 1.39 1.04 0.42 -0.13 1.76 1.40 0.83 0.50 -0.23 4.01 Note:  Newly-enrolled  girls  are  omitted  from  all  analyses  and  students  absent  at  baseline  are  omitted  from  the \nanalysis of performance at baseline. For the subgroup analyses by caste category, gender, and block, mean values \nrepresent total learning gains (across all subjects). As a reminder, students who are absent at baseline are imputed \nthe lowest possible score (3 out of 16 points), which explains the high learning gains for subgroups including those \nstudents (since this imputation is done for both Treatment and Control students it does not affect the unbiasedness \nof the ATE estimator). For the subgroup analyses by baseline scores, mean values represent learning gains in the \nrespective  subject  (for  students  present  at  baseline).  The  p-values  in  this  table  are  the  likelihoods  that,  if  the \ntreatment effect is zero, then the difference in means between treatment and control could be this large by random \nchance.", " Appendix 13: Assessment Location of Students Student Type At School At Home Students Present at Baseline Students Absent at Baseline 78% 73% 22% 27% Appendix 14: Newly Enrolled Girls since Baseline By Year 1 Endline By Year 2 Endline By Year 3 Endline A: Girls Enrolled \nB: Girls Eligible for Enrollment \nC: Share of Girls Enrolled against \nFinal Target (837 Girls)  \nD: Share of Target (D=C/79%) 322 \n744 \n38% 48% 613 \n835 \n73% 92% 768 \n837 \n92% 116% Appendix 15: Changes to Year 1 and Year 2 Results IDinsight made updates to the data from Year 1 and Year 2, leading to small changes in \nthe calculated aggregate learning gains by Year 1 and 2. These changes represent 0.01% \n(Year 1) and 2.1% (Year 2) of the final target.", " \u2022  Students in grades 4 and 5 at Baseline were expected to progress to grades 6 and 7 by \nYear 3. However, 32 students from Baseline grades 4 and 5 were still in grades 3-5 at \nthe time of the Year 3 Endline, and thus assessed this year. Likewise, two students from \nBaseline grade 5 were still in grade 5 during the Year 2 Endline. We included these \nassessments in the final calculation of learning gains, leading to changes in the learning \ngains of students in grades 4Y1 and 5Y1 despite these cohorts generally not being part of \nthe Year 3 student assessments.", " \u2022  26  children  present  at  Baseline  subsequently  dropped  out  of  school  and  were  later \nenrolled by EG. We shifted these students from Type I-III to the Newly Enrolled Girls \ncategory.  Since  100%  of  Newly  Enrolled  Girls  were  sampled,  their sampling  weight \nwas changed to 1. The remaining Type I-III students in the cohorts from which these \nstudents were removed kept their original sampling weights.", " \u2022  During the third round of student assessments, we identified 64 students who were \nlisted  twice  on  our  student  lists.  While  none  of  them  have  been  assessed  twice, \nremoving these duplicates affects sampling weights.", " \u2022  We  made  updates  to  school  assignments  for  several  students  who  were  incorrectly \nattributed  to  schools  with  similar  names  (e.g.,  Ragunathpura  vs.  Ragunathpra  and \nRampuriya vs. Rampuria) leading to small changes in sampling weights.", " Appendix 16: Descriptive Statistics of Schools Surveyed in Year 3 Variable  \n(* indicates average if answer to \npreceding question is \u201cyes\u201d) Average \n(All) Std. Dev. \n(All) Average \n(Treatment) Average \n(Control) p-Value of \nDifference 11.34 11.18 # of Headmasters (Appointed) # of Headmasters (Observed) # of Teachers (Appointed) # of Teachers (Observed) # of Parateachers (Appointed) # of Parateachers (Observed) Existence of SMC # of SMC Members* # of SMC Meetings* Mid-Day Meal Served School Kitchen Available Observed Food Served Evidence of Mid-Day Meal # of Pucca (Permanent) Rooms # of Rooms for Teaching Play Area Usable Equipment in Play Area Sports Equipment Library Books Children Using Books* Handpump/Tap Handpump in Usable Condition* Source of Drinking Water Electricity Electricity at Visit* School Wall/Boundary Computers Children Using Computers* Tables and Chairs Available Dari (Carpet) for Seating Usable Blackboard Other Learning Materials in Classroom Common Toilet Girls Toilet Boys Toilet 0.32 0.3 2.88 2.52 0.12 0.08 12.6 0.84 0.95 0.84 0.73 5.69 2.56 0.72 0.15 0.91 0.75 0.7 0.66 0.83 0.46 0.46 0.66 0.5 0.03 0.27 0.06 0.97 0.92 0.21 0.94 0.93 0.47 0.46 1.82 1.76 0.41 0.28 3.68 55.39 0.37 0.21 0.37 0.44 2.47 1.83 0.45 0.36 0.89 0.83 0.46 0.47 0.37 0.57 0.5 0.47 0.5 0.18 0.47 0.24 0.16 0.27 0.41 0.23 0.26 0.31 0.31 3.01 2.6 0.11 0.08 11.5 15.55 0.87 0.95 0.83 0.73 5.64 2.55 0.7 0.15 0.8 0.79 0.71 0.69 0.84 0.43 0.44 0.67 0.5 0.04 0.17 0.06 0.97 0.93 0.18 0.95 0.92 0.32 0.29 2.75 2.44 0.12 0.08 9.54 0.82 0.96 0.84 0.73 5.74 2.57 0.74 0.15 1.03 0.71 0.68 0.64 0.83 0.48 0.47 0.66 0.5 0.03 0.4 0.06 0.97 0.9 0.24 0.94 0.93 0.97 0.69 0.24 0.53 0.79 0.85 0.53 0.32 0.23 0.78 0.74 0.97 0.69 0.94 0.43 0.86 0.02 0.39 0.77 0.25 0.93 0.4 0.61 0.95 0.96 0.99 0.14 0.8 0.98 0.42 0.13 0.64 0.66 0.79 Total Enrollment Grades 1 to 5 45.18 23.31 45.63 44.71 Note: Data from 320 schools from Year 3 Endline. The p-values in this table are the likelihoods to observe differences \nin means between treatment and control this large (or larger) by random chance if there were no mean differences \nbetween treatment and control schools.", " Appendix 17: Merged Schools Treatment-Control Merge Cases \nAs per the Working Group\u2019s decision from 2017, in cases where a treatment school closed and \nmerged with a control school or a control school closed and merged with a treatment school, \nstudents have not been assessed after the school merge occurred. However, all learning gains \nthat  were  captured  before  the  schools  merged  are  included  in  the  calculation  of  outcome \npayments.", " Treatment school closed and merged with control school School G.P.S. NAYA GAU DISE Code School Merged With G.P.S.JORA JI KA KHERA DISE Code Control school closed and merged with treatment school School G.P.S. HIMMAT PURA DISE Code School Merged With DISE Code G.P.S. BHEROO KA RADHA Out-of-Sample Merge Cases \nIn cases where an in-sample school merged with an out-of-sample school, we continued to \nassess all sampled students from the in-sample school. IDinsight did not assess any students \nthat were previously enrolled in out-of-sample schools.", " Treatment school closed and merged with out-of-sample school DISE Code DISE Code School G.P.S. GOPALPURA G.P.S. MANAK CHOUK G.P.S NANA BABA KA JHUPRA G.P.S. PIPALDA School Merged With G.P.S. MAGANPURA G.G.U.P.S. MAHUO G.P.S. BHAIRU KA KHERA G.S.K.P.S. RAMPURIYA Control school closed and merged with out-of-sample school School School Merged With G.P.S. KANJORA KA JOPARA G.S.S.S. RAJGARH SARTHALA G.P.S. BAGTHALA G.S.S.S. RAJGARH Out-of-sample school closed and merged with in-sample school School G.P.S. LAXMIPURA G.P.S. RATANPURA School Merged With DISE CODE G.U.P.S. DAGARIYA G.P.S. JAJARPURA In-Sample Merge Cases \nIn cases where a treatment school merged with another treatment school or a control school \nmerged with another control school, IDinsight continued to assess all sampled students from \nboth schools.", " School Treatment school closed and merged with another treatment school \nDISE Code School Merged With DISE Code Year Y2 G.P.S. BHIL BASTI G.P.S. MEENA KA \nJHONPARIYA G.P.S. BILIYA KA JOPHDA G.U.P.S. BILIYA Y2 Control school closed and merged with another control school School DISE Code School Merged With G.P.S. BHARJI KA KHERA DISE Code Year Y2 G.U.P.S. SHAKTA JI KA \nKHERA Year Y2 Year Y2 Year Y2 Y2 Y2 Y3 Year Y3 Y3 Year Y3 Y3 Appendix 18a: ASER Testing Tool for Hindi in Year 3 Endline HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 \u0936\u092c\u094d\u0926 \u0905\u0915\u094d\u0937\u0930 \u0905\u0928\u0941\u091a\u094d\u091b\u0947 \u0926 \u0917\u093e\u0928\u093e \u0916\u0941\u0936 \u092c \u0935 \u0930\u093e\u0928\u0940 \u0928\u0926\u0940 \u0915\u093f\u0928\u093e\u0930\u0947 \u0930\u0939\u0924\u0940 \u0939\u0948| \u092e\u094c\u0938\u0940 \u0916 \u0906\u093f\u0942 \u0916\u0947\u0924 \u0939 \u091d \u0928\u0926\u0940 \u092e\u0947\u0902 \u092c\u0939\u0941\u0924 \u092e\u091b\u0932\u093f\u092f\u093e\u093e\u0901 \u0939\u0948\u0902| \u0930\u093e\u0928\u0940 \u0909\u0928\u093f\u094b \u0926\u093e\u0928\u093e \u0926\u0947\u0924\u0940 \u0939\u0948| \u0926\u0926\u0928 \u0938 \u0935\u0947 \u0938\u092c \u092e\u091c\u0947 \u0938\u0947 \u0926\u093e\u0928\u093e \u0916\u093e\u0924\u0940 \u0939\u0948\u0902| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) HINDI ASSESSMENT: LEVELS 0-5 HINDI ASSESSMENT: LEVELS 0-5 \u093f\u0939\u093e\u0928\u0940 1 \u093f\u0939\u093e\u0928\u0940 2 \u0930\u093e\u091c\u0942 \u0928\u093e\u092e \u093f\u093e \u090f\u093f \u093f\u095c\u093f\u093e \u0925\u093e| \u0909\u0938\u093f\u0940 \u090f\u093f \u092c\u095c\u0940 \u092c\u0939\u0928 \u0935 \u090f\u093f \u091b\u094b\u091f\u093e \u092d\u093e\u0908 \u0925\u093e| \u0909\u0938\u093f\u093e \u092d\u093e\u0908 \u0917\u093e\u093e\u0901\u0935 \u093f\u0947  \u092a\u093e\u0938 \u093f\u0947  \u0935\u0935\u0926\u094d\u092f\u093e\u093f\u092f \u092e\u0947\u0902 \n \n\u092a\u095d\u0928\u0947 \u091c\u093e\u0924\u093e \u0925\u093e| \u0935\u0939 \u0916\u0942\u092c \u092e\u0947\u0939\u0928\u0924 \u093f\u0930\u0924\u093e \u0925\u093e| \u0909\u0938\u093f\u0940 \u092c\u0939\u0928 \n \n\u092c\u0939\u0941\u0924 \u0905\u091a\u094d\u091b\u0940 \u0916\u0916\u093f\u093e\u095c\u0940 \u0925\u0940| \u0909\u0938\u0947 \u093f\u092e\u094d\u092c\u0940 \u0926\u094c\u095c \u093f\u0917\u093e\u0928\u093e \u0905\u091a\u094d\u091b\u093e \n \n\u093f\u0917\u0924\u093e \u0925\u093e| \u0935\u0947 \u0924\u0940\u0928\u094b\u0902 \u0930\u094b\u091c \u0938\u093e\u0925-\u0938\u093e\u0925 \u092e\u094c\u091c-\u092e\u0938\u094d\u0924\u0940 \u093f\u0930\u0924\u0947 \u0925\u0947| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) \u090f\u093f \u093f\u095c\u093f\u093e \u0930\u094b\u091c \u0938\u0941\u092c\u0939 \u090f\u093f \u092c\u0942\u095d\u0940 \u092e\u0926\u0939\u093f\u093e \u093f\u094b \u0924\u093e\u093f\u093e\u092c \u093f\u0947  \u0915\u093f\u0928\u093e\u0930\u0947 \u0926\u0947\u0916\u0924\u093e \u0925\u093e| \u0935\u0939 \u092e\u0926\u0939\u093f\u093e \n\u0930\u094b\u091c \u091b\u094b\u091f\u0947 \u091b\u094b\u091f\u0947 \u093f\u091b\u0941 \u0935\u094b\u0902 \u093f\u0940 \u092a\u0940\u0920 \u093f\u094b \u0938\u093e\u095e \u093f\u0930\u0924\u0940 \u0925\u0940| \u090f\u093f \u0926\u0926\u0928 \u0909\u0938 \u093f\u095c\u093f\u0947  \u0928\u0947 \u0907\u0938\u093f\u0947  \u092a\u0940\u091b\u0947  \n\u093f\u093e \u093f\u093e\u0930\u0923 \u091c\u093e\u0928\u0928\u0947 \u093f\u093e \u092e\u0928 \u092c\u0928\u093e\u092f\u093e| \u0909\u0938\u0928\u0947 \u092e\u0926\u0939\u093f\u093e \u093f\u0947  \u092a\u093e\u0938 \u091c\u093e\u093f\u0930 \u093f\u0939\u093e, \u201d\u0928\u092e\u0938\u094d\u0924\u0947 \u0906\u0902\u091f\u0940!", " \u0906\u092a \u0939\u092e\u0947\u0936\u093e \u0907\u0928 \u093f\u091b\u0941 \u0935\u094b\u0902 \u093f\u0940 \u092a\u0940\u0920 \u0915\u094d\u092f\u094b\u0902 \u0938\u093e\u092b \u093f\u0930\u0924\u0940 \u0939\u0948\u0902?\u201d \u092e\u0926\u0939\u093f\u093e \u0928\u0947 \u092c\u094b\u093f\u093e, \u201d\u0907\u0928 \u093f\u091b\u0941 \u0935\u094b\u0902 \u093f\u0940 \u092a\u0940\u0920 \u0938\u093e\u095e \u093f\u0930\u0924\u0947 \u0939\u0941\u090f \u092e\u0948\u0902 \u0938\u0941\u0916 \u0936\u093e\u0902\u0924\u0924 \u093f\u093e \u0905\u0928\u0941\u092d\u0935 \u093f\u0947\u0924\u0940 \u0939\u0942 \u093e\u0901|\u201d \u0907\u0928 \u093f\u091b\u0941 \u0935\u094b\u0902 \u093f\u0940 \u092a\u0940\u0920 \u092a\u0930 \u091c\u094b \u093f\u0935\u091a \u0939\u094b\u0924\u093e \u0939\u0948 \u0909\u0938 \u092a\u0930 \u093f\u091a\u0930\u093e \u091c\u092e\u093e \u0939\u094b \u091c\u093e\u0924\u093e \u0939\u0948| \u091c\u091c\u0938\u093f\u0940 \u0935\u091c\u0939 \u0938\u0947 \u0907\u0928\u093f\u0940 \u0917\u092e\u0940 \u092a\u0948\u0926\u093e \u093f\u0930\u0928\u0947 \u093f\u0940 \u0915\u094d\u0937\u092e\u0924\u093e \u093f\u092e \u0939\u094b \u091c\u093e\u0924\u0940 \u0939\u0948| \u093f\u092e\u094d\u092c\u0947 \u0938\u092e\u092f \u0924\u093f \u0905\u0917\u0930 \u0910\u0938\u093e \u0939\u0940 \u0930\u0939\u0947 \u0924\u094b \u092f\u0947 \u093f\u0935\u091a \u093f\u092e\u091c\u094b\u0930 \u093e\u0901| \u092f\u0939 \u0938\u0941\u0928\u093f\u0930 \u093f\u095c\u093f\u093e \u0906\u0936\u094d\u091a\u092f\u092f \u0938\u0947 \n\u092d\u0940 \u0939\u094b \u091c\u093e\u0924\u0947 \u0939\u0948\u0902| \u0907\u0938\u0932\u093f\u090f \u092e\u0948\u0902 \u093f\u0935\u091a \u093f\u094b \u0938\u093e\u095e \u093f\u0930\u0924\u0940 \u0939\u0942\n\u092c\u094b\u093f\u093e, \u201c\u0906\u092a\u093f\u0947  \u0905\u093f\u0947 \u093f\u0947 \u093f\u0947  \u092c\u0926\u093f\u0928\u0947 \u0938\u0947 \u0924\u094b \u093f\u094b\u0908 \u092c\u095c\u093e \u092a\u0930\u0930\u0935\u0924\u092f\u0928 \u0928\u0939\u0940\u0902 \u0906\u092f\u0947\u0917\u093e|\u201d \u092e\u0926\u0939\u093f\u093e \u0928\u0947 \u0938\u0902\u0915\u094d\u0937\u0915\u094d\u0937\u092a\u094d\u0924 \u092e\u0947\u0902 \u091c\u0935\u093e\u092c \u0926\u0926\u092f\u093e, \u201c\u092d\u093f\u0947 \u092e\u0947\u0930\u0947 \u0907\u0938 \u093f\u092e\u092f \u0938\u0947 \u093f\u094b\u0908 \u092c\u095c\u093e \u092c\u0926\u093f\u093e\u0935 \u0928\u0939\u0940\u0902 \u0906\u092f\u0947\u0917\u093e \u093f\u0947\u0915\u093f\u0928 \u0907\u0938 \u090f\u093f \u093f\u091b\u0941 \u0935\u0947 \u093f\u0940 \u091c\u091c\u0928\u094d\u0926\u0917\u0940 \u092e\u0947\u0902 \u0924\u094b \u092c\u0926\u093f\u093e\u0935 \u0906\u092f\u0947\u0917\u093e |\u201d \u0907\u0938\u0932\u093f\u090f \u0939\u092e\u0947\u0902 \u091b\u094b\u091f\u0947 \u092c\u0926\u093f\u093e\u0935 \u0938\u0947 \u0939\u0940 \n\u0936\u0941\u0930\u0941\u0906\u0924 \u093f\u0930\u0928\u0940 \u091a\u093e\u0926\u0939\u090f| All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 18b: ASER Testing Tool for Math in Year 3 Endline MATH ASSESSMENT (Version A): LEVELS 0-4 MATH ASSESSMENT (Version A): LEVELS 0-4 Number recognition 1 \u2013 9 Number recognition  \n10 \u2013 99 Subtraction 2 digit with borrowing Division \n3 digit by 1 digit \u2212 29 \u2212 28 \u2212 76 \u2212 15 \u2212 39 \u2212 17 \u2212 57 \u2212 49 All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) All assessments except for Hindi Level 5 developed by the ASER Centre (www.asercentre.org) !\n!\n!\n!", " Appendix 18c: ASER Testing Tool for English in Year 3 Endline ENGLISH ASSESSMENT: LEVELS 0-4 ENGLISH ASSESSMENT: LEVELS 0-4 D U G i t x M          R a           y hen old What is your name?", " sit This is a big bus.", " run bag I like to sing.", " S         Z          L n         h          c fox I have a sister.", " All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) All assessments except of Hindi Level 5 developed by ASER Centre (www.asercentre.org) Appendix 19: Map of Bhilwara District Note: \u201cBeejoliya\u201d and \u201cJahazpur\u201d are alternative spellings of Bijoliya and Jahajpur, respectively."]}]